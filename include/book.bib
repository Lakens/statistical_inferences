@article{abelson_value_2003,
  title = {The {{Value}} of {{Life}} and {{Health}} for {{Public Policy}}},
  author = {Abelson, Peter},
  year = {2003},
  month = jun,
  journal = {Economic Record},
  volume = {79},
  pages = {S2-S13},
  issn = {00130249, 14754932},
  doi = {10.1111/1475-4932.00087},
  urldate = {2020-03-07},
  langid = {english}
}

@book{aberson_applied_2019,
  title = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}},
  shorttitle = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Aberson, Christopher L.},
  year = {2019},
  month = feb,
  edition = {2},
  publisher = {Routledge},
  address = {New York},
  abstract = {Applied Power Analysis for the Behavioral Sciences is a practical "how-to" guide to conducting statistical power analyses for psychology and related fields. The book provides a guide to conducting analyses that is appropriate for researchers and students, including those with limited quantitative backgrounds. With practical use in mind, the text provides detailed coverage of topics such as how to estimate expected effect sizes and power analyses for complex designs. The topical coverage of the text, an applied approach, in-depth coverage of popular statistical procedures, and a focus on conducting analyses using R make the text a unique contribution to the power literature. To facilitate application and usability, the text includes ready-to-use R code developed for the text. An accompanying R package called pwr2ppl (available at https://github.com/chrisaberson/pwr2ppl) provides tools for conducting power analyses across each topic covered in the text.},
  isbn = {978-1-138-04456-2},
  langid = {english},
  annotation = {00000}
}

@misc{aert_correcting_2018,
  title = {Correcting for {{Publication Bias}} in a {{Meta-Analysis}} with the {{P-uniform}}* {{Method}}},
  author = {van Aert, Robbie C. M. and van Assen, Marcel A. L. M.},
  year = {2018},
  month = oct,
  institution = {MetaArXiv},
  doi = {10.31222/osf.io/zqjr9},
  urldate = {2022-03-16},
  abstract = {Publication bias is a major threat to the validity of a meta-analysis resulting in overestimated effect sizes. We propose an extension and improvement of the publication bias method p-uniform called p-uniform*. P-uniform* improves upon p-uniform in three ways, as it (i) entails a more efficient estimator, (ii) eliminates the overestimation of effect size caused by between-study variance in true effect size, and (iii) enables estimating and testing for the presence of the between-study variance. We compared the statistical properties of p-uniform* with p-uniform, the selection model approach of Hedges (1992), and the random-effects model. Statistical properties of p-uniform* and the selection model approach were comparable and generally outperformed p-uniform and the random-effects model if publication bias was present. We demonstrate that p-uniform* and the selection model approach estimate average effect size and between-study variance rather well with ten or more studies in the meta-analysis when publication bias is not extreme. P-uniform* generally provides more accurate estimates of the between-study variance in meta-analyses containing many studies (e.g., 60 or more) and if publication bias is present. We offer recommendations for applied researchers, provide an R package as well as an easy-to-use web application for applying p-uniform*.},
  langid = {american},
  keywords = {meta-analysis,Other Statistics and Probability,p-uniform,Physical Sciences and Mathematics,Psychology,publication bias,Quantitative Psychology,selection model approach,Social and Behavioral Sciences,Statistics and Probability}
}

@article{agnoli_questionable_2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  year = {2017},
  month = mar,
  journal = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0172792},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0172792},
  urldate = {2022-03-09},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  langid = {english},
  keywords = {Behavior,Experimental psychology,Italian people,Psychologists,Psychology,Psychometrics,Questionnaires,United States}
}

@misc{akker_effectiveness_2023,
  title = {The Effectiveness of Preregistration in Psychology: {{Assessing}} Preregistration Strictness and Preregistration-Study Consistency},
  shorttitle = {The Effectiveness of Preregistration in Psychology},
  author = {van den Akker, Olmo and Bakker, Marjan and van Assen, Marcel A. L. M. and Pennington, Charlotte Rebecca and Verweij, Leone and Elsherif, Mahmoud and Claesen, Aline and Gaillard, Stefan Daniel Michel and Yeung, Siu Kit and Frankenberger, Jan-Luca and Krautter, Kai and Cockcroft, Jamie P. and Kreuer, Katharina Sybille and Evans, Thomas Rhys and Heppel, Fr{\'e}d{\'e}rique and Schoch, Sarah Fiona and Korbmacher, Max and Yamada, Yuki and {Albayrak-Aydemir}, Nihan and Alzahawi, Shilaan and Sarafoglou, Alexandra and Sitnikov, Maksim and Dechterenko, Filip and Wingen, Sophia and Grinschgl, Sandra and Hartmann, Helena and Stewart, Suzanne and Oliveira, Catia Margarida and {Ashcroft-Jones}, Sarah and Baker, Bradley James and Wicherts, Jelte},
  year = {2023},
  month = may,
  publisher = {MetaArXiv},
  doi = {10.31222/osf.io/h8xjw},
  urldate = {2023-05-13},
  abstract = {Study preregistration has become increasingly popular in psychology, but its effectiveness in restricting potentially biasing researcher degrees of freedom remains unclear. We used an extensive protocol to assess the strictness of preregistrations and the consistency between preregistration and publications of 300 preregistered psychology studies. We found that preregistrations often lack methodological details and that undisclosed deviations from preregistered plans are frequent. Combining the strictness and consistency results highlights that biases due to researcher degrees of freedom are prevalent and likely in many preregistered studies. More comprehensive registration templates typically yielded stricter and hence better preregistrations. We did not find that effectiveness of preregistrations differed over time or between original and replication studies. Furthermore, we found that operationalizations of variables were generally more effectively preregistered than other study parts. Inconsistencies between preregistrations and published studies were mainly encountered for data collection procedures, statistical models, and exclusion criteria. Our results indicate that, to unlock the full potential of preregistration, researchers in psychology should aim to write stricter preregistrations, adhere to these preregistrations more faithfully, and more transparently report any deviations from the preregistrations. This could be facilitated by training and education to improve preregistration skills, as well as the development of more comprehensive templates.},
  langid = {american},
  keywords = {preregistration,Psychology,Social and Behavioral Sciences}
}

@article{albers_credible_2018,
  title = {Credible {{Confidence}}: {{A Pragmatic View}} on the {{Frequentist}} vs {{Bayesian Debate}}},
  shorttitle = {Credible {{Confidence}}},
  author = {Albers, Casper J. and Kiers, Henk A. L. and van Ravenzwaaij, Don},
  year = {2018},
  month = aug,
  journal = {Collabra: Psychology},
  volume = {4},
  number = {1},
  pages = {31},
  publisher = {The Regents of the University of California},
  issn = {2474-7394},
  doi = {10.1525/collabra.149},
  urldate = {2020-07-07},
  abstract = {Article: Credible Confidence: A Pragmatic View on the Frequentist vs Bayesian Debate},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english}
}

@article{albers_when_2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper J. and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  urldate = {2017-10-16},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\varepsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis}
}

@misc{aldhous_journal_2011,
  title = {Journal Rejects Studies Contradicting Precognition},
  author = {Aldhous, Peter},
  year = {2011},
  journal = {New Scientist},
  urldate = {2023-07-24},
  abstract = {The journal which published astonishing evidence that people can see the future has controversially rejected attempts to repeat the work},
  howpublished = {https://www.newscientist.com/article/dn20447-journal-rejects-studies-contradicting-precognition/},
  langid = {american}
}

@article{aldrich_fisher_1997,
  title = {R.{{A}}. {{Fisher}} and the Making of Maximum Likelihood 1912-1922},
  author = {Aldrich, John},
  year = {1997},
  month = sep,
  journal = {Statistical Science},
  volume = {12},
  number = {3},
  pages = {162--176},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1030037906},
  urldate = {2022-02-06},
  abstract = {In 1922 R. A. Fisher introduced the method of maximum likelihood. He first presented the numerical procedure in 1912. This paper considers Fisher's changing justifications for the method, the concepts he developed around it (including likelihood, sufficiency, efficiency and information) and the approaches he discarded (including inverse probability).},
  keywords = {Bayes's postulate,efficiency,Fisher,Information,inverse probability,maximum likelihood,Pearson,student,sufficiency}
}

@article{allison_power_1997,
  title = {Power and Money: {{Designing}} Statistically Powerful Studies While Minimizing Financial Costs},
  shorttitle = {Power and Money},
  author = {Allison, David B. and Allison, Ronald L. and Faith, Myles S. and Paultre, Furcy and {Pi-Sunyer}, F. Xavier},
  year = {1997},
  journal = {Psychological Methods},
  volume = {2},
  number = {1},
  pages = {20--33},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/1082-989X.2.1.20},
  abstract = {Adequate statistical power is increasingly demanded in research designs. However, obtaining adequate research funding is increasingly difficult. This places researchers in a difficult position. In response, the authors advocate an approach to designing studies that considers statistical power and financial concerns simultaneously. Their purpose is twofold: (a) to introduce the general paradigm of cost optimization in the context of power analysis and (b) to present techniques for such optimization. Techniques are presented in the context of a randomized clinical trial. The authors consider (a) selecting optimal cutpoints for subject screening tests; (b) optimally allocating subjects to different treatment conditions; (c) choosing between obtaining more subjects or taking more replicate measurements; and (d) using prerandomization covariates. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cost Containment,Experimentation,Experimenters,Statistical Power}
}

@article{altman_statistics_1995,
  title = {Statistics Notes: {{Absence}} of Evidence Is Not Evidence of Absence},
  shorttitle = {Statistics Notes},
  author = {Altman, Douglas G. and Bland, J. Martin},
  year = {1995},
  month = aug,
  journal = {BMJ},
  volume = {311},
  number = {7003},
  pages = {485},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.311.7003.485},
  urldate = {2018-02-23},
  abstract = {The non-equivalence of statistical significance and clinical importance has long been recognised, but this error of interpretation remains common. Although a significant result in a large study may sometimes not be clinically important, a far greater problem arises from misinterpretation of non-significant findings. By convention a P value greater than 5\% (P{$>$}0.05) is called ``not significant.'' Randomised controlled clinical trials that do not show a significant difference between the treatments being compared are often called ``negative.'' This term wrongly implies that the study has shown that there is no difference, whereas usually all that has been shown is an absence of evidence of a difference. These are quite different statements. The sample size of controlled trials is generally inadequate, with a consequent lack of power to detect real, and clinically worthwhile, differences in treatment. Freiman et al1 found that only {\dots}},
  copyright = {{\copyright} 1995 BMJ Publishing Group Ltd.},
  langid = {english},
  pmid = {7647644}
}

@article{altoe_enhancing_2020,
  title = {Enhancing {{Statistical Inference}} in {{Psychological Research}} via {{Prospective}} and {{Retrospective Design Analysis}}},
  author = {Alto{\`e}, Gianmarco and Bertoldo, Giulia and Zandonella Callegher, Claudio and Toffalini, Enrico and Calcagn{\`i}, Antonio and Finos, Livio and Pastore, Massimiliano},
  year = {2020},
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  urldate = {2022-10-10},
  abstract = {In the past two decades, psychological science has experienced an unprecedented replicability crisis, which has uncovered several issues. Among others, the use and misuse of statistical inference plays a key role in this crisis. Indeed, statistical inference is too often viewed as an isolated procedure limited to the analysis of data that have already been collected. Instead, statistical reasoning is necessary both at the planning stage and when interpreting the results of a research project. Based on these considerations, we build on and further develop an idea proposed by Gelman and Carlin (2014) termed ``prospective and retrospective design analysis.'' Rather than focusing only on the statistical significance of a result and on the classical control of type I and type II errors, a comprehensive design analysis involves reasoning about what can be considered a plausible effect size. Furthermore, it introduces two relevant inferential risks: the exaggeration ratio or Type M error (i.e., the predictable average overestimation of an effect that emerges as statistically significant) and the sign error or Type S error (i.e., the risk that a statistically significant effect is estimated in the wrong direction). Another important aspect of design analysis is that it can be usefully carried out both in the planning phase of a study and for the evaluation of studies that have already been conducted, thus increasing researchers' awareness during all phases of a research project. To illustrate the benefits of a design analysis to the widest possible audience, we use a familiar example in psychology where the researcher is interested in analyzing the differences between two independent groups considering Cohen's d as an effect size measure. We examine the case in which the plausible effect size is formalized as a single value, and we propose a method in which uncertainty concerning the magnitude of the effect is formalized via probability distributions. Through several examples and an application to a real case study, we show that, even though a design analysis requires significant effort, it has the potential to contribute to planning more robust and replicable studies. Finally, future developments in the Bayesian framework are discussed.}
}

@article{anderson_addressing_2017,
  title = {Addressing the ``{{Replication Crisis}}'': {{Using Original Studies}} to {{Design Replication Studies}} with {{Appropriate Statistical Power}}},
  shorttitle = {Addressing the ``{{Replication Crisis}}''},
  author = {Anderson, Samantha F. and Maxwell, Scott E.},
  year = {2017},
  month = mar,
  journal = {Multivariate Behavioral Research},
  pages = {1--20},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171.2017.1289361},
  urldate = {2017-05-01},
  langid = {english}
}

@incollection{anderson_group_2014,
  title = {Group {{Sequential Design}} in {{R}}},
  booktitle = {Clinical {{Trial Biostatistics}} and {{Biopharmaceutical Applications}}},
  author = {Anderson, Keaven M.},
  year = {2014},
  pages = {179--209},
  publisher = {CRC Press},
  address = {New York},
  isbn = {978-1-4822-1218-1}
}

@article{anderson_normative_2007,
  title = {Normative Dissonance in Science: {{Results}} from a National Survey of {{US}} Scientists},
  shorttitle = {Normative Dissonance in Science},
  author = {Anderson, Melissa S. and Martinson, Brian C. and De Vries, Raymond},
  year = {2007},
  journal = {Journal of Empirical Research on Human Research Ethics},
  volume = {2},
  number = {4},
  pages = {3--14},
  urldate = {2015-11-30}
}

@article{anderson_perverse_2007,
  title = {The Perverse Effects of Competition on Scientists' Work and Relationships},
  author = {Anderson, Melissa S. and Ronning, Emily A. and De Vries, Raymond and Martinson, Brian C.},
  year = {2007},
  journal = {Science and engineering ethics},
  volume = {13},
  number = {4},
  pages = {437--461},
  urldate = {2015-12-11}
}

@article{anderson_samplesize_2017,
  title = {Sample-Size Planning for More Accurate Statistical Power: {{A}} Method Adjusting Sample Effect Sizes for Publication Bias and Uncertainty},
  shorttitle = {Sample-Size Planning for More Accurate Statistical Power},
  author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
  year = {2017},
  journal = {Psychological science},
  volume = {28},
  number = {11},
  pages = {1547--1562},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  doi = {10.1177/0956797617723724}
}

@article{anderson_there_2016,
  title = {There's More than One Way to Conduct a Replication Study: {{Beyond}} Statistical Significance.},
  shorttitle = {There's More than One Way to Conduct a Replication Study},
  author = {Anderson, Samantha F. and Maxwell, Scott E.},
  year = {2016},
  journal = {Psychological Methods},
  volume = {21},
  number = {1},
  pages = {1--12},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000051},
  urldate = {2016-11-19},
  langid = {english}
}

@article{anvari_not_2021,
  title = {Not All Effects Are Indispensable: {{Psychological}} Science Requires Verifiable Lines of Reasoning for Whether an Effect Matters.},
  shorttitle = {Not All Effects Are Indispensable},
  author = {Anvari, Farid and Kievit, Rogier and Lakens, Dani{\"e}l and Pennington, Charlotte Rebecca and Przybylski, Andrew K. and Tiokhin, Leo and Wiernik, Brenton M. and Orben, Amy},
  year = {2021},
  month = jun,
  journal = {Perspectives on Psychological Science},
  doi = {10.31234/osf.io/g3vtr},
  urldate = {2022-03-18},
  abstract = {Psychological researchers currently lack guidance for how to make claims about and evaluate the practical relevance and significance of observed effect sizes, i.e. whether a finding will have impact when translated to a different context of application. Although psychologists have recently highlighted theoretical justifications for why small effect sizes might be practically relevant, such justifications fail to provide the information necessary for evaluation and falsification. Claims about whether an observed effect size is practically relevant need to consider both the mechanisms amplifying and counteracting practical relevance, as well as the assumptions underlying each mechanism at play. To provide guidance for making claims about whether an observed effect size is practically relevant in such a way that the claims can be systematically evaluated, we present examples of widely applicable mechanisms and the key assumptions needed for justifying whether an observed effect size can be expected to generalize to different contexts. Routine use of these mechanisms to justify claims about practical relevance has the potential to make researchers' claims about generalizability substantially more transparent. This transparency can help move psychological science towards a more rigorous assessment of when psychological findings can be applied in the world.},
  langid = {american},
  keywords = {benchmarks,effect size,evaluation,Meta-science,practical significance,Social and Behavioral Sciences}
}

@article{anvari_replicability_2018,
  title = {The Replicability Crisis and Public Trust in Psychological Science},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2018},
  month = sep,
  journal = {Comprehensive Results in Social Psychology},
  volume = {3},
  number = {3},
  pages = {266--286},
  publisher = {Routledge},
  issn = {2374-3603},
  doi = {10.1080/23743603.2019.1684822},
  urldate = {2020-02-26},
  abstract = {Replication failures of past findings in several scientific disciplines, including psychology, medicine, and experimental economics, have created a ``crisis of confidence'' among scientists. Psychological science has been at the forefront of tackling these issues, with discussions about replication failures and scientific self-criticisms of questionable research practices (QRPs) increasingly taking place in public forums. How this replicability crisis impacts the public's trust is a question yet to be answered by research. Whereas some researchers believe that the public's trust will be positively impacted or maintained, others believe trust will be diminished. Because it is our field of expertise, we focus on trust in psychological science. We performed a study testing how public trust in past and future psychological research would be impacted by being informed about (i) replication failures (replications group), (ii) replication failures and criticisms of QRPs (QRPs group), and (iii) replication failures, criticisms of QRPs, and proposed reforms (reforms group). Results from a mostly European sample (N = 1129) showed that, compared to a control group, people in the replications, QRPs, and reforms groups self-reported less trust in past research. Regarding trust in future research, the replications and QRPs groups did not significantly differ from the control group. Surprisingly, the reforms group had less trust in future research than the control group. Nevertheless, people in the replications, QRPs, and reforms groups did not significantly differ from the control group in how much they believed future research in psychological science should be supported by public funding. Potential explanations are discussed.},
  keywords = {crisis of confidence,open science,Replicability crisis,reproducibility crisis,trust in science}
}

@article{anvari_using_2021,
  title = {Using Anchor-Based Methods to Determine the Smallest Effect Size of Interest},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2021},
  month = sep,
  journal = {Journal of Experimental Social Psychology},
  volume = {96},
  pages = {104159},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2021.104159},
  urldate = {2022-01-10},
  abstract = {Effect sizes are an important outcome of quantitative research, but few guidelines exist that explain how researchers can determine which effect sizes are meaningful. Psychologists often want to study effects that are large enough to make a difference to people's subjective experience. Thus, subjective experience is one way to gauge the meaningfulness of an effect. We propose and illustrate one method for how to quantify the smallest subjectively experienced difference---the smallest change in an outcome measure that individuals consider to be meaningful enough in their subjective experience such that they are willing to rate themselves as feeling different---using an anchor-based method with a global rating of change question applied to the positive and negative affect scale. We provide a step-by-step guide for the questions that researchers need to consider in deciding whether and how to use the anchor-based method, and we make explicit the assumptions of the method that future research can examine. For researchers interested in people's subjective experiences, this anchor-based method provides one way to specify a smallest effect size of interest, which allows researchers to interpret observed results in terms of their theoretical and practical significance.},
  langid = {english},
  keywords = {Minimum important difference,Negative affect,Positive affect,Practical significance,Smallest effect size of interest,Smallest subjectively experienced difference,Subjectively experienced difference}
}

@article{appelbaum_journal_2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  shorttitle = {Journal Article Reporting Standards for Quantitative Research in Psychology},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  month = jan,
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3},
  publisher = {US: American Psychological Association},
  issn = {1935-990X},
  doi = {10.1037/amp0000191},
  urldate = {2020-07-25}
}

@article{armitage_repeated_1969,
  title = {Repeated Significance Tests on Accumulating Data},
  author = {Armitage, Peter and McPherson, C. K. and Rowe, B. C.},
  year = {1969},
  journal = {Journal of the Royal Statistical Society: Series A (General)},
  volume = {132},
  number = {2},
  pages = {235--244},
  publisher = {Wiley Online Library}
}

@article{arpinon_practical_2023,
  title = {A Practical Guide to {{Registered Reports}} for Economists},
  author = {Arpinon, Thibaut and Espinosa, Romain},
  year = {2023},
  journal = {Journal of the Economic Science Association},
  pages = {1--33},
  publisher = {Springer}
}

@article{arslan_how_2019,
  title = {How to {{Automatically Document Data With}} the Codebook {{Package}} to {{Facilitate Data Reuse}}},
  author = {Arslan, Ruben C.},
  year = {2019},
  month = may,
  journal = {Advances in Methods and Practices in Psychological Science},
  pages = {2515245919838783},
  issn = {2515-2459},
  doi = {10.1177/2515245919838783},
  urldate = {2019-05-22},
  abstract = {Data documentation in psychology lags behind not only many other disciplines, but also basic standards of usefulness. Psychological scientists often prefer to invest the time and effort that would be necessary to document existing data well in other duties, such as writing and collecting more data. Codebooks therefore tend to be unstandardized and stored in proprietary formats, and they are rarely properly indexed in search engines. This means that rich data sets are sometimes used only once---by their creators---and left to disappear into oblivion. Even if they can find an existing data set, researchers are unlikely to publish analyses based on it if they cannot be confident that they understand it well enough. My codebook package makes it easier to generate rich metadata in human- and machine-readable codebooks. It uses metadata from existing sources and automates some tedious tasks, such as documenting psychological scales and reliabilities, summarizing descriptive statistics, and identifying patterns of missingness. The codebook R package and Web app make it possible to generate a rich codebook in a few minutes and just three clicks. Over time, its use could lead to psychological data becoming findable, accessible, interoperable, and reusable, thereby reducing research waste and benefiting both its users and the scientific community as a whole.},
  langid = {english}
}

@article{artino_ethical_2019,
  title = {Ethical {{Shades}} of {{Gray}}: {{International Frequency}} of {{Scientific Misconduct}} and {{Questionable Research Practices}} in {{Health Professions Education}}},
  shorttitle = {Ethical {{Shades}} of {{Gray}}},
  author = {Artino, Anthony R. and Driessen, Erik W. and Maggio, Lauren A.},
  year = {2019},
  month = jan,
  journal = {Academic Medicine: Journal of the Association of American Medical Colleges},
  volume = {94},
  number = {1},
  pages = {76--84},
  issn = {1938-808X},
  doi = {10.1097/ACM.0000000000002412},
  abstract = {PURPOSE: To maintain scientific integrity and engender public confidence, research must be conducted responsibly. Whereas deliberate scientific misconduct such as data fabrication is clearly unethical, other behaviors-often referred to as questionable research practices (QRPs)-exploit the ethical shades of gray that color acceptable practice. This study aimed to measure the frequency of self-reported misconduct and QRPs in a diverse, international sample of health professions education (HPE) researchers. METHOD: In 2017, the authors conducted an anonymous, cross-sectional survey study. The web-based survey contained 43 items that asked respondents to rate how often they had engaged in a variety of irresponsible research behaviors. The items were adapted from previously published surveys. RESULTS: In total, 590 HPE researchers took the survey. The mean age was 46 years (SD = 11.6), and the majority of participants were from the United States (26.4\%), Europe (23.2\%), and Canada (15.3\%). The three most frequently reported irresponsible research behaviors were adding authors who did not qualify for authorship (60.6\%), citing articles that were not read (49.5\%), and selectively citing papers to please editors or reviewers (49.4\%). Additionally, respondents reported misrepresenting a participant's words (6.7\%), plagiarizing (5.5\%), inappropriately modifying results (5.3\%), deleting data without disclosure (3.4\%), and fabricating data (2.4\%). Overall, 533 (90.3\%) respondents reported at least one irresponsible behavior. CONCLUSIONS: Notwithstanding the methodological limitations of survey research, these findings indicate that a substantial proportion of HPE researchers report a range of misconduct and QRPs. Consequently, reforms may be needed to improve the conduct of HPE research.},
  langid = {english},
  pmid = {30113363},
  keywords = {Adult,Authorship,Biomedical Research,Cross-Sectional Studies,Education Medical,Ethics Research,Female,Humans,Male,Middle Aged,Publishing,Scientific Misconduct}
}

@article{azrin_control_1961,
  title = {The Control of the Content of Conversation through Reinforcement},
  author = {Azrin, N. H. and Holz, W. and Ulrich, R. and Goldiamond, I.},
  year = {1961},
  journal = {Journal of the Experimental Analysis of Behavior},
  volume = {4},
  pages = {25--30},
  publisher = {Journal of the Experimental Analysis of Behavior},
  address = {US},
  issn = {1938-3711},
  doi = {10.1901/jeab.1961.4-25},
  abstract = {3 experiments were performed attempting to demonstrate the control of conversation through reinforcement. The results indicate that only with adjective recording and programing procedures can valid results be obtained in this area. From Psyc Abstracts 36:01:1CJ25A. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@book{babbage_reflections_1830,
  title = {Reflections on the {{Decline}} of {{Science}} in {{England}}: {{And}} on {{Some}} of {{Its Causes}}},
  shorttitle = {Reflections on the {{Decline}} of {{Science}} in {{England}}},
  author = {Babbage, Charles},
  year = {1830},
  publisher = {B. Fellowes},
  urldate = {2017-10-26},
  abstract = {Book digitized by Google and uploaded to the Internet Archive by user tpb.},
  collaborator = {{unknown library}},
  langid = {english}
}

@article{bacchetti_current_2010,
  title = {Current Sample Size Conventions: {{Flaws}}, Harms, and Alternatives},
  shorttitle = {Current Sample Size Conventions},
  author = {Bacchetti, Peter},
  year = {2010},
  month = mar,
  journal = {BMC Medicine},
  volume = {8},
  number = {1},
  pages = {17},
  issn = {1741-7015},
  doi = {10.1186/1741-7015-8-17},
  urldate = {2020-12-31},
  abstract = {The belief remains widespread that medical research studies must have statistical power of at least 80\% in order to be scientifically sound, and peer reviewers often question whether power is high enough.},
  keywords = {Current Convention,Inadequate Sample Size,Information Method,Marginal Return,Sample Size Planning}
}

@book{baguley_serious_2012,
  title = {Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences},
  shorttitle = {Serious Stats},
  author = {Baguley, Thom},
  year = {2012},
  publisher = {Palgrave Macmillan},
  address = {Houndmills, Basingstoke, Hampshire [England] ; New York},
  isbn = {978-0-230-57717-6 978-0-230-57718-3},
  lccn = {BF39 .B3175 2012},
  keywords = {Psychology,Psychometrics,Social sciences,Statistical methods}
}

@article{baguley_standardized_2009,
  title = {Standardized or Simple Effect Size: {{What}} Should Be Reported?},
  shorttitle = {Standardized or Simple Effect Size},
  author = {Baguley, Thom},
  year = {2009},
  month = aug,
  journal = {British Journal of Psychology},
  volume = {100},
  number = {3},
  pages = {603--617},
  issn = {2044-8295},
  doi = {10.1348/000712608X377117},
  urldate = {2018-02-13},
  abstract = {It is regarded as best practice for psychologists to report effect size when disseminating quantitative research findings. Reporting of effect size in the psychological literature is patchy -- though this may be changing -- and when reported it is far from clear that appropriate effect size statistics are employed. This paper considers the practice of reporting point estimates of standardized effect size and explores factors such as reliability, range restriction and differences in design that distort standardized effect size unless suitable corrections are employed. For most purposes simple (unstandardized) effect size is more robust and versatile than standardized effect size. Guidelines for deciding what effect size metric to use and how to report it are outlined. Foremost among these are: (i) a preference for simple effect size over standardized effect size, and (ii) the use of confidence intervals to indicate a plausible range of values the effect might take. Deciding on the appropriate effect size statistic to report always requires careful thought and should be influenced by the goals of the researcher, the context of the research and the potential needs of readers.},
  langid = {english}
}

@article{baguley_understanding_2004,
  title = {Understanding Statistical Power in the Context of Applied Research},
  author = {Baguley, Thom},
  year = {2004},
  month = mar,
  journal = {Applied Ergonomics},
  volume = {35},
  number = {2},
  pages = {73--80},
  issn = {0003-6870},
  doi = {10.1016/j.apergo.2004.01.002},
  urldate = {2020-12-28},
  abstract = {Estimates of statistical power are widely used in applied research for purposes such as sample size calculations. This paper reviews the benefits of power and sample size estimation and considers several problems with the use of power calculations in applied research that result from misunderstandings or misapplications of statistical power. These problems include the use of retrospective power calculations and standardized measures of effect size. Methods of increasing the power of proposed research that do not involve merely increasing sample size (such as reduction in measurement error, increasing `dose' of the independent variable and optimizing the design) are noted. It is concluded that applied researchers should consider a broader range of factors (other than sample size) that influence statistical power, and that the use of standardized measures of effect size should be avoided (except as intermediate stages in prospective power or sample size calculations).},
  langid = {english},
  keywords = {Applied research,Experimental design,Statistical power}
}

@book{bakan_method_1967,
  title = {On Method: Toward a Reconstruction of Psychological Investigation},
  shorttitle = {On Method},
  author = {Bakan, David},
  year = {1967},
  publisher = {San Francisco, Jossey-Bass},
  urldate = {2023-07-23},
  abstract = {xviii, 187 pages 24 cm; Includes bibliographical references (pages 171-178); The test of significance in psychological research -- Group and individual functions -- The general and the aggregate -- The mystery-mastery complex in contemporary psychology -- Learning and the scientific enterprise -- Learning and the principle of inverse probability -- The exponential growth function in Herbart and Hull -- Clinical psychology and logic -- A reconsideration of the problem of introspection -- Suicide and the method of introspection -- Psychotherapist: Healer or repairman? -- A perspective on psychoanalytic theory -- Science, mysticism, and psychoanalysis -- Idolatry in religion and science -- Psychological characteristics of man projected in the image of Satan},
  collaborator = {{Internet Archive}},
  isbn = {978-0-87589-008-1},
  langid = {english},
  keywords = {Psychology -- Methodology}
}

@article{bakan_test_1966,
  title = {The Test of Significance in Psychological Research.},
  author = {Bakan, David},
  year = {1966},
  journal = {Psychological bulletin},
  volume = {66},
  number = {6},
  pages = {423--437},
  doi = {10.1037/h0020412}
}

@article{bakker_questionable_2021,
  title = {Questionable and {{Open Research Practices}}: {{Attitudes}} and {{Perceptions}} among {{Quantitative Communication Researchers}}},
  shorttitle = {Questionable and {{Open Research Practices}}},
  author = {Bakker, Bert N and Kokil, Jaidka and D{\"o}rr, Timothy and Fasching, Neil and Lelkes, Yphtach},
  year = {2021},
  month = oct,
  journal = {Journal of Communication},
  volume = {71},
  number = {5},
  pages = {715--738},
  issn = {0021-9916},
  doi = {10.1093/joc/jqab031},
  urldate = {2021-11-29},
  abstract = {Recent contributions have questioned the credibility of quantitative communication research. While questionable research practices (QRPs) are believed to be widespread, evidence for this belief is, primarily, derived from other disciplines. Therefore, it is largely unknown to what extent QRPs are used in quantitative communication research and whether researchers embrace open research practices (ORPs). We surveyed first and corresponding authors of publications in the top-20 journals in communication science. Many researchers report using one or more QRPs. We find widespread pluralistic ignorance: QRPs are generally rejected, but researchers believe they are prevalent. At the same time, we find optimism about the use of open science practices. In all, our study has implications for theories in communication that rely upon a cumulative body of empirical work: these theories are negatively affected by QRPs but can gain credibility if based upon ORPs. We outline an agenda to move forward as a discipline.}
}

@article{ball_effects_2002,
  title = {Effects of Cognitive Training Interventions with Older Adults: A Randomized Controlled Trial},
  shorttitle = {Effects of Cognitive Training Interventions with Older Adults},
  author = {Ball, Karlene and Berch, Daniel B. and Helmers, Karin F. and Jobe, Jared B. and Leveck, Mary D. and Marsiske, Michael and Morris, John N. and Rebok, George W. and Smith, David M. and Tennstedt, Sharon L.},
  year = {2002},
  journal = {Jama},
  volume = {288},
  number = {18},
  pages = {2271--2281},
  publisher = {American Medical Association}
}

@book{barber_pitfalls_1976,
  title = {Pitfalls in {{Human Research}}: {{Ten Pivotal Points}}},
  shorttitle = {Pitfalls in {{Human Research}}},
  author = {Barber, Theodore Xenophon},
  year = {1976},
  publisher = {Pergamon Press},
  googlebooks = {UBN9AAAAMAAJ},
  isbn = {978-0-08-020935-7},
  langid = {english},
  keywords = {Psychology / General}
}

@article{bartos_zcurve_2020,
  title = {Z-{{Curve}}.2.0: {{Estimating Replication Rates}} and {{Discovery Rates}}},
  shorttitle = {Z-{{Curve}}.2.0},
  author = {Barto{\v s}, Franti{\v s}ek and Schimmack, Ulrich},
  year = {2020},
  month = jan,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/urgtn},
  urldate = {2020-06-19},
  abstract = {This article introduces z-curve.2.0 as a method that estimates the expected replication rate (ERR) and the expected discovery rate (EDR) based on the test-statistics of studies selected for significance. Z-curve.2.0 extends the work by Brunner and Schimmack (2019) in several ways. First, we show that a new estimation method using expectation-maximization outperforms the kernel-density approach of z-curve.1.0. Second, we examine the coverage of bootstrapped confidence intervals to provide information about the uncertainty in z-curve estimates. Third, we extended z-curve to estimate the number of all studies that were conducted, including studies with non-significant results that may not have been reported, solely on the basis of significant results. This allows us to estimate the EDR; that is, the percentage of significant results that were obtained in all studies. EDR can be used to assess the size of the file-drawer, estimate the maximum number of false positive results, and may provide a better estimate of the success rate in actual replication studies than the ERR because exact replications are impossible.}
}

@article{bauer_unifying_1996,
  title = {A Unifying Approach for Confidence Intervals and Testing of Equivalence and Difference},
  author = {Bauer, Peter and Kieser, Meinhard},
  year = {1996},
  journal = {Biometrika},
  volume = {83},
  number = {4},
  pages = {934--937},
  urldate = {2016-08-03}
}

@book{bausell_power_2002,
  title = {Power {{Analysis}} for {{Experimental Research}}: {{A Practical Guide}} for the {{Biological}}, {{Medical}} and {{Social Sciences}}},
  shorttitle = {Power {{Analysis}} for {{Experimental Research}}},
  author = {Bausell, R. Barker and Li, Yu-Fang},
  year = {2002},
  month = sep,
  edition = {1st edition},
  publisher = {Cambridge University Press},
  abstract = {Power analysis is an essential tool for determining whether a statistically significant result can be expected in a scientific experiment prior to the experiment being performed. Many funding agencies and institutional review boards now require power analyses to be carried out before they will approve experiments, particularly where they involve the use of human subjects. This comprehensive, yet accessible, book provides practising researchers with step-by-step instructions for conducting power/sample size analyses, assuming only basic prior knowledge of summary statistics and the normal distribution. It contains a unified approach to statistical power analysis, with numerous easy-to-use tables to guide the reader without the need for further calculations or statistical expertise. This will be an indispensable text for researchers and graduates in the medical and biological sciences needing to apply power analysis in the design of their experiments.},
  langid = {english}
}

@book{beck_modern_1957,
  title = {Modern {{Science}} and the Nature of Life},
  author = {Beck, William Samson},
  year = {1957},
  month = jan,
  edition = {First Edition},
  publisher = {Harcourt, Brace},
  langid = {english}
}

@incollection{becker_failsafe_2005,
  title = {Failsafe {{N}} or {{File-Drawer Number}}},
  booktitle = {Publication {{Bias}} in {{Meta-Analysis}}},
  author = {Becker, Betsy Jane},
  year = {2005},
  pages = {111--125},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/0470870168.ch7},
  urldate = {2022-03-15},
  abstract = {This chapter contains sections titled: Introduction Definition of the Failsafe N Examples Assumptions of the Failsafe N Variations on the Failsafe N Summary of the Examples Applications of the Failsafe N Conclusions Acknowledgement References},
  chapter = {7},
  isbn = {978-0-470-87016-7},
  langid = {english},
  keywords = {'file-drawer' analysis,'offset publication bias',augmented Fisher test statistic,failsafe N or file-drawer number,file-drawer effect,Raudenbush's teacher expectancy data set,Stouffer tests}
}

@article{bem_feeling_2011,
  title = {Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect},
  shorttitle = {Feeling the Future},
  author = {Bem, Daryl J.},
  year = {2011},
  month = mar,
  journal = {Journal of Personality and Social Psychology},
  volume = {100},
  number = {3},
  pages = {407--425},
  issn = {1939-1315},
  doi = {10.1037/a0021524},
  abstract = {The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by "time-reversing" well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.},
  langid = {english},
  pmid = {21280961},
  keywords = {Affect,Awareness,Boredom,Cognition,Erotica,Escape Reaction,Female,Habituation Psychophysiologic,Humans,Male,Mental Recall,Parapsychology,Subliminal Stimulation,Time Factors}
}

@article{bem_must_2011,
  title = {Must Psychologists Change the Way They Analyze Their Data?},
  author = {Bem, Daryl J. and Utts, Jessica and Johnson, Wesley O.},
  year = {2011},
  month = oct,
  journal = {Journal of Personality and Social Psychology},
  volume = {101},
  number = {4},
  pages = {716--719},
  issn = {1939-1315},
  doi = {10.1037/a0024777},
  abstract = {Wagenmakers, Wetzels, Borsboom, and van der Maas (2011) argued that psychologists should replace the familiar "frequentist" statistical analyses of their data with bayesian analyses. To illustrate their argument, they reanalyzed a set of psi experiments published recently in this journal by Bem (2011), maintaining that, contrary to his conclusion, his data do not yield evidence in favor of the psi hypothesis. We argue that they have incorrectly selected an unrealistic prior distribution for their analysis and that a bayesian analysis using a more reasonable distribution yields strong evidence in favor of the psi hypothesis. More generally, we argue that there are advantages to bayesian analyses that merit their increased use in the future. However, as Wagenmakers et al.'s analysis inadvertently revealed, they contain hidden traps that must be better understood before being more widely substituted for the familiar frequentist analyses currently employed by most research psychologists.},
  langid = {english},
  pmid = {21928916},
  keywords = {Data Interpretation Statistical,Humans,Psychology}
}

@article{ben-shachar_effectsize_2020,
  title = {Effectsize: {{Estimation}} of {{Effect Size Indices}} and {{Standardized Parameters}}},
  shorttitle = {Effectsize},
  author = {{Ben-Shachar}, Mattan S. and L{\"u}decke, Daniel and Makowski, Dominique},
  year = {2020},
  month = dec,
  journal = {Journal of Open Source Software},
  volume = {5},
  number = {56},
  pages = {2815},
  issn = {2475-9066},
  doi = {10.21105/joss.02815},
  urldate = {2022-02-13},
  abstract = {Ben-Shachar et al., (2020). effectsize: Estimation of Effect Size Indices and Standardized Parameters. Journal of Open Source Software, 5(56), 2815, https://doi.org/10.21105/joss.02815},
  langid = {english}
}

@article{bender_adjusting_2001,
  title = {Adjusting for Multiple Testing---When and How?},
  author = {Bender, Ralf and Lange, Stefan},
  year = {2001},
  journal = {Journal of clinical epidemiology},
  volume = {54},
  number = {4},
  pages = {343--349},
  urldate = {2016-01-01}
}

@article{benjamini_controlling_1995,
  title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
  shorttitle = {Controlling the False Discovery Rate},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  year = {1995},
  journal = {Journal of the royal statistical society. Series B (Methodological)},
  eprint = {2346101},
  eprinttype = {jstor},
  pages = {289--300},
  urldate = {2016-09-07}
}

@article{benjamini_it_2016,
  title = {It's {{Not}} the p-Values' {{Fault}}},
  author = {Benjamini, Yoav},
  year = {2016},
  journal = {The American Statistician: Supplemental Material to the ASA Statement on P-Values and Statistical Significance},
  volume = {70},
  pages = {1--2}
}

@article{berger_interplay_2004,
  title = {The {{Interplay}} of {{Bayesian}} and {{Frequentist Analysis}}},
  author = {Berger, J. O. and Bayarri, M. J.},
  year = {2004},
  month = feb,
  journal = {Statistical Science},
  volume = {19},
  number = {1},
  pages = {58--80},
  issn = {0883-4237},
  doi = {10.1214/088342304000000116},
  urldate = {2016-01-11},
  langid = {english}
}

@book{berkeley_defence_1735,
  title = {A Defence of Free-Thinking in Mathematics, in Answer to a Pamphlet of {{Philalethes Cantabrigiensis}} Entitled {{Geometry No Friend}} to {{Infidelity}}. {{Also}} an Appendix Concerning Mr. {{Walton}}'s {{Vindication}} of the Principles of Fluxions against the Objections Contained in {{The}} Analyst. {{By}} the Author of {{The}} Minute Philosopher},
  author = {Berkeley, George},
  year = {1735},
  volume = {3}
}

@article{bird_selfplagiarism_2008,
  title = {Self-Plagiarism, Recycling Fraud, and the Intent to Mislead},
  author = {Bird, Steven B. and Sivilotti, Marco L. A.},
  year = {2008},
  month = jun,
  journal = {Journal of Medical Toxicology},
  volume = {4},
  number = {2},
  pages = {69--70},
  issn = {1556-9039},
  doi = {10.1007/BF03160957},
  urldate = {2022-09-27},
  pmcid = {PMC3550132},
  pmid = {18570164}
}

@article{bishop_fallibility_2018,
  title = {Fallibility in {{Science}}: {{Responding}} to {{Errors}} in the {{Work}} of {{Oneself}} and {{Others}}},
  shorttitle = {Fallibility in {{Science}}},
  author = {Bishop, D. V. M.},
  year = {2018},
  month = jul,
  journal = {Advances in Methods and Practices in Psychological Science},
  pages = {2515245918776632},
  issn = {2515-2459},
  doi = {10.1177/2515245918776632},
  urldate = {2018-07-04},
  langid = {english},
  annotation = {00000}
}

@book{bland_introduction_2015,
  title = {An Introduction to Medical Statistics},
  author = {Bland, Martin},
  year = {2015},
  series = {Oxford Medical Publications},
  edition = {Fourth edition},
  publisher = {Oxford University Press},
  address = {Oxford},
  isbn = {978-0-19-958992-0},
  langid = {english},
  lccn = {RA409 .B55 2015},
  keywords = {Medical statistics}
}

@article{bonett_replicationextension_2012,
  title = {Replication-{{Extension Studies}}},
  author = {Bonett, D. G.},
  year = {2012},
  month = dec,
  journal = {Current Directions in Psychological Science},
  volume = {21},
  number = {6},
  pages = {409--412},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/0963721412459512},
  urldate = {2015-11-30},
  langid = {english}
}

@book{borenstein_introduction_2009,
  title = {Introduction to Meta-Analysis},
  editor = {Borenstein, Michael},
  year = {2009},
  publisher = {John Wiley \& Sons},
  address = {Chichester, U.K},
  abstract = {This text provides a concise and clearly presented discussion of all the elements in a meta-analysis. It is illustrated with worked examples throughout, with visual explanations, using screenshots from Excel spreadsheets and computer programs such as Comprehensive Meta-Analysis (CMA) or Strata},
  isbn = {978-0-470-05724-7},
  lccn = {R853.M48 I58 2009},
  keywords = {Meta-analysis,Meta-Analysis as Topic}
}

@article{bosco_correlational_2015,
  title = {Correlational Effect Size Benchmarks},
  author = {Bosco, Frank A. and Aguinis, Herman and Singh, Kulraj and Field, James G. and Pierce, Charles A.},
  year = {2015},
  month = mar,
  journal = {The Journal of Applied Psychology},
  volume = {100},
  number = {2},
  pages = {431--449},
  issn = {1939-1854},
  doi = {10.1037/a0038047},
  abstract = {Effect size information is essential for the scientific enterprise and plays an increasingly central role in the scientific process. We extracted 147,328 correlations and developed a hierarchical taxonomy of variables reported in Journal of Applied Psychology and Personnel Psychology from 1980 to 2010 to produce empirical effect size benchmarks at the omnibus level, for 20 common research domains, and for an even finer grained level of generality. Results indicate that the usual interpretation and classification of effect sizes as small, medium, and large bear almost no resemblance to findings in the field, because distributions of effect sizes exhibit tertile partitions at values approximately one-half to one-third those intuited by Cohen (1988). Our results offer information that can be used for research planning and design purposes, such as producing better informed non-nil hypotheses and estimating statistical power and planning sample size accordingly. We also offer information useful for understanding the relative importance of the effect sizes found in a particular study in relationship to others and which research domains have advanced more or less, given that larger effect sizes indicate a better understanding of a phenomenon. Also, our study offers information about research domains for which the investigation of moderating effects may be more fruitful and provide information that is likely to facilitate the implementation of Bayesian analysis. Finally, our study offers information that practitioners can use to evaluate the relative effectiveness of various types of interventions.},
  langid = {english},
  pmid = {25314367},
  keywords = {Behavioral Research,Benchmarking,Data Interpretation Statistical,Humans}
}

@article{bozarth_signifying_1972,
  title = {Signifying Significant Significance.},
  author = {Bozarth, Jerold D. and Roberts, Ralph R.},
  year = {1972},
  journal = {American Psychologist},
  volume = {27},
  number = {8},
  pages = {774},
  publisher = {American Psychological Association}
}

@article{brachem_replikationskrise_2022,
  title = {Replikationskrise, p-Hacking Und {{Open Science}}},
  author = {Brachem, Johannes and Frank, Maximilian and Kvetnaya, Tatiana and Schramm, Leonhard F. F. and Volz, Leonhard},
  year = {2022},
  month = jan,
  journal = {Psychologische Rundschau},
  volume = {73},
  number = {1},
  pages = {1--17},
  publisher = {Hogrefe Verlag},
  issn = {0033-3042},
  doi = {10.1026/0033-3042/a000562},
  urldate = {2022-09-21},
  abstract = {Zusammenfassung. In den letzten Jahren gab es innerhalb der Psychologie eine intensive Auseinandersetzung mit den Auswirkungen der Replikationskrise sowie dem hieraus entstandenen Diskurs {\"u}ber die Weiterentwicklung der Disziplin. Als ein Grund f{\"u}r die mangelnde Replizierbarkeit psychologischer Forschung wurde die Verwendung fragw{\"u}rdiger Forschungspraktiken (eng. QRPs) identifiziert. W{\"a}hrend es umfangreiche Untersuchungen zur Pr{\"a}valenz von QRPs unter Wissenschaftler*innen gibt, ist bisher wenig {\"u}ber die Verbreitung dieser Praktiken unter Studierenden bekannt. Mit der hier vorgestellten Arbeit wurde erstmals eine gr{\"o}{\ss}ere Befragung unter 1397 Psychologie-Studierenden im deutschsprachigen Raum durchgef{\"u}hrt, um die Verbreitung von QRPs in studentischen Projekten sowie den aktuellen Stand der akademischen Lehre in Bezug auf die Replikationskrise und Open Science zu erheben. Die gemeinsame Betrachtung der Lehre und des Einsatzes fragw{\"u}rdiger Forschungspraktiken versprechen Aufschluss dar{\"u}ber, wie die psychologische Lehre mit dem empirischen Vorgehen der Studierenden zusammenh{\"a}ngt. Die Ergebnisse zeigen, dass QRPs auch in studentischen Projekten vorkommen, wobei gro{\ss}e Unterschiede in der Verbreitung einzelner QRPs bestehen. Auch zwischen den verschiedenen Projekttypen zeigten sich Unterschiede, so war die Anwendung von QRPs in Experimentalpraktika am st{\"a}rksten und in Masterarbeiten am schw{\"a}chsten ausgepr{\"a}gt. Unsere Daten weisen insgesamt darauf hin, dass die selbstberichtete Verbreitung von QRPs {\"u}ber den Studienverlauf abnimmt. Zudem scheint ein Gro{\ss}teil der Studierenden bereits mit der Thematik der Replikationskrise in der Lehre in Ber{\"u}hrung gekommen zu sein. Deren Behandlung findet gr{\"o}{\ss}tenteils in der Methodenlehre und weniger in inhaltlich spezialisierten Lehrveranstaltungen statt. Wir geben abschlie{\ss}end Impulse zur Weiterentwicklung der psychologischen Lehre, in denen die Prinzipien der Offenheit, Transparenz und Kollaboration beim Hervorbringen inhaltlich robuster Forschung bereits w{\"a}hrend des Studiums im Vordergrund stehen.},
  keywords = {Abschlussarbeiten,academic teaching,empirical projects,empirische Projekte,final year theses,fragewurdige Forschungspraktiken,Lehre,open science,Open Science,QRPs,questionable research practices,replication crisis,Replikationskrise,Studium}
}

@book{bretz_multiple_2011,
  title = {Multiple Comparisons Using {{R}}},
  author = {Bretz, Frank and Hothorn, Torsten and Westfall, Peter H.},
  year = {2011},
  publisher = {CRC Press},
  address = {Boca Raton, FL},
  abstract = {"Adopting a unifying theme based on maximum statistics, Multiple Comparisons Using R describes the common underlying theory of multiple comparison procedures through numerous examples. After giving examples of multiplicity problems, the book covers general concepts and basic multiple comparisons procedures, including the Bonferroni method and Simes' test. It then shows how to perform parametric multiple comparisons in standard linear models and general parametric models. It also introduces the multcomp package in R, which offers a convenient interface to perform multiple comparisons in a general context. Following this theoretical framework, the book explores applications involving the Dunnett test, Tukey's all pairwise comparisons, and general multiple contrast tests for standard regression models, mixed-effects models, and parametric survival models. The last chapter reviews other multiple comparison procedures, such as resampling-based procedures, methods for group sequential or adaptive designs, and the combination of multiple comparison procedures with modeling techniques. Controlling multiplicity in experiments ensures better decision making and safeguards against false claims. A self-contained introduction to multiple comparison procedures, this book offers strategies for constructing the procedures and illustrates the framework for multiple hypotheses testing in general parametric models. It is suitable for readers with R experience but limited knowledge of multiple comparison procedures and vice versa."--Publisher's description},
  isbn = {978-1-58488-574-0},
  lccn = {QA278.4 .B74 2011},
  keywords = {Multiple comparisons (Statistics),R (Computer program language)},
  annotation = {OCLC: ocn643322735}
}

@incollection{bross_critical_1971,
  title = {Critical Levels, Statistical Language and Scientific Inference},
  booktitle = {Foundations of Statistical Inference},
  author = {Bross, Irwin DJ},
  year = {1971},
  pages = {500--513},
  publisher = {{Holt, Rinehart and Winston}},
  address = {Toronto}
}

@article{brown_errors_1983,
  title = {Errors, {{Types I}} and {{II}}},
  author = {Brown, George W.},
  year = {1983},
  month = jun,
  journal = {American Journal of Diseases of Children},
  volume = {137},
  number = {6},
  pages = {586--591},
  issn = {0002-922X},
  doi = {10.1001/archpedi.1983.02140320062014},
  urldate = {2020-12-16},
  abstract = {{$\bullet$} The practicing physician and the clinical investigator regularly confront therapeutic trials, diagnostic tests, and other hypothesis-testing situations. The clinical literature increasingly displays statistical notations and concepts related to decision making in medicine. For these reasons, the physician is obligated to have some familiarity with the principles behind the null hypothesis, Type I and II errors, statistical power, and related elements of hypothesis testing.(Am J Dis Child 1983;137:586-591)}
}

@article{brown_grim_2017,
  title = {The {{GRIM Test}}: {{A Simple Technique Detects Numerous Anomalies}} in the {{Reporting}} of {{Results}} in {{Psychology}}},
  shorttitle = {The {{GRIM Test}}},
  author = {Brown, Nicholas J. L. and Heathers, James A. J.},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {363--369},
  issn = {1948-5506},
  doi = {10.1177/1948550616673876},
  urldate = {2019-07-13},
  abstract = {We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20\% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.},
  langid = {english}
}

@article{brunner_estimating_2020,
  title = {Estimating {{Population Mean Power Under Conditions}} of {{Heterogeneity}} and {{Selection}} for {{Significance}}},
  author = {Brunner, Jerry and Schimmack, Ulrich},
  year = {2020},
  month = may,
  journal = {Meta-Psychology},
  volume = {4},
  issn = {2003-2714},
  doi = {10.15626/MP.2018.874},
  urldate = {2022-03-16},
  abstract = {In scientific fields that use significance tests, statistical power is important for successful replications of significant results because it is the long-run success rate in a series of exact replication studies. For any population of significant results, there is a population of power values of the statistical tests on which conclusions are based. We give exact theoretical results showing how selection for significance affects the distribution of statistical power in a heterogeneous population of significance tests. In a set of large-scale simulation studies, we compare four methods for estimating population mean power of a set of studies selected for significance (a maximum likelihood model, extensions of p-curve and p-uniform, \&amp; z-curve). The p-uniform and p-curve methods performed well with a fixed effects size and varying sample sizes. However, when there was substantial variability in effect sizes as well as sample sizes, both methods systematically overestimate mean power. With heterogeneity in effect sizes, the maximum likelihood model produced the most accurate estimates when the distribution of effect sizes matched the assumptions of the model, but z-curve produced more accurate estimates when the assumptions of the maximum likelihood model were not met. We recommend the use of z-curve to estimate the typical power of significant results, which has implications for the replicability of significant results in psychology journals.},
  copyright = {Copyright (c) 2020 Jerry Brunner, Ulrich Schimmack},
  langid = {english},
  keywords = {Effect size,Maximum likelihood,Meta-analysis,P-curve,P-uniform,Post-hoc power analysis,Power estimation,Publication bias,Replicability,Z-curve}
}

@article{bryan_behavioural_2021,
  title = {Behavioural Science Is Unlikely to Change the World without a Heterogeneity Revolution},
  author = {Bryan, Christopher J. and Tipton, Elizabeth and Yeager, David S.},
  year = {2021},
  month = jul,
  journal = {Nature Human Behaviour},
  pages = {1--10},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01143-3},
  urldate = {2021-07-27},
  abstract = {In the past decade, behavioural science has gained influence in policymaking but suffered a crisis of confidence in the replicability of its findings. Here, we describe a nascent heterogeneity revolution that we believe these twin historical trends have triggered. This revolution will be defined by the recognition that most treatment effects are heterogeneous, so the variation in effect estimates across studies that defines the replication crisis is to be expected as long as heterogeneous effects are studied without a systematic approach to sampling and moderation. When studied systematically, heterogeneity can be leveraged to build more complete theories of causal mechanism that could inform nuanced and dependable guidance to policymakers. We recommend investment in shared research infrastructure to make it feasible to study behavioural interventions in heterogeneous and generalizable samples, and suggest low-cost steps researchers can take immediately to avoid being misled by heterogeneity and begin to learn from it instead.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Human behaviour;Policy;Research management;Science, technology and society\\
Subject\_term\_id: human-behaviour;policy;research-management;science-technology-and-society}
}

@article{brysbaert_how_2019,
  title = {How Many Participants Do We Have to Include in Properly Powered Experiments? {{A}} Tutorial of Power Analysis with Reference Tables},
  shorttitle = {How Many Participants Do We Have to Include in Properly Powered Experiments?},
  author = {Brysbaert, Marc},
  year = {2019},
  month = jul,
  journal = {Journal of Cognition},
  volume = {2},
  number = {1},
  pages = {16},
  issn = {2514-4820},
  doi = {10.5334/joc.72},
  urldate = {2019-07-19},
  abstract = {Article: How many participants do we have to include in properly powered experiments?  A tutorial of power analysis with reference tables},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english}
}

@article{brysbaert_power_2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  urldate = {2018-09-07},
  abstract = {Article: Power Analysis and Effect Size in Mixed Effects Models: A Tutorial},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  annotation = {00012}
}

@misc{buchanan_mote_2017,
  title = {{{MOTE}}: {{Effect Size}} and {{Confidence Interval Calculator}}.},
  author = {Buchanan, Erin M. and Scofield, J and Valentine, K. D.},
  year = {2017}
}

@article{bulus_bound_2021,
  title = {Bound {{Constrained Optimization}} of {{Sample Sizes Subject}} to {{Monetary Restrictions}} in {{Planning Multilevel Randomized Trials}} and {{Regression Discontinuity Studies}}},
  author = {Bulus, Metin and Dong, Nianbo},
  year = {2021},
  month = apr,
  journal = {The Journal of Experimental Education},
  volume = {89},
  number = {2},
  pages = {379--401},
  publisher = {Routledge},
  issn = {0022-0973},
  doi = {10.1080/00220973.2019.1636197},
  urldate = {2022-01-22},
  abstract = {Sample size determination in multilevel randomized trials (MRTs) and multilevel regression discontinuity designs (MRDDs) can be complicated due to multilevel structure, monetary restrictions, differing marginal costs per treatment and control units, and range restrictions in sample size at one or more levels. These issues have sparked a set of studies under optimal design literature where scholars consider sample size determination as an allocation problem. The literature on optimal design of MRTs and MRDDs and their implementation in software packages has been scarce, scattered, and incomplete. This study unifies optimal design literature and extends currently available software under bound constrained optimal sample allocation (BCOSA) framework via bound constrained optimization technique. The BCOSA framework, introduction to the cosa R library, and an illustration that replicates and extends minimum required sample size determination for an evaluation report is provided.},
  keywords = {bound constrained optimal sample allocation,conditional optimal design,multilevel randomized trials,multilevel regression discontinuity designs}
}

@article{bulus_pwrss_2023,
  title = {{pwrss R paketi ile istatistiksel g{\"u}{\c c} analizi [Statistical power analysis with pwrss R package]}},
  author = {Bulus, M. and Polat, C.},
  year = {2023},
  month = feb,
  publisher = {Open Science Framework},
  urldate = {2024-07-07},
  abstract = {This   study   presents   the   theoretical foundations and   computational   approachestostatistical  power  analysis. Ten  hypothesis  tests  and  their  derivatives  are  reviewed, including the test   of oneproportion   against   a   constant,   difference   betweentwo proportions, onemean  against  a  constant,  difference  between two  means  (independent and  matched samples), onecorrelationagainst  a  constant,  difference  between two correlations, R-squared deviation from zero in linear regression, difference between twoR-squared values in hierarchical linear regression, analyses of variance/covariance (one-way, two-way and three-way ANOVA or ANCOVA) for comparingmeans of two or more groups, and repeated measures ANOVA. The concept of statistical power and sample size calculations for these tests are consolidated with practical examples. The hypothesis tests of  non-inferiority,  superiority,  and  equivalence,  which  are  widely  used  in  medical  and pharmaceutical  research  are  also  introduced,and  their  applications  are  demonstrated using examples from behavioral and educational research. Calculations were performed with the pwrss R package(https://pwrss.shinyapps.io/lang-en/).},
  langid = {tur}
}

@article{burriss_changes_2015,
  title = {Changes in Women's Facial Skin Color over the Ovulatory Cycle Are Not Detectable by the Human Visual System},
  author = {Burriss, Robert P. and Troscianko, Jolyon and Lovell, P. George and Fulford, Anthony J. C. and Stevens, Martin and Quigley, Rachael and Payne, Jenny and Saxton, Tamsin K. and Rowland, Hannah M.},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130093},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130093},
  urldate = {2017-10-01},
  abstract = {Human ovulation is not advertised, as it is in several primate species, by conspicuous sexual swellings. However, there is increasing evidence that the attractiveness of women's body odor, voice, and facial appearance peak during the fertile phase of their ovulatory cycle. Cycle effects on facial attractiveness may be underpinned by changes in facial skin color, but it is not clear if skin color varies cyclically in humans or if any changes are detectable. To test these questions we photographed women daily for at least one cycle. Changes in facial skin redness and luminance were then quantified by mapping the digital images to human long, medium, and shortwave visual receptors. We find cyclic variation in skin redness, but not luminance. Redness decreases rapidly after menstrual onset, increases in the days before ovulation, and remains high through the luteal phase. However, we also show that this variation is unlikely to be detectable by the human visual system. We conclude that changes in skin color are not responsible for the effects of the ovulatory cycle on women's attractiveness.},
  keywords = {Cameras,Color vision,Estradiol,Estrogens,Face,Luminance,Ovulation,Visual system}
}

@article{button_minimal_2015,
  title = {Minimal Clinically Important Difference on the {{Beck Depression Inventory}} - {{II}} According to the Patient's Perspective},
  author = {Button, K. S. and Kounali, D. and Thomas, L. and Wiles, N. J. and Peters, T. J. and Welton, N. J. and Ades, A. E. and Lewis, G.},
  year = {2015},
  month = nov,
  journal = {Psychological Medicine},
  volume = {45},
  number = {15},
  pages = {3269--3279},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291715001270},
  urldate = {2017-10-01},
  abstract = {Background The Beck Depression Inventory, 2nd edition (BDI-II) is widely used in research on depression. However, the minimal clinically important difference (MCID) is unknown. MCID can be estimated in several ways. Here we take a patient-centred approach, anchoring the change on the BDI-II to the patient's global report of improvement. Method  We used data collected (n = 1039) from three randomized controlled trials for the management of depression. Improvement on a `global rating of change' question was compared with changes in BDI-II scores using general linear modelling to explore baseline dependency, assessing whether MCID is best measured in absolute terms (i.e. difference) or as percent reduction in scores from baseline (i.e. ratio), and receiver operator characteristics (ROC) to estimate MCID according to the optimal threshold above which individuals report feeling `better'. Results Improvement in BDI-II scores associated with reporting feeling `better' depended on initial depression severity, and statistical modelling indicated that MCID is best measured on a ratio scale as a percentage reduction of score. We estimated a MCID of a 17.5\% reduction in scores from baseline from ROC analyses. The corresponding estimate for individuals with longer duration depression who had not responded to antidepressants was higher at 32\%. Conclusions MCID on the BDI-II is dependent on baseline severity, is best measured on a ratio scale, and the MCID for treatment-resistant depression is larger than that for more typical depression. This has important implications for clinical trials and practice.},
  keywords = {2nd edition (BDI-II),Beck Depression Inventory,depression,minimal clinically important difference,outcome assessment,primary care}
}

@article{button_power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, K. S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = apr,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2015-11-30}
}

@article{caplan_how_2021,
  title = {How {{Should We Regard Information Gathered}} in {{Nazi Experiments}}?},
  author = {Caplan, Arthur L.},
  year = {2021},
  month = jan,
  journal = {AMA Journal of Ethics},
  volume = {23},
  number = {1},
  pages = {55--58},
  publisher = {American Medical Association},
  issn = {2376-6980},
  doi = {10.1001/amajethics.2021.55},
  urldate = {2022-09-25},
  abstract = {Immorally acquired information from Nazi experimentation or other sources infects the body of scientific and biomedical knowledge. Responding to this reality ethically means insisting on good teaching about the horrific history of such information's sources and careful deliberation about how it is referenced and described.}
}

@article{carter_correcting_2019,
  title = {Correcting for {{Bias}} in {{Psychology}}: {{A Comparison}} of {{Meta-Analytic Methods}}},
  shorttitle = {Correcting for {{Bias}} in {{Psychology}}},
  author = {Carter, Evan C. and Sch{\"o}nbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
  year = {2019},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {115--144},
  issn = {2515-2459},
  doi = {10.1177/2515245919847196},
  urldate = {2019-07-11},
  abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses---that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
  langid = {english}
}

@article{carter_publication_2014,
  title = {Publication Bias and the Limited Strength Model of Self-Control: Has the Evidence for Ego Depletion Been Overestimated?},
  shorttitle = {Publication Bias and the Limited Strength Model of Self-Control},
  author = {Carter, Evan C. and McCullough, Michael E.},
  year = {2014},
  month = jul,
  journal = {Frontiers in Psychology},
  volume = {5},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00823},
  urldate = {2015-11-30}
}

@article{cascio_open_1983,
  title = {Open a {{New Window}} in {{Rational Research Planning}}: {{Adjust Alpha}} to {{Maximize Statistical Power}}},
  shorttitle = {Open a {{New Window}} in {{Rational Research Planning}}},
  author = {Cascio, Wayne F. and Zedeck, Sheldon},
  year = {1983},
  journal = {Personnel Psychology},
  volume = {36},
  number = {3},
  pages = {517--526},
  issn = {1744-6570},
  doi = {10.1111/j.1744-6570.1983.tb02233.x},
  urldate = {2020-12-16},
  abstract = {Alternative strategies for optimizing statistical power in applied psychological research are considered. Increasing sample size and combining predictors in order to yield a useful effect size are well-known tactics for increasing power. A third approach, increasing alpha, is rarely used because of zealous adherence to convention. There are two related aspects in setting the alpha level. First, the relative seriousness of Type I and Type II errors must be considered. This assessment must then be qualified and redetermined after taking into account the prior probability that an effect exists. Procedures that make these processes objective are demonstrated. When sample size and effect size are both fixed, increasing alpha may be the only feasible strategy for maximizing power. It is concluded that a priori power analysis should be a major consideration in any test of an hypothesis, and that alpha level adjustment should be viewed as a useful strategy for increasing power.},
  langid = {english}
}

@article{ceci_psychological_2000,
  title = {Psychological {{Science}} in the {{Public Interest}}: {{The Case}} for {{Juried Analyses}}},
  shorttitle = {Psychological {{Science}} in the {{Public Interest}}},
  author = {Ceci, Stephen J. and Bjork, Robert A.},
  year = {2000},
  month = may,
  journal = {Psychological Science},
  volume = {11},
  number = {3},
  pages = {177--178},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00237},
  urldate = {2023-08-26},
  abstract = {The inaugural issue of Psychological Science in the Public Interest (PSPI), a new publishing initiative by the American Psychological Society, accompanies this issue of Psychological Science. The report it contains, ``Psychological Science Can Improve Diagnostic Decisions,'' by John Swets, Robyn Dawes, and John Monahan, represents a careful effort by those authors to summarize the potential of modern psychological science to enhance real-world diagnostic decisions. Such decisions (Is a cancer present? Will this individual commit violence? Will an impending storm strike? Will this applicant succeed?) are prevalent and crucial to the lives of individuals and to the well-being of our society. Subsequent issues of PSPI will address other important topics of public interest in areas where psychological science may have the potential to inform and improve public policy. Each of those reports will also represent the efforts of a distinguished team of scientists to report the available evidence, and the implications of that evidence, fairly and comprehensively. In this article, we describe the goals, procedures, and potential of PSPI.},
  langid = {english}
}

@article{cevolani_verisimilitude_2011,
  title = {Verisimilitude and Belief Change for Conjunctive Theories},
  author = {Cevolani, Gustavo and Crupi, Vincenzo and Festa, Roberto},
  year = {2011},
  journal = {Erkenntnis},
  volume = {75},
  number = {2},
  pages = {183},
  urldate = {2017-06-18}
}

@article{chalmers_avoidable_2009,
  title = {Avoidable Waste in the Production and Reporting of Research Evidence},
  author = {Chalmers, Iain and Glasziou, Paul},
  year = {2009},
  journal = {The Lancet},
  volume = {374},
  number = {9683},
  pages = {86--89}
}

@article{chamberlin_method_1890,
  title = {The {{Method}} of {{Multiple Working Hypotheses}}},
  author = {Chamberlin, T. C.},
  year = {1890},
  month = feb,
  journal = {Science},
  volume = {ns-15},
  number = {366},
  pages = {92--96},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.ns-15.366.92},
  urldate = {2023-08-26}
}

@article{chambers_present_2022,
  title = {The Past, Present and Future of {{Registered Reports}}},
  author = {Chambers, Christopher D. and Tzavella, Loukia},
  year = {2022},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {1},
  pages = {29--42},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01193-7},
  urldate = {2022-04-06},
  abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Publishing}
}

@book{chang_adaptive_2016,
  title = {Adaptive {{Design Theory}} and {{Implementation Using SAS}} and {{R}}},
  author = {Chang, Mark},
  year = {2016},
  month = oct,
  edition = {2nd edition},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Get Up to Speed on Many Types of Adaptive Designs  Since the publication of the first edition, there have been remarkable advances in the methodology and application of adaptive trials. Incorporating many of these new developments, Adaptive Design Theory and Implementation Using SAS and R, Second Edition offers a detailed framework to understand the use of various adaptive design methods in clinical trials.  New to the Second Edition  Twelve new chapters covering blinded and semi-blinded sample size reestimation design, pick-the-winners design, biomarker-informed adaptive design, Bayesian designs, adaptive multiregional trial design, SAS and R for group sequential design, and much more More analytical methods for K-stage adaptive designs, multiple-endpoint adaptive design, survival modeling, and adaptive treatment switching New material on sequential parallel designs with rerandomization and the skeleton approach in adaptive dose-escalation trials Twenty new SAS macros and R functions Enhanced end-of-chapter problems that give readers hands-on practice addressing issues encountered in designing real-life adaptive trials  Covering even more adaptive designs, this book provides biostatisticians, clinical scientists, and regulatory reviewers with up-to-date details on this innovative area in pharmaceutical research and development. Practitioners will be able to improve the efficiency of their trial design, thereby reducing the time and cost of drug development.},
  isbn = {978-1-138-03423-5},
  langid = {english}
}

@book{chang_realism_2022,
  title = {Realism for {{Realistic People}}: {{A New Pragmatist Philosophy}} of {{Science}}},
  shorttitle = {Realism for {{Realistic People}}},
  author = {Chang, Hasok},
  year = {2022},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9781108635738},
  urldate = {2023-08-06},
  abstract = {In this innovative book, Hasok Chang constructs a philosophy of science for 'realistic people' interested in understanding and promoting the actual practices of inquiry in science and other knowledge-focused areas of life. Inspired by pragmatist philosophy, he reconceives the very notions of reality and truth on the basis of his concept of the 'operational coherence' of epistemic activities, and offers new pragmatist conceptions of truth and reality as operational ideals achievable in actual scientific practice. Rejecting the version of scientific realism that is concerned with claiming that our theories correspond to an ultimate reality, he proposes instead an 'activist realism': a commitment to do all that we can actually do to improve our knowledge of realities. His book will appeal to scholars and students in philosophy, science and the history of science, and all who are concerned about the place of science and empirical truth in society.},
  isbn = {978-1-108-47038-4}
}

@techreport{chatziathanasiou_beware_2022,
  type = {{{SSRN Scholarly Paper}}},
  title = {Beware the {{Lure}} of {{Narratives}}: `{{Hungry Judges}}' {{Should}} Not {{Motivate}} the {{Use}} of `{{Artificial Intelligence}}' in {{Law}}},
  shorttitle = {Beware the {{Lure}} of {{Narratives}}},
  author = {Chatziathanasiou, Konstantin},
  year = {2022},
  month = jan,
  number = {ID 4011603},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  doi = {10.2139/ssrn.4011603},
  urldate = {2022-02-14},
  abstract = {The `hungry judge' effect, as presented by a famous study, is a common point of reference to underline human bias in judicial decision-making. This is particularly pronounced in the literature on `artificial intelligence' (AI) in law. Here, the effect is invoked to counter concerns about bias in automated decision-aids and to motivate their use. However, the validity of the `hungry judge' effect is doubtful. In our context, this is problematic for, at least, two reasons. First, shaky evidence leads to a misconstruction of the problem that may warrant an AI intervention. Second, painting the justice system worse than it actually is, is a dangerous argumentative strategy as it undermines institu-tional trust. Against this background, this article revisits the original `hungry judge' study and argues that it cannot be relied on as an argument in the AI discourse or beyond. The case of `hungry judges' demonstrates the lure of narratives, the dangers of `problem gerrymandering', and ultimately the need for a careful reception of social science.},
  langid = {english},
  keywords = {extra-legal influences,hungry judge,judicial decision-making,social science in law}
}

@article{chin_questionable_2021,
  title = {Questionable {{Research Practices}} and {{Open Science}} in {{Quantitative Criminology}}},
  author = {Chin, Jason M. and Pickett, Justin T. and Vazire, Simine and Holcombe, Alex O.},
  year = {2021},
  month = aug,
  journal = {Journal of Quantitative Criminology},
  issn = {1573-7799},
  doi = {10.1007/s10940-021-09525-6},
  urldate = {2022-02-07},
  abstract = {Questionable research practices (QRPs) lead to incorrect research results and contribute to irreproducibility in science. Researchers and institutions have proposed open science practices (OSPs) to improve the detectability of QRPs and the credibility of science. We examine the prevalence of QRPs and OSPs in criminology, and researchers' opinions of those practices.},
  langid = {english}
}

@article{cho_twotailed_2013,
  title = {Is Two-Tailed Testing for Directional Research Hypotheses Tests Legitimate?},
  author = {Cho, Hyun-Chul and Abe, Shuzo},
  year = {2013},
  month = sep,
  journal = {Journal of Business Research},
  series = {Advancing {{Research Methods}} in {{Marketing}}},
  volume = {66},
  number = {9},
  pages = {1261--1266},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2012.02.023},
  urldate = {2016-09-02},
  abstract = {This paper demonstrates that there is currently a widespread misuse of two-tailed testing for directional research hypotheses tests. One probable reason for this overuse of two-tailed testing is the seemingly valid beliefs that two-tailed testing is more conservative and safer than one-tailed testing. However, the authors examine the legitimacy of this notion and find it to be flawed. A second and more fundamental cause of the current problem is the pervasive oversight in making a clear distinction between the research hypothesis and the statistical hypothesis. Based upon the explicated, sound relationship between the research and statistical hypotheses, the authors propose a new scheme of hypothesis classification to facilitate and clarify the proper use of statistical hypothesis testing in empirical research.},
  keywords = {hypothesis testing,one-tailed testing,Research hypothesis in existential form,Research hypothesis in non-existential form,Statistical hypothesis,two-tailed testing}
}

@article{cohen_earth_1994,
  title = {The Earth Is Round (p {$<$} .05).},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/0003-066X.49.12.997},
  urldate = {2017-06-05},
  langid = {english}
}

@book{cohen_statistical_1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {2nd ed},
  publisher = {L. Erlbaum Associates},
  address = {Hillsdale, N.J},
  isbn = {978-0-8058-0283-2},
  lccn = {HA29 .C66 1988},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis}
}

@article{cohen_things_1990,
  title = {Things {{I}} Have Learned (so Far)},
  author = {Cohen, Jacob},
  year = {1990},
  journal = {American Psychologist},
  volume = {45},
  number = {12},
  pages = {1304--1312},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.45.12.1304},
  abstract = {This is an account of what I have learned (so far) about the application of statistics to psychology and the other sociobiomedical sciences. It includes the principles "less is more" (fewer variables, more highly targeted issues, sharp rounding off), "simple is better" (graphic representation, unit weighting for linear composites), and "some things you learn aren't so." I have learned to avoid the many misconceptions that surround Fisherian null hypothesis testing. I have also learned the importance of power analysis and the determination of just how big (rather than how statistically significant) are the effects that we study. Finally, I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data, and that things take time. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Psychology,Social Sciences,Statistics}
}

@article{coles_multi-lab_2022,
  title = {A Multi-Lab Test of the Facial Feedback Hypothesis by the {{Many Smiles Collaboration}}},
  author = {Coles, Nicholas A. and March, David S. and {Marmolejo-Ramos}, Fernando and Larsen, Jeff T. and Arinze, Nwadiogo C. and Ndukaihe, Izuchukwu L. G. and Willis, Megan L. and Foroni, Francesco and Reggev, Niv and Mokady, Aviv and Forscher, Patrick S. and Hunter, John F. and Kaminski, Gwena{\"e}l and Y{\"u}vr{\"u}k, Elif and Kapucu, Aycan and Nagy, Tam{\'a}s and Hajdu, Nandor and Tejada, Julian and Freitag, Raquel M. K. and Zambrano, Danilo and Som, Bidisha and Aczel, Balazs and Barzykowski, Krystian and Adamus, Sylwia and Filip, Katarzyna and Yamada, Yuki and Ikeda, Ayumi and Eaves, Daniel L. and Levitan, Carmel A. and Leiweke, Sydney and Parzuchowski, Michal and Butcher, Natalie and Pfuhl, Gerit and {Basnight-Brown}, Dana M. and Hinojosa, Jos{\'e} A. and Montoro, Pedro R. and Javela D, Lady G. and Vezirian, Kevin and IJzerman, Hans and Trujillo, Natalia and Pressman, Sarah D. and Gygax, Pascal M. and {\"O}zdo{\u g}ru, Asil A. and {Ruiz-Fernandez}, Susana and Ellsworth, Phoebe C. and Gaertner, Lowell and Strack, Fritz and Marozzi, Marco and Liuzza, Marco Tullio},
  year = {2022},
  month = dec,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {12},
  pages = {1731--1742},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01458-9},
  urldate = {2023-03-24},
  abstract = {Following theories of emotional embodiment, the facial feedback hypothesis suggests that individuals' subjective experiences of emotion are influenced by their facial expressions. However, evidence for this hypothesis has been mixed. We thus formed a global adversarial collaboration and carried out a preregistered, multicentre study designed to specify and test the conditions that should most reliably produce facial feedback effects. Data from n\,=\,3,878 participants spanning 19 countries indicated that a facial mimicry and voluntary facial action task could both amplify and initiate feelings of happiness. However, evidence of facial feedback effects was less conclusive when facial feedback was manipulated unobtrusively via a pen-in-mouth task.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Psychology}
}

@article{colling_registered_2020,
  title = {Registered {{Replication Report}} on {{Fischer}}, {{Castel}}, {{Dodd}}, and {{Pratt}} (2003)},
  author = {Colling, Lincoln J. and Sz{\H u}cs, D{\'e}nes and De Marco, Damiano and Cipora, Krzysztof and Ulrich, Rolf and Nuerk, Hans-Christoph and Soltanlou, Mojtaba and Bryce, Donna and Chen, Sau-Chin and Schroeder, Philipp Alexander and Henare, Dion T. and Chrystall, Christine K. and Corballis, Paul M. and Ansari, Daniel and Goffin, Celia and Sokolowski, H. Moriah and Hancock, Peter J. B. and Millen, Ailsa E. and Langton, Stephen R. H. and Holmes, Kevin J. and Saviano, Mark S. and Tummino, Tia A. and Lindemann, Oliver and Zwaan, Rolf A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Beckov{\'a}, Ad{\'e}la and Vranka, Marek A. and Cutini, Simone and Mammarella, Irene Cristina and Mulatti, Claudio and Bell, Raoul and Buchner, Axel and Mieth, Laura and R{\"o}er, Jan Philipp and Klein, Elise and Huber, Stefan and Moeller, Korbinian and Ocampo, Brenda and Lupi{\'a}{\~n}ez, Juan and {Ortiz-Tudela}, Javier and {de la Fuente}, Juanma and Santiago, Julio and Ouellet, Marc and Hubbard, Edward M. and Toomarian, Elizabeth Y. and Job, Remo and Treccani, Barbara and McShane, Blakeley B.},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {143--162},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920903079},
  urldate = {2020-10-01},
  abstract = {The attentional spatial-numerical association of response codes (Att-SNARC) effect (Fischer, Castel, Dodd, \& Pratt, 2003)?the finding that participants are quicker to detect left-side targets when the targets are preceded by small numbers and quicker to detect right-side targets when they are preceded by large numbers?has been used as evidence for embodied number representations and to support strong claims about the link between number and space (e.g., a mental number line). We attempted to replicate Experiment 2 of Fischer et al. by collecting data from 1,105 participants at 17 labs. Across all 1,105 participants and four interstimulus-interval conditions, the proportion of times the effect we observed was positive (i.e., directionally consistent with the original effect) was .50. Further, the effects we observed both within and across labs were minuscule and incompatible with those observed by Fischer et al. Given this, we conclude that we failed to replicate the effect reported by Fischer et al. In addition, our analysis of several participant-level moderators (finger-counting habits, reading and writing direction, handedness, and mathematics fluency and mathematics anxiety) revealed no substantial moderating effects. Our results indicate that the Att-SNARC effect cannot be used as evidence to support strong claims about the link between number and space.}
}

@article{colquhoun_false_2019,
  title = {The {{False Positive Risk}}: {{A Proposal Concerning What}} to {{Do About}} p-{{Values}}},
  shorttitle = {The {{False Positive Risk}}},
  author = {Colquhoun, David},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {192--201},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1529622},
  urldate = {2019-04-08},
  abstract = {It is widely acknowledged that the biomedical literature suffers from a surfeit of false positive results. Part of the reason for this is the persistence of the myth that observation of p {$<$} 0.05 is sufficient justification to claim that you have made a discovery. It is hopeless to expect users to change their reliance on p-values unless they are offered an alternative way of judging the reliability of their conclusions. If the alternative method is to have a chance of being adopted widely, it will have to be easy to understand and to calculate. One such proposal is based on calculation of false positive risk(FPR). It is suggested that p-values and confidence intervals should continue to be given, but that they should be supplemented by a single additional number that conveys the strength of the evidence better than the p-value. This number could be the minimum FPR (that calculated on the assumption of a prior probability of 0.5, the largest value that can be assumed in the absence of hard prior data). Alternatively one could specify the prior probability that it would be necessary to believe in order to achieve an FPR of, say, 0.05.},
  keywords = {Bayes,False positive,False positive report probability,False positive risk,FPR,Likelihood ratio,Point null,Positive predictive value}
}

@article{cook_assessing_2014,
  title = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial: {{DELTA}} ({{Difference ELicitation}} in {{TriAls}}) Review},
  shorttitle = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial},
  author = {Cook, Jonathan and Hislop, Jennifer and Adewuyi, Temitope and Harrild, Kirsten and Altman, Douglas and Ramsay, Craig and Fraser, Cynthia and Buckley, Brian and Fayers, Peter and Harvey, Ian and Briggs, Andrew and Norrie, John and Fergusson, Dean and Ford, Ian and Vale, Luke},
  year = {2014},
  month = jan,
  journal = {Health Technology Assessment},
  volume = {18},
  number = {28},
  issn = {1366-5278, 2046-4924},
  doi = {10.3310/hta18280},
  urldate = {2020-12-11},
  langid = {english}
}

@article{cook_pvalue_2002,
  title = {P-{{Value Adjustment}} in {{Sequential Clinical Trials}}},
  author = {Cook, Thomas D.},
  year = {2002},
  journal = {Biometrics},
  volume = {58},
  number = {4},
  pages = {1005--1011},
  publisher = {Wiley Online Library}
}

@book{cooper_handbook_2009,
  title = {The Handbook of Research Synthesis and Meta-Analysis},
  editor = {Cooper, Harris M. and Hedges, Larry V. and Valentine, Jeff C.},
  year = {2009},
  edition = {2nd ed},
  publisher = {Russell Sage Foundation},
  address = {New York},
  isbn = {978-0-87154-163-5},
  lccn = {Q180.55.M4 H35 2009},
  keywords = {Information storage and retrieval systems,Methodology,Research,Statistical methods}
}

@book{cooper_reporting_2020,
  title = {Reporting Quantitative Research in Psychology: {{How}} to Meet {{APA Style Journal Article Reporting Standards}} (2nd Ed.).},
  shorttitle = {Reporting Quantitative Research in Psychology},
  author = {Cooper, Harris},
  year = {2020},
  publisher = {American Psychological Association},
  address = {Washington},
  doi = {10.1037/0000178-000},
  urldate = {2020-07-25},
  isbn = {978-1-4338-3283-3 978-1-4338-3342-7},
  langid = {english}
}

@article{copay_understanding_2007,
  title = {Understanding the Minimum Clinically Important Difference: A Review of Concepts and Methods},
  shorttitle = {Understanding the Minimum Clinically Important Difference},
  author = {Copay, Anne G. and Subach, Brian R. and Glassman, Steven D. and Polly, David W. and Schuler, Thomas C.},
  year = {2007},
  journal = {The Spine Journal},
  volume = {7},
  number = {5},
  pages = {541--546},
  doi = {10.1016/j.spinee.2007.01.008},
  urldate = {2017-07-21}
}

@article{corneille_beware_2023,
  title = {Beware `Persuasive Communication Devices' When Writing and Reading Scientific Articles},
  author = {Corneille, Olivier and Havemann, Jo and Henderson, Emma L and IJzerman, Hans and Hussey, Ian and {Orban de Xivry}, Jean-Jacques and Jussim, Lee and Holmes, Nicholas P and Pilacinski, Artur and Beffara, Brice and Carroll, Harriet and Outa, Nicholas Otieno and Lush, Peter and Lotter, Leon D},
  year = {2023},
  month = may,
  journal = {eLife},
  volume = {12},
  pages = {e88654},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.88654},
  urldate = {2023-09-02},
  abstract = {Authors rely on a range of devices and techniques to attract and maintain the interest of readers, and to convince them of the merits of the author's point of view. However, when writing a scientific article, authors must use these `persuasive communication devices' carefully. In particular, they must be explicit about the limitations of their work, avoid obfuscation, and resist the temptation to oversell their results. Here we discuss a list of persuasive communication devices and we encourage authors, as well as reviewers and editors, to think carefully about their use.},
  keywords = {citation,language,point of view,reporting,scientific publishing,scientific writing}
}

@article{correll_avoid_2020,
  title = {Avoid {{Cohen}}'s `{{Small}}', `{{Medium}}', and `{{Large}}' for {{Power Analysis}}},
  author = {Correll, Joshua and Mellinger, Christopher and McClelland, Gary H. and Judd, Charles M.},
  year = {2020},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {3},
  pages = {200--207},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.12.009},
  urldate = {2020-08-11},
  abstract = {One of the most difficult and important decisions in power analysis involves specifying an effect size. Researchers frequently employ definitions of small, medium, and large that were proposed by Jacob Cohen. These definitions are problematic for two reasons. First, they are arbitrary, based on non-scientific criteria. Second, they are inconsistent, changing dramatically and illogically as a function of the statistical test a researcher plans to use (e.g., t-test versus regression). These problems may be unknown to many researchers, but they have a huge impact on power analyses. Estimates of the required n may be inappropriately doubled or cut in half. For power analyses to have any meaning, these definitions of effect size should be avoided.},
  langid = {english},
  keywords = {effect size,research design,research methods}
}

@manual{cousineau_superb_2019,
  type = {Manual},
  title = {Superb: {{Computes}} Standard Error and Confidence Interval of Means under Various Designs and Sampling Schemes},
  author = {Cousineau, Denis and Chiasson, Felix},
  year = {2019}
}

@article{cowles_origins_1982,
  title = {On the Origins of the. 05 Level of Statistical Significance.},
  author = {Cowles, Michael and Davis, Caroline},
  year = {1982},
  journal = {American Psychologist},
  volume = {37},
  number = {5},
  pages = {553},
  urldate = {2015-12-03}
}

@article{cox_problems_1958,
  title = {Some {{Problems Connected}} with {{Statistical Inference}}},
  author = {Cox, D. R.},
  year = {1958},
  month = jun,
  journal = {Annals of Mathematical Statistics},
  volume = {29},
  number = {2},
  pages = {357--372},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177706618},
  urldate = {2020-05-21},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  mrnumber = {MR94890},
  zmnumber = {0088.11702}
}

@article{cribbie_recommendations_2004,
  title = {Recommendations for Applying Tests of Equivalence},
  author = {Cribbie, Robert A. and Gruman, Jamie A. and {Arpin-Cribbie}, Chantal A.},
  year = {2004},
  journal = {Journal of clinical psychology},
  volume = {60},
  number = {1},
  pages = {1--10},
  publisher = {Wiley Online Library}
}

@article{crusius_envy_2020,
  title = {Envy: {{An Adversarial Review}} and {{Comparison}} of {{Two Competing Views}}},
  shorttitle = {Envy},
  author = {Crusius, Jan and Gonzalez, Manuel F. and Lange, Jens and {Cohen-Charash}, Yochi},
  year = {2020},
  month = jan,
  journal = {Emotion Review},
  volume = {12},
  number = {1},
  pages = {3--21},
  publisher = {SAGE Publications},
  issn = {1754-0739},
  doi = {10.1177/1754073919873131},
  urldate = {2023-08-26},
  abstract = {The nature of envy has recently been the subject of a heated debate. Some researchers see envy as a complex, yet unitary construct that despite being hostile in nature can lead to both hostile and nonhostile reactions. Others offer a dual approach to envy, in which envy's outcomes reflect two types of envy: benign envy, involving upward motivation, and malicious envy, involving hostility against superior others. We compare these competing conceptualizations of envy in an adversarial (yet collaborative) review. Our goal is to aid the consumers of envy research in navigating the intricacies of this debate. We identify agreements and disagreements and describe implications for theory, methodology, and measurement, as well as challenges and opportunities for future work.},
  langid = {english}
}

@article{cruwell_what_2023,
  title = {What's in a {{Badge}}? {{A Computational Reproducibility Investigation}} of the {{Open Data Badge Policy}} in {{One Issue}} of {{Psychological Science}}},
  shorttitle = {What's in a {{Badge}}?},
  author = {Cr{\"u}well, Sophia and Apthorp, Deborah and Baker, Bradley J. and Colling, Lincoln and Elson, Malte and Geiger, Sandra J. and Lobentanzer, Sebastian and Mon{\'e}ger, Jean and Patterson, Alex and Schwarzkopf, D. Samuel and Zaneva, Mirela and Brown, Nicholas J. L.},
  year = {2023},
  month = feb,
  journal = {Psychological Science},
  pages = {09567976221140828},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/09567976221140828},
  urldate = {2023-03-12},
  abstract = {In April 2019, Psychological Science published its first issue in which all Research Articles received the Open Data badge. We used that issue to investigate the effectiveness of this badge, focusing on the adherence to its aim at Psychological Science: sharing both data and code to ensure reproducibility of results. Twelve researchers of varying experience levels attempted to reproduce the results of the empirical articles in the target issue (at least three researchers per article). We found that all 14 articles provided at least some data and six provided analysis code, but only one article was rated to be exactly reproducible, and three were rated as essentially reproducible with minor deviations. We suggest that researchers should be encouraged to adhere to the higher standard in force at Psychological Science. Moreover, a check of reproducibility during peer review may be preferable to the disclosure method of awarding badges.},
  langid = {english}
}

@article{cumming_confidence_2006,
  title = {Confidence Intervals and Replication: {{Where}} Will the next Mean Fall?},
  shorttitle = {Confidence Intervals and Replication},
  author = {Cumming, Geoff and Maillardet, Robert},
  year = {2006},
  journal = {Psychological Methods},
  volume = {11},
  number = {3},
  pages = {217--227},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.3.217},
  urldate = {2015-11-30},
  langid = {english}
}

@book{cumming_introduction_2016,
  title = {Introduction to the {{New Statistics}}: {{Estimation}}, {{Open Science}}, and {{Beyond}}},
  shorttitle = {Introduction to the {{New Statistics}}},
  author = {Cumming, Geoff and {Calin-Jageman}, Robert},
  year = {2016},
  month = oct,
  publisher = {Routledge},
  abstract = {This is the first introductory statistics text to use an estimation approach from the start to help readers understand effect sizes, confidence intervals (CIs), and meta-analysis (`the new statistics'). It is also the first text to explain the new and exciting Open Science practices, which encourage replication and enhance the trustworthiness of research. In addition, the book explains NHST fully so students can understand published research. Numerous real research examples are used throughout. The book uses today's most effective learning strategies and promotes critical thinking, comprehension, and retention, to deepen users' understanding of statistics and modern research methods. The free ESCI (Exploratory Software for Confidence Intervals) software makes concepts visually vivid, and provides calculation and graphing facilities. The book can be used with or without ESCI.  Other highlights include: - Coverage of both estimation and NHST approaches, and how to easily translate between the two.  - Some exercises use ESCI to analyze data and create graphs including CIs, for best understanding of estimation methods.  -Videos of the authors describing key concepts and demonstrating use of ESCI provide an engaging learning tool for traditional or flipped classrooms. -In-chapter exercises and quizzes with related commentary allow students to learn by doing, and to monitor their progress. -End-of-chapter exercises and commentary, many using real data, give practice for using the new statistics to analyze data, as well as for applying research judgment in realistic contexts.  -Don't fool yourself tips help students avoid common errors.  -Red Flags highlight the meaning of "significance" and what p values actually mean.  -Chapter outlines, defined key terms, sidebars of key points, and summarized take-home messages provide a study tool at exam time.  -http://www.routledge.com/cw/cumming offers for students: ESCI downloads; data sets; key term flashcards; tips for using SPSS for analyzing data; and videos. For instructors it offers: tips for teaching the new statistics and Open Science; additional homework exercises; assessment items; answer keys for homework and assessment items; and downloadable text images; and PowerPoint lecture slides.  Intended for introduction to statistics, data analysis, or quantitative methods courses in psychology, education, and other social and health sciences, researchers interested in understanding the new statistics will also appreciate this book. No familiarity with introductory statistics is assumed.},
  googlebooks = {KR8xDQAAQBAJ},
  isbn = {978-1-317-48337-3},
  langid = {english},
  keywords = {Business & Economics / Statistics,Education / Statistics,Medical / Biostatistics,Psychology / Statistics}
}

@article{cumming_new_2014,
  title = {The {{New Statistics}}: {{Why}} and {{How}}},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, Geoff},
  year = {2014},
  month = jan,
  journal = {Psychological Science},
  volume = {25},
  number = {1},
  pages = {7--29},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797613504966},
  urldate = {2015-11-30},
  langid = {english}
}

@article{cumming_replication_2008,
  title = {Replication and {\emph{p}} {{Intervals}}: {\emph{p}} {{Values Predict}} the {{Future Only Vaguely}}, but {{Confidence Intervals Do Much Better}}},
  shorttitle = {Replication and {\emph{p}} {{Intervals}}},
  author = {Cumming, Geoff},
  year = {2008},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {3},
  number = {4},
  pages = {286--300},
  issn = {17456916, 17456924},
  doi = {10.1111/j.1745-6924.2008.00079.x},
  urldate = {2015-11-30},
  langid = {english}
}

@book{cumming_understanding_2013,
  title = {Understanding the New Statistics: {{Effect}} Sizes, Confidence Intervals, and Meta-Analysis},
  shorttitle = {Understanding the New Statistics},
  author = {Cumming, Geoff},
  year = {2013},
  publisher = {Routledge},
  urldate = {2016-09-07}
}

@article{danziger_extraneous_2011,
  title = {Extraneous Factors in Judicial Decisions},
  author = {Danziger, S. and Levav, J. and {Avnaim-Pesso}, L.},
  year = {2011},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {17},
  pages = {6889--6892},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/PNAS.1018033108},
  urldate = {2018-07-24},
  langid = {english},
  annotation = {00711}
}

@article{debruine_understanding_2021,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa M. and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  urldate = {2022-02-16},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  keywords = {lme4,mixed-effects models,open materials,power,R,simulation}
}

@book{degroot_methodology_1969,
  title = {Methodology},
  author = {{de Groot}, Adrianus Dingeman},
  year = {1969},
  volume = {6},
  publisher = {Mouton \& Co.},
  address = {The Hague}
}

@article{deheide_why_2017,
  title = {Why Optional Stopping Is a Problem for {{Bayesians}}},
  author = {{de Heide}, Rianne and Gr{\"u}nwald, Peter D.},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.08278 [math, stat]},
  eprint = {1708.08278},
  primaryclass = {math, stat},
  urldate = {2018-02-25},
  abstract = {Recently, optional stopping has been a subject of debate in the Bayesian psychology community. Rouder (2014) argues that optional stopping is no problem for Bayesians, and even recommends the use of optional stopping in practice, as do Wagenmakers et al. (2012). This article addresses the question whether optional stopping is problematic for Bayesian methods, and specifies under which circumstances and in which sense it is and is not. By slightly varying and extending Rouder's (2014) experiment, we illustrate that, as soon as the parameters of interest are equipped with default or pragmatic priors - which means, in most practical applications of Bayes Factor hypothesis testing - resilience to optional stopping can break down. We distinguish between four types of default priors, each having their own specific issues with optional stopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type II and III priors).},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{delacre_why_2017,
  title = {Why {{Psychologists Should}} by {{Default Use Welch}}'s {\emph{t}}-Test {{Instead}} of {{Student}}'s {\emph{t}}-Test},
  author = {Delacre, Marie and Lakens, Dani{\"e}l and Leys, Christophe},
  year = {2017},
  journal = {International Review of Social Psychology},
  volume = {30},
  number = {1},
  issn = {2119-4130},
  doi = {10.5334/irsp.82},
  urldate = {2017-04-08},
  abstract = {When comparing two independent groups, psychology researchers commonly use Student's t-tests. Assumptions of normality and homogeneity of variance underlie this test. More often than not, when these conditions are not met, Student's t-test can be severely biased and lead to invalid statistical inferences. Moreover, we argue that the assumption of equal variances will seldom hold in psychological research, and choosing between Student's t-test and Welch's t-test based on the outcomes of a test of the equality of variances often fails to provide an appropriate answer. We show that the Welch's t-test provides a better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student's t-test when the assumptions are met. We argue that Welch's t-test should be used as a default strategy.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  keywords = {homogeneity of variance,Homoscedasticity,Levene's test,statistical power,Student's t-test,type 1 error,type 2 error,Welch's t-test}
}

@misc{delacre_why_2021,
  title = {Why {{Hedges}}' G*s Based on the Non-Pooled Standard Deviation Should Be Reported with {{Welch}}'s t-Test},
  author = {Delacre, Marie and Lakens, Dani{\"e}l and Ley, Christophe and Liu, Limin and Leys, Christophe},
  year = {2021},
  month = may,
  institution = {PsyArXiv},
  doi = {10.31234/osf.io/tu6mp},
  urldate = {2021-10-15},
  abstract = {Researchers are generally required to report and interpret effect sizes and associated confidence intervals. When comparing two independent groups, the most commonly used estimator of effect size is Cohen's ds where sample mean difference is divided by the pooled standard deviation. However, computing the pooled error term is not valid when both groups do not share common population variances. Furthermore, the assumption of equal population variances is unlikely in many psychological fields. Consequently, researchers shift to the use of Welch's t-test over Student's t-test in the context of hypothesis testing. Meanwhile, the question which effect size to report when equal variances are not assumed remains open. Based on Monte Carlo simulations, we compare Hedges' gs (i.e. Cohen's ds with correction for bias) to Glass's gs, Shieh's gs and Hedges' g\_s{\textasciicircum}*. Comparisons are made under normality as well as under realistic deviations from the assumptions of normality and equal variances. Although it is not directly related with Welch's t-test (unlike Shieh's gs), we recommend the use of Hedges' g\_s{\textasciicircum}* because it shows better properties than all other estimators. Practical recommendations, R package and Shiny App in order to compute effect size estimators and confidence intervals are provided.},
  keywords = {Effect size,Monte Carlo Simulations,Parametric Assumptions,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods}
}

@article{detsky_using_1990,
  title = {Using Cost-Effectiveness Analysis to Improve the Efficiency of Allocating Funds to Clinical Trials},
  author = {Detsky, Allan S.},
  year = {1990},
  journal = {Statistics in Medicine},
  volume = {9},
  number = {1-2},
  pages = {173--184},
  issn = {1097-0258},
  doi = {10.1002/sim.4780090124},
  urldate = {2020-12-31},
  abstract = {This study applied a cost-effectiveness model to seven randomized trials. The model demonstrates the effect of design choices made in the planning stages of a clinical trial on the costs and benefits derived from conducting the trial. The study focused on one parameter used to calculate sample size: the minimum clinically important difference in event rates between control and experimental therapies. The study shows that the model can be operationalized to these trials. A computerized software package and manual has been developed to simplify the calculations. While there was some variation in the incremental cost-effectiveness ratios across the seven trials in this study, all ratios may be below the funding threshold. This analytical technique can be used to demonstrate explicitly the resource consequences of the design of randomized trials and perhaps to set funding priorities.},
  copyright = {Copyright {\copyright} 1990 John Wiley \& Sons, Ltd.},
  langid = {english}
}

@book{dienes_understanding_2008,
  title = {Understanding Psychology as a Science: {{An}} Introduction to Scientific and Statistical Inference},
  shorttitle = {Understanding Psychology as a Science},
  author = {Dienes, Zoltan},
  year = {2008},
  publisher = {Palgrave Macmillan},
  urldate = {2017-06-20}
}

@article{dienes_using_2014,
  title = {Using {{Bayes}} to Get the Most out of Non-Significant Results},
  author = {Dienes, Zoltan},
  year = {2014},
  journal = {Frontiers in Psychology},
  volume = {5},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00781},
  urldate = {2021-01-02},
  abstract = {No scientific conclusion follows automatically from a statistically non-significant result, yet people routinely use non-significant results to guide conclusions about the status of theories (or the effectiveness of practices). To know whether a non-significant result counts against a theory, or if it just indicates data insensitivity, researchers must use one of: power, intervals (such as confidence or credibility intervals), or else an indicator of the relative evidence for one theory over another, such as a Bayes factor. I argue Bayes factors allow theory to be linked to data in a way that overcomes the weaknesses of the other approaches. Specifically, Bayes factors use the data themselves to determine their sensitivity in distinguishing theories (unlike power), and they make use of those aspects of a theory's predictions that are often easiest to specify (unlike power and intervals, which require specifying the minimal interesting value in order to address theory). Bayes factors provide a coherent approach to determining whether non-significant results support a null hypothesis over a theory, or whether the data are just insensitive. They allow accepting and rejecting the null hypothesis to be put on an equal footing. Concrete examples are provided to indicate the range of application of a simple online Bayes calculator, which reveal both the strengths and weaknesses of Bayes factors.},
  langid = {english},
  keywords = {Bayes factor,confidence interval,credibility interval,Significance testing,statistical inference}
}

@article{ditroilo_exploratory_2025,
  title = {Exploratory Research in Sport and Exercise Science: {{Perceptions}}, Challenges, and Recommendations},
  shorttitle = {Exploratory Research in Sport and Exercise Science},
  author = {Ditroilo, Massimiliano and , Cristian, Mesquida and , Grant, Abt and {and Lakens}, Dani{\"e}l},
  year = {2025},
  month = jun,
  journal = {Journal of Sports Sciences},
  volume = {43},
  number = {12},
  pages = {1108--1120},
  publisher = {Routledge},
  issn = {0264-0414},
  doi = {10.1080/02640414.2025.2486871},
  urldate = {2025-05-17},
  abstract = {Quantitative exploratory research implies a flexible examination of a dataset with the purpose of finding patterns, associations, and interactions between variables to help formulate a hypothesis, which should be severely tested in a future confirmatory study. In many fields, including sport and exercise science, exploratory research is not openly reported, a practice that leads to serious problems. At the same time, exploration is a crucial step in scientific knowledge generation, and a substantial proportion of studies will be exploratory in nature, or include both confirmatory and exploratory analyses. Using a flowchart, we review how data are typically collected and used, and we distinguish exploratory from confirmatory studies by arguing that data-driven analyses, where the Type I and Type II error cannot be controlled, is what characterises exploratory research. We ask which factors increase the quality and value of exploratory analyses, and highlight large sample sizes, uncommon sample compositions, rigorous data collection, widely used measures, observing a logical and coherent pattern across multiple variables, and the potential for generating new research questions as the main factors. Finally, we provide guidelines for carrying out and transparently writing up an exploratory study.},
  pmid = {40197233},
  keywords = {confirmatory,data analysis,error control,Hypothesis testing,questionable research practices,theory-driven analysis}
}

@article{dmitrienko_traditional_2013,
  title = {Traditional Multiplicity Adjustment Methods in Clinical Trials},
  author = {Dmitrienko, Alex and D'Agostino Sr, Ralph},
  year = {2013},
  journal = {Statistics in Medicine},
  volume = {32},
  number = {29},
  pages = {5172--5218},
  issn = {1097-0258},
  doi = {10.1002/sim.5990},
  urldate = {2022-02-08},
  abstract = {This tutorial discusses important statistical problems arising in clinical trials with multiple clinical objectives based on different clinical variables, evaluation of several doses or regiments of a new treatment, analysis of multiple patient subgroups, etc. Simultaneous assessment of several objectives in a single trial gives rise to multiplicity. If unaddressed, problems of multiplicity can undermine integrity of statistical inferences. The tutorial reviews key concepts in multiple hypothesis testing and introduces main classes of methods for addressing multiplicity in a clinical trial setting. General guidelines for the development of relevant and efficient multiple testing procedures are presented on the basis of application-specific clinical and statistical information. Case studies with common multiplicity problems are used to motivate and illustrate the statistical methods presented in the tutorial, and software implementation of the multiplicity adjustment methods is discussed. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {clinical trials,multiple testing procedures,multiplicity adjustments,multiplicity problems,type I error rate}
}

@article{dodge_method_1929,
  title = {A {{Method}} of {{Sampling Inspection}}},
  author = {Dodge, H. F. and Romig, H. G.},
  year = {1929},
  month = oct,
  journal = {Bell System Technical Journal},
  volume = {8},
  number = {4},
  pages = {613--631},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1929.tb01240.x},
  urldate = {2017-06-29},
  abstract = {This paper outlines some of the general considerations which must be taken into account in setting up any practical sampling inspection plan. An economical method of inspection is developed in detail for the case where the purpose of the inspection is to determine the acceptability of discrete lots of a product submitted by a producer. By employing probability theory, the method places a definite barrier in the path of material of defective quality and gives this protection to the consumer with a minimum of inspection expense.},
  langid = {english}
}

@article{dongen_multiple_2019,
  title = {Multiple {{Perspectives}} on {{Inference}} for {{Two Simple Statistical Scenarios}}},
  author = {van Dongen, Noah N. N. and van Doorn, Johnny B. and Gronau, Quentin F. and van Ravenzwaaij, Don and Hoekstra, Rink and Haucke, Matthias N. and Lakens, Dani{\"e}l and Hennig, Christian and Morey, Richard D. and Homer, Saskia and Gelman, Andrew and Sprenger, Jan and Wagenmakers, Eric-Jan},
  year = {2019},
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {328--339},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1565553},
  urldate = {2019-04-08},
  abstract = {When data analysts operate within different statistical frameworks (e.g., frequentist versus Bayesian, emphasis on estimation versus emphasis on testing), how does this impact the qualitative conclusions that are drawn for real data? To study this question empirically we selected from the literature two simple scenarios---involving a comparison of two proportions and a Pearson correlation---and asked four teams of statisticians to provide a concise analysis and a qualitative interpretation of the outcome. The results showed considerable overall agreement; nevertheless, this agreement did not appear to diminish the intensity of the subsequent debate over which statistical framework is more appropriate to address the questions at hand.},
  keywords = {Frequentist or Bayesian,Multilab analysis,Statistical paradigms,Testing or estimation}
}

@book{douglas_science_2009,
  title = {Science, Policy, and the Value-Free Ideal},
  author = {Douglas, Heather E.},
  year = {2009},
  publisher = {University of Pittsburgh Press},
  address = {Pittsburgh, Pa},
  isbn = {978-0-8229-6026-3},
  langid = {english},
  lccn = {Q175.5 .D68 2009},
  keywords = {Moral and ethical aspects,Professional ethics,Science,Scientists,Social aspects},
  annotation = {OCLC: ocn297144848}
}

@book{dubin_theory_1969,
  title = {Theory Building},
  author = {Dubin, Robert},
  year = {1969},
  publisher = {Free Press},
  address = {New York},
  urldate = {2022-02-09},
  langid = {english},
  annotation = {OCLC: 609596212}
}

@book{duhem_aim_1954,
  title = {The Aim and Structure of Physical Theory},
  author = {Duhem, Pierre},
  year = {1954},
  publisher = {Princeton University Press},
  address = {Princeton}
}

@article{dunn_multiple_1961,
  title = {Multiple {{Comparisons}} among {{Means}}},
  author = {Dunn, Olive Jean},
  year = {1961},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {56},
  number = {293},
  pages = {52--64},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1961.10482090},
  urldate = {2022-02-07},
  abstract = {Methods for constructing simultaneous confidence intervals for all possible linear contrasts among several means of normally distributed variables have been given by Scheff{\'e} and Tukey. In this paper the possibility is considered of picking in advance a number (say m) of linear contrasts among k means, and then estimating these m linear contrasts by confidence intervals based on a Student t statistic, in such a way that the overall confidence level for the m intervals is greater than or equal to a preassigned value. It is found that for some values of k, and for m not too large, intervals obtained in this way are shorter than those using the F distribution or the Studentized range. When this is so, the experimenter may be willing to select the linear combinations in advance which he wishes to estimate in order to have m shorter intervals instead of an infinite number of longer intervals.}
}

@article{dupont_sequential_1983,
  title = {Sequential Stopping Rules and Sequentially Adjusted {{P}} Values: {{Does}} One Require the Other?},
  shorttitle = {Sequential Stopping Rules and Sequentially Adjusted {{P}} Values},
  author = {Dupont, William D.},
  year = {1983},
  month = jan,
  journal = {Controlled Clinical Trials},
  volume = {4},
  number = {1},
  pages = {3--10},
  issn = {0197-2456},
  doi = {10.1016/S0197-2456(83)80003-8},
  urldate = {2020-09-25},
  abstract = {During the course of a clinical trial it is normally necessary to conduct periodic reviews of the data in order to determine whether the trial should be terminated. Since these reviews affect the probability of the final outcome, many statisticians recommend that the P values quoted for a clinical trial be sequentially adjusted to account for the possibility of premature termination. In this article it is argued that the sequentially adjusted P value is an inappropriate measure of the strength of evidence justified by a clinical trial. This is because the size of sequentially adjusted P values will vary according to actions that might have been taken if the trial had gone differently than it in fact did. Although such contingencies will effect the frequency of occurrence of certain events in hypothetical sequence of trial replications, it is hard to see why decisions that would have been made in response to outcomes that did not occur should have any bearing on the strength of evidence that can be attributed to the results that were actually observed. The credibility merited by a clinical trial depends not only on the implausibility of the observed results under the null hypothesis, but also on factors such as the medical plausibility of hypothesis well supported by the data, and the extent to which observed results have been predicted in advance. It is argued that publishing these factors along with fixed sample P values is the best way to indicate the degree of certainty that should be attributed to the conclusions of a clinical trial.},
  langid = {english},
  keywords = {Clinical trials,foundations of statistical inference,likelihood principle,Sequential stopping rules}
}

@article{duyx_scientific_2017,
  title = {Scientific Citations Favor Positive Results: A Systematic Review and Meta-Analysis},
  shorttitle = {Scientific Citations Favor Positive Results},
  author = {Duyx, Bram and Urlings, Miriam J. E. and Swaen, Gerard M. H. and Bouter, Lex M. and Zeegers, Maurice P.},
  year = {2017},
  month = aug,
  journal = {Journal of Clinical Epidemiology},
  volume = {88},
  pages = {92--101},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2017.06.002},
  urldate = {2023-08-29},
  abstract = {Objectives Citation bias concerns the selective citation of scientific articles based on their results. We brought together all available evidence on citation bias across scientific disciplines and quantified its impact. Study Design and Setting An extensive search strategy was applied to the Web of Science Core Collection and Medline, yielding 52 studies in total. We classified these studies on scientific discipline, selection method, and other variables. We also performed random-effects meta-analyses to pool the effect of positive vs. negative results on subsequent citations. Finally, we checked for other determinants of citation as reported in the citation bias literature. Results Evidence for the occurrence of citation bias was most prominent in the biomedical sciences and least in the natural sciences. Articles with statistically significant results were cited 1.6 (95\% confidence interval [CI] 1.3-1.8) times more often than articles with nonsignificant results. Articles in which the authors explicitly conclude to have found support for their hypothesis were cited 2.7 (CI 2.0-3.7) times as often. Article results and journal impact factor were associated with citation more often than any other reported determinant. Conclusion Similar to what we already know on publication bias, also citation bias can lead to an overrepresentation of positive results and unfounded beliefs.},
  keywords = {Citation bias,Meta-analysis,Outcome bias,Questionable research practices,Research integrity,Systematic review}
}

@article{ebersole_many_2016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  shorttitle = {Many {{Labs}} 3},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and {Joy-Gaba}, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and {van Allen}, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  urldate = {2021-03-03},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences---conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  langid = {english},
  keywords = {Cognitive psychology,Individual differences,Participant pool,Replication,Sampling effects,Situational effects,Social psychology}
}

@article{ebersole_many_2020,
  title = {Many {{Labs}} 5: {{Testing Pre-Data-Collection Peer Review}} as an {{Intervention}} to {{Increase Replicability}}},
  shorttitle = {Many {{Labs}} 5},
  author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and {Bart-Plange}, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazarevi{\'c}, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Ban{\'i}k, Gabriel and Baskin, Ernest and Belopavlovi{\'c}, Radomir and Bernstein, Michael H. and Bia{\l}ek, Micha{\l} and Bloxsom, Nicholas G. and Bodro{\v z}a, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Br{\"u}hlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and {\v C}oli{\'c}, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falc{\~a}o, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, M{\'a}ire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Ha{\l}asa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and H{\"u}ffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and {Jim{\'e}nez-Leal}, William and Johannesson, Magnus and {Joy-Gaba}, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Ko{\l}odziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and {de Lima}, Tiago Jess{\'e} Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, {\L}ukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafa{\l} and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orli{\'c}, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedovi{\'c}, Ivana and P{\k e}kala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrovi{\'c}, Boban and Pfeiffer, Thomas and Pie{\'n}kosz, Damian and Preti, Emanuele and Puri{\'c}, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemys{\l}aw and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and {Schulz-Hardt}, Stefan and Sch{\"u}tz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, R{\'u}ben and Sioma, Barbara and Skorb, Lauren and {de Souza}, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilovi{\'c}, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Sz{\"o}ke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and {\v Z}e{\v z}elj, Iris and Zrubka, Mark and Nosek, Brian A.},
  year = {2020},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {3},
  pages = {309--331},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920958687},
  urldate = {2023-08-10},
  abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {$<$} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3--9; median total sample = 1,279.5, range = 276--3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols ({$\Delta$}r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00--.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19--.50).},
  langid = {english}
}

@article{eckermann_value_2010,
  title = {The {{Value}} of {{Value}} of {{Information}}},
  author = {Eckermann, Simon and Karnon, Jon and Willan, Andrew R.},
  year = {2010},
  month = sep,
  journal = {PharmacoEconomics},
  volume = {28},
  number = {9},
  pages = {699--709},
  issn = {1179-2027},
  doi = {10.2165/11537370-000000000-00000},
  urldate = {2020-06-27},
  abstract = {Value of information (VOI) methods have been proposed as a systematic approach to inform optimal research design and prioritization. Four related questions arise that VOI methods could address. (i) Is further research for a health technology assessment (HTA) potentially worthwhile? (ii) Is the cost of a given research design less than its expected value? (iii) What is the optimal research design for an HTA? (iv) How can research funding be best prioritized across alternative HTAs?},
  langid = {english}
}

@article{edwards_academic_2017,
  title = {Academic {{Research}} in the 21st {{Century}}: {{Maintaining Scientific Integrity}} in a {{Climate}} of {{Perverse Incentives}} and {{Hypercompetition}}},
  shorttitle = {Academic {{Research}} in the 21st {{Century}}},
  author = {Edwards, Marc A. and Roy, Siddhartha},
  year = {2017},
  month = jan,
  journal = {Environmental Engineering Science},
  volume = {34},
  number = {1},
  pages = {51--61},
  publisher = {Mary Ann Liebert, Inc., publishers},
  doi = {10.1089/ees.2016.0223},
  urldate = {2022-09-13},
  abstract = {Over the last 50 years, we argue that incentives for academic scientists have become increasingly perverse in terms of competition for research funding, development of quantitative metrics to measure performance, and a changing business model for higher education itself. Furthermore, decreased discretionary funding at the federal and state level is creating a hypercompetitive environment between government agencies (e.g., EPA, NIH, CDC), for scientists in these agencies, and for academics seeking funding from all sources---the combination of perverse incentives and decreased funding increases pressures that can lead to unethical behavior. If a critical mass of scientists become untrustworthy, a tipping point is possible in which the scientific enterprise itself becomes inherently corrupt and public trust is lost, risking a new dark age with devastating consequences to humanity. Academia and federal agencies should better support science as a public good, and incentivize altruistic and ethical outcomes, while de-emphasizing output.},
  keywords = {academic research,funding,misconduct,perverse incentives,scientific integrity}
}

@article{elson_press_2014,
  title = {Press {{CRTT}} to Measure Aggressive Behavior: The Unstandardized Use of the Competitive Reaction Time Task in Aggression Research},
  shorttitle = {Press {{CRTT}} to Measure Aggressive Behavior},
  author = {Elson, Malte and Mohseni, M. Rohangis and Breuer, Johannes and Scharkow, Michael and Quandt, Thorsten},
  year = {2014},
  month = jun,
  journal = {Psychological Assessment},
  volume = {26},
  number = {2},
  pages = {419--432},
  issn = {1939-134X},
  doi = {10.1037/a0035569},
  abstract = {The competitive reaction time task (CRTT) is the measure of aggressive behavior most commonly used in laboratory research. However, the test has been criticized for issues in standardization because there are many different test procedures and at least 13 variants to calculate a score for aggressive behavior. We compared the different published analyses of the CRTT using data from 3 different studies to scrutinize whether it would yield the same results. The comparisons revealed large differences in significance levels and effect sizes between analysis procedures, suggesting that the unstandardized use and analysis of the CRTT have substantial impacts on the results obtained, as well as their interpretations. Based on the outcome of our comparisons, we provide suggestions on how to address some of the issues associated with the CRTT, as well as a guideline for researchers studying aggressive behavior in the laboratory.},
  langid = {english},
  pmid = {24447279},
  keywords = {Adult,Aggression,Competitive Behavior,Female,Humans,Male,Reaction Time,Social Behavior,Task Performance and Analysis,Young Adult}
}

@article{ensinck_inceptioncohort_2025,
  title = {An {{Inception-Cohort Study Quantifying How Many Registered Studies Are Publicly Shared}}},
  author = {Ensinck, Eline N. F. and Lakens, Dani{\"e}l},
  year = {2025},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {8},
  number = {1},
  pages = {25152459241296031},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459241296031},
  urldate = {2025-02-13},
  abstract = {We quantified how many studies registered on the OSF up to November 2017 are performed but not shared after at least 4 years. Examining a sample of 169 registered research studies, we found that 104 (61.5\%) were publicly shared. We estimate that 5,316 out of 9,172 (58\%) registered studies on the OSF are publicly shared. Researchers use registries to make unpublished studies public, and the OSF policy to open registrations after a 4-year embargo substantially increases the number of studies that become known to the scientific community. In responses to emails asking researchers why studies remained unpublished, logistical issues (e.g., lack of time, researchers changing jobs) were the most common cause, followed by null results and rejections during peer review. Our exploratory study shows that a substantial amount of studies that researchers perform remain unpublished.},
  langid = {english}
}

@article{epstein_stability_1980,
  title = {The Stability of Behavior: {{II}}. {{Implications}} for Psychological Research},
  shorttitle = {The Stability of Behavior},
  author = {Epstein, Seymour},
  year = {1980},
  journal = {American Psychologist},
  volume = {35},
  number = {9},
  pages = {790--806},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.35.9.790},
  abstract = {Argues that one solution to the problem of establishing replicable generalizations in psychological research lies in aggregating behavior over situations and/or occasions, thereby canceling out incidental, uncontrollable factors relative to experimental factors. Such a procedure increases reliability without introducing excessive constraints into the experimental situation. The value of such aggregation was demonstrated in 4 studies in Part I of this paper (Epstein, 1979) that examined a variety of data, including the subjective and objective measurement of behavior in the field and in the laboratory. Four kinds of aggregation are discussed, each of which reduces a specific source of error. The degree of aggregation that is required varies inversely with the degree to which the events studied are ego-involving, implicitly or explicitly include an adequate sample of behavioral observation, or have been demonstrated to be robust over incidental sources of variation. (58 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Behavior,Experimentation,Methodology}
}

@article{erdfelder_gpower_1996,
  title = {{{GPOWER}}: {{A}} General Power Analysis Program},
  shorttitle = {{{GPOWER}}},
  author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  year = {1996},
  month = mar,
  journal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {28},
  number = {1},
  pages = {1--11},
  issn = {1532-5970},
  doi = {10.3758/BF03203630},
  urldate = {2020-08-04},
  abstract = {GPOWER is a completely interactive, menu-driven program for IBM-compatible and Apple Macintosh personal computers. It performs high-precision statistical power analyses for the most common statistical tests in behavioral research, that is,t tests,F tests, and{$\chi$}2 tests. GPOWER computes (1) power values for given sample sizes, effect sizes and{$\alpha$} levels (post hoc power analyses); (2) sample sizes for given effect sizes,{$\alpha$} levels, and power values (a priori power analyses); and (3){$\alpha$} and{$\beta$} values for given sample sizes, effect sizes, and{$\beta$}/{$\alpha$} ratios (compromise power analyses). The program may be used to display graphically the relation between any two of the relevant variables, and it offers the opportunity to compute the effect size measures from basic parameters defining the alternative hypothesis. This article delineates reasons for the development of GPOWER and describes the program's capabilities and handling.},
  langid = {english}
}

@article{eysenck_exercise_1978,
  title = {An Exercise in Mega-Silliness},
  author = {Eysenck, H. J.},
  year = {1978},
  journal = {American Psychologist},
  volume = {33},
  number = {5},
  pages = {517--517},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.33.5.517.a},
  abstract = {Disputes the methodology and conclusions of M. L. Smith and G. V. Glass (see record 1978-10341-001) in their meta-analysis of psychotherapy outcome studies. Smith and Glass's use of a compilation of studies, mostly of poor design, is an abandonment of scholarship. There remains no acceptable evidence for the efficacy of psychotherapy. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Meta Analysis,Psychotherapeutic Outcomes,Psychotherapeutic Techniques,Psychotherapy}
}

@article{fanelli_how_2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta-Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  year = {2009},
  month = may,
  journal = {PLOS ONE},
  volume = {4},
  number = {5},
  pages = {e5738},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005738},
  urldate = {2022-03-15},
  abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, ``cooking'' of data, etc{\dots} Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86--4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once --a serious form of misconduct by any standard-- and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91--19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words ``falsification'' or ``fabrication'', and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
  langid = {english},
  keywords = {Deception,Medical journals,Medicine and health sciences,Metaanalysis,Scientific misconduct,Scientists,Social research,Surveys}
}

@article{fanelli_positive_2010,
  title = {``{{Positive}}'' {{Results Increase Down}} the {{Hierarchy}} of the {{Sciences}}},
  author = {Fanelli, Daniele},
  year = {2010},
  month = apr,
  journal = {PLoS ONE},
  volume = {5},
  number = {4},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010068},
  urldate = {2015-12-11},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the ``hardness'' of scientific research---i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors---is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a ``positive'' (full or partial) or ``negative'' support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in ``softer'' sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  pmcid = {PMC2850928},
  pmid = {20383332}
}

@article{faul_gpower_2007,
  title = {{{GPower}} 3: {{A}} Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences},
  shorttitle = {G*{{Power}} 3},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  year = {2007},
  month = may,
  journal = {Behavior Research Methods},
  volume = {39},
  number = {2},
  pages = {175--191},
  issn = {1554-351X, 1554-3528},
  doi = {10.3758/BF03193146},
  urldate = {2018-05-24},
  langid = {english}
}

@article{ferguson_comment_2014,
  title = {Comment: {{Why}} Meta-Analyses Rarely Resolve Ideological Debates},
  shorttitle = {Comment},
  author = {Ferguson, Christopher J.},
  year = {2014},
  journal = {Emotion Review},
  volume = {6},
  number = {3},
  pages = {251--252},
  urldate = {2017-03-31}
}

@article{ferguson_providing_2021,
  title = {Providing a Lower-Bound Estimate for Psychology's ``Crud Factor'': {{The}} Case of Aggression},
  shorttitle = {Providing a Lower-Bound Estimate for Psychology's ``Crud Factor''},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2021},
  month = dec,
  journal = {Professional Psychology: Research and Practice},
  volume = {52},
  number = {6},
  pages = {620--626},
  publisher = {American Psychological Association},
  address = {Washington, US},
  issn = {0735-7028},
  doi = {http://dx.doi.org/10.1037/pro0000386},
  urldate = {2022-03-29},
  abstract = {When conducting research on large data sets, statistically significant findings having only trivial interpretive meaning may appear. Little consensus exists whether such small effects can be meaningfully interpreted. The current analysis examines the possibility that trivial effects may emerge in large datasets, but that some such effects may lack interpretive value. When such results match an investigator's hypothesis, they may be over-interpreted. The current study examines this issue as related to aggression research in two large samples. Specifically, in the first study, the National Longitudinal Study of Adolescent to Adult Health (AddHeath) dataset was used. Fifteen variables with little theoretical relevance to aggression were selected, then correlated with self-reported delinquency. For the second study, the Understanding Society database was used. As with Study 1, 14 nonsensical variables were correlated with conduct problems. Many variables achieved ``statistical significance'' and some effect sizes approached or exceeded r = .10, despite little theoretical relevance between the variables. It is recommended that effect sizes below r = .10 should not be interpreted as hypothesis supportive. (PsycInfo Database Record (c) 2021 APA, all rights reserved) (Source: journal abstract)},
  copyright = {{\copyright} 2021, American Psychological Association},
  langid = {english},
  keywords = {Aggressive Behavior (major),Behavior Problems (major),Data Sets,Effect Size (Statistical) (major),Self-Report (major),Statistical Significance (major)},
  annotation = {(US)}
}

@article{ferguson_publishing_2014,
  title = {Publishing: {{The}} Peer-Review Scam},
  shorttitle = {Publishing},
  author = {Ferguson, Cat and Marcus, Adam and Oransky, Ivan},
  year = {2014},
  month = nov,
  journal = {Nature},
  volume = {515},
  number = {7528},
  pages = {480--482},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/515480a},
  urldate = {2023-08-26},
  abstract = {When a handful of authors were caught reviewing their own papers, it exposed weaknesses in modern publishing systems. Editors are trying to plug the holes.},
  copyright = {2014 Springer Nature Limited},
  langid = {english},
  keywords = {Authorship,Peer review,Publishing}
}

@article{ferguson_vast_2012,
  title = {A Vast Graveyard of Undead Theories Publication Bias and Psychological Science's Aversion to the Null},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2012},
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {555--561},
  urldate = {2015-12-11}
}

@article{ferron_power_1996,
  title = {The {{Power}} of {{Randomization Tests}} for {{Single-Case Phase Designs}}},
  author = {Ferron, John and Onghena, Patrick},
  year = {1996},
  month = apr,
  journal = {The Journal of Experimental Education},
  volume = {64},
  number = {3},
  pages = {231--239},
  publisher = {Routledge},
  issn = {0022-0973},
  doi = {10.1080/00220973.1996.9943805},
  urldate = {2021-02-07},
  abstract = {Monte Carlo methods were used to estimate the power of randomization tests used with single-case designs involving the random assignment of treatments to phases. The design studied involved 2 treatments and 6 phases. The power was studied for 6 standardized effect sizes (0, .2, .5, .8, 1.1, and 1.4), 4 levels of autocorrelation (1st order autocorrelation coefficients of -.3, 0, .3, and .6), and 5 different phase lengths (4, 5, 6, 7, and 8 observations). Power was estimated for each condition by simulating 10,000 experiments. The results showed an adequate level of power ({$>$} .80) when effect sizes were large (1.1 and 1.4), phase lengths exceeded 5, and autocorrelation was not negative.}
}

@book{feyerabend_method_1993,
  title = {Against Method},
  author = {Feyerabend, Paul},
  year = {1993},
  edition = {3rd ed},
  publisher = {Verso},
  address = {London ; New York},
  isbn = {978-0-86091-481-5 978-0-86091-646-8},
  lccn = {Q175 .F42 1993},
  keywords = {Methodology,Philosophy,Rationalism,Science}
}

@article{feynman_cargo_1974,
  title = {Cargo Cult Science},
  author = {Feynman, Richard P.},
  year = {1974},
  journal = {Engineering and Science},
  volume = {37},
  number = {7},
  pages = {10--13}
}

@article{fiedler_questionable_2015,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2015},
  month = oct,
  journal = {Social Psychological and Personality Science},
  pages = {1948550615612150},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615612150},
  urldate = {2015-12-11},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english},
  keywords = {ethics/morality,language,research methods,research practices,survey methodology}
}

@article{fiedler_questionable_2016,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2016},
  month = jan,
  journal = {Social Psychological and Personality Science},
  volume = {7},
  number = {1},
  pages = {45--52},
  publisher = {SAGE Publications Inc},
  issn = {1948-5506},
  doi = {10.1177/1948550615612150},
  urldate = {2021-01-12},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english},
  keywords = {ethics/morality,language,research methods,research practices,survey methodology}
}

@article{fiedler_tools_2004,
  title = {Tools, Toys, Truisms, and Theories: {{Some}} Thoughts on the Creative Cycle of Theory Formation},
  shorttitle = {Tools, Toys, Truisms, and Theories},
  author = {Fiedler, Klaus},
  year = {2004},
  journal = {Personality and Social Psychology Review},
  volume = {8},
  number = {2},
  pages = {123--131},
  doi = {10.1207/s15327957pspr0802_5},
  urldate = {2016-09-07}
}

@article{field_minimizing_2004,
  title = {Minimizing the Cost of Environmental Management Decisions by Optimizing Statistical Thresholds},
  author = {Field, Scott A. and Tyre, Andrew J. and Jonz{\'e}n, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
  year = {2004},
  month = aug,
  journal = {Ecology Letters},
  volume = {7},
  number = {8},
  pages = {669--675},
  issn = {1461-0248},
  doi = {10.1111/j.1461-0248.2004.00625.x},
  urldate = {2017-08-24},
  abstract = {Environmental management decisions are prone to expensive mistakes if they are triggered by hypothesis tests using the conventional Type I error rate ({$\alpha$}) of 0.05. We derive optimal {$\alpha$}-levels for decision-making by minimizing a cost function that specifies the overall cost of monitoring and management. When managing an economically valuable koala population, it shows that a decision based on {$\alpha~$}=~0.05 carries an expected cost over \$5~million greater than the optimal decision. For a species of such value, there is never any benefit in guarding against the spurious detection of declines and therefore management should proceed directly to recovery action. This result holds in most circumstances where the species' value substantially exceeds its recovery costs. For species of lower economic value, we show that the conventional {$\alpha$}-level of 0.05 rarely approximates the optimal decision-making threshold. This analysis supports calls for reversing the statistical `burden of proof' in environmental decision-making when the cost of Type II errors is relatively high.},
  langid = {english},
  keywords = {Koala,management,optimal monitoring,statistical power,Statistical Significance,type I error,type II error}
}

@book{fisher_design_1935,
  title = {The Design of Experiments},
  author = {Fisher, Ronald Aylmer},
  year = {1935},
  publisher = {Oliver And Boyd; Edinburgh; London}
}

@article{fisher_has_1936,
  title = {Has {{Mendel}}'s Work Been Rediscovered?},
  author = {Fisher, Ronald A.},
  year = {1936},
  journal = {Annals of science},
  volume = {1},
  number = {2},
  pages = {115--137},
  publisher = {Taylor \& Francis}
}

@book{fisher_statistical_1956,
  title = {Statistical Methods and Scientific Inference},
  author = {Fisher, Ronald A.},
  year = {1956},
  volume = {viii},
  publisher = {Hafner Publishing Co.},
  address = {Oxford, England},
  abstract = {An explicit statement of the logical nature of statistical reasoning that has been implicitly required in the development and use of statistical techniques in the making of uncertain inferences and in the design of experiments. Included is a consideration of the concept of mathematical probability; a comparison of fiducial and confidence intervals; a comparison of the logic of tests of significance with the acceptance decision approach; and a discussion of the principles of prediction and estimation.},
  copyright = {(c) 2016 APA, all rights reserved}
}

@article{fishman_american_1982,
  title = {American Psychology in the Eighties: {{Who}} Will Buy?},
  shorttitle = {American Psychology in the Eighties},
  author = {Fishman, Daniel B. and Neigher, William D.},
  year = {1982},
  journal = {American Psychologist},
  volume = {37},
  number = {5},
  pages = {533--546},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.37.5.533},
  abstract = {Presents a planning model for American psychology (AP) that leads to 9 concrete action proposals. AP is viewed as a nonprofit business whose products involve the creation, application, and dissemination of scientifically and technologically derived information about the human mind and behavior. Investigation of the present status of these knowledge bases finds psychology wanting and strongly suggests 3 disquieting conclusions: (a) Potential achievements of basic research are being undermined by a misdirected incentive system of publication criteria that disregards many of this field's basic precepts; (b) psychological problem solution can be served by a technological research paradigm qualitatively different from that of basic research; and thus (c) if AP is to compete successfully for future public support, the technological paradigm must be much better articulated, expanded, and integrated into the organization of the discipline. (75 ref) (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Models,Psychology}
}

@article{fraley_npact_2014,
  title = {The {{N-Pact Factor}}: {{Evaluating}} the {{Quality}} of {{Empirical Journals}} with {{Respect}} to {{Sample Size}} and {{Statistical Power}}},
  shorttitle = {The {{N-Pact Factor}}},
  author = {Fraley, R. Chris and Vazire, Simine},
  year = {2014},
  month = oct,
  journal = {PLOS ONE},
  volume = {9},
  number = {10},
  pages = {e109019},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0109019},
  urldate = {2016-04-01},
  abstract = {The authors evaluate the quality of research reported in major journals in social-personality psychology by ranking those journals with respect to their  N -pact Factors (NF)---the statistical power of the empirical studies they publish to detect typical effect sizes. Power is a particularly important attribute for evaluating research quality because, relative to studies that have low power, studies that have high power are more likely to (a) to provide accurate estimates of effects, (b) to produce literatures with low false positive rates, and (c) to lead to replicable findings. The authors show that the average sample size in social-personality research is 104 and that the power to detect the typical effect size in the field is approximately 50\%. Moreover, they show that there is considerable variation among journals in sample sizes and power of the studies they publish, with some journals consistently publishing higher power studies than others. The authors hope that these rankings will be of use to authors who are choosing where to submit their best work, provide hiring and promotion committees with a superior way of quantifying journal quality, and encourage competition among journals to improve their NF rankings.},
  keywords = {Personality,Personality differences,Psychology,Research Design,Research integrity,Research quality assessment,Social psychology,Social research}
}

@article{francis_equivalent_2016,
  title = {Equivalent Statistics and Data Interpretation},
  author = {Francis, Gregory},
  year = {2016},
  month = oct,
  journal = {Behavior Research Methods},
  pages = {1--15},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0812-3},
  urldate = {2017-04-12},
  abstract = {Recent reform efforts in psychological science have led to a plethora of choices for scientists to analyze their data. A scientist making an inference about their data must now decide whether to report a p value, summarize the data with a standardized effect size and its confidence interval, report a Bayes Factor, or use other model comparison methods. To make good choices among these options, it is necessary for researchers to understand the characteristics of the various statistics used by the different analysis frameworks. Toward that end, this paper makes two contributions. First, it shows that for the case of a two-sample t test with known sample sizes, many different summary statistics are mathematically equivalent in the sense that they are based on the very same information in the data set. When the sample sizes are known, the p value provides as much information about a data set as the confidence interval of Cohen's d or a JZS Bayes factor. Second, this equivalence means that different analysis methods differ only in their interpretation of the empirical data. At first glance, it might seem that mathematical equivalence of the statistics suggests that it does not matter much which statistic is reported, but the opposite is true because the appropriateness of a reported statistic is relative to the inference it promotes. Accordingly, scientists should choose an analysis method appropriate for their scientific investigation. A direct comparison of the different inferential frameworks provides some guidance for scientists to make good choices and improve scientific practice.},
  langid = {english}
}

@article{francis_frequency_2014,
  title = {The Frequency of Excess Success for Articles in {{Psychological Science}}},
  author = {Francis, Gregory},
  year = {2014},
  month = mar,
  journal = {Psychonomic Bulletin \& Review},
  volume = {21},
  number = {5},
  pages = {1180--1187},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-014-0601-x},
  urldate = {2015-12-11},
  abstract = {Recent controversies have questioned the quality of scientific practice in the field of psychology, but these concerns are often based on anecdotes and seemingly isolated cases. To gain a broader perspective, this article applies an objective test for excess success to a large set of articles published in the journal Psychological Science between 2009 and 2012. When empirical studies succeed at a rate much higher than is appropriate for the estimated effects and sample sizes, readers should suspect that unsuccessful findings have been suppressed, the experiments or analyses were improper, or the theory does not properly account for the data. In total, problems appeared for 82 \% (36 out of 44) of the articles in Psychological Science that had four or more experiments and could be analyzed.},
  langid = {english},
  keywords = {Cognitive psychology,Probabilistic reasoning,Statistical inference,Statistics}
}

@article{franco_publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  year = {2014},
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  doi = {10.1126/SCIENCE.1255484},
  annotation = {00377}
}

@article{frankenhuis_strategic_2022,
  title = {Strategic Ambiguity in the Social Sciences},
  author = {Frankenhuis, Willem E. and Panchanathan, Karthik and Smaldino, Paul E.},
  year = {2022},
  month = nov,
  journal = {Social Psychological Bulletin},
  issn = {2569-653X},
  urldate = {2023-08-14},
  langid = {english}
}

@article{fraser_questionable_2018,
  title = {Questionable Research Practices in Ecology and Evolution},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  year = {2018},
  month = jul,
  journal = {PLOS ONE},
  volume = {13},
  number = {7},
  pages = {e0200303},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200303},
  urldate = {2019-11-01},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  langid = {english},
  keywords = {Behavioral ecology,Community ecology,Ecology and environmental sciences,Evolutionary biology,Evolutionary ecology,Psychology,Publication ethics,Statistical data}
}

@article{freedman_compliance_1966,
  title = {Compliance without Pressure: {{The}} Foot-in-the-Door Technique},
  shorttitle = {Compliance without Pressure},
  author = {Freedman, Jonathan L. and Fraser, Scott C.},
  year = {1966},
  month = aug,
  journal = {Journal of Personality and Social Psychology},
  volume = {4},
  number = {2},
  pages = {195--202},
  publisher = {American Psychological Association},
  address = {Washington, US},
  issn = {0022-3514},
  doi = {10.1037/h0023552},
  urldate = {2023-03-25},
  abstract = {2 experiments were conducted to test the proposition that once someone has agreed to a small request he is more likely to comply with a larger request. Exp. I demonstrated this effect when the same person made both requests; Exp. II extended this to the situation in which different people made the 2 requests. Several experimental groups were run in an effort to explain these results, and possible explanations are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  copyright = {{\copyright} 1966, American Psychological Association},
  langid = {english},
  keywords = {Compliance (major),Perception (major)},
  annotation = {(US)}
}

@article{freiman_importance_1978,
  title = {The Importance of Beta, the Type {{II}} Error and Sample Size in the Design and Interpretation of the Randomized Control Trial. {{Survey}} of 71 "Negative" Trials},
  author = {Freiman, J. A. and Chalmers, T. C. and Smith, H. and Kuebler, R. R.},
  year = {1978},
  month = sep,
  journal = {The New England Journal of Medicine},
  volume = {299},
  number = {13},
  pages = {690--694},
  issn = {0028-4793},
  doi = {10.1056/NEJM197809282991304},
  abstract = {Seventy-one "negative" randomized control trials were re-examined to determine if the investigators had studied large enough samples to give a high probability (greater than 0.90) of detecting a 25 per cent and 50 per cent therapeutic improvement in the response. Sixty-seven of the trials had a greater than 10 per cent risk of missing a true 25 per cent therapeutic improvement, and with the same risk, 50 of the trials could have missed a 50 per cent improvement. Estimates of 90 per cent confidence intervals for the true improvement in each trial showed that in 57 of these "negative" trials, a potential 25 per cent improvement was possible, and 34 of the trials showed a potential 50 per cent improvement. Many of the therapies labeled as "no different from control" in trials using inadequate samples have not received a fair test. Concern for the probability of missing an important therapeutic improvement because of small sample sizes deserves more attention in the planning of clinical trials.},
  langid = {english},
  pmid = {355881},
  keywords = {Clinical Trials as Topic,Humans,Probability,Research,Research Design,Therapeutics}
}

@article{frick_appropriate_1996,
  title = {The Appropriate Use of Null Hypothesis Testing.},
  author = {Frick, Robert W.},
  year = {1996},
  journal = {Psychological Methods},
  volume = {1},
  number = {4},
  pages = {379--390},
  doi = {10.1037/1082-989X.1.4.379}
}

@article{fricker_assessing_2019,
  title = {Assessing the {{Statistical Analyses Used}} in {{Basic}} and {{Applied Social Psychology After Their}} P-{{Value Ban}}},
  author = {Fricker, Ronald D. and Burke, Katherine and Han, Xiaoyan and Woodall, William H.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {374--384},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537892},
  urldate = {2019-04-08},
  abstract = {In this article, we assess the 31 articles published in Basic and Applied Social Psychology (BASP) in 2016, which is one full year after the BASP editors banned the use of inferential statistics. We discuss how the authors collected their data, how they reported and summarized their data, and how they used their data to reach conclusions. We found multiple instances of authors overstating conclusions beyond what the data would support if statistical significance had been considered. Readers would be largely unable to recognize this because the necessary information to do so was not readily available.},
  keywords = {Effect size,Inference ban,NHST,Psychology,Statistical significance}
}

@article{fried_method_1993,
  title = {A Method for Achieving Consensus on Rheumatoid Arthritis Outcome Measures: The {{OMERACT}} Conference Process.},
  shorttitle = {A Method for Achieving Consensus on Rheumatoid Arthritis Outcome Measures},
  author = {Fried, B. J. and Boers, M. and Baker, P. R.},
  year = {1993},
  journal = {The Journal of rheumatology},
  volume = {20},
  number = {3},
  pages = {548--551}
}

@article{friede_sample_2006,
  title = {Sample Size Recalculation in Internal Pilot Study Designs: A Review},
  shorttitle = {Sample Size Recalculation in Internal Pilot Study Designs},
  author = {Friede, Tim and Kieser, Meinhard},
  year = {2006},
  journal = {Biometrical Journal: Journal of Mathematical Methods in Biosciences},
  volume = {48},
  number = {4},
  pages = {537--555},
  publisher = {Wiley Online Library},
  doi = {10.1002/bimj.200510238}
}

@article{friedlander_type_1964,
  title = {Type {{I}} and {{Type II Bias}}},
  author = {Friedlander, Frank},
  year = {1964},
  month = mar,
  journal = {American Psychologist},
  volume = {19},
  number = {3},
  pages = {198--199},
  publisher = {American Psychological Association},
  address = {Washington, US},
  issn = {0003-066X},
  doi = {10.1037/h0038977},
  urldate = {2023-07-31},
  abstract = {Errors in research do occur. Their prevalence should be viewed with alarm rather than passive acceptance as an essential concomitant of humans conducting research. The author looks at the discussion of errors in an article by Leroy Wolins (Amer. Psychologist, 1962, 17, 657-658) and an article by Emanuel Berger (Amer. Psychologist, 1962, 17, 657). The first article suggests that we may be quite unaware of the large proportion of the iceberg (erroneous research results) which is never perceived or reanalyzed. The second article pleads for experimental evaluation of research results. It would seem as though these two articles are both positively and negatively related. On the one hand, subjective evaluation and interpretation of research results may possibly expose inadvertent errors in computations (or in design, sampling, etc.). Thus, results which do not readily blend with an already established nomological network might call for statistical recomputations. On the other hand, subjective evaluation can become quite blinding when one identifies with or strongly favors certain results. In such cases, errors in computation would never be suspected, much less discovered. The author discusses experimental errors further, taking a look at Type I and Type II bias. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  copyright = {{\copyright} 1964, American Psychological Association},
  langid = {english},
  keywords = {Errors (major),Experimentation (major),Statistical Analysis,Test Bias (major)},
  annotation = {(US)}
}

@article{fugard_supporting_2015,
  title = {Supporting Thinking on Sample Sizes for Thematic Analyses: A Quantitative Tool},
  shorttitle = {Supporting Thinking on Sample Sizes for Thematic Analyses},
  author = {Fugard, Andrew J. B. and Potts, Henry W. W.},
  year = {2015},
  month = nov,
  journal = {International Journal of Social Research Methodology},
  volume = {18},
  number = {6},
  pages = {669--684},
  publisher = {Routledge},
  issn = {1364-5579},
  doi = {10.1080/13645579.2015.1005453},
  urldate = {2020-08-26},
  abstract = {Thematic analysis is frequently used to analyse qualitative data in psychology, healthcare, social research and beyond. An important stage in planning a study is determining how large a sample size may be required, however current guidelines for thematic analysis are varied, ranging from around 2 to over 400 and it is unclear how to choose a value from the space in between. Some guidance can also not be applied prospectively. This paper introduces a tool to help users think about what would be a useful sample size for their particular context when investigating patterns across participants. The calculation depends on (a) the expected population theme prevalence of the least prevalent theme, derived either from prior knowledge or based on the prevalence of the rarest themes considered worth uncovering, e.g. 1 in 10, 1 in 100; (b) the number of desired instances of the theme; and (c) the power of the study. An adequately powered study will have a high likelihood of finding sufficient themes of the desired prevalence. This calculation can then be used alongside other considerations. We illustrate how to use the method to calculate sample size before starting a study and achieved power given a sample size, providing tables of answers and code for use in the free software, R. Sample sizes are comparable to those found in the literature, for example to have 80\% power to detect two instances of a theme with 10\% prevalence, 29 participants are required. Increasing power, increasing the number of instances or decreasing prevalence increases the sample size needed. We do not propose this as a ritualistic requirement for study design, but rather as a pragmatic supporting tool to help plan studies using thematic analysis.},
  keywords = {Corrigendum,power analysis,sample size determination,thematic analysis}
}

@article{funder_evaluating_2019,
  title = {Evaluating Effect Size in Psychological Research: {{Sense}} and Nonsense},
  shorttitle = {Evaluating Effect Size in Psychological Research},
  author = {Funder, David C. and Ozer, Daniel J.},
  year = {2019},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {156--168},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  doi = {10.1177/2515245919847202}
}

@article{gannon_blending_2019,
  title = {Blending {{Bayesian}} and {{Classical Tools}} to {{Define Optimal Sample-Size-Dependent Significance Levels}}},
  author = {Gannon, Mark Andrew and {de Bragan{\c c}a Pereira}, Carlos Alberto and Polpo, Adriano},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {213--222},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1518268},
  urldate = {2023-01-05},
  abstract = {This article argues that researchers do not need to completely abandon the p-value, the best-known significance index, but should instead stop using significance levels that do not depend on sample sizes. A testing procedure is developed using a mixture of frequentist and Bayesian tools, with a significance level that is a function of sample size, obtained from a generalized form of the Neyman--Pearson Lemma that minimizes a linear combination of {$\alpha$}, the probability of rejecting a true null hypothesis, and {$\beta$}, the probability of failing to reject a false null, instead of fixing {$\alpha$} and minimizing {$\beta$}. The resulting hypothesis tests do not violate the Likelihood Principle and do not require any constraints on the dimensionalities of the sample space and parameter space. The procedure includes an ordering of the entire sample space and uses predictive probability (density) functions, allowing for testing of both simple and compound hypotheses. Accessible examples are presented to highlight specific characteristics of the new tests.},
  keywords = {Hardy-Weinberg equilibrium,Neyman-Pearson lemma,Predictive distribution,Significance test}
}

@article{garcia-garzon_exploring_2022,
  title = {Exploring {{COVID-19}} Research Credibility among {{Spanish}} Scientists},
  author = {{Garcia-Garzon}, Eduardo and {Angulo-Brunet}, Ariadna and Lecuona, Oscar and Barrada, Juan Ram{\'o}n and Corradi, Guido},
  year = {2022},
  month = feb,
  journal = {Current Psychology},
  issn = {1936-4733},
  doi = {10.1007/s12144-022-02797-6},
  urldate = {2022-09-20},
  abstract = {Amidst a worldwide vaccination campaign, trust in science plays a significant role when addressing the COVID-19 pandemic. Given current concerns regarding research standards, we were interested in how Spanish scholars perceived COVID-19 research and the extent to which questionable research practices and potentially problematic academic incentives are commonplace. We asked researchers to evaluate~the expected quality of their COVID-19 projects and other peers' research and compared these assessments with those from scholars not involved in COVID-19 research. We investigated self-admitting and estimated rates of questionable research practices and attitudes towards current research status. Responses from 131 researchers suggested that COVID-19 evaluations followed partisan lines, with scholars~being more pessimistic~about others' colleagues' research~than their own. Additionally,researchers~not involved in COVID-19 projects were more negative~than~their participating peers. These differences were particularly notable for areas such as the expected theoretical foundations or overall quality of the research, among others. Most Spanish scholars expected questionable research practices and inadequate incentives to be widespread. In these two aspects, researchers tended to agree regardless of their involvement in COVID-19 research. We provide specific recommendations for improving future meta-science studies, such as redefining QRPs as inadequate research practices (IRP). This change could help avoid key controversies regarding QRPs' definition while highlighting their detrimental impact. Lastly, we join previous calls to improve transparency and academic career incentives as a cornerstone for generating trust in science.},
  langid = {english},
  keywords = {COVID-19,Inadequate incentives,Questionable research practices,Reproducibility crisis,Trust in science}
}

@article{gelman_power_2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = {2014},
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  urldate = {2017-01-02}
}

@article{gergen_social_1973,
  title = {Social Psychology as History},
  author = {Gergen, Kenneth J.},
  year = {1973},
  journal = {Journal of Personality and Social Psychology},
  volume = {26},
  pages = {309--320},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315},
  doi = {10.1037/h0034436},
  abstract = {Presents an analysis of theory and research in social psychology which reveals that while methods of research are scientific in character, theories of social behavior are primarily reflections of contemporary history. The dissemination of psychological knowledge modifies the patterns of behavior upon which the knowledge is based. This modification occurs because of the prescriptive bias of psychological theorizing, the liberating effects of knowledge, and the resistance based on common values of freedom and individuality. In addition, theoretical premises are based primarily on acquired dispositions. As the culture changes, such dispositions are altered, and the premises are often invalidated. Several modifications in the scope and methods of social psychology are derived from this analysis. (53 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Culture Change,History,Social Processes,Social Psychology}
}

@article{gerring_mere_2012,
  title = {Mere {{Description}}},
  author = {Gerring, John},
  year = {2012},
  month = oct,
  journal = {British Journal of Political Science},
  volume = {42},
  number = {4},
  pages = {721--746},
  publisher = {Cambridge University Press},
  issn = {0007-1234, 1469-2112},
  doi = {10.1017/S0007123412000130},
  urldate = {2020-02-27},
  abstract = {This article attempts to reformulate and resuscitate the seemingly prosaic methodological task of description, which is often derided in favour of causal analysis. First, the problem of definition is addressed: what does this category of analysis (`description') refer to? Secondly, a taxonomy of descriptive arguments is offered, emphasizing the diversity contained within this genre of empirical analysis. Thirdly, the demise of description within political science is charted over the past century, with comparisons to other disciplines. Fourthly, it is argued that the task of description ought to be approached independently, not merely as a handmaiden of causal theories. Fifthly, the methodological difficulties of descriptive inference are addressed. Finally, fruitful research areas within the rubric of description are reviewed.},
  langid = {english}
}

@article{gillon_medical_1994,
  title = {Medical Ethics: Four Principles plus Attention to Scope},
  shorttitle = {Medical Ethics},
  author = {Gillon, R.},
  year = {1994},
  month = jul,
  journal = {BMJ},
  volume = {309},
  number = {6948},
  pages = {184},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.309.6948.184},
  urldate = {2022-09-27},
  abstract = {The ``four principles plus scope'' approach provides a simple, accessible, and culturally neutral approach to thinking about ethical issues in health care. The approach, developed in the United States, is based on four common, basic prima facie moral commitments - respect for autonomy, beneficence, non-maleficence, and justice - plus concern for their scope of application. It offers a common, basic moral analytical framework and a common, basic moral language. Although they do not provide ordered rules, these principles can help doctors and other health care workers to make decisions when reflecting on moral issues that arise at work. Nine years ago the BMJ allowed me to introduce to its readers1 an approach to medical ethics developed by the Americans Beauchamp and Childress,2 which is based on four prima facie moral principles and attention to these principles' scope of application. Since then I have often been asked for a summary of this approach by doctors and other health care workers who find it helpful for organising their thoughts about medical ethics. This paper, based on the preface of a large multiauthor textbook on medical ethics,3 offers a brief account of this ``four principles plus scope'' approach. The four principles plus scope approach claims that whatever our personal philosophy, politics, religion, moral theory, or life stance, we will find no difficulty in committing ourselves to four prima facie moral principles plus a reflective concern about their scope of application. Moreover, these four principles, plus attention to their scope of application, encompass most of the moral issues that arise in health care. The four prima facie principles are respect for autonomy, beneficence, non-maleficence, and justice. ``Prima facie,'' a term introduced by the English philosopher W D Ross, means that the principle is binding unless it conflicts with another moral principle {\dots}},
  chapter = {Education and debate},
  copyright = {{\copyright} 1994 BMJ Publishing Group Ltd.},
  langid = {english},
  pmid = {8044100}
}

@article{glockner_irrational_2016,
  title = {The Irrational Hungry Judge Effect Revisited: {{Simulations}} Reveal That the Magnitude of the Effect Is Overestimated},
  shorttitle = {The Irrational Hungry Judge Effect Revisited},
  author = {Gl{\"o}ckner, Andreas},
  year = {2016},
  journal = {Judgment and Decision Making},
  volume = {11},
  number = {6},
  pages = {601--610}
}

@article{glover_likelihood_2004,
  title = {Likelihood Ratios: {{A}} Simple and Flexible Statistic for Empirical Psychologists},
  shorttitle = {Likelihood Ratios},
  author = {Glover, Scott and Dixon, Peter},
  year = {2004},
  journal = {Psychonomic Bulletin \& Review},
  volume = {11},
  number = {5},
  pages = {791--806},
  publisher = {Springer}
}

@article{goldacre_compliance_2018,
  title = {Compliance with Requirement to Report Results on the {{EU Clinical Trials Register}}: Cohort Study and Web Resource},
  shorttitle = {Compliance with Requirement to Report Results on the {{EU Clinical Trials Register}}},
  author = {Goldacre, Ben and DeVito, Nicholas J. and Heneghan, Carl and Irving, Francis and Bacon, Seb and Fleminger, Jessica and Curtis, Helen},
  year = {2018},
  month = sep,
  journal = {BMJ},
  volume = {362},
  pages = {k3218},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1756-1833},
  doi = {10.1136/bmj.k3218},
  urldate = {2022-09-13},
  abstract = {Objectives To ascertain compliance rates with the European Commission's requirement that all trials on the EU Clinical Trials Register (EUCTR) post results to the registry within 12 months of completion (final compliance date 21 December 2016); to identify features associated with non-compliance; to rank sponsors by compliance; and to build a tool for live ongoing audit of compliance. Design Retrospective cohort study. Setting EUCTR. Participants 7274 of 11 531 trials listed as completed on EUCTR and where results could be established as due. Main outcome measure Publication of results on EUCTR. Results Of 7274 trials where results were due, 49.5\% (95\% confidence interval 48.4\% to 50.7\%) reported results. Trials with a commercial sponsor were substantially more likely to post results than those with a non-commercial sponsor (68.1\% v 11.0\%, adjusted odds ratio 23.2, 95\% confidence interval 19.2 to 28.2); as were trials by a sponsor who conducted a large number of trials (77.9\% v 18.4\%, adjusted odds ratio 18.4, 15.3 to 22.1). More recent trials were more likely to report results (per year odds ratio 1.05, 95\% confidence interval 1.03 to 1.07). Extensive evidence was found of errors, omissions, and contradictory entries in EUCTR data that prevented ascertainment of compliance for some trials. Conclusions Compliance with the European Commission requirement for all trials to post results on to the EUCTR within 12 months of completion has been poor, with half of all trials non-compliant. EU registry data commonly contain inconsistencies that might prevent even regulators assessing compliance. Accessible and timely information on the compliance status of each individual trial and sponsor may help to improve reporting rates. Video Player is loading.Play VideoPlayMuteCurrent Time 0:00/Duration 13:49Loaded: 1.20\%00:00Stream Type LIVESeek to live, currently behind liveLIVERemaining Time -13:49 1xPlayback Rate2x1.75x1.5x1.25x1x, selected0.75x0.5xChaptersChaptersDescriptionsdescriptions off, selectedCaptionscaptions settings, opens captions settings dialogcaptions off, selectedAudio Tracken (Main), selectedFullscreenThis is a modal window.Beginning of dialog window. Escape will cancel and close the window.TextColorWhiteBlackRedGreenBlueYellowMagentaCyanTransparencyOpaqueSemi-TransparentBackgroundColorBlackWhiteRedGreenBlueYellowMagentaCyanTransparencyOpaqueSemi-TransparentTransparentWindowColorBlackWhiteRedGreenBlueYellowMagentaCyanTransparencyTransparentSemi-TransparentOpaqueFont Size50\%75\%100\%125\%150\%175\%200\%300\%400\%Text Edge StyleNoneRaisedDepressedUniformDropshadowFont FamilyProportional Sans-SerifMonospace Sans-SerifProportional SerifMonospace SerifCasualScriptSmall CapsReset restore all settings to the default valuesDoneClose Modal DialogEnd of dialog window.Close Modal DialogThis is a modal window. This modal can be closed by pressing the Escape key or activating the close button.},
  chapter = {Research},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  pmid = {30209058}
}

@article{good_bayes_1992,
  title = {The {{Bayes}}/{{Non-Bayes}} Compromise: {{A}} Brief Review},
  shorttitle = {The {{Bayes}}/{{Non-Bayes Compromise}}},
  author = {Good, I. J.},
  year = {1992},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {87},
  number = {419},
  eprint = {2290192},
  eprinttype = {jstor},
  pages = {597--606},
  issn = {01621459},
  doi = {10.2307/2290192},
  urldate = {2016-01-11}
}

@article{goodyear-smith_analysis_2012,
  title = {Analysis of Decisions Made in Meta-Analyses of Depression Screening and the Risk of Confirmation Bias: {{A}} Case Study},
  shorttitle = {Analysis of Decisions Made in Meta-Analyses of Depression Screening and the Risk of Confirmation Bias},
  author = {{Goodyear-Smith}, Felicity A and {van Driel}, Mieke L and Arroll, Bruce and Del Mar, Chris},
  year = {2012},
  month = jun,
  journal = {BMC Medical Research Methodology},
  volume = {12},
  pages = {76},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-76},
  urldate = {2019-09-07},
  abstract = {Background Depression is common in primary care and clinicians are encouraged to screen their patients. Meta-analyses have evaluated the effectiveness of screening, but two author groups consistently reached completely opposite conclusions. Methods We identified five systematic reviews on depression screening conducted between 2001 and 2009, three by Gilbody and colleagues and two by the United States Preventive Task Force. The two author groups consistently reached completely opposite conclusions. We analyzed two contemporaneous systematic reviews, applying a stepwise approach to unravel their methods. Decision points were identified, and discrepancies between systematic reviews authors' justification of choices made were recorded. Results Two systematic reviews each addressing three research questions included 26 randomized controlled trials with different combinations in each review. For the outcome depression screening resulting in treatment, both reviews undertook meta-analyses of imperfectly overlapping studies. Two in particular, pooled each by only one of the reviews, influenced the recommendations in opposite directions. Justification for inclusion or exclusion of studies was obtuse. Conclusion Systematic reviews may be less objective than assumed. Based on this analysis of two meta-analyses we hypothesise that strongly held prior beliefs (confirmation bias) may have influenced inclusion and exclusion criteria of studies, and their interpretation. Authors should be required to declare a priori any strongly held prior beliefs within their hypotheses, before embarking on systematic reviews.},
  pmcid = {PMC3464667},
  pmid = {22691262}
}

@article{gopalakrishna_prevalence_2022,
  title = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors: {{A}} Survey among Academic Researchers in {{The Netherlands}}},
  shorttitle = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors},
  author = {Gopalakrishna, Gowri and ter Riet, Gerben and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
  year = {2022},
  month = feb,
  journal = {PLOS ONE},
  volume = {17},
  number = {2},
  pages = {e0263023},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0263023},
  urldate = {2022-09-20},
  abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the ``publish or perish'' incentive system promotes research integrity.},
  langid = {english},
  keywords = {Deception,Linear regression analysis,Medical humanities,Medicine and health sciences,Open science,Research integrity,Scientific misconduct,Surveys}
}

@techreport{gosset_application_1904,
  title = {The {{Application}} of the "{{Law}} of {{Error}}" to the {{Work}} of the {{Brewery}}},
  author = {Gosset, W. S.},
  year = {1904},
  month = mar,
  number = {1 vol 8},
  pages = {3--16},
  institution = {Arthur Guinness \& Son, Ltd.}
}

@article{gotz_small_2022,
  title = {Small {{Effects}}: {{The Indispensable Foundation}} for a {{Cumulative Psychological Science}}},
  shorttitle = {Small {{Effects}}},
  author = {G{\"o}tz, Friedrich M. and Gosling, Samuel D. and Rentfrow, Peter J.},
  year = {2022},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {17},
  number = {1},
  pages = {205--215},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620984483},
  urldate = {2022-04-06},
  abstract = {We draw on genetics research to argue that complex psychological phenomena are most likely determined by a multitude of causes and that any individual cause is likely to have only a small effect. Building on this, we highlight the dangers of a publication culture that continues to demand large effects. First, it rewards inflated effects that are unlikely to be real and encourages practices likely to yield such effects. Second, it overlooks the small effects that are most likely to be real, hindering attempts to identify and understand the actual determinants of complex psychological phenomena. We then explain the theoretical and practical relevance of small effects, which can have substantial consequences, especially when considered at scale and over time. Finally, we suggest ways in which scholars can harness these insights to advance research and practices in psychology (i.e., leveraging the power of big data, machine learning, and crowdsourcing science; promoting rigorous preregistration, including prespecifying the smallest effect size of interest; contextualizing effects; changing cultural norms to reward accurate and meaningful effects rather than exaggerated and unreliable effects). Only once small effects are accepted as the norm, rather than the exception, can a reliable and reproducible cumulative psychological science be built.},
  langid = {english},
  keywords = {questionable research practices,research culture,scientific community,small effects}
}

@article{green_how_1991,
  title = {How {{Many Subjects Does It Take To Do A Regression Analysis}}},
  author = {Green, S. B.},
  year = {1991},
  month = jul,
  journal = {Multivariate Behavioral Research},
  volume = {26},
  number = {3},
  pages = {499--510},
  issn = {0027-3171},
  doi = {10.1207/s15327906mbr2603_7},
  abstract = {Numerous rules-of-thumb have been suggested for determining the minimum number of subjects required to conduct multiple regression analyses. These rules-of-thumb are evaluated by comparing their results against those based on power analyses for tests of hypotheses of multiple and partial correlations. The results did not support the use of rules-of-thumb that simply specify some constant (e.g., 100 subjects) as the minimum number of subjects or a minimum ratio of number of subjects (N) to number of predictors (m). Some support was obtained for a rule-of-thumb that N {$\geq$} 50 + 8 m for the multiple correlation and N {$\geq$}104 + m for the partial correlation. However, the rule-of-thumb for the multiple correlation yields values too large for N when m {$\geq$} 7, and both rules-of-thumb assume all studies have a medium-size relationship between criterion and predictors. Accordingly, a slightly more complex rule-of thumb is introduced that estimates minimum sample size as function of effect size as well as the number of predictors. It is argued that researchers should use methods to determine sample size that incorporate effect size.},
  langid = {english},
  pmid = {26776715}
}

@article{green_simr_2016,
  title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  shorttitle = {{{SIMR}}},
  author = {Green, Peter and MacLeod, Catriona J.},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12504},
  urldate = {2019-03-22},
  abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  copyright = {{\copyright} 2015 The Authors. Methods in Ecology and Evolution {\copyright} 2015 British Ecological Society},
  langid = {english},
  keywords = {experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
  annotation = {00148}
}

@article{greenland_statistical_2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  issn = {0393-2990, 1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  urldate = {2016-07-02},
  langid = {english}
}

@article{greenwald_consequences_1975,
  title = {Consequences of Prejudice against the Null Hypothesis.},
  author = {Greenwald, Anthony G.},
  year = {1975},
  journal = {Psychological Bulletin},
  volume = {82},
  number = {1},
  pages = {1--20},
  urldate = {2015-12-23}
}

@article{greenwald_editorial_1976,
  title = {An Editorial},
  editor = {Greenwald, Anthony G.},
  year = {1976},
  journal = {Journal of Personality and Social Psychology},
  volume = {33},
  number = {1},
  pages = {1--7},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315},
  doi = {10.1037/h0078635},
  abstract = {In 1974, the American Psychological Association's Publications and Communications Board appointed a committee of personality and social psychologists, chaired by Albert Hastorf, to make recommendations regarding the future of the Journal of Personality and Social Psychology (JPSP). The editor-elect lists the problems he will seek to deal with at the level of policies and procedures for JPSP. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Personality,Scientific Communication,Social Psychology}
}

@article{grunwald_safe_2019,
  title = {Safe {{Testing}}},
  author = {Gr{\"u}nwald, Peter and {de Heide}, Rianne and Koolen, Wouter},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.07801 [cs, math, stat]},
  eprint = {1906.07801},
  primaryclass = {cs, math, stat},
  urldate = {2019-10-21},
  abstract = {We present a new theory of hypothesis testing. The main concept is the S-value, a notion of evidence which, unlike p-values, allows for effortlessly combining evidence from several tests, even in the common scenario where the decision to perform a new test depends on the previous test outcome: safe tests based on S-values generally preserve Type-I error guarantees under such "optional continuation". S-values exist for completely general testing problems with composite null and alternatives. Their prime interpretation is in terms of gambling or investing, each S-value corresponding to a particular investment. Surprisingly, optimal "GROW" S-values, which lead to fastest capital growth, are fully characterized by the joint information projection (JIPr) between the set of all Bayes marginal distributions on H0 and H1. Thus, optimal S-values also have an interpretation as Bayes factors, with priors given by the JIPr. We illustrate the theory using two classical testing scenarios: the one-sample t-test and the 2x2 contingency table. In the t-test setting, GROW s-values correspond to adopting the right Haar prior on the variance, like in Jeffreys' Bayesian t-test. However, unlike Jeffreys', the "default" safe t-test puts a discrete 2-point prior on the effect size, leading to better behavior in terms of statistical power. Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, S-values and safe tests may provide a methodology acceptable to adherents of all three schools.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{gupta_intention_2011,
  title = {Intention-to-Treat Concept: {{A}} Review},
  shorttitle = {Intention-to-Treat Concept},
  author = {Gupta, Sandeep K.},
  year = {2011},
  journal = {Perspectives in Clinical Research},
  volume = {2},
  number = {3},
  pages = {109--112},
  issn = {2229-3485},
  doi = {10.4103/2229-3485.83221},
  urldate = {2020-12-15},
  abstract = {Randomized controlled trials often suffer from two major complications, i.e., noncompliance and missing outcomes. One potential solution to this problem is a statistical concept called intention-to-treat (ITT) analysis. ITT analysis includes every subject who is randomized according to randomized treatment assignment. It ignores noncompliance, protocol deviations, withdrawal, and anything that happens after randomization. ITT analysis maintains prognostic balance generated from the original random treatment allocation. In ITT analysis, estimate of treatment effect is generally conservative. A better application of the ITT approach is possible if complete outcome data are available for all randomized subjects. Per-protocol population is defined as a subset of the ITT population who completed the study without any major protocol violations.},
  pmcid = {PMC3159210},
  pmid = {21897887}
}

@book{hacking_logic_1965,
  title = {Logic of {{Statistical Inference}}},
  author = {Hacking, Ian},
  year = {1965},
  publisher = {Cambridge University Press},
  address = {New York},
  abstract = {One of Ian Hacking's earliest publications, this book showcases his early ideas on the central concepts and questions surrounding statistical reasoning. He explores the basic principles of statistical reasoning and tests them, both at a philosophical level and in terms of their practical consequences for statisticians. Presented in a fresh twenty-first-century series livery, and including a specially commissioned preface written by Jan-Willem Romeijn, illuminating its enduring importance and relevance to philosophical enquiry, Hacking's influential and original work has been revived for a new generation of readers.},
  isbn = {978-1-316-50814-5},
  langid = {english},
  annotation = {01160}
}

@article{hagger_multilab_2016,
  title = {A {{Multilab Preregistered Replication}} of the {{Ego-Depletion Effect}}},
  author = {Hagger, M. S. and Chatzisarantis, N. L. D. and Alberts, H. and Anggono, C. O. and Batailler, C. and Birt, A. R. and Brand, R. and Brandt, M. J. and Brewer, G. and Bruyneel, S. and Calvillo, D. P. and Campbell, W. K. and Cannon, P. R. and Carlucci, M. and Carruth, N. P. and Cheung, T. and Crowell, A. and De Ridder, D. T. D. and Dewitte, S. and Elson, M. and Evans, J. R. and Fay, B. A. and Fennis, B. M. and Finley, A. and Francis, Z. and Heise, E. and Hoemann, H. and Inzlicht, M. and Koole, S. L. and Koppel, L. and Kroese, F. and Lange, F. and Lau, K. and Lynch, B. P. and Martijn, C. and Merckelbach, H. and Mills, N. V. and Michirev, A. and Miyake, A. and Mosser, A. E. and Muise, M. and Muller, D. and Muzi, M. and Nalis, D. and Nurwanti, R. and Otgaar, H. and Philipp, M. C. and Primoceri, P. and Rentzsch, K. and Ringos, L. and Schlinkert, C. and Schmeichel, B. J. and Schoch, S. F. and Schrama, M. and Sch{\"u}tz, A. and Stamos, A. and Tingh{\"o}g, G. and Ullrich, J. and {vanDellen}, M. and Wimbarti, S. and Wolff, W. and Yusainy, C. and Zerhouni, O. and Zwienenberg, M.},
  year = {2016},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {4},
  pages = {546--573},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691616652873},
  urldate = {2019-08-16},
  langid = {english},
  pmid = {27474142},
  keywords = {Adult,energy model,Humans,Meta-analysis,Meta-Analysis as Topic,Reproducibility of Results,Research Design,resource depletion,Self-control,self-regulation,strength model,Task Performance and Analysis,Young Adult}
}

@article{hallahan_statistical_1996,
  title = {Statistical Power: {{Concepts}}, Procedures, and Applications},
  shorttitle = {Statistical Power},
  author = {Hallahan, Mark and Rosenthal, Robert},
  year = {1996},
  month = may,
  journal = {Behaviour Research and Therapy},
  volume = {34},
  number = {5},
  pages = {489--499},
  issn = {0005-7967},
  doi = {10.1016/0005-7967(95)00082-8},
  urldate = {2020-12-16},
  abstract = {This paper discusses the concept of statistical power and its application to psychological research. Power, the probability that a significance test will produce a significant result when the null hypothesis is false, often is neglected with potentially serious consequences. The concept of power should be considered as part of planning and interpreting research. This article provides explication of the concept of power and suggestions for researchers to increase the power of their investigations.},
  langid = {english}
}

@article{hallinan_information_2023,
  title = {Information {{Provision}} for {{Informed Consent Procedures}} in {{Psychological Research Under}} the {{General Data Protection Regulation}}: {{A Practical Guide}}},
  shorttitle = {Information {{Provision}} for {{Informed Consent Procedures}} in {{Psychological Research Under}} the {{General Data Protection Regulation}}},
  author = {Hallinan, Dara and Boehm, Franziska and K{\"u}lpmann, Annika and Elson, Malte},
  year = {2023},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {6},
  number = {1},
  pages = {25152459231151944},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459231151944},
  urldate = {2023-03-12},
  abstract = {Psychological research often involves the collection and processing of personal data from human research participants. The European General Data Protection Regulation (GDPR) applies, as a rule, to psychological research conducted on personal data in the European Economic Area (EEA)?and even, in certain cases, to psychological research conducted on personal data outside the EEA. The GDPR elaborates requirements concerning the forms of information that should be communicated to research participants whenever personal data are collected directly from them. There is a general norm that informed consent should be obtained before psychological research involving the collection of personal data directly from research participants is conducted. The information required to be provided under the GDPR is normally communicated in the context of an informed consent procedure. There is reason to believe, however, that the information required by the GDPR may not always be provided. Our aim in this tutorial is thus to provide general practical guidance to psychological researchers allowing them to understand the forms of information that must be provided to research participants under the GDPR in informed consent procedures.},
  langid = {english}
}

@article{halpern_continuing_2002,
  title = {The Continuing Unethical Conduct of Underpowered Clinical Trials},
  author = {Halpern, Scott D. and Karlawish, Jason HT and Berlin, Jesse A.},
  year = {2002},
  journal = {Jama},
  volume = {288},
  number = {3},
  pages = {358--362},
  publisher = {American Medical Association},
  doi = {doi:10.1001/jama.288.3.358}
}

@article{halpern_sample_2001,
  title = {The Sample Size for a Clinical Trial: {{A Bayesian}} Decision Theoretic Approach},
  shorttitle = {The Sample Size for a Clinical Trial},
  author = {Halpern, Jerry and Brown Jr, Byron Wm and Hornberger, John},
  year = {2001},
  journal = {Statistics in Medicine},
  volume = {20},
  number = {6},
  pages = {841--858},
  publisher = {Wiley Online Library},
  doi = {10.1002/sim.703}
}

@article{hand_deconstructing_1994,
  title = {Deconstructing {{Statistical Questions}}},
  author = {Hand, David J.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {157},
  number = {3},
  eprint = {10.2307/2983526},
  eprinttype = {jstor},
  pages = {317--356},
  issn = {09641998},
  doi = {10.2307/2983526},
  urldate = {2018-06-16},
  annotation = {00153}
}

@article{hardwicke_data_2018,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal {{Cognition}}},
  shorttitle = {Data Availability, Reusability, and Analytic Reproducibility},
  author = {Hardwicke, Tom E. and Mathur, Maya B. and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Mohr, Alicia Hofelich and Clayton, Elizabeth and Yoon, Erica J. and Tessler, Michael Henry and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
  year = {2018},
  month = aug,
  journal = {Open Science},
  volume = {5},
  number = {8},
  pages = {180448},
  issn = {2054-5703},
  doi = {10.1098/rsos.180448},
  urldate = {2018-10-02},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (`analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  copyright = {{\copyright} 2018 The Authors.. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
  langid = {english},
  annotation = {00006}
}

@article{harms_making_2018,
  title = {Making 'null Effects' Informative: Statistical Techniques and Inferential Frameworks},
  shorttitle = {Making 'null Effects' Informative},
  author = {Harms, Christopher and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Clinical and Translational Research},
  number = {3},
  pages = {382--393},
  issn = {2424810X},
  doi = {10.18053/jctres.03.2017S2.007},
  urldate = {2019-04-10},
  abstract = {Being able to interpret `null effects' is important for cumulative knowledge generation in science. To draw informative conclusions from null-effects, researchers need to move beyond the incorrect interpretation of a non-significant result in a null-hypothesis significance test as evidence of the absence of an effect. We explain how to statistically evaluate null-results using equivalence tests, Bayesian estimation, and Bayes factors. A worked example demonstrates how to apply these statistical tools and interpret the results. Finally, we explain how no statistical approach can actually prove that the null-hypothesis is true, and briefly discuss the philosophical differences between statistical approaches to examine null-effects. The increasing availability of easy-to-use software and online tools to perform equivalence tests, Bayesian estimation, and calculate Bayes factors make it timely and feasible to complement or move beyond traditional null-hypothesis tests, and allow researchers to draw more informative conclusions about null-effects.},
  langid = {english}
}

@book{harrer_doing_2021,
  title = {Doing {{Meta-Analysis}} with {{R}}: {{A Hands-On Guide}}},
  shorttitle = {Doing {{Meta-Analysis}} with {{R}}},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi A. and Ebert, David D.},
  year = {2021},
  month = sep,
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton},
  doi = {10.1201/9781003107347},
  abstract = {Doing Meta-Analysis with R: A Hands-On Guide serves as an accessible introduction on how meta-analyses can be conducted in R. Essential steps for meta-analysis are covered, including calculation and pooling of outcome measures, forest plots, heterogeneity diagnostics, subgroup analyses, meta-regression, methods to control for publication bias, risk of bias assessments and plotting tools. Advanced but highly relevant topics such as network meta-analysis, multi-three-level meta-analyses, Bayesian meta-analysis approaches and SEM meta-analysis are also covered. A companion R package, dmetar, is introduced at the beginning of the guide. It contains data sets and several helper functions for the meta and metafor package used in the guide.  The programming and statistical background covered in the book are kept at a non-expert level, making the book widely accessible.  Features{$\bullet$} Contains two introductory chapters on how to set up an R environment and do basic imports/manipulations of meta-analysis data, including exercises{$\bullet$} Describes statistical concepts clearly and concisely before applying them in R{$\bullet$} Includes step-by-step guidance through the coding required to perform meta-analyses, and a companion R package for the book},
  isbn = {978-1-003-10734-7}
}

@article{hauck_new_1984,
  title = {A New Statistical Procedure for Testing Equivalence in Two-Group Comparative Bioavailability Trials},
  author = {Hauck, Dr Walter W. and Anderson, Sharon},
  year = {1984},
  month = feb,
  journal = {Journal of Pharmacokinetics and Biopharmaceutics},
  volume = {12},
  number = {1},
  pages = {83--91},
  issn = {0090-466X},
  doi = {10.1007/BF01063612},
  urldate = {2016-07-27},
  abstract = {The clinical problem of testing for equivalence in comparative bioavailability trials is restated in terms of the proper statistical hypotheses. A simple t-test procedure for these hypotheses has been devloped that is more powerful than the methods based on usual (shortest) and symmetric confidence intervals. In this note, this new procedure is explained and an example is given, including the method for sample size determination.},
  langid = {english},
  keywords = {bioavailability,Biochemistry general,bioequivalence,Biomedical Engineering,hypothesis tests,Pharmacology/Toxicology,Pharmacy,sample size determination,Veterinary Medicine}
}

@article{hedges_power_2001,
  title = {The Power of Statistical Tests in Meta-Analysis.},
  author = {Hedges, Larry V. and Pigott, Therese D.},
  year = {2001},
  journal = {Psychological methods},
  volume = {6},
  number = {3},
  pages = {203--217},
  publisher = {American Psychological Association},
  doi = {10.1037/1082-989X.6.3.203}
}

@book{hempel_philosophy_1966,
  title = {Philosophy of Natural Science},
  author = {Hempel, Carl G.},
  year = {1966},
  series = {Prentice-{{Hall}} Foundations of Philosophy Series},
  edition = {Nachdr.},
  publisher = {Prentice-Hall},
  address = {Upper Saddle River, NJ},
  isbn = {978-0-13-663823-0},
  langid = {english},
  annotation = {OCLC: 837122937}
}

@article{hilgard_maximal_2021,
  title = {Maximal Positive Controls: {{A}} Method for Estimating the Largest Plausible Effect Size},
  shorttitle = {Maximal Positive Controls},
  author = {Hilgard, Joseph},
  year = {2021},
  month = mar,
  journal = {Journal of Experimental Social Psychology},
  volume = {93},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2020.104082},
  urldate = {2020-12-12},
  abstract = {Effect sizes in social psychology are generally not large and are limited by error variance in manipulation and measurement. Effect sizes exceeding these limits are implausible and should be viewed with skepticism. Maximal positive controls, experimental conditions that should show an obvious and predictable effect, can provide estimates of the upper limits of plausible effect sizes on a measure. In this work, maximal positive controls are conducted for three measures of aggressive cognition, and the effect sizes obtained are compared to studies found through systematic review. Questions are raised regarding the plausibility of certain reports with effect sizes comparable to, or in excess of, the effect sizes found in maximal positive controls. Maximal positive controls may provide a means to identify implausible study results at lower cost than direct replication.},
  langid = {english},
  keywords = {Aggression,Aggressive thought,Positive controls,Scientific self-correction,Violent video games}
}

@article{hill_empirical_2008,
  title = {Empirical {{Benchmarks}} for {{Interpreting Effect Sizes}} in {{Research}}},
  author = {Hill, Carolyn J. and Bloom, Howard S. and Black, Alison Rebeck and Lipsey, Mark W.},
  year = {2008},
  journal = {Child Development Perspectives},
  volume = {2},
  number = {3},
  pages = {172--177},
  issn = {1750-8606},
  doi = {10.1111/j.1750-8606.2008.00061.x},
  urldate = {2020-03-08},
  abstract = {ABSTRACT--- There is no universal guideline or rule of thumb for judging the practical importance or substantive significance of a standardized effect size estimate for an intervention. Instead, one must develop empirical benchmarks of comparison that reflect the nature of the intervention being evaluated, its target population, and the outcome measure or measures being used. This approach is applied to the assessment of effect size measures for educational interventions designed to improve student academic achievement. Three types of empirical benchmarks are illustrated: (a) normative expectations for growth over time in student achievement, (b) policy-relevant gaps in student achievement by demographic group or school performance, and (c) effect size results from past research for similar interventions and target populations. The findings can be used to help assess educational interventions, and the process of doing so can provide guidelines for how to develop and use such benchmarks in other fields.},
  copyright = {{\copyright} 2008, Copyright the Author(s); Journal Compilation {\copyright} 2008, Society for Research in Child Development with Exclusive License to Print by MDRC},
  langid = {english},
  keywords = {educational evaluation,effect size,student performance}
}

@article{hodges_testing_1954,
  title = {Testing the {{Approximate Validity}} of {{Statistical Hypotheses}}},
  author = {Hodges, J. L. and Lehmann, E. L.},
  year = {1954},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {16},
  number = {2},
  eprint = {2984052},
  eprinttype = {jstor},
  pages = {261--268},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1954.tb00169.x},
  urldate = {2019-04-08},
  abstract = {[The distinction between statistical significance and material significance in hypotheses testing is discussed. Modifications of the customary tests, in order to test for the absence of material significance, are derived for several parametric problems, for the chi-square test of goodness of fit, and for Student's hypothesis. The latter permits one to test the hypothesis that the means of two normal populations of equal variance, do not differ by more than a stated amount.]}
}

@article{hoenig_abuse_2001,
  title = {The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis},
  shorttitle = {The Abuse of Power},
  author = {Hoenig, John M. and Heisey, Dennis M.},
  year = {2001},
  journal = {The American Statistician},
  volume = {55},
  number = {1},
  pages = {19--24},
  doi = {10.1198/000313001300339897},
  urldate = {2017-09-30}
}

@article{huedo-medina_assessing_2006,
  title = {Assessing Heterogeneity in Meta-Analysis: {{Q}} Statistic or {{I}}\${\textasciicircum}2\$ Index?},
  shorttitle = {Assessing Heterogeneity in Meta-Analysis},
  author = {{Huedo-Medina}, Tania B. and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and Botella, Juan},
  year = {2006},
  journal = {Psychological methods},
  volume = {11},
  number = {2},
  pages = {193},
  urldate = {2016-05-01},
  annotation = {00000}
}

@article{hung_behavior_1997,
  title = {The {{Behavior}} of the {{P-Value When}} the {{Alternative Hypothesis}} Is {{True}}},
  author = {Hung, H. M. James and O'Neill, Robert T. and Bauer, Peter and Kohne, Karl},
  year = {1997},
  journal = {Biometrics},
  volume = {53},
  number = {1},
  eprint = {2533093},
  eprinttype = {jstor},
  pages = {11--22},
  issn = {0006-341X},
  doi = {10.2307/2533093},
  urldate = {2016-04-23},
  abstract = {The P-value is a random variable derived from the distribution of the test statistic used to analyze a data set and to test a null hypothesis. Under the null hypothesis, the P-value based on a continuous test statistic has a uniform distribution over the interval [0, 1], regardless of the sample size of the experiment. In contrast, the distribution of the P-value under the alternative hypothesis is a function of both sample size and the true value or range of true values of the tested parameter. The characteristics, such as mean and percentiles, of the P-value distribution can give valuable insight into how the P-value behaves for a variety of parameter values and sample sizes. Potential applications of the P-value distribution under the alternative hypothesis to the design, analysis, and interpretation of results of clinical trials are considered.},
  annotation = {00148}
}

@article{hunt_we_1975,
  title = {Do We Really Need More Replications?},
  author = {Hunt, Karl},
  year = {1975},
  journal = {Psychological Reports},
  volume = {36},
  number = {2},
  pages = {587--593},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}

@article{hyde_gender_2008,
  title = {Gender {{Similarities Characterize Math Performance}}},
  author = {Hyde, Janet S. and Lindberg, Sara M. and Linn, Marcia C. and Ellis, Amy B. and Williams, Caroline C.},
  year = {2008},
  month = jul,
  journal = {Science},
  volume = {321},
  number = {5888},
  pages = {494--495},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1160364},
  urldate = {2017-11-23},
  abstract = {{$<$}p{$>$} Standardized tests in the U.S. indicate that girls now score just as well as boys in math. {$<$}/p{$>$}},
  copyright = {{\copyright} 2008 American Association for the Advancement of Science},
  langid = {english},
  pmid = {18653867},
  annotation = {00845}
}

@article{ioannidis_exploratory_2007,
  title = {An Exploratory Test for an Excess of Significant Findings},
  author = {Ioannidis, John P. A. and Trikalinos, T. A},
  year = {2007},
  month = jun,
  journal = {Clinical Trials},
  volume = {4},
  number = {3},
  pages = {245--253},
  issn = {1740-7745},
  doi = {10.1177/1740774507079441},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00322}
}

@article{ioannidis_why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLoS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {05886}
}

@article{isager_deciding_2023,
  title = {Deciding What to Replicate: {{A}} Decision Model for Replication Study Selection under Resource and Knowledge Constraints},
  shorttitle = {Deciding What to Replicate},
  author = {Isager, Peder Mortvedt and {van Aert}, Robbie C. M. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Brandt, Mark J. and DeSoto, K. Andrew and {Giner-Sorolla}, Roger and Krueger, Joachim I. and Perugini, Marco and Ropovik, Ivan and {van 't Veer}, Anna E. and Vranka, Marek and Lakens, Dani{\"e}l},
  year = {2023},
  month = apr,
  journal = {Psychological Methods},
  volume = {28},
  number = {2},
  pages = {438--451},
  issn = {1939-1463},
  doi = {10.1037/met0000438},
  abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, replicating researchers are constrained by resources, and will almost always have to choose one replication effort to focus on from a set of potential candidates. To select a candidate efficiently in these cases, we need methods for deciding which out of all candidates considered would be the most useful to replicate, given some overall goal researchers wish to achieve. In this article we assume that the overall goal researchers wish to achieve is to maximize the utility gained by conducting the replication study. We then propose a general rule for study selection in replication research based on the replication value of the set of claims considered for replication. The replication value of a claim is defined as the maximum expected utility we could gain by conducting a replication of the claim, and is a function of (a) the value of being certain about the claim, and (b) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using replication value as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of replication value. Our goal in this article is not to define concrete guidelines for study selection, but to provide the necessary theoretical foundations on which such concrete guidelines could be built. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  langid = {english},
  pmid = {34928679},
  keywords = {Humans,Knowledge,Models Theoretical,Uncertainty}
}

@article{isbell_misconduct_2022,
  title = {Misconduct and {{Questionable Research Practices}}: {{The Ethics}} of {{Quantitative Data Handling}} and {{Reporting}} in {{Applied Linguistics}}},
  shorttitle = {Misconduct and {{Questionable Research Practices}}},
  author = {Isbell, Daniel R. and Brown, Dan and Chen, Meishan and Derrick, Deirdre J. and Ghanem, Romy and Arvizu, Mar{\'i}a Nelly Guti{\'e}rrez and Schnur, Erin and Zhang, Meixiu and Plonsky, Luke},
  year = {2022},
  journal = {The Modern Language Journal},
  volume = {106},
  number = {1},
  pages = {172--195},
  issn = {1540-4781},
  doi = {10.1111/modl.12760},
  urldate = {2022-09-20},
  abstract = {Scientific progress depends on the integrity of data and research findings. Intentionally distorting research data and findings constitutes scientific misconduct and introduces falsehoods into the scientific record. Unintentional distortions arising from questionable research practices (QRPs), such as unsystematically deleting outliers, pose similar obstacles to knowledge advancement. To investigate the extent of misconduct and QRPs in quantitative applied linguistics research, we surveyed 351 applied linguists who conduct quantitative research about their practices related to data handling and reporting. We found that 17\% of respondents (approximately 1 in 6) admitted to 1 or more forms of scientific misconduct and that 94\% admitted to 1 or more QRPs relevant to quantitative research. We also examined these practices in relation to participant background and training. Researchers admitting to misconduct tended to be earlier in their careers and had experienced publication rejection due to lack of statistically significant results. Quantitative training had generally desirable associations with QRPs. Publication rate and experience with publication rejection were associated with admission of several QRPs related to omitting statistical results. We discuss these findings and offer 5 recommendations for the field of applied linguistics to improve ethical quantitative data handling and reporting in research.},
  langid = {english},
  keywords = {applied linguistics,ethics,methodology,questionable research practices (QRPs),research misconduct,statistics}
}

@article{iyengar_selection_1988,
  title = {Selection {{Models}} and the {{File Drawer Problem}}},
  author = {Iyengar, Satish and Greenhouse, Joel B.},
  year = {1988},
  journal = {Statistical Science},
  volume = {3},
  number = {1},
  eprint = {2245925},
  eprinttype = {jstor},
  pages = {109--117},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2022-03-16},
  abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.}
}

@article{jaeschke_measurement_1989,
  title = {Measurement of Health Status: {{Ascertaining}} the Minimal Clinically Important Difference},
  shorttitle = {Measurement of Health Status},
  author = {Jaeschke, Roman and Singer, Joel and Guyatt, Gordon H.},
  year = {1989},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {10},
  number = {4},
  pages = {407--415},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(89)90005-6},
  urldate = {2018-02-09},
  langid = {english},
  pmid = {2691207},
  keywords = {Measurement,quality of life,responsiveness},
  annotation = {02971}
}

@article{janke_dark_2019,
  title = {Dark {{Pathways}} to {{Achievement}} in {{Science}}: {{Researchers}}' {{Achievement Goals Predict Engagement}} in {{Questionable Research Practices}}},
  shorttitle = {Dark {{Pathways}} to {{Achievement}} in {{Science}}},
  author = {Janke, Stefan and Daumiller, Martin and Rudert, Selma Carolin},
  year = {2019},
  month = aug,
  journal = {Social Psychological and Personality Science},
  volume = {10},
  number = {6},
  pages = {783--791},
  publisher = {SAGE Publications Inc},
  issn = {1948-5506},
  doi = {10.1177/1948550618790227},
  urldate = {2023-06-04},
  abstract = {Questionable research practices (QRPs) are a strongly debated topic in the scientific community. Hypotheses about the relationship between individual differences and QRPs are plentiful but have rarely been empirically tested. Here, we investigate whether researchers' personal motivation (expressed by achievement goals) is associated with self-reported engagement in QRPs within a sample of 217 psychology researchers. Appearance approach goals (striving for skill demonstration) positively predicted engagement in QRPs, while learning approach goals (striving for skill development) were a negative predictor. These effects remained stable when also considering Machiavellianism, narcissism, and psychopathy in a latent multiple regression model. Additional moderation analyses revealed that the more researchers favored publishing over scientific rigor, the stronger the association between appearance approach goals and engagement in QRPs. The findings deliver first insights into the nature of the relationship between personal motivation and scientific malpractice.},
  langid = {english}
}

@book{jeffreys_theory_1939,
  title = {Theory of Probability},
  author = {Jeffreys, Harold},
  year = {1939},
  series = {The {{International}} Series of Monographs on Physics},
  edition = {1st ed},
  publisher = {Oxford University Press},
  address = {Oxford [Oxfordshire]: New York},
  isbn = {978-0-19-853193-7},
  lccn = {QA273 .J4 1983},
  keywords = {Probabilities}
}

@book{jennison_group_2000,
  title = {Group Sequential Methods with Applications to Clinical Trials},
  author = {Jennison, Christopher and Turnbull, Bruce W.},
  year = {2000},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton},
  isbn = {978-0-8493-0316-6},
  lccn = {R853.C55 J46 2000},
  keywords = {Clinical Trials,Decision Theory,methods,Models Statistical,Statistical methods,Statistics}
}

@article{johansson_hail_2011,
  title = {Hail the Impossible: P-Values, Evidence, and Likelihood},
  shorttitle = {Hail the Impossible},
  author = {Johansson, Tobias},
  year = {2011},
  journal = {Scandinavian Journal of Psychology},
  volume = {52},
  number = {2},
  pages = {113--125},
  issn = {1467-9450},
  doi = {10.1111/j.1467-9450.2010.00852.x},
  urldate = {2022-12-05},
  abstract = {Johansson, T. (2011). Hail the impossible: p-values, evidence, and likelihood. Scandinavian Journal of Psychology 52, 113--125. Significance testing based on p-values is standard in psychological research and teaching. Typically, research articles and textbooks present and use p as a measure of statistical evidence against the null hypothesis (the Fisherian interpretation), although using concepts and tools based on a completely different usage of p as a tool for controlling long-term decision errors (the Neyman--Pearson interpretation). There are four major problems with using p as a measure of evidence and these problems are often overlooked in the domain of psychology. First, p is uniformly distributed under the null hypothesis and can therefore never indicate evidence for the null. Second, p is conditioned solely on the null hypothesis and is therefore unsuited to quantify evidence, because evidence is always relative in the sense of being evidence for or against a hypothesis relative to another hypothesis. Third, p designates probability of obtaining evidence (given the null), rather than strength of evidence. Fourth, p depends on unobserved data and subjective intentions and therefore implies, given the evidential interpretation, that the evidential strength of observed data depends on things that did not happen and subjective intentions. In sum, using p in the Fisherian sense as a measure of statistical evidence is deeply problematic, both statistically and conceptually, while the Neyman--Pearson interpretation is not about evidence at all. In contrast, the likelihood ratio escapes the above problems and is recommended as a tool for psychologists to represent the statistical evidence conveyed by obtained data relative to two hypotheses.},
  langid = {english},
  keywords = {error control,evidence,likelihood,p-value,significance testing,subjectivity}
}

@article{john_measuring_2012,
  title = {Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  journal = {Psychological science},
  volume = {23},
  number = {5},
  pages = {524--532},
  doi = {10.1177/0956797611430953},
  urldate = {2017-08-15},
  annotation = {00795}
}

@article{johnson_revised_2013,
  title = {Revised Standards for Statistical Evidence},
  author = {Johnson, V. E.},
  year = {2013},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {48},
  pages = {19313--19317},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1313476110},
  urldate = {2016-01-11},
  langid = {english},
  annotation = {00466}
}

@article{jones_test_1952,
  title = {Test of Hypotheses: One-Sided vs. Two-Sided Alternatives.},
  shorttitle = {Test of Hypotheses},
  author = {Jones, L. V.},
  year = {1952},
  month = jan,
  journal = {Psychological Bulletin},
  volume = {49},
  number = {1},
  pages = {43--46},
  issn = {0033-2909},
  doi = {http://dx.doi.org/10.1037/h0056832},
  urldate = {2016-03-16},
  abstract = {"The failure, among psychologists, to utilize the one-tailed statistical test, where it is appropriate, very likely is due to the propagation of the two-tailed model by writers of text-books in psychological statistics. It is typical, in such texts, to find little or no attention given to one-tailed tests. Since the test of the null hypothesis against a one-sided alternative is the most powerful test for all directional hypotheses, it is strongly recommended that the one-tailed model be adopted wherever its use is appropriate." (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  copyright = {{\copyright} American Psychological Association 1952},
  langid = {english},
  keywords = {Hypothesis Testing (major),Psychologists,Statistical Tests (major),Statistics (major)},
  annotation = {00095}
}

@article{jostmann_short_2016,
  title = {A Short History of the Weight-Importance Effect and a Recommendation for Pre-Testing: {{Commentary}} on {{Ebersole}} et al. (2016)},
  shorttitle = {A Short History of the Weight-Importance Effect and a Recommendation for Pre-Testing},
  author = {Jostmann, Nils B. and Lakens, Dani{\"e}l and Schubert, Thomas W.},
  year = {2016},
  journal = {Journal of Experimental Social Psychology},
  volume = {67},
  pages = {93--94},
  issn = {00221031},
  doi = {10.1016/j.jesp.2015.12.001},
  urldate = {2018-05-24},
  langid = {english}
}

@article{jostmann_weight_2009,
  title = {Weight as an {{Embodiment}} of {{Importance}}},
  author = {Jostmann, Nils B. and Lakens, Dani{\"e}l and Schubert, Thomas W.},
  year = {2009},
  journal = {Psychological Science},
  volume = {20},
  number = {9},
  pages = {1169--1174},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2009.02426.x},
  urldate = {2018-05-24},
  abstract = {Four studies show that the abstract concept of importance is grounded in bodily experiences of weight. Participants provided judgments of importance while they held either a heavy or a light clipboard. Holding a heavy clipboard increased judgments of monetary value (Study 1) and made participants consider fair decision-making procedures to be more important (Study 2). It also caused more elaborate thinking, as indicated by higher consistency between related judgments (Study 3) and by greater polarization of agreement ratings for strong versus weak arguments (Study 4). In line with an embodied perspective on cognition, these findings suggest that, much as weight makes people invest more physical effort in dealing with concrete objects, it also makes people invest more cognitive effort in dealing with abstract issues.},
  langid = {english},
  annotation = {00397}
}

@article{julious_sample_2004,
  title = {Sample Sizes for Clinical Trials with Normal Data},
  author = {Julious, Steven A.},
  year = {2004},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {23},
  number = {12},
  pages = {1921--1986},
  issn = {0277-6715},
  doi = {10.1002/sim.1783},
  abstract = {This article gives an overview of sample size calculations for parallel group and cross-over studies with Normal data. Sample size derivation is given for trials where the objective is to demonstrate: superiority, equivalence, non-inferiority, bioequivalence and estimation to a given precision, for different types I and II errors. It is demonstrated how the different trial objectives influence the null and alternative hypotheses of the trials and how these hypotheses influence the calculations. Sample size tables for the different types of trials and worked examples are given.},
  langid = {english},
  pmid = {15195324},
  keywords = {Biometry,Cross-Over Studies,Humans,Randomized Controlled Trials as Topic,Research Design,Sample Size,Therapeutic equivalency},
  annotation = {00317}
}

@article{junk_reproducibility_2020,
  title = {Reproducibility and {{Replication}} of {{Experimental Particle Physics Results}}},
  author = {Junk, Thomas and Lyons, Louis},
  year = {2020},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {2},
  number = {4},
  issn = {2644-2353, 2688-8513},
  doi = {10.1162/99608f92.250f995b},
  urldate = {2023-08-09},
  abstract = {Recently, much attention has been focused on the replicability of scientific results, causing scientists, statisticians, and journal editors to examine closely their methodologies and publishing criteria. Experimental particle physicists have been aware of the precursors of nonreplicable research for many decades and have many safeguards to ensure that the published results are as reliable as possible. The experiments require large investments of time and effort to design, construct, and operate. Large collaborations produce and check the results, and many papers are signed by more than 3,000 authors. This article gives an introduction to what experimental particle physics is and to some of the tools that are used to analyze the data. It describes the procedures used to ensure that results can be computationally reproduced, both by collaborators and by noncollaborators. It describes the status of publicly available data sets and analysis tools that aid in reproduction and recasting of experimental results. It also describes methods particle physicists use to maximize the reliability of the results, which increases the probability that they can be replicated by other collaborations or even the same collaborations with more data and new personnel. Examples of results that were later found to be false are given, both with failed replication attempts and one with alarmingly successful replications. While some of the characteristics of particle physics experiments are unique, many of the procedures and techniques can be and are used in other fields.},
  langid = {english}
}

@article{kaiser_directional_1960,
  title = {Directional Statistical Decisions},
  author = {Kaiser, Henry F.},
  year = {1960},
  journal = {Psychological Review},
  volume = {67},
  number = {3},
  pages = {160--167},
  issn = {1939-1471 0033-295X},
  doi = {10.1037/h0047595},
  abstract = {Concerning the traditional nondirectional 2-sided test of significance, the author argues that "we cannot logically make a directional statistical decision or statement when the null hypothesis is rejected on the basis of the direction of the difference in the observed means." Thus, this test "should almost never be used." He proposes that "almost without exception the directional two-sided test should replace" it (18 ref.)},
  copyright = {(c) 2016 APA, all rights reserved},
  langid = {english},
  keywords = {*Null Hypothesis Testing,Statistical Significance},
  annotation = {00198}
}

@article{kaplan_likelihood_2015,
  title = {Likelihood of {{Null Effects}} of {{Large NHLBI Clinical Trials Has Increased}} over {{Time}}},
  author = {Kaplan, Robert M. and Irvin, Veronica L.},
  year = {2015},
  month = aug,
  journal = {PLOS ONE},
  volume = {10},
  number = {8},
  pages = {e0132382},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0132382},
  urldate = {2018-02-03},
  abstract = {Background We explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time.   Methods We identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs {$>\$$}500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality.   Results 17 of 30 studies (57\%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8\%) trials published after 2000 ({$\chi$}2=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings.   Conclusions The number NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.},
  langid = {english},
  keywords = {Blood pressure,Cardiovascular diseases,Comparators,Coronary heart disease,Drug therapy,Myocardial infarction,Sudden cardiac death,Women's health},
  annotation = {00067}
}

@article{kass_bayes_1995,
  title = {Bayes Factors},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  year = {1995},
  journal = {Journal of the american statistical association},
  volume = {90},
  number = {430},
  pages = {773--795},
  doi = {10.1080/01621459.1995.10476572},
  urldate = {2017-08-13},
  annotation = {11249}
}

@article{keefe_defining_2013,
  title = {Defining a {{Clinically Meaningful Effect}} for the {{Design}} and {{Interpretation}} of {{Randomized Controlled Trials}}},
  author = {Keefe, Richard S. E. and Kraemer, Helena C. and Epstein, Robert S. and Frank, Ellen and Haynes, Ginger and Laughren, Thomas P. and Mcnulty, James and Reed, Shelby D. and Sanchez, Juan and Leon, Andrew C.},
  year = {2013},
  journal = {Innovations in Clinical Neuroscience},
  volume = {10},
  number = {5-6 Suppl A},
  pages = {4S-19S},
  issn = {2158-8333},
  urldate = {2018-03-10},
  abstract = {Objective: This article captures the proceedings of a meeting aimed at defining clinically meaningful effects for use in randomized controlled trials for psychopharmacological agents., Design: Experts from a variety of disciplines defined clinically meaningful effects from their perspectives along with viewpoints about how to design and interpret randomized controlled trials., Setting: The article offers relevant, practical, and sometimes anecdotal information about clinically meaningful effects and how to interpret them., Participants: The concept for this session was the work of co-chairs Richard Keefe and the late Andy Leon. Faculty included Richard Keefe, PhD; James McNulty, AbScB; Robert S. Epstein, MD, MS; Shelby D. Reed, PhD; Juan Sanchez, MD; Ginger Haynes, PhD; Andrew C. Leon, PhD; Helena Chmura Kraemer, PhD; Ellen Frank, PhD, and Kenneth L. Davis, MD., Results: The term clinically meaningful effect is an important aspect of designing and interpreting randomized controlled trials but can be particularly difficult in the setting of psychopharmacology where effect size may be modest, particularly over the short term, because of a strong response to placebo. Payers, regulators, patients, and clinicians have different concerns about clinically meaningful effects and may describe these terms differently. The use of moderators in success rate differences may help better delineate clinically meaningful effects., Conclusion: There is no clear consensus on a single definition for clinically meaningful differences in randomized controlled trials, and investigators must be sensitive to specific concerns of stakeholders in psychopharmacology in order to design and execute appropriate clinical trials.},
  pmcid = {PMC3719483},
  pmid = {23882433},
  annotation = {00022}
}

@article{kelley_confidence_2007,
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  author = {Kelley, Ken},
  year = {2007},
  journal = {Journal of Statistical Software},
  volume = {20},
  number = {8},
  issn = {1548-7660},
  doi = {10.18637/JSS.V020.I08},
  urldate = {2018-07-19},
  abstract = {The behavioral, educational, and social sciences are undergoing a paradigmatic shift in methodology, from disciplines that focus on the dichotomous outcome of null hypothesis significance tests to disciplines that report and interpret effect sizes and their corresponding confidence intervals. Due to the arbitrariness of many measurement instruments used in the behavioral, educational, and social sciences, some of the most widely reported effect sizes are standardized. Although forming confidence intervals for standardized effect sizes can be very beneficial, such confidence interval procedures are generally difficult to implement because they depend on noncentral t, F , and {$\chi$}2 distributions. At present, no main-stream statistical package provides exact confidence intervals for standardized effects without the use of specialized programming scripts. Methods for the Behavioral, Educational, and Social Sciences (MBESS) is an R package that has routines for calculating confidence intervals for noncentral t, F , and {$\chi$}2 distributions, which are then used in the calculation of exact confidence intervals for standardized effect sizes by using the confidence interval transformation and inversion principles. The present article discusses the way in which confidence intervals are formed for standardized effect sizes and illustrates how such confidence intervals can be easily formed using MBESS in R.},
  langid = {english},
  annotation = {00163}
}

@article{kelley_effect_2012,
  title = {On Effect Size},
  author = {Kelley, Ken and Preacher, Kristopher J.},
  year = {2012},
  journal = {Psychological methods},
  volume = {17},
  number = {2},
  pages = {137--152},
  publisher = {American Psychological Association},
  doi = {10.1037/a0028086}
}

@article{kelley_sample_2006,
  title = {Sample Size Planning for the Standardized Mean Difference: Accuracy in Parameter Estimation via Narrow Confidence Intervals.},
  shorttitle = {Sample Size Planning for the Standardized Mean Difference},
  author = {Kelley, Ken and Rausch, Joseph R.},
  year = {2006},
  journal = {Psychological methods},
  volume = {11},
  number = {4},
  pages = {363--385},
  doi = {10.1037},
  urldate = {2016-07-27},
  annotation = {00083}
}

@article{kelter_analysis_2021,
  title = {Analysis of Type {{I}} and {{II}} Error Rates of {{Bayesian}} and Frequentist Parametric and Nonparametric Two-Sample Hypothesis Tests under Preliminary Assessment of Normality},
  author = {Kelter, Riko},
  year = {2021},
  month = jun,
  journal = {Computational Statistics},
  volume = {36},
  number = {2},
  pages = {1263--1288},
  issn = {1613-9658},
  doi = {10.1007/s00180-020-01034-7},
  urldate = {2023-04-13},
  abstract = {Testing for differences between two groups is among the most frequently carried out statistical methods in empirical research. The traditional frequentist approach is to make use of null hypothesis significance tests which use p values to reject a null hypothesis. Recently, a lot of research has emerged which proposes Bayesian versions of the most common parametric and nonparametric frequentist two-sample tests. These proposals include Student's two-sample t-test and its nonparametric counterpart, the Mann--Whitney U test. In this paper, the underlying assumptions, models and their implications for practical research of recently proposed Bayesian two-sample tests are explored and contrasted with the frequentist solutions. An extensive simulation study is provided, the results of which demonstrate that the proposed Bayesian tests achieve better type I error control at slightly increased type II error rates. These results are important, because balancing the type I and II errors is a crucial goal in a variety of research, and shifting towards the Bayesian two-sample tests while simultaneously increasing the sample size yields smaller type I error rates. What is more, the results highlight that the differences in type II error rates between frequentist and Bayesian two-sample tests depend on the magnitude of the underlying effect.},
  langid = {english},
  keywords = {Bayesian hypothesis testing,Null hypothesis significance testing,Parametric and non-parametric two-sample tests,Two-sample hypothesis tests,Type I and II error rates}
}

@book{kenett_information_2016,
  title = {Information {{Quality}}: {{The Potential}} of {{Data}} and {{Analytics}} to {{Generate Knowledge}}},
  shorttitle = {Information {{Quality}}},
  author = {Kenett, Ron S. and Shmueli, Galit and Kenett, Ron},
  year = {2016},
  month = dec,
  edition = {1st edition},
  publisher = {Wiley},
  address = {Chichester, West Sussex},
  isbn = {978-1-118-87444-8},
  langid = {english}
}

@article{kennedy-shaffer_05_2019,
  title = {Before p {$<$} 0.05 to {{Beyond}} p {$<$} 0.05: {{Using History}} to {{Contextualize}} p-{{Values}} and {{Significance Testing}}},
  shorttitle = {Before p {$<$} 0.05 to {{Beyond}} p {$<$} 0.05},
  author = {{Kennedy-Shaffer}, Lee},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {82--90},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537891},
  urldate = {2021-02-18},
  abstract = {As statisticians and scientists consider a world beyond p {$<$} 0.05, it is important to not lose sight of how we got to this point. Although significance testing and p-values are often presented as prescriptive procedures, they came about through a process of refinement and extension to other disciplines. Ronald A. Fisher and his contemporaries formalized these methods in the early twentieth century and Fisher's 1925 Statistical Methods for Research Workers brought the techniques to experimentalists in a variety of disciplines. Understanding how these methods arose, spread, and were argued over since then illuminates how p {$<$} 0.05 came to be a standard for scientific inference, the advantage it offered at the time, and how it was interpreted. This historical perspective can inform the work of statisticians today by encouraging thoughtful consideration of how their work, including proposed alternatives to the p-value, will be perceived and used by scientists. And it can engage students more fully and encourage critical thinking rather than rote applications of formulae. Incorporating history enables students, practitioners, and statisticians to treat the discipline as an ongoing endeavor, crafted by fallible humans, and provides a deeper understanding of the subject and its consequences for science and society.},
  pmid = {31413381},
  keywords = {Education,Foundational issues,Hypothesis testing,Inference,Probability}
}

@article{kenny_unappreciated_2019,
  title = {The Unappreciated Heterogeneity of Effect Sizes: {{Implications}} for Power, Precision, Planning of Research, and Replication},
  shorttitle = {The Unappreciated Heterogeneity of Effect Sizes},
  author = {Kenny, David A. and Judd, Charles M.},
  year = {2019},
  journal = {Psychological Methods},
  volume = {24},
  number = {5},
  pages = {578--589},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/met0000209},
  abstract = {Repeated investigations of the same phenomenon typically yield effect sizes that vary more than one would expect from sampling error alone. Such variation is even found in exact replication studies, suggesting that it is not only because of identifiable moderators but also to subtler random variation across studies. Such heterogeneity of effect sizes is typically ignored, with unfortunate consequences. We consider its implications for power analyses, the precision of estimated effects, and the planning of original and replication research. With heterogeneity and an interest in generalizing to a population of studies, the usual power calculations and confidence intervals are likely misleading, and the preference for single definitive large-N studies is misguided. Researchers and methodologists need to recognize that effects are often heterogeneous and plan accordingly. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Experimental Replication,Population (Statistics),Prediction,Statistical Power}
}

@book{keppel_design_1991,
  title = {Design and Analysis: {{A}} Researcher's Handbook, 3rd Ed},
  shorttitle = {Design and Analysis},
  author = {Keppel, Geoffrey},
  year = {1991},
  series = {Design and Analysis: {{A}} Researcher's Handbook, 3rd Ed},
  pages = {xiii, 594},
  publisher = {Prentice-Hall, Inc},
  address = {Englewood Cliffs, NJ, US},
  abstract = {My major purpose in writing this book was to present the design and analysis of experiments from a researcher's point of view. The book was not intended to be a primary statistical reference, but rather to be a useful source of information and explanation of design and statistical matters rarely touched on by more mathematically sophisticated books. I wrote the book with a particular reader in mind, namely, a student who is about to engage in experimental research, but who possesses only the most fundamental mathematical skills and has little or no formal statistical background. What I offer in effect are research tutorials that provide the basic information necessary to design and to analyze meaningful experiments in the behavioral, social, and biological sciences. . . . Thus, a large proportion of the book is devoted to a detailed discussion of matters of experimental design and to the practical use of statistical procedures that will assist researchers in drawing inferences from experimental data. Statistical arguments are not neglected, however, but are covered in the context of data analysis and data interpretation; references to more mathematically oriented sources are liberally provided.  The major difference between my book and others at this level is its coverage of the detailed analysis of experiments. The book considers the reasons behind these analyses and provides numerous illustrations of their creative application to experimental problems.  This book is intended for use in a one-semester course or a two-quarter sequence in experimental design and statistical analysis.  This book presents a comprehensive coverage of the detailed analysis of basic experimental designs and is intended as a text at the graduate level as a reference tool for researchers. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-13-200775-7},
  keywords = {Experimental Design,Statistical Analysis}
}

@article{kerr_harking_1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  issn = {1088-8683, 1532-7957},
  doi = {10.1207/s15327957pspr0203_4},
  urldate = {2016-03-25},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  langid = {english},
  pmid = {15647155},
  annotation = {00553}
}

@article{king_point_2011,
  title = {A Point of Minimal Important Difference ({{MID}}): A Critique of Terminology and Methods},
  shorttitle = {A Point of Minimal Important Difference ({{MID}})},
  author = {King, Madeleine T.},
  year = {2011},
  month = apr,
  journal = {Expert Review of Pharmacoeconomics \& Outcomes Research},
  volume = {11},
  number = {2},
  pages = {171--184},
  issn = {1473-7167},
  doi = {10.1586/erp.11.9},
  urldate = {2018-12-21},
  abstract = {The minimal important difference (MID) is a phrase with instant appeal in a field struggling to interpret health-related quality of life and other patient-reported outcomes. The terminology can be confusing, with several terms differing only slightly in definition (e.g., minimal clinically important difference, clinically important difference, minimally detectable difference, the subjectively significant difference), and others that seem similar despite having quite different meanings (minimally detectable difference versus minimum detectable change). Often, nuances of definition are of little consequence in the way that these quantities are estimated and used. Four methods are commonly employed to estimate MIDs: patient rating of change (global transition items); clinical anchors; standard error of measurement; and effect size. These are described and critiqued in this article. There is no universal MID, despite the appeal of the notion. Indeed, for a particular patient-reported outcome instrument or scale, the MID is not an immutable characteristic, but may vary by population and context. At both the group and individual level, the MID may depend on the clinical context and decision at hand, the baseline from which the patient starts, and whether they are improving or deteriorating. Specific estimates of MIDs should therefore not be overinterpreted. For a given health-related quality-of-life scale, all available MID estimates (and their confidence intervals) should be considered, amalgamated into general guidelines and applied judiciously to any particular clinical or research context.},
  pmid = {21476819},
  keywords = {clinical significance,health-related quality of life,HRQOL,interpretation,MCID,MID,minimal clinically important difference,minimal important difference,patient-reported outcome,PRO},
  annotation = {00223}
}

@article{kish_statistical_1959,
  title = {Some {{Statistical Problems}} in {{Research Design}}},
  author = {Kish, Leslie},
  year = {1959},
  journal = {American Sociological Review},
  volume = {24},
  number = {3},
  eprint = {2089381},
  eprinttype = {jstor},
  pages = {328--338},
  publisher = {[American Sociological Association, Sage Publications, Inc.]},
  issn = {0003-1224},
  doi = {10.2307/2089381},
  urldate = {2023-08-14},
  abstract = {Several statistical problems in the design of research are discussed: (1) The use of statistical tests and the search for causation in survey research are examined; for this we suggest separating four classes of variables: explanatory, controlled, confounded, and randomized. (2) The relative advantages of experiments, surveys, and other investigations are shown to derive respectively from better control, representation, and measurement. (3) Finally, three common misuses of statistical tests are examined: "hunting with a shot-gun for significant differences," confusing statistical significance with substantive importance, and overemphasis on the primitive level of merely finding differences.}
}

@book{kish_survey_1965,
  title = {Survey {{Sampling}}},
  author = {Kish, Leslie},
  year = {1965},
  publisher = {Wiley},
  address = {New York}
}

@article{klein_practical_2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and Ijzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  year = {2018},
  month = jun,
  journal = {Collabra: Psychology},
  volume = {4},
  number = {1},
  pages = {20},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  urldate = {2020-01-13},
  abstract = {Article: A Practical Guide for Transparency in Psychological Science},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english}
}

@article{komic_research_2015,
  title = {Research {{Integrity}} and {{Research Ethics}} in {{Professional Codes}} of {{Ethics}}: {{Survey}} of {{Terminology Used}} by {{Professional Organizations}} across {{Research Disciplines}}},
  shorttitle = {Research {{Integrity}} and {{Research Ethics}} in {{Professional Codes}} of {{Ethics}}},
  author = {Komi{\'c}, Dubravka and Maru{\v s}i{\'c}, Stjepan Ljudevit and Maru{\v s}i{\'c}, Ana},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0133662},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0133662},
  urldate = {2022-09-27},
  abstract = {Professional codes of ethics are social contracts among members of a professional group, which aim to instigate, encourage and nurture ethical behaviour and prevent professional misconduct, including research and publication. Despite the existence of codes of ethics, research misconduct remains a serious problem. A survey of codes of ethics from 795 professional organizations from the Illinois Institute of Technology's Codes of Ethics Collection showed that 182 of them (23\%) used research integrity and research ethics terminology in their codes, with differences across disciplines: while the terminology was common in professional organizations in social sciences (82\%), mental health (71\%), sciences (61\%), other organizations had no statements (construction trades, fraternal social organizations, real estate) or a few of them (management, media, engineering). A subsample of 158 professional organizations we judged to be directly involved in research significantly more often had statements on research integrity/ethics terminology than the whole sample: an average of 10.4\% of organizations with a statement (95\% CI = 10.4-23-5\%) on any of the 27 research integrity/ethics terms compared to 3.3\% (95\% CI = 2.1--4.6\%), respectively (P{$<$}0.001). Overall, 62\% of all statements addressing research integrity/ethics concepts used prescriptive language in describing the standard of practice. Professional organizations should define research integrity and research ethics issues in their ethics codes and collaborate within and across disciplines to adequately address responsible conduct of research and meet contemporary needs of their communities.},
  langid = {english},
  keywords = {Careers in research,Conflicts of interest,Language,Professions,Publication ethics,Research ethics,Research integrity,Scientific misconduct}
}

@article{koole_rewarding_2012,
  title = {Rewarding Replications {{A}} Sure and Simple Way to Improve Psychological Science},
  author = {Koole, Sander L. and Lakens, Dani{\"e}l},
  year = {2012},
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {608--614},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612462586},
  urldate = {2015-12-04},
  abstract = {Although replications are vital to scientific progress, psychologists rarely engage in systematic replication efforts. In this article, we consider psychologists' narrative approach to scientific publications as an underlying reason for this neglect and propose an incentive structure for replications within psychology. First, researchers need accessible outlets for publishing replications. To accomplish this, psychology journals could publish replication reports in files that are electronically linked to reports of the original research. Second, replications should get cited. This can be achieved by cociting replications along with original research reports. Third, replications should become a valued collaborative effort. This can be realized by incorporating replications in teaching programs and by stimulating adversarial collaborations. The proposed incentive structure for replications can be developed in a relatively simple and cost-effective manner. By promoting replications, this incentive structure may greatly enhance the dependability of psychology's knowledge base.},
  langid = {english},
  keywords = {philosophy of science,publication bias,replication,scientific fraud,selective reporting}
}

@article{krafczyk_learning_2021,
  title = {Learning from Reproducing Computational Results: Introducing Three Principles and the {{{\emph{Reproduction Package}}}}},
  shorttitle = {Learning from Reproducing Computational Results},
  author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
  year = {2021},
  month = may,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2197},
  pages = {rsta.2020.0069, 20200069},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2020.0069},
  urldate = {2021-03-31},
  abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel               Reproduction Package               , a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar               Reproduction Packages               .                                         This article is part of the theme issue `Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification               in silico               '.},
  langid = {english}
}

@article{kraft_interpreting_2020,
  title = {Interpreting Effect Sizes of Education Interventions},
  author = {Kraft, Matthew A.},
  year = {2020},
  journal = {Educational Researcher},
  volume = {49},
  number = {4},
  pages = {241--253},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  doi = {10.3102/0013189X20912798}
}

@article{kruschke_bayesian_2011,
  title = {Bayesian Assessment of Null Values via Parameter Estimation and Model Comparison},
  author = {Kruschke, John K.},
  year = {2011},
  journal = {Perspectives on Psychological Science},
  volume = {6},
  number = {3},
  pages = {299--312},
  annotation = {00203}
}

@article{kruschke_bayesian_2013,
  title = {Bayesian Estimation Supersedes the t Test.},
  author = {Kruschke, John K.},
  year = {2013},
  journal = {Journal of Experimental Psychology: General},
  volume = {142},
  number = {2},
  pages = {573--603},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0029146},
  urldate = {2015-12-01},
  langid = {english},
  annotation = {00483}
}

@article{kruschke_bayesian_2017,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {Kruschke, John K. and Liddell, Torrin M.},
  year = {2017},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  urldate = {2017-02-10},
  langid = {english},
  annotation = {00059}
}

@book{kruschke_doing_2014,
  title = {Doing {{Bayesian Data Analysis}}, {{Second Edition}}: {{A Tutorial}} with {{R}}, {{JAGS}}, and {{Stan}}},
  shorttitle = {Doing {{Bayesian Data Analysis}}, {{Second Edition}}},
  author = {Kruschke, John K.},
  year = {2014},
  month = nov,
  edition = {2 edition},
  publisher = {Academic Press},
  address = {Boston},
  abstract = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition provides an accessible approach for conducting Bayesian data analysis, as material is explained clearly with concrete examples. Included are step-by-step instructions on how to carry out Bayesian data analyses in the popular and free software R and WinBugs, as well as new programs in JAGS and Stan. The new programs are designed to be much easier to use than the scripts in the first edition. In particular, there are now compact high-level scripts that make it easy to run the programs on your own data sets. The book is divided into three parts and begins with the basics: models, probability, Bayes' rule, and the R programming language. The discussion then moves to the fundamentals applied to inferring a binomial probability, before concluding with chapters on the generalized linear model. Topics include metric-predicted variable on one or two groups; metric-predicted variable with one metric predictor; metric-predicted variable with multiple metric predictors; metric-predicted variable with one nominal predictor; and metric-predicted variable with multiple nominal predictors. The exercises found in the text have explicit purposes and guidelines for accomplishment. This book is intended for first-year graduate students or advanced undergraduates in statistics, data analysis, psychology, cognitive science, social sciences, clinical sciences, and consumer sciences in business. Accessible, including the basics of essential concepts of probability and random samplingExamples with R programming language and JAGS softwareComprehensive coverage of all scenarios addressed by non-Bayesian textbooks: t-tests, analysis of variance (ANOVA) and comparisons in ANOVA, multiple regression, and chi-square (contingency table analysis)Coverage of experiment planningR and JAGS computer programming code on websiteExercises have explicit purposes and guidelines for accomplishment Provides step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs},
  isbn = {978-0-12-405888-0},
  langid = {english}
}

@article{kruschke_rejecting_2018,
  title = {Rejecting or {{Accepting Parameter Values}} in {{Bayesian Estimation}}},
  author = {Kruschke, John K.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {270--280},
  issn = {2515-2459},
  doi = {10.1177/2515245918771304},
  urldate = {2019-04-24},
  abstract = {This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.},
  langid = {english}
}

@book{kuhn_structure_1962,
  title = {The {{Structure}} of {{Scientific Revolutions}}},
  author = {Kuhn, Thomas S.},
  year = {1962},
  publisher = {University of Chicago Press},
  address = {Chicago, IL},
  abstract = {"A landmark in intellectual history which has attracted attention far beyond its own immediate field. . . . It is written with a combination of depth and clarity that make it an almost unbroken series of aphorisms. . . . Kuhn does not permit truth to be a criterion of scientific theories, he would presumably not claim his own theory to be true. But if causing a revolution is the hallmark of a superior paradigm, [this book] has been a resounding success." ---Nicholas Wade, Science~"Perhaps the best explanation of [the] process of discovery." ---William Erwin Thompson, New York Times Book Review~"Occasionally there emerges a book which has an influence far beyond its originally intended audience. . . . Thomas Kuhn's The Structure of Scientific Revolutions . . . has clearly emerged as just such a work." ---Ron Johnston, Times Higher Education Supplement~"Among the most influential academic books in this century." ---Choice~One of "The Hundred Most Influential Books Since the Second World War," Times Literary Supplement},
  isbn = {978-0-226-45808-3},
  langid = {english}
}

@article{kuipers_models_2016,
  title = {Models, Postulates, and Generalized Nomic Truth Approximation},
  author = {Kuipers, Theo A. F.},
  year = {2016},
  month = oct,
  journal = {Synthese},
  volume = {193},
  number = {10},
  pages = {3057--3077},
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-015-0916-9},
  urldate = {2017-06-17},
  abstract = {The qualitative theory of nomic truth approximation, presented in Kuipers in his (from instrumentalism to constructive realism, 2000), in which `the truth' concerns the distinction between nomic, e.g. physical, possibilities and impossibilities, rests on a very restrictive assumption, viz. that theories always claim to characterize the boundary between nomic possibilities and impossibilities. Fully recognizing two different functions of theories, viz. excluding and representing, this paper drops this assumption by conceiving theories in development as tuples of postulates and models, where the postulates claim to exclude nomic impossibilities and the (not-excluded) models claim to represent nomic possibilities. Revising theories becomes then a matter of adding or revising models and/or postulates in the light of increasing evidence, captured by a special kind of theories, viz. `data-theories'. Under the assumption that the data-theory is true, achieving empirical progress in this way provides good reasons for the abductive conclusion that truth approximation has been achieved as well. Here, the notions of truth approximation and empirical progress are formally direct generalizations of the earlier ones. However, truth approximation is now explicitly defined in terms of increasing truth-content and decreasing falsity-content of theories, whereas empirical progress is defined in terms of lasting increased accepted and decreased rejected content in the light of increasing evidence. These definitions are strongly inspired by a paper of Gustavo Cevolani, Vincenzo Crupi and Roberto Festa, viz., ``Verisimilitude and belief change for conjunctive theories'' (Cevolani et al. in Erkenntnis 75(2):183--222, 2011).},
  langid = {english},
  annotation = {00004}
}

@book{lakatos_methodology_1978,
  title = {The Methodology of Scientific Research Programmes: {{Volume}} 1: {{Philosophical}} Papers},
  shorttitle = {The Methodology of Scientific Research Programmes},
  author = {Lakatos, Imre},
  year = {1978},
  publisher = {Cambridge University Press},
  urldate = {2017-06-14}
}

@article{lakens_benefits_2024,
  title = {The Benefits of Preregistration and {{Registered Reports}}},
  author = {Lakens, Dani{\"e}l and Mesquida, Cristian and Rasti, Sajedeh and Ditroilo, Massimiliano},
  year = {2024},
  month = dec,
  journal = {Evidence-Based Toxicology},
  volume = {2},
  number = {1},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/2833373X.2024.2376046},
  urldate = {2024-07-23},
  abstract = {Practices that introduce systematic bias are common in most scientific disciplines, including toxicology. Selective reporting of results and publication bias are two of the most prevalent sources of bias and lead to unreliable scientific claims. Preregistration and Registered Reports are recent developments that aim to counteract systematic bias and allow other scientists to transparently evaluate how severely a claim has been tested. We review metascientific research confirming that preregistration and Registered Reports achieve their goals, and have additional benefits, such as improving the quality of studies. We then reflect on criticisms of preregistration. Beyond the valid concern that the mere presence of a preregistration may be mindlessly used as a proxy for high quality, we identify conflicting viewpoints, several misunderstandings, and a general lack of empirical support for the criticisms that have been raised. We conclude with general recommendations to increase the quality and practice of preregistration.},
  keywords = {preregistration,publication bias,Registered Reports,selective reporting,severe testing}
}

@article{lakens_calculating_2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  author = {Lakens, Dani{\"e}l},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00863},
  urldate = {2020-02-03},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  langid = {english},
  keywords = {Cohen's d,effect sizes,eta-squared,power analysis,sample size planning}
}

@article{lakens_challenges_2015,
  title = {On the Challenges of Drawing Conclusions from {\emph{p}} -Values Just below 0.05},
  author = {Lakens, Dani{\"e}l},
  year = {2015},
  journal = {PeerJ},
  volume = {3},
  pages = {e1142},
  issn = {2167-8359},
  doi = {10.7717/peerj.1142},
  urldate = {2015-11-30},
  langid = {english}
}

@article{lakens_concerns_2025,
  title = {Concerns {{About Theorizing}}, {{Relevance}}, {{Generalizability}}, and {{Methodology Across Two Crises}} in {{Social Psychology}}},
  author = {Lakens, Dani{\"e}l},
  year = {2025},
  month = may,
  journal = {International Review of Social Psychology},
  volume = {38},
  number = {1},
  issn = {2397-8570},
  doi = {10.5334/irsp.1038},
  urldate = {2025-05-15},
  abstract = {During two crises in social psychology, the first from the 1960s to the end of the 1970s, and the second starting in 2010 and still ongoing, researchers discussed the strength of theories in the field, the societal relevance of research, the generalizability of effects, and problematic methodological and statistical practices. Continuing on the first part of this review, which focused on replicability, I compare similarities in the concerns raised across both crises. I consider which issues have prompted meaningful reforms and which have yet to result in significant progress. Finally, I reflect on the extent that the incentives contributed to these crises and argue that a more coordinated approach to scientific research is needed to prevent these concerns from resurfacing in a future third crisis.},
  langid = {american}
}

@article{lakens_concerns_2025a,
  title = {Concerns {{About Replicability Across Two Crises}} in {{Social Psychology}}},
  author = {Lakens, Dani{\"e}l},
  year = {2025},
  month = may,
  journal = {International Review of Social Psychology},
  volume = {38},
  number = {1},
  issn = {2397-8570},
  doi = {10.5334/irsp.1036},
  urldate = {2025-05-15},
  abstract = {Twice in the history of social psychology has there been a crisis of confidence. The first started in the 1960s and lasted until the end of the 1970s, and the second crisis dominated the 2010s. Drawing on extensive quotes from articles published during both crises, I examine the similarities and differences between these psychological crises. In this first of two articles, I focus on how researchers discussed fundamental concerns about the replicability of findings across the two crises. I reflect on five possible reasons why concerns about failed replications received more attention during the second crisis, the continuing lack of incentives to perform replication studies, and the importance of large-scale research projects to instigate change.},
  langid = {american}
}

@article{lakens_equivalence_2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta-Analyses}}},
  shorttitle = {Equivalence {{Tests}}},
  author = {Lakens, Dani{\"e}l},
  year = {2017},
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {355--362},
  issn = {1948-5506},
  doi = {10.1177/1948550617697177},
  urldate = {2017-07-08},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  langid = {english}
}

@article{lakens_equivalence_2018,
  title = {Equivalence Testing for Psychological Research: {{A}} Tutorial},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  urldate = {2018-11-30},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english}
}

@article{lakens_improving_2020,
  title = {Improving {{Inferences About Null Effects With Bayes Factors}} and {{Equivalence Tests}}},
  author = {Lakens, Dani{\"e}l and McLatchie, Neil and Isager, Peder M and Scheel, Anne M and Dienes, Zoltan},
  year = {2020},
  month = jan,
  journal = {The Journals of Gerontology: Series B},
  volume = {75},
  number = {1},
  pages = {45--57},
  issn = {1079-5014},
  doi = {10.1093/geronb/gby065},
  urldate = {2021-04-05},
  abstract = {Researchers often conclude an effect is absent when a null-hypothesis significance test yields a nonsignificant p value. However, it is neither logically nor statistically correct to conclude an effect is absent when a hypothesis test is not significant. We present two methods to evaluate the presence or absence of effects: Equivalence testing (based on frequentist statistics) and Bayes factors (based on Bayesian statistics). In four examples from the gerontology literature, we illustrate different ways to specify alternative models that can be used to reject the presence of a meaningful or predicted effect in hypothesis tests. We provide detailed explanations of how to calculate, report, and interpret Bayes factors and equivalence tests. We also discuss how to design informative studies that can provide support for a null model or for the absence of a meaningful effect. The conceptual differences between Bayes factors and equivalence tests are discussed, and we also note when and why they might lead to similar or different inferences in practice. It is important that researchers are able to falsify predictions or can quantify the support for predicted null effects. Bayes factors and equivalence tests provide useful statistical tools to improve inferences about null effects.}
}

@article{lakens_improving_2020a,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigour}} by {{Making Hypothesis Tests Machine Readable}}},
  author = {Lakens, Dani{\"e}l and DeBruine, Lisa},
  year = {2020},
  month = jan,
  doi = {10.31234/osf.io/5xcda},
  urldate = {2020-01-31},
  abstract = {Making scientific information machine-readable greatly facilitates its re-use. Many scientific articles have the goal to test a hypothesis, and making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine readable. We believe there are two benefits to specifying a hypothesis test in a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis test will become more transparent, falsifiable, and rigorous. Second, scientists will benefit if information related to hypothesis tests in scientific articles is easily findable and re-usable, for example when performing meta-analyses, during peer review, and when examining meta-scientific research questions. We examine what a machine readable hypothesis test should looks like, and demonstrate the feasibility of machine readable hypothesis tests in a real-life example.}
}

@article{lakens_justify_2018,
  title = {Justify Your Alpha},
  author = {Lakens, Dani{\"e}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and Oliveira, Cilene Lino and Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k a}tkowski, Wojciech and Vadillo, Miguel A. and Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = feb,
  journal = {Nature Human Behaviour},
  volume = {2},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  urldate = {2018-02-28},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  copyright = {2018 The Publisher},
  langid = {english}
}

@article{lakens_my_2023,
  title = {Is My Study Useless? {{Why}} Researchers Need Methodological Review Boards},
  shorttitle = {Is My Study Useless?},
  author = {Lakens, Dani{\"e}l},
  year = {2023},
  month = jan,
  journal = {Nature},
  volume = {613},
  number = {7942},
  pages = {9--9},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-022-04504-8},
  urldate = {2023-01-04},
  abstract = {Making researchers account for their methods before data collection is a long-overdue step.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Lab life,Peer review,Publishing,Research management},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: World View\\
Subject\_term: Publishing, Peer review, Research management, Lab life}
}

@article{lakens_pandemic_2020,
  title = {Pandemic Researchers --- Recruit Your Own Best Critics},
  author = {Lakens, Dani{\"e}l},
  year = {2020},
  month = may,
  journal = {Nature},
  volume = {581},
  number = {7807},
  pages = {121--121},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-020-01392-8},
  urldate = {2020-05-12},
  abstract = {To guard against rushed and sloppy science, build pressure testing into your research.},
  copyright = {2020 Nature},
  langid = {english}
}

@article{lakens_performing_2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: {{Sequential}} Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  urldate = {2015-11-30},
  langid = {english}
}

@article{lakens_practical_2021,
  title = {The Practical Alternative to the p Value Is the Correctly Used p Value},
  author = {Lakens, Dani{\"e}l},
  year = {2021},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {3},
  pages = {639--648},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620958012},
  urldate = {2021-05-11},
  abstract = {Because of the strong overreliance on p values in the scientific literature, some researchers have argued that we need to move beyond p values and embrace practical alternatives. When proposing alternatives to p values statisticians often commit the ``statistician's fallacy,'' whereby they declare which statistic researchers really ``want to know.'' Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p value. As long as null-hypothesis tests have been criticized, researchers have suggested including minimum-effect tests and equivalence tests in our statistical toolbox, and these tests have the potential to greatly improve the questions researchers ask. If anyone believes p values affect the quality of scientific research, preventing the misinterpretation of p values by developing better evidence-based education and user-centered statistical software should be a top priority. Polarized discussions about which statistic scientists should use has distracted us from examining more important questions, such as asking researchers what they want to know when they conduct scientific research. Before we can improve our statistical inferences, we need to improve our statistical questions.},
  langid = {english},
  keywords = {equivalence tests,null-hypothesis testing,p values,statistical inferences}
}

@article{lakens_reproducibility_2016,
  title = {On the Reproducibility of Meta-Analyses: Six Practical Recommendations},
  shorttitle = {On the Reproducibility of Meta-Analyses},
  author = {Lakens, Dani{\"e}l and Hilgard, Joe and Staaks, Janneke},
  year = {2016},
  journal = {BMC Psychology},
  volume = {4},
  pages = {24},
  issn = {2050-7283},
  doi = {10.1186/s40359-016-0126-3},
  urldate = {2016-06-07},
  abstract = {Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions.},
  keywords = {Meta-analysis,Open science,Reporting guidelines,Reproducibility}
}

@article{lakens_sample_2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  journal = {Collabra: Psychology},
  doi = {10.31234/osf.io/9d3yf},
  urldate = {2021-05-10},
  abstract = {An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  keywords = {Experimental Design and Sample Surveys,power analysis,Quantitative Methods,sample size justification,Social and Behavioral Sciences,study design,value of information}
}

@article{lakens_simulationbased_2021,
  title = {Simulation-{{Based Power Analysis}} for {{Factorial Analysis}} of {{Variance Designs}}},
  author = {Lakens, Dani{\"e}l and Caldwell, Aaron R.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920951503},
  urldate = {2021-03-23},
  abstract = {Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure that a study is adequately powered to yield informative results with an ANOVA, researchers can perform an a priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not allow power analyses for complex designs with several within-participants factors. Moreover, power analyses often need {$\eta$}2{$p\eta$}p2{$<$}math display="inline" id="math1-2515245920951503" overflow="scroll" altimg="eq-00001.gif"{$><$}mrow{$><$}msubsup{$><$}mi mathvariant="normal"{$>\eta<$}/mi{$><$}mi{$>$}p{$<$}/mi{$><$}mn{$>$}2{$<$}/mn{$><$}/msubsup{$><$}/mrow{$><$}/math{$>$} or Cohen's f as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package Superpower and online Shiny apps to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-participants factors. Predicted effects are entered by specifying means, standard deviations, and, for within-participants factors, the correlations. The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons. The software can plot power across a range of sample sizes, can control for multiple comparisons, and can compute power when the homogeneity or sphericity assumption is violated. This Tutorial demonstrates how to perform a priori power analysis to design informative studies for main effects, interactions, and individual comparisons and highlights important factors that determine the statistical power for factorial ANOVA designs.},
  langid = {english},
  keywords = {ANOVA,hypothesis test,open materials,power analysis,sample-size justification}
}

@article{lakens_too_2017,
  title = {Too {{True}} to Be {{Bad}}: {{When Sets}} of {{Studies With Significant}} and {{Nonsignificant Findings Are Probably True}}},
  shorttitle = {Too {{True}} to Be {{Bad}}},
  author = {Lakens, Dani{\"e}l and Etz, Alexander J.},
  year = {2017},
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {8},
  pages = {875--881},
  issn = {1948-5506},
  doi = {10.1177/1948550617693058},
  urldate = {2018-02-28},
  abstract = {Psychology journals rarely publish nonsignificant results. At the same time, it is often very unlikely (or ``too good to be true'') that a set of studies yields exclusively significant results. Here, we use likelihood ratios to explain when sets of studies that contain a mix of significant and nonsignificant results are likely to be true or ``too true to be bad.'' As we show, mixed results are not only likely to be observed in lines of research but also, when observed, often provide evidence for the alternative hypothesis, given reasonable levels of statistical power and an adequately controlled low Type 1 error rate. Researchers should feel comfortable submitting such lines of research with an internal meta-analysis for publication. A better understanding of probabilities, accompanied by more realistic expectations of what real sets of studies look like, might be an important step in mitigating publication bias in the scientific literature.},
  langid = {english}
}

@article{lakens_value_2019,
  title = {The Value of Preregistration for Psychological Science: {{A}} Conceptual Analysis},
  shorttitle = {The Value of Preregistration for Psychological Science},
  author = {Lakens, Dani{\"e}l},
  year = {2019},
  journal = {Japanese Psychological Review},
  volume = {62},
  number = {3},
  pages = {221--230},
  doi = {10.24602/sjpr.62.3_221},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.},
  keywords = {hypothesis testing,meta-science,preregistration,registered reports,severity}
}

@misc{lakens_when_2023,
  title = {When and {{How}} to {{Deviate}} from a {{Preregistration}}},
  author = {Lakens, Daniel},
  year = {2023},
  month = dec,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/ha29k},
  urldate = {2023-12-18},
  abstract = {As the practice of preregistration becomes more common, researchers need guidance in how to report deviations from their preregistered statistical analysis plan. A principled approach to the use of preregistration should not treat all deviations as problematic. Deviations from a preregistered analysis plan can both reduce and increase the severity of a test, as well as increase the validity of inferences. I provide examples of how researchers can present deviations from preregistrations and evaluate the consequences of the deviation when encountering 1) unforeseen events, 2) errors in the preregistration, 3) missing information, 4) violations of untested assumptions, and 5) falsification of auxiliary hypotheses. The current manuscript aims to provide a principled approach to deciding when to deviate from a preregistration and how to report deviations from on an error-statistical philosophy grounded in methodological falsificationism. The goal is to help researchers reflect on the consequence of deviations from preregistrations by evaluating the test's severity and the validity of the inference.},
  langid = {american},
  keywords = {error control,Meta-science,open science,preregistration,Registered Reports}
}

@article{lakens_when_2024,
  title = {When and {{How}} to {{Deviate From}} a {{Preregistration}}},
  author = {Lakens, Dani{\"e}l},
  year = {2024},
  month = may,
  journal = {Collabra: Psychology},
  volume = {10},
  number = {1},
  pages = {117094},
  issn = {2474-7394},
  doi = {10.1525/collabra.117094},
  urldate = {2024-05-14},
  abstract = {As the practice of preregistration becomes more common, researchers need guidance in how to report deviations from their preregistered statistical analysis plan. A principled approach to the use of preregistration should not treat all deviations as problematic. Deviations from a preregistered analysis plan can both reduce and increase the severity of a test, as well as increase the validity of inferences. I provide examples of how researchers can present deviations from preregistrations and evaluate the consequences of the deviation when encountering 1) unforeseen events, 2) errors in the preregistration, 3) missing information, 4) violations of untested assumptions, and 5) falsification of auxiliary hypotheses. The current manuscript aims to provide a principled approach to deciding when to deviate from a preregistration and how to report deviations from an error-statistical philosophy grounded in methodological falsificationism. The goal is to help researchers reflect on the consequence of deviations from preregistrations by evaluating the test's severity and the validity of the inference.}
}

@article{lakens_why_2022,
  title = {Why {{P}} Values Are Not Measures of Evidence},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  journal = {Trends in Ecology \& Evolution},
  volume = {37},
  number = {4},
  pages = {289--290},
  issn = {0169-5347},
  doi = {10.1016/j.tree.2021.12.006},
  urldate = {2023-05-25},
  langid = {english}
}

@article{lan_discrete_1983,
  title = {Discrete {{Sequential Boundaries}} for {{Clinical Trials}}},
  author = {Lan, K. K. Gordon and DeMets, David L.},
  year = {1983},
  month = dec,
  journal = {Biometrika},
  volume = {70},
  number = {3},
  eprint = {2336502},
  eprinttype = {jstor},
  pages = {659},
  issn = {00063444},
  doi = {10.2307/2336502},
  urldate = {2015-12-03},
  annotation = {01888}
}

@article{langmuir_pathological_1989,
  title = {Pathological {{Science}}},
  author = {Langmuir, Irving and Hall, Robert N.},
  year = {1989},
  month = oct,
  journal = {Physics Today},
  volume = {42},
  number = {10},
  pages = {36--48},
  issn = {0031-9228, 1945-0699},
  doi = {10.1063/1.881205},
  urldate = {2023-07-10},
  abstract = {Irving Langmuir spent many productive years pursuing Nobel-caliber research (see the photo on the opposite page). Over the years, he also explored the subject of what he called ``pathological science.'' Although he never published his investigations in this area, on 18December 1953 at General Electric's Knolls Atomic Power Laboratory, he gave a colloquium on the subject that will long be remembered by those in his audience. This talk was a colorful account of a particular kind of pitfall into which scientists may stumble.},
  langid = {english}
}

@article{latan_crossing_2021,
  title = {Crossing the {{Red Line}}? {{Empirical Evidence}} and {{Useful Recommendations}} on {{Questionable Research Practices}} among {{Business Scholars}}},
  shorttitle = {Crossing the {{Red Line}}?},
  author = {Latan, Hengky and Chiappetta Jabbour, Charbel Jose and {Lopes de Sousa Jabbour}, Ana Beatriz and Ali, Murad},
  year = {2021},
  month = nov,
  journal = {Journal of Business Ethics},
  pages = {1--21},
  publisher = {Springer Netherlands},
  issn = {1573-0697},
  doi = {10.1007/s10551-021-04961-7},
  urldate = {2022-09-17},
  abstract = {Academic leaders in management from all over the world---including recent calls by the Academy of Management Shaw (Academy of Management Journal 60(3): 819--822, 2017)---have urged further research into the extent and use of questionable research practices (QRPs). In order to provide empirical evidence on the topic of QRPs, this work presents two linked studies. Study 1 determines the level of use of QRPs based on self-admission rates and estimated prevalence among business scholars in Indonesia. It was determined that if the level of QRP use identified in Study 1 was quite high, Study 2 would be conducted to follow-up on this result, and this was indeed the case. Study 2 examines the factors that encourage and discourage the use of QRPs in the sample analyzed. The main research findings are as follows: (a) in Study 1, we found the self-admission rates and estimated prevalence of business scholars' involvement in QRPs to be quite high when compared with studies conducted in other countries and (b) in Study 2, we found pressure for publication from universities, fear of rejection of manuscripts, meeting the expectations of reviewers, and available rewards to be the main reasons for the use of QRPs in Indonesia, whereas (c) formal sanctions and prevention efforts are factors that discourage QRPs. Recommendations for stakeholders (in this case, reviewers, editors, funders, supervisors, chancellors and others) are also provided in order to reduce the use of QRPs.},
  copyright = {2021 The Author(s)},
  langid = {english}
}

@book{laudan_science_1981,
  title = {Science and {{Hypothesis}}},
  author = {Laudan, Larry},
  year = {1981},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-015-7288-0},
  urldate = {2020-05-15},
  isbn = {978-94-015-7290-3 978-94-015-7288-0},
  langid = {english}
}

@book{laudan_science_1986,
  title = {Science and {{Values}}: {{The Aims}} of {{Science}} and {{Their Role}} in {{Scientific Debate}}},
  shorttitle = {Science and {{Values}}},
  author = {Laudan, Larry},
  year = {1986},
  month = jan,
  urldate = {2023-02-28},
  abstract = {Laudan constructs a fresh approach to a longtime problem for the philosopher of science: how to explain the simultaneous and widespread presence of both agreement and disagreement in science. Laudan critiques the logical empiricists and the post-positivists as he stresses the need for centrality and values and the interdependence of values, methods, and facts as prerequisites to solving the problems of consensus and dissent in science.},
  copyright = {Available worldwide},
  isbn = {978-0-520-05743-2},
  langid = {english}
}

@article{lawrence_lesson_2021,
  title = {The Lesson of Ivermectin: Meta-Analyses Based on Summary Data Alone Are Inherently Unreliable},
  shorttitle = {The Lesson of Ivermectin},
  author = {Lawrence, Jack M. and {Meyerowitz-Katz}, Gideon and Heathers, James A. J. and Brown, Nicholas J. L. and Sheldrick, Kyle A.},
  year = {2021},
  month = nov,
  journal = {Nature Medicine},
  volume = {27},
  number = {11},
  pages = {1853--1854},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-021-01535-y},
  urldate = {2022-04-07},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Medical research,Randomized controlled trials}
}

@book{leamer_specification_1978,
  title = {Specification {{Searches}}: {{Ad Hoc Inference}} with {{Nonexperimental Data}}},
  shorttitle = {Specification {{Searches}}},
  author = {Leamer, Edward E.},
  year = {1978},
  month = apr,
  edition = {1 edition},
  publisher = {Wiley},
  address = {New York usw.},
  abstract = {Offers a radically new approach to inference with nonexperimental data when the statistical model is ambiguously defined. Examines the process of model searching and its implications for inference. Identifies six different varieties of specification searches, discussing the inferential consequences of each in detail.},
  isbn = {978-0-471-01520-8},
  langid = {english}
}

@book{lehmann_testing_2005,
  title = {Testing Statistical Hypotheses},
  author = {Lehmann, E. L. and Romano, Joseph P.},
  year = {2005},
  series = {Springer Texts in Statistics},
  edition = {3rd ed},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-98864-1},
  lccn = {QA277 .L425 2005},
  keywords = {Statistical hypothesis testing}
}

@article{lenth_post_2007,
  title = {Post Hoc Power: Tables and Commentary},
  shorttitle = {Post Hoc Power},
  author = {Lenth, Russell V.},
  year = {2007},
  journal = {Iowa City: Department of Statistics and Actuarial Science, University of Iowa},
  urldate = {2017-09-30}
}

@article{lenth_practical_2001,
  title = {Some Practical Guidelines for Effective Sample Size Determination},
  author = {Lenth, Russell V.},
  year = {2001},
  journal = {The American Statistician},
  volume = {55},
  number = {3},
  pages = {187--193},
  doi = {10.1198/000313001317098149},
  urldate = {2017-09-30}
}

@article{leon_role_2011,
  title = {The {{Role}} and {{Interpretation}} of {{Pilot Studies}} in {{Clinical Research}}},
  author = {Leon, Andrew C. and Davis, Lori L. and Kraemer, Helena C.},
  year = {2011},
  month = may,
  journal = {Journal of psychiatric research},
  volume = {45},
  number = {5},
  pages = {626--629},
  issn = {0022-3956},
  doi = {10.1016/j.jpsychires.2010.10.008},
  urldate = {2016-02-23},
  abstract = {Pilot studies represent a fundamental phase of the research process. The purpose of conducting a pilot study is to examine the feasibility of an approach that is intended to be used in a larger scale study. The roles and limitations of pilot studies are described here using a clinical trial as an example. A pilot study can be used to evaluate the feasibility of recruitment, randomization, retention, assessment procedures, new methods, and implementation of the novel intervention., A pilot study is not a hypothesis testing study. Safety, efficacy and effectiveness are not evaluated in a pilot. Contrary to tradition, a pilot study does not provide a meaningful effect size estimate for planning subsequent studies due to the imprecision inherent in data from small samples. Feasibility results do not necessarily generalize beyond the inclusion and exclusion criteria of the pilot design., A pilot study is a requisite initial step in exploring a novel intervention or an innovative application of an intervention. Pilot results can inform feasibility and identify modifications needed in the design of a larger, ensuing hypothesis testing study. Investigators should be forthright in stating these objectives of a pilot study. Grant reviewers and other stakeholders should expect no more.},
  pmcid = {PMC3081994},
  pmid = {21035130}
}

@article{letrud_affirmative_2019,
  title = {Affirmative Citation Bias in Scientific Myth Debunking: {{A}} Three-in-One Case Study},
  shorttitle = {Affirmative Citation Bias in Scientific Myth Debunking},
  author = {Letrud, K{\aa}re and Hernes, Sigbj{\o}rn},
  year = {2019},
  month = sep,
  journal = {PLOS ONE},
  volume = {14},
  number = {9},
  pages = {e0222213},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0222213},
  urldate = {2023-08-29},
  abstract = {Several uncorroborated, false, or misinterpreted conceptions have for years been widely distributed in academic publications, thus becoming scientific myths. How can such misconceptions persist and proliferate within the inimical environment of academic criticism? Examining 613 articles we demonstrate that the reception of three myth-exposing publications is skewed by an `affirmative citation bias': The vast majority of articles citing the critical article will affirm the idea criticized. 468 affirmed the myth, 105 were neutral, while 40 took a negative stance. Once misconceptions proliferate wide and long enough, criticizing them not only becomes increasingly difficult, efforts may even contribute to the continued spreading of the myths.},
  langid = {english},
  keywords = {Citation analysis,Database searching,Encyclopedias,Learning,Opioids,Pain,Peer review,Prescription drug addiction}
}

@article{leung_1980_2017,
  title = {A 1980 {{Letter}} on the {{Risk}} of {{Opioid Addiction}}},
  author = {Leung, Pamela T.M. and Macdonald, Erin M. and Stanbrook, Matthew B. and Dhalla, Irfan A. and Juurlink, David N.},
  year = {2017},
  month = jun,
  journal = {New England Journal of Medicine},
  volume = {376},
  number = {22},
  pages = {2194--2195},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMc1700150},
  urldate = {2023-08-29},
  pmid = {28564561}
}

@article{levine_communication_2008,
  title = {A Communication Researchers' Guide to Null Hypothesis Significance Testing and Alternatives},
  author = {Levine, Timothy R. and Weber, Ren{\'e} and Park, Hee Sun and Hullett, Craig R.},
  year = {2008},
  journal = {Human Communication Research},
  volume = {34},
  number = {2},
  pages = {188--209},
  urldate = {2016-05-19}
}

@article{leys_how_2019,
  title = {How to {{Classify}}, {{Detect}}, and {{Manage Univariate}} and {{Multivariate Outliers}}, {{With Emphasis}} on {{Pre-Registration}}},
  author = {Leys, Christophe and Delacre, Marie and Mora, Youri L. and Lakens, Dani{\"e}l and Ley, Christophe},
  year = {2019},
  month = apr,
  journal = {International Review of Social Psychology},
  volume = {32},
  number = {1},
  pages = {5},
  issn = {2397-8570},
  doi = {10.5334/irsp.289},
  urldate = {2019-06-19},
  abstract = {Researchers often lack knowledge about how to deal with outliers when analyzing their data. Even more frequently, researchers do not pre-specify how they plan to manage outliers. In this paper we aim to improve research practices by outlining what you need to know about outliers. We start by providing a functional definition of outliers. We then lay down an appropriate nomenclature/classification of outliers.~This nomenclature is used to understand what kinds of outliers can be encountered and serves as a guideline to make appropriate decisions regarding the conservation, deletion, or recoding of outliers. These decisions might impact the validity of statistical inferences as well as the reproducibility of our experiments. To be able to make informed decisions about outliers you first need proper detection tools. We remind readers why the most common outlier detection methods are problematic and recommend the use of the median absolute deviation to detect univariate outliers, and of the Mahalanobis-MCD distance to detect multivariate outliers. An R package was created that can be used to easily perform these detection tests. Finally, we promote the use of pre-registration to avoid flexibility in data analysis when handling outliers. ~ Publishers note: due to a typesetting error, this paper was originally published with incorrect table numbering, where tables 2, 3, and 4 were incorrectly labelled. This was corrected soon after publication.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  keywords = {Malahanobis distance,median absolute deviation,minimum covariance determinant,outliers,preregistration,robust detection}
}

@article{linden_heterogeneity_2021,
  title = {Heterogeneity of {{Research Results}}: {{A New Perspective From Which}} to {{Assess}} and {{Promote Progress}} in {{Psychological Science}}},
  shorttitle = {Heterogeneity of {{Research Results}}},
  author = {Linden, Audrey Helen and H{\"o}nekopp, Johannes},
  year = {2021},
  month = mar,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {2},
  pages = {358--376},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620964193},
  urldate = {2021-06-03},
  abstract = {Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.},
  langid = {english},
  keywords = {heterogeneity,meta-analysis,philosophy of science,psychological research,replication,statistical power}
}

@article{lindley_statistical_1957,
  title = {A Statistical Paradox},
  author = {Lindley, Dennis V.},
  year = {1957},
  journal = {Biometrika},
  volume = {44},
  number = {1/2},
  pages = {187--192}
}

@article{lindsay_replication_2015,
  title = {Replication in {{Psychological Science}}},
  author = {Lindsay, D. Stephen},
  year = {2015},
  month = dec,
  journal = {Psychological Science},
  volume = {26},
  number = {12},
  pages = {1827--1832},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797615616374},
  urldate = {2016-03-25},
  langid = {english},
  pmid = {26553013}
}

@article{loevinger_information_1968,
  title = {The "Information Explosion."},
  author = {Loevinger, Jane},
  year = {1968},
  journal = {American Psychologist},
  volume = {23},
  number = {6},
  pages = {455--455},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/h0020800},
  abstract = {In this column, the author discusses psychological research. The author believes that too much research is published. The question explored here is, can one raise publication standards without freezing a modal conformity and excluding the spark of novelty and dissent? Two suggestions are presented. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Experimentation,Information,Psychology,Scientific Communication}
}

@book{longino_science_1990,
  title = {Science as {{Social Knowledge}}: {{Values}} and {{Objectivity}} in {{Scientific Inquiry}}},
  shorttitle = {Science as {{Social Knowledge}}},
  author = {Longino, Helen E.},
  year = {1990},
  publisher = {Princeton University Press},
  abstract = {"Helen Longino has written a timely book that fills a critical gap in the existing literature between philosophy of science and the social studies of science. Her exposition of scientific inquiry as a context-laden process provides the conceptual tools we need to understand how social expectations shape the development of science while at the same time recognizing the dependence of scientific inquiry on its interactions with natural phenomena. This is an important book precisely because there is none other quite like it." --Evelyn Fox Keller, author of "Reflections on Gender and Science" Conventional wisdom has it that the sciences, properly pursued, constitute a pure, value-free method of obtaining knowledge about the natural world. In light of the social and normative dimensions of many scientific debates, Helen Longino finds that general accounts of scientific methodology cannot support this common belief. Focusing on the notion of evidence, the author argues that a methodology powerful enough to account for theories of any scope and depth is incapable of ruling out the influence of social and cultural values in the very structuring of knowledge. The objectivity of scientific inquiry can nevertheless be maintained, she proposes, by understanding scientific inquiry as a social rather than an individual process. Seeking to open a dialogue between methodologists and social critics of the sciences, Longino develops this concept of "contextual empiricism" in an analysis of research programs that have drawn criticism from feminists. Examining theories of human evolution and of prenatal hormonal determination of "gender-role" behavior, of sex differences in cognition, and of sexualorientation, the author shows how assumptions laden with social values affect the description, presentation, and interpretation of data. In particular, Longino argues that research on the hormonal basis of "sex-differentiated behavior" involves assumptions not only about gender relations but also about human action and agency. She concludes with a discussion of the relation between science, values, and ideology, based on the work of Habermas, Foucault, Keller, and Haraway.},
  googlebooks = {S8fIbD19BisC},
  isbn = {978-0-691-02051-8},
  langid = {english},
  keywords = {Philosophy / Epistemology}
}

@article{louis_effective_2009,
  title = {Effective Communication of Standard Errors and Confidence Intervals},
  author = {Louis, Thomas A. and Zeger, Scott L.},
  year = {2009},
  month = jan,
  journal = {Biostatistics},
  volume = {10},
  number = {1},
  pages = {1--2},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxn014},
  urldate = {2022-11-15}
}

@article{lovakov_empirically_2021,
  title = {Empirically Derived Guidelines for Effect Size Interpretation in Social Psychology},
  author = {Lovakov, Andrey and Agadullina, Elena R.},
  year = {2021},
  journal = {European Journal of Social Psychology},
  volume = {51},
  number = {3},
  pages = {485--504},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2752},
  urldate = {2022-11-08},
  abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
  langid = {english},
  keywords = {Cohen's d,correlation,effect size,sample size}
}

@article{lubin_replicability_1957,
  title = {Replicability as a Publication Criterion},
  author = {Lubin, Ardie},
  year = {1957},
  journal = {American Psychologist},
  volume = {12},
  pages = {519--520},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/h0039746},
  abstract = {From time to time various suggestions have been made on how to reduce publication lag. It has been pointed out that raising the methodological standards by which articles are judged would help solve the lag problem and, at the same time, increase the quality of published articles. One particular methodological criterion which might help the present publication lag to disappear is that of replicability--requiring every author to satisfy the editor that his major results have been repeated before the article is considered for publication. Articles using replication designs which are not satisfactory to the editor could be given lowest publication priority. Articles with no attempt at replication would be rejected. There is good reason to anticipate that use of the repeatability criterion might do more than simply increase rejections and reduce publication lag. It seems likely that, under such a built-in reward system, more studies would be designed with replication in mind, the quality of replication (and generalization) designs would improve, and a great deal of overelaborate statistical analysis will disappear. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Empirical Methods,Experimental Replication,Scientific Communication}
}

@article{luttrell_replicating_2017,
  title = {Replicating and Fixing Failed Replications: {{The}} Case of Need for Cognition and Argument Quality},
  shorttitle = {Replicating and Fixing Failed Replications},
  author = {Luttrell, Andrew and Petty, Richard E. and Xu, Mengran},
  year = {2017},
  month = mar,
  journal = {Journal of Experimental Social Psychology},
  volume = {69},
  pages = {178--183},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2016.09.006},
  urldate = {2022-10-08},
  abstract = {Recent large-scale replication efforts have raised the question: how are we to interpret failures to replicate? Many have responded by pointing out conceptual or methodological discrepancies between the original and replication studies as potential explanations for divergent results as well as emphasizing the importance of contextual moderators. To illustrate the importance of accounting for discrepancies between original and replication studies as well as moderators, we turn to a recent example of a failed replication effort. Previous research has shown that individual differences in need for cognition interact with a message's argument quality to affect evaluation (Cacioppo, Petty, \& Morris, 1983). However, a recent attempt failed to replicate this outcome (Ebersole et al., 2016). We propose that the latter study's null result was due to conducting a non-optimal replication attempt. We thus conducted a new study that manipulated the key features that we propose created non-optimal conditions in the replication effort. The current results replicated the original need for cognition{\texttimes}argument quality interaction but only under the ``optimal'' conditions (closer to the original study's method and accounting for subsequently identified moderators). Under the non-optimal conditions, mirroring those used by Ebersole et al., results replicated the failure to replicate the target interaction. These findings emphasize the importance of informed replication, an approach to replication that pays close attention to ongoing developments identified in an effect's broader literature.},
  langid = {english},
  keywords = {Elaboration likelihood model,Need for cognition,Replication,Reproducibility}
}

@article{lykken_statistical_1968,
  title = {Statistical Significance in Psychological Research},
  author = {Lykken, David T.},
  year = {1968},
  journal = {Psychological Bulletin},
  volume = {70},
  number = {3, Pt.1},
  pages = {151--159},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/h0026141},
  abstract = {MOST THEORIES IN THE AREAS OF PERSONALITY, CLINICAL, AND SOCIAL PSYCHOLOGY PREDICT ONLY THE DIRECTION OF A CORRELATION, GROUP DIFFERENCE, OR TREATMENT EFFECT. SINCE THE NULL HYPOTHESIS IS NEVER STRICTLY TRUE, SUCH PREDICTIONS HAVE ABOUT A 50-50 CHANCE OF BEING CONFIRMED BY EXPERIMENT WHEN THE THEORY IN QUESTION IS FALSE, SINCE THE STATISTICAL SIGNIFICANCE OF THE RESULT IS A FUNCTION OF THE SAMPLE SIZE. CONFIRMATION OF 1 DIRECTIONAL PREDICTION GENERALLY BUILDS LITTLE CONFIDENCE IN THE THEORY BEING TESTED. MOST THEORIES SHOULD BE TESTED BY MULTIPLE CORROBORATION AND MOST EMPIRICAL GENERALIZATIONS BY CONSTRUCTIVE REPLICATION. STATISTICAL SIGNIFICANCE, PERHAPS THE LEAST IMPORTANT ATTRIBUTE OF A GOOD EXPERIMENT, IS NEVER A SUFFICIENT CONDITION FOR CLAIMING THAT (1) A THEORY HAS BEEN USEFULLY CORROBORATED, (2) A MEANINGFUL EMPIRICAL FACT HAS BEEN ESTABLISHED, OR (3) AN EXPERIMENTAL REPORT OUGHT TO BE PUBLISHED. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Mathematics (Concepts),Population (Statistics),Statistical Analysis,Statistical Correlation,Statistical Reliability,Statistical Validity,Statistical Variables,Theories}
}

@article{lyons_rethinking_2015,
  title = {Rethinking the Implications of Numerical Ratio Effects for Understanding the Development of Representational Precision and Numerical Processing across Formats},
  author = {Lyons, Ian M. and Nuerk, Hans-Christoph and Ansari, Daniel},
  year = {2015},
  journal = {Journal of Experimental Psychology: General},
  volume = {144},
  number = {5},
  pages = {1021--1035},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2222},
  doi = {10.1037/xge0000094},
  abstract = {Numerical ratio effects are a hallmark of numerical comparison tasks. Moreover, ratio effects have been used to draw strong conclusions about the nature of numerical representations, how these representations develop, and the degree to which they generalize across stimulus formats. Here, we compute ratio effects for 1,719 children from Grades K--6 for each individual separately by computing not just the average ratio effect for each person, but also the variability and statistical magnitude (effect-size) of their ratio effect. We find that individuals' ratio effect-sizes in fact increase over development, calling into question the view that decreasing ratio effects over development indicate increasing representational precision. Our data also strongly caution against the use of ratio effects in inferring the nature of symbolic number representation. While 75\% of children showed a statistically significant ratio effect for nonsymbolic comparisons, only 30\% did so for symbolic comparisons. Furthermore, whether a child's nonsymbolic ratio effect was significant did not predict whether the same was true of their symbolic ratio effect. These results undercut the notions (a) that individuals' ratio effects are indicative of representational precision in symbolic numbers, and (b) that a common process generates ratio effects in symbolic and nonsymbolic formats. Finally, for both formats, it was the variability of an individual child's ratio effect (not its slope or even effect-size) that correlated with arithmetic ability. Taken together, these results call into question many of the long-held tenets regarding the interpretation of ratio effects---especially with respect to symbolic numbers. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Development,Individual Differences,Mathematics,Mathematics (Concepts),Numbers (Numerals)}
}

@article{maccoun_blind_2015,
  title = {Blind Analysis: {{Hide}} Results to Seek the Truth},
  shorttitle = {Blind Analysis},
  author = {MacCoun, Robert and Perlmutter, Saul},
  year = {2015},
  month = oct,
  journal = {Nature},
  volume = {526},
  number = {7572},
  pages = {187--189},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/526187a},
  urldate = {2023-08-26},
  abstract = {More fields should, like particle physics, adopt blind analysis to thwart bias, urge Robert MacCoun and Saul Perlmutter.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Research management}
}

@article{mack_need_1951,
  title = {The {{Need}} for {{Replication Research}} in {{Sociology}}},
  author = {Mack, Raymond W.},
  year = {1951},
  journal = {American Sociological Review},
  volume = {16},
  number = {1},
  eprint = {2087978},
  eprinttype = {jstor},
  pages = {93--94},
  publisher = {[American Sociological Association, Sage Publications, Inc.]},
  issn = {0003-1224},
  doi = {10.2307/2087978},
  urldate = {2023-08-09}
}

@article{mahoney_psychology_1979,
  title = {Psychology of the Scientist: {{An}} Evaluative Review},
  shorttitle = {Review {{Paper}}},
  author = {Mahoney, Michael J.},
  year = {1979},
  month = aug,
  journal = {Social Studies of Science},
  volume = {9},
  number = {3},
  pages = {349--375},
  publisher = {SAGE Publications Ltd},
  issn = {0306-3127},
  doi = {10.1177/030631277900900304},
  urldate = {2023-07-16},
  abstract = {Although the social processes in scientific inquiry have received extensive analysis, psychologists have devoted relatively little attention to the thoughts, feelings, and actions of the individual scientist. This neglect has resulted in an unfortunate failure to evaluate long held assumptions about scientist behaviour. This article reviews sociological, archival, and recent experimental evidence bearing on the psychology of the scientist. These data suggest that the correspondence between scientist behaviour and accepted scientific 'ideals' may be far less than has been presumed. After briefly reappraising those ideals, it is argued that psychological research - and particularly psychological theorizing- are critical to an adequate understanding and refinement of human factors in science.},
  langid = {english}
}

@article{maier_justify_2022,
  title = {Justify Your Alpha: {{A}} Primer on Two Practical Approaches},
  shorttitle = {Justify {{Your Alpha}}},
  author = {Maier, Maximilian and Lakens, Dani{\"e}l},
  year = {2022},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.31234/osf.io/ts4r6},
  urldate = {2021-12-30},
  abstract = {The default use of an alpha level of 0.05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power p-values lower than the alpha level can be more likely when the null hypothesis is true, than when the alternative hypothesis is true (i.e., Lindley's paradox). This manuscript explains two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of 0.05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley's paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors), but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that have a better justification should improve statistical inferences and can increase the efficiency and informativeness of scientific research.},
  langid = {american},
  keywords = {Hypothesis Testing,Meta-science,Quantitative Methods,Social and Behavioral Sciences,Statistical Power,Type 1 Error,Type 2 Error}
}

@article{makel_both_2021,
  title = {Both {{Questionable}} and {{Open Research Practices Are Prevalent}} in {{Education Research}}},
  author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan A.},
  year = {2021},
  month = nov,
  journal = {Educational Researcher},
  volume = {50},
  number = {8},
  pages = {493--504},
  publisher = {American Educational Research Association},
  issn = {0013-189X},
  doi = {10.3102/0013189X211001356},
  urldate = {2022-02-07},
  abstract = {Concerns about the conduct of research are pervasive in many fields, including education. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and five open research practices. We asked them to estimate the prevalence of the practices in the field, to self-report their own use of such practices, and to estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are used by many education researchers. This baseline information will be useful as education researchers seek to understand existing social norms and grapple with whether and how to improve research practices.},
  langid = {english},
  keywords = {ethics,globalization,open science,psychology,questionable research practices,replication,research methodology,survey research}
}

@article{marshall_does_2013,
  title = {Does {{Sample Size Matter}} in {{Qualitative Research}}?: {{A Review}} of {{Qualitative Interviews}} in Is {{Research}}},
  shorttitle = {Does {{Sample Size Matter}} in {{Qualitative Research}}?},
  author = {Marshall, Bryan and Cardon, Peter and Poddar, Amit and Fontenot, Renee},
  year = {2013},
  month = sep,
  journal = {Journal of Computer Information Systems},
  volume = {54},
  number = {1},
  pages = {11--22},
  publisher = {Taylor \& Francis},
  issn = {0887-4417},
  doi = {10.1080/08874417.2013.11645667},
  urldate = {2020-12-31},
  abstract = {This study examines 83 IS qualitative studies in leading IS journals for the following purposes: (a) identifying the extent to which IS qualitative studies employ best practices of justifying sample size; (b) identifying optimal ranges of interviews for various types of qualitative research; and (c) identifying the extent to which cultural factors (such as journal of publication, number of authors, world region) impact sample size of interviews. Little or no rigor for justifying sample size was shown for virtually all of the IS studies in this dataset. Furthermore, the number of interviews conducted for qualitative studies is correlated with cultural factors, implying the subjective nature of sample size in qualitative IS studies. Recommendations are provided for minimally acceptable practices of justifying sample size of interviews in qualitative IS studies.},
  keywords = {data saturation,qualitative interviews,qualitative methodology,sample size}
}

@book{maxwell_designing_2004,
  title = {Designing Experiments and Analyzing Data: A Model Comparison Perspective},
  shorttitle = {Designing Experiments and Analyzing Data},
  author = {Maxwell, Scott E. and Delaney, Harold D.},
  year = {2004},
  edition = {2nd ed},
  publisher = {Lawrence Erlbaum Associates},
  address = {Mahwah, N.J},
  isbn = {978-0-8058-3718-6},
  lccn = {QA279 .M384 2004},
  keywords = {Experimental design}
}

@book{maxwell_designing_2017,
  title = {Designing {{Experiments}} and {{Analyzing Data}}: {{A Model Comparison Perspective}}, {{Third Edition}}},
  shorttitle = {Designing {{Experiments}} and {{Analyzing Data}}},
  author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
  year = {2017},
  month = aug,
  edition = {3 edition},
  publisher = {Routledge},
  address = {New York, NY},
  abstract = {Designing Experiments and Analyzing Data: A Model Comparison Perspective (3rd edition) offers an integrative conceptual framework for understanding experimental design and data analysis. Maxwell, Delaney, and Kelley first apply fundamental principles to simple experimental designs followed by an application of the same principles to more complicated designs. Their integrative conceptual framework better prepares readers to understand the logic behind a general strategy of data analysis that is appropriate for a wide variety of designs, which allows for the introduction of more complex topics that are generally omitted from other books. Numerous pedagogical features further facilitate understanding: examples of published research demonstrate the applicability of each chapter's content; flowcharts assist in choosing the most appropriate procedure; end-of-chapter lists of important formulas highlight key ideas and assist readers in locating the initial presentation of equations; useful programming code and tips are provided throughout the book and in associated resources available online, and extensive sets of exercises help develop a deeper understanding of the subject. Detailed solutions for some of the exercises and realistic data sets are included on the website (DesigningExperiments.com). The pedagogical approach used throughout the book enables readers to gain an overview of experimental design, from conceptualization of the research question to analysis of the data. The book and its companion website with web apps, tutorials, and detailed code are ideal for students and researchers seeking the optimal way to design their studies and analyze the resulting data.},
  isbn = {978-1-138-89228-6},
  langid = {english},
  annotation = {00000}
}

@incollection{maxwell_ethics_2011,
  title = {Ethics and Sample Size Planning},
  booktitle = {Handbook of Ethics in Quantitative Methodology},
  author = {Maxwell, Scott E. and Kelley, Ken},
  year = {2011},
  pages = {179--204},
  publisher = {Routledge}
}

@article{maxwell_sample_2008,
  title = {Sample {{Size Planning}} for {{Statistical Power}} and {{Accuracy}} in {{Parameter Estimation}}},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  year = {2008},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {59},
  number = {1},
  pages = {537--563},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.59.103006.093735},
  urldate = {2015-11-30},
  langid = {english}
}

@book{mayo_error_1996,
  title = {Error and the Growth of Experimental Knowledge},
  author = {Mayo, Deborah G.},
  year = {1996},
  series = {Science and Its Conceptual Foundations},
  publisher = {University of Chicago Press},
  address = {Chicago},
  isbn = {978-0-226-51197-9 978-0-226-51198-6},
  langid = {english},
  lccn = {QA275 .M347 1996},
  keywords = {Bayesian statistical decision theory,Error analysis (Mathematics),Philosophy,Science}
}

@article{mayo_error_2011,
  title = {Error Statistics},
  author = {Mayo, Deborah G. and Spanos, Aris},
  year = {2011},
  journal = {Philosophy of statistics},
  volume = {7},
  pages = {152--198},
  urldate = {2016-01-11}
}

@book{mayo_statistical_2018,
  title = {Statistical Inference as Severe Testing: How to Get beyond the Statistics Wars},
  shorttitle = {Statistical Inference as Severe Testing},
  author = {Mayo, Deborah G.},
  year = {2018},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  isbn = {978-1-107-05413-4},
  langid = {english},
  lccn = {QA276 .M3755 2018},
  keywords = {Deviation (Mathematics),Error analysis (Mathematics),Fallacies (Logic),Inference,Mathematical statistics},
  annotation = {00004}
}

@article{mazzolari_myths_2022,
  title = {Myths and Methodologies: {{The}} Use of Equivalence and Non-Inferiority Tests for Interventional Studies in Exercise Physiology and Sport Science},
  shorttitle = {Myths and Methodologies},
  author = {Mazzolari, Raffaele and Porcelli, Simone and Bishop, David J. and Lakens, Dani{\"e}l},
  year = {2022},
  journal = {Experimental Physiology},
  volume = {107},
  number = {3},
  pages = {201--212},
  issn = {1469-445X},
  doi = {10.1113/EP090171},
  urldate = {2022-03-05},
  abstract = {Exercise physiology and sport science have traditionally made use of the null hypothesis of no difference to make decisions about experimental interventions. In this article, we aim to review current statistical approaches typically used by exercise physiologists and sport scientists for the design and analysis of experimental interventions and to highlight the importance of including equivalence and non-inferiority studies, which address different research questions from deciding whether an effect is present. Initially, we briefly describe the most common approaches, along with their rationale, to investigate the effects of different interventions. We then discuss the main steps involved in the design and analysis of equivalence and non-inferiority studies, commonly performed in other research fields, with worked examples from exercise physiology and sport science scenarios. Finally, we provide recommendations to exercise physiologists and sport scientists who would like to apply the different approaches in future research. We hope this work will promote the correct use of equivalence and non-inferiority designs in exercise physiology and sport science whenever the research context, conditions, applications, researchers' interests or reasonable beliefs justify these approaches.},
  langid = {english},
  keywords = {intervention efficacy,methodology,statistical review}
}

@article{mccarthy_registered_2018,
  title = {Registered {{Replication Report}} on {{Srull}} and {{Wyer}} (1979)},
  author = {McCarthy, Randy J. and Skowronski, John J. and Verschuere, Bruno and Meijer, Ewout H. and Jim, Ariane and Hoogesteyn, Katherine and Orthey, Robin and Acar, Oguz A. and Aczel, Balazs and Bakos, Bence E. and Barbosa, Fernando and Baskin, Ernest and B{\`e}gue, Laurent and {Ben-Shakhar}, Gershon and Birt, Angie R. and Blatz, Lisa and Charman, Steve D. and Claesen, Aline and Clay, Samuel L. and Coary, Sean P. and Crusius, Jan and Evans, Jacqueline R. and Feldman, Noa and {Ferreira-Santos}, Fernando and Gamer, Matthias and Gerlsma, Coby and Gomes, Sara and {Gonz{\'a}lez-Iraizoz}, Marta and Holzmeister, Felix and Huber, Juergen and Huntjens, Rafaele J. C. and Isoni, Andrea and Jessup, Ryan K. and Kirchler, Michael and {klein Selle}, Nathalie and Koppel, Lina and Kovacs, Marton and Laine, Tei and Lentz, Frank and Loschelder, David D. and Ludvig, Elliot A. and Lynn, Monty L. and Martin, Scott D. and McLatchie, Neil M. and Mechtel, Mario and Nahari, Galit and {\"O}zdo{\u g}ru, Asil Ali and Pasion, Rita and Pennington, Charlotte R. and Roets, Arne and Rozmann, Nir and Scopelliti, Irene and Spiegelman, Eli and Suchotzki, Kristina and Sutan, Angela and Szecsi, Peter and Tingh{\"o}g, Gustav and Tisserand, Jean-Christian and Tran, Ulrich S. and Van Hiel, Alain and Vanpaemel, Wolf and V{\"a}stfj{\"a}ll, Daniel and Verliefde, Thomas and Vezirian, K{\'e}vin and Voracek, Martin and Warmelink, Lara and Wick, Katherine and Wiggins, Bradford J. and Wylie, Keith and Y{\i}ld{\i}z, Ezgi},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {321--336},
  issn = {2515-2459},
  doi = {10.1177/2515245918777487},
  urldate = {2020-02-07},
  abstract = {Srull and Wyer (1979) demonstrated that exposing participants to more hostility-related stimuli caused them subsequently to interpret ambiguous behaviors as more hostile. In their Experiment 1, participants descrambled sets of words to form sentences. In one condition, 80\% of the descrambled sentences described hostile behaviors, and in another condition, 20\% described hostile behaviors. Following the descrambling task, all participants read a vignette about a man named Donald who behaved in an ambiguously hostile manner and then rated him on a set of personality traits. Next, participants rated the hostility of various ambiguously hostile behaviors (all ratings on scales from 0 to 10). Participants who descrambled mostly hostile sentences rated Donald and the ambiguous behaviors as approximately 3 scale points more hostile than did those who descrambled mostly neutral sentences. This Registered Replication Report describes the results of 26 independent replications (N = 7,373 in the total sample; k = 22 labs and N = 5,610 in the primary analyses) of Srull and Wyer's Experiment 1, each of which followed a preregistered and vetted protocol. A random-effects meta-analysis showed that the protagonist was seen as 0.08 scale points more hostile when participants were primed with 80\% hostile sentences than when they were primed with 20\% hostile sentences (95\% confidence interval, CI = [0.004, 0.16]). The ambiguously hostile behaviors were seen as 0.08 points less hostile when participants were primed with 80\% hostile sentences than when they were primed with 20\% hostile sentences (95\% CI = [-0.18, 0.01]). Although the confidence interval for one outcome excluded zero and the observed effect was in the predicted direction, these results suggest that the currently used methods do not produce an assimilative priming effect that is practically and routinely detectable.},
  langid = {english},
  keywords = {hostility,impression formation,Many Labs,open data,open materials,preregistered,priming,replication}
}

@book{mcelreath_statistical_2016,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2016},
  volume = {122},
  publisher = {CRC Press},
  urldate = {2016-03-28}
}

@article{mcgrath_when_2006,
  title = {When Effect Sizes Disagree: {{The}} Case of r and d.},
  shorttitle = {When Effect Sizes Disagree},
  author = {McGrath, Robert E. and Meyer, Gregory J.},
  year = {2006},
  journal = {Psychological Methods},
  volume = {11},
  number = {4},
  pages = {386--401},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.4.386},
  urldate = {2015-11-30},
  langid = {english}
}

@article{mcgraw_common_1992,
  title = {A Common Language Effect Size Statistic},
  author = {McGraw, Kenneth O. and Wong, S. P.},
  year = {1992},
  journal = {Psychological Bulletin},
  volume = {111},
  number = {2},
  pages = {361--365},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.111.2.361},
  abstract = {Some of the shortcomings in interpretability and generalizability of the effect size statistics currently available to researchers can be overcome by a statistic that expresses how often a score sampled from one distribution will be greater than a score sampled from another distribution. The statistic, the common language effect size indicator, is easily calculated from sample means and variances (or from proportions in the case of nominal-level data). It can be used for expressing the effect observed in both independent and related sample designs and in both 2-group and n-group designs. Empirical tests show it to be robust to violations of the normality assumption, particularly when the variances in the 2 parent distributions are equal. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Effect Size (Statistical)}
}

@article{mcguire_perspectivist_2004,
  title = {A {{Perspectivist Approach}} to {{Theory Construction}}},
  author = {McGuire, William J.},
  year = {2004},
  month = may,
  journal = {Personality and Social Psychology Review},
  volume = {8},
  number = {2},
  pages = {173--182},
  issn = {1088-8683, 1532-7957},
  doi = {10.1207/s15327957pspr0802_11},
  urldate = {2015-11-30},
  langid = {english}
}

@article{mcintosh_power_2021,
  title = {Power Calculations in Single-Case Neuropsychology: {{A}} Practical Primer},
  shorttitle = {Power Calculations in Single-Case Neuropsychology},
  author = {McIntosh, Robert D. and Rittmo, Jonathan {\"O}.},
  year = {2021},
  month = feb,
  journal = {Cortex},
  volume = {135},
  pages = {146--158},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2020.11.005},
  urldate = {2022-11-08},
  abstract = {Researchers and clinicians in neuropsychology often compare individual patients against healthy control samples, to quantify evidence for cognitive-behavioural deficits and dissociations. Statistical methods for these comparisons have been developed that control Type I (false positive) errors effectively. However, remarkably little attention has been given to the power of these tests. In this practical primer, we describe, in minimally technical terms, the origins and limits of power for case--control comparisons. We argue that power calculations can play useful roles in single-case study design and interpretation, and we make suggestions for optimising power in practice. As well as providing figures, tables and tools for estimating the power of case--control comparisons, we hope to assist researchers in setting realistic expectations for what such tests can achieve in general.},
  langid = {english},
  keywords = {Deficit,Dissociation,Power,Single-case,Statistical methods}
}

@article{meehl_appraising_1990,
  title = {Appraising and Amending Theories: {{The}} Strategy of {{Lakatosian}} Defense and Two Principles That Warrant It},
  shorttitle = {Appraising and Amending Theories},
  author = {Meehl, Paul E.},
  year = {1990},
  journal = {Psychological Inquiry},
  volume = {1},
  number = {2},
  pages = {108--141},
  doi = {10.1207/s15327965pli0102_1},
  urldate = {2016-04-29}
}

@article{meehl_cliometric_2004,
  title = {Cliometric Metatheory {{III}}: {{Peircean}} Consensus, Verisimilitude and Asymptotic Method},
  shorttitle = {Cliometric Metatheory {{III}}},
  author = {Meehl, Paul E.},
  year = {2004},
  journal = {The British journal for the philosophy of science},
  volume = {55},
  number = {4},
  pages = {615--643}
}

@article{meehl_theoretical_1978,
  title = {Theoretical {{Risks}} and {{Tabular Asterisks}}: {{Sir Karl}}, {{Sir Ronald}}, and the {{Slow Progress}} of {{Soft Psychology}}},
  shorttitle = {Theoretical {{Risks}} and {{Tabular Asterisks}}},
  author = {Meehl, Paul E.},
  year = {1978},
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {46},
  number = {4},
  pages = {806--834},
  doi = {10.1037/0022-006X.46.4.806},
  urldate = {2016-04-10}
}

@article{meehl_why_1990,
  title = {Why {{Summaries}} of {{Research}} on {{Psychological Theories}} Are {{Often Uninterpretable}}:},
  shorttitle = {Why {{Summaries}} of {{Research}} on {{Psychological Theories}} Are {{Often Uninterpretable}}},
  author = {Meehl, Paul E.},
  year = {1990},
  journal = {Psychological Reports},
  volume = {66},
  number = {1},
  pages = {195--244},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  doi = {10.2466/pr0.1990.66.1.195},
  urldate = {2020-02-28},
  abstract = {Null hypothesis testing of correlational predictions from weak substantive theories in soft psychology is subject to the influence of ten obfuscating factors wh...},
  langid = {english}
}

@article{meehl-theory-testing_1967,
  title = {Theory-Testing in Psychology and Physics: {{A}} Methodological Paradox},
  shorttitle = {Theory-Testing in Psychology and Physics},
  author = {Meehl, Paul E.},
  year = {1967},
  journal = {Philosophy of science},
  eprint = {186099},
  eprinttype = {jstor},
  pages = {103--115},
  urldate = {2016-04-10}
}

@article{melara_driven_2003,
  title = {Driven by Information: {{A}} Tectonic Theory of {{Stroop}} Effects.},
  shorttitle = {Driven by Information},
  author = {Melara, Robert D. and Algom, Daniel},
  year = {2003},
  journal = {Psychological Review},
  volume = {110},
  number = {3},
  pages = {422--471},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.110.3.422},
  urldate = {2015-12-01},
  langid = {english}
}

@article{mellers_frequency_2001,
  title = {Do Frequency Representations Eliminate Conjunction Effects? {{An}} Exercise in Adversarial Collaboration},
  shorttitle = {Do {{Frequency Representations Eliminate Conjunction Effects}}?},
  author = {Mellers, Barbara and Hertwig, Ralph and Kahneman, Daniel},
  year = {2001},
  month = jul,
  journal = {Psychological Science},
  volume = {12},
  number = {4},
  pages = {269--275},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00350},
  urldate = {2021-04-12},
  abstract = {The present article offers an approach to scientific debate called adversarial collaboration. The approach requires both parties to agree on empirical tests for resolving a dispute and to conduct these tests with the help of an arbiter. In dispute were Hertwig's claims that frequency formats eliminate conjunction effects and that the conjunction effects previously reported by Kahneman and Tversky occurred because some participants interpreted the word ``and'' in ``bank tellers and feminists'' as a union operator. Hertwig proposed two new conjunction phrases, ``and are'' and ``who are,'' that would eliminate the ambiguity. Kahneman disagreed with Hertwig's predictions for ``and are,'' but agreed with his predictions for ``who are.'' Mellers served as arbiter. Frequency formats by themselves did not eliminate conjunction effects with any of the phrases, but when filler items were removed, conjunction effects disappeared with Hertwig's phrases. Kahneman and Hertwig offer different interpretations of the findings. We discuss the benefits of adversarial collaboration over replies and rejoinders, and present a suggested protocol for adversarial collaboration.},
  langid = {english}
}

@article{merton_note_1942,
  title = {A {{Note}} on {{Science}} and {{Democracy}}},
  author = {Merton, Robert K.},
  year = {1942},
  journal = {Journal of Legal and Political Sociology},
  volume = {1},
  pages = {115--126}
}

@article{meyners_equivalence_2012,
  title = {Equivalence Tests -- {{A}} Review},
  author = {Meyners, Michael},
  year = {2012},
  month = dec,
  journal = {Food Quality and Preference},
  volume = {26},
  number = {2},
  pages = {231--245},
  issn = {09503293},
  doi = {10.1016/j.foodqual.2012.05.003},
  urldate = {2016-08-06},
  langid = {english}
}

@article{meyvis_increasing_2018,
  title = {Increasing the {{Power}} of {{Your Study}} by {{Increasing}} the {{Effect Size}}},
  author = {Meyvis, Tom and Van Osselaer, Stijn M J},
  year = {2018},
  month = feb,
  journal = {Journal of Consumer Research},
  volume = {44},
  number = {5},
  pages = {1157--1173},
  issn = {0093-5301},
  doi = {10.1093/jcr/ucx110},
  urldate = {2020-12-30},
  abstract = {As in other social sciences, published findings in consumer research tend to overestimate the size of the effect being investigated, due to both file drawer effects and abuse of researcher degrees of freedom, including opportunistic analysis decisions. Given that most effect sizes are substantially smaller than would be apparent from published research, there has been a widespread call to increase power by increasing sample size. We propose that, aside from increasing sample size, researchers can also increase power by boosting the effect size. If done correctly, removing participants, using covariates, and optimizing experimental designs, stimuli, and measures can boost effect size without inflating researcher degrees of freedom. In fact, careful planning of studies and analyses to maximize effect size is essential to be able to study many psychologically interesting phenomena when massive sample sizes are not feasible.}
}

@book{millar_maximum_2011,
  title = {Maximum Likelihood Estimation and Inference: With Examples in {{R}}, {{SAS}}, and {{ADMB}}},
  shorttitle = {Maximum Likelihood Estimation and Inference},
  author = {Millar, R. B.},
  year = {2011},
  series = {Statistics in Practice},
  publisher = {Wiley},
  address = {Chichester, West Sussex},
  isbn = {978-0-470-09482-2},
  lccn = {QA276.8 .M55 2011},
  keywords = {Chance,Estimation theory,Mathematical models}
}

@article{miller_quest_2019,
  title = {The Quest for an Optimal Alpha},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2019},
  month = jan,
  journal = {PLOS ONE},
  volume = {14},
  number = {1},
  pages = {e0208631},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0208631},
  urldate = {2019-01-04},
  abstract = {Researchers who analyze data within the framework of null hypothesis significance testing must choose a critical ``alpha'' level, {$\alpha$}, to use as a cutoff for deciding whether a given set of data demonstrates the presence of a particular effect. In most fields, {$\alpha$} = 0.05 has traditionally been used as the standard cutoff. Many researchers have recently argued for a change to a more stringent evidence cutoff such as {$\alpha$} = 0.01, 0.005, or 0.001, noting that this change would tend to reduce the rate of false positives, which are of growing concern in many research areas. Other researchers oppose this proposed change, however, because it would correspondingly tend to increase the rate of false negatives. We show how a simple statistical model can be used to explore the quantitative tradeoff between reducing false positives and increasing false negatives. In particular, the model shows how the optimal {$\alpha$} level depends on numerous characteristics of the research area, and it reveals that although {$\alpha$} = 0.05 would indeed be approximately the optimal value in some realistic situations, the optimal {$\alpha$} could actually be substantially larger or smaller in other situations. The importance of the model lies in making it clear what characteristics of the research area have to be specified to make a principled argument for using one {$\alpha$} level rather than another, and the model thereby provides a blueprint for researchers seeking to justify a particular {$\alpha$} level.},
  langid = {english},
  keywords = {Decision theory,Economic growth,Health economics,Medicine and health sciences,Psychology,Publication ethics,Statistical methods,Statistical models},
  annotation = {00000}
}

@article{miller_what_2009,
  title = {What Is the Probability of Replicating a Statistically Significant Effect?},
  author = {Miller, Jeff},
  year = {2009},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {4},
  pages = {617--640},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.4.617},
  urldate = {2015-12-01},
  langid = {english}
}

@article{mitroff_norms_1974,
  title = {Norms and {{Counter-Norms}} in a {{Select Group}} of the {{Apollo Moon Scientists}}: {{A Case Study}} of the {{Ambivalence}} of {{Scientists}}},
  shorttitle = {Norms and {{Counter-Norms}} in a {{Select Group}} of the {{Apollo Moon Scientists}}},
  author = {Mitroff, Ian I.},
  year = {1974},
  journal = {American Sociological Review},
  volume = {39},
  number = {4},
  eprint = {2094423},
  eprinttype = {jstor},
  pages = {579--595},
  publisher = {[American Sociological Association, Sage Publications, Inc.]},
  issn = {0003-1224},
  doi = {10.2307/2094423},
  urldate = {2023-08-04},
  abstract = {This paper describes a three and a half year study conducted over the course of the Apollo lunar missions with forty-two of the most prestigious scientists who studied the lunar rocks. The paper supports the Merton-E. Barber concept of sociological ambivalence, that social institutions reflect potentially conflicting sets of norms. The paper offers a set of counter-norms for science, arguing that if the norm of universalism is rooted in the impersonal character of science, an opposing counter-norm is rooted in the personal character of science. The paper also argues that not only is sociological ambivalence a characteristic of science, but it seems necessary for the existence and ultimate rationality of science.}
}

@article{moe_should_1984,
  title = {Should the {{Nazi Research Data Be Cited}}?},
  author = {Moe, Kristine},
  year = {1984},
  journal = {The Hastings Center Report},
  volume = {14},
  number = {6},
  eprint = {3561733},
  eprinttype = {jstor},
  pages = {5--7},
  publisher = {[Hastings Center, Wiley]},
  issn = {0093-0334},
  doi = {10.2307/3561733},
  urldate = {2022-09-25}
}

@article{moran_know_2022,
  title = {I Know It's Bad, but {{I}} Have Been Pressured into It: {{Questionable}} Research Practices among Psychology Students in {{Canada}}},
  shorttitle = {I Know It's Bad, but {{I}} Have Been Pressured into It},
  author = {Moran, Chelsea and link will open in a new window {Link to external site}, this and Richard, Alexandra and link will open in a new window {Link to external site}, this and Wilson, Kaitlin and Twomey, Rosie and link will open in a new window {Link to external site}, this and Coroiu, Adina and link will open in a new window {Link to external site}, this},
  year = {2022},
  month = mar,
  journal = {Canadian Psychology/Psychologie canadienne},
  publisher = {Educational Publishing Foundation},
  issn = {0708-5591},
  doi = {10.1037/cap0000326},
  urldate = {2022-09-17},
  abstract = {Questionable research practices (QRPs) have been identified as a driving force of the replication crisis in the field of psychological science. The aim of this study was to assess the frequency and reasons for QRP use among psychology students in Canadian universities. Participants were psychology students attending Canadian universities recruited via online advertising and email invitations. Respondents were asked how often they and others engaged in seven QRPs, to estimate the proportion of psychology research impacted by each QRP and how acceptable they found each QRP. Data were collected through Likert-type scale survey items and open-ended text responses between May 2020 and January 2021, and analyzed using descriptive statistics and thematic analysis. Data from 425 psychology students are summarized (40\% undergraduate, 59\% graduate, 1\% postdoctoral fellows). Overall, 64\% of participants reported using at least one QRP, whereas 79\% reported having observed others engaging in at least one QRP. The most frequently reported QRPs were p-hacking (46\%), not submitting null results for publication (31\%), and excluding outcome measures (30\%). These QRPs were also the most frequently observed in others, estimated to be the most prevalent in the field, and rated as the most acceptable. Qualitative findings revealed that students are aware of external pressures that promote QRP use and offered several ideas for alternatives and solutions. The results of this study highlight the need to examine the pedagogical standards and cultural norms in academia that may promote or normalize QRPs in psychological science. (PsycInfo Database Record (c) 2022 APA, all rights reserved) (Source: journal abstract) Certain research practices are deemed ``questionable'' because they can contribute to unreliable conclusions that fail to replicate in subsequent research. Most Canadian psychology students in this study have used questionable research practices (QRPs). Results of this study suggest that students are observing other researchers use QRPs and estimate that these practices are prevalent in psychological science more broadly. Despite this, most students in this study believe that QRPs are not acceptable. Results also suggest that students use QRPs in response to pressure and incentives from the academic system. There is a need to address the factors contributing to the use of QRPs in Canadian psychology settings to improve the quality of psychological science. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  copyright = {{\copyright} 2022, Canadian Psychological Association},
  langid = {english},
  keywords = {Behavioral Sciences (major),College Students (major),Experimental Ethics (major),Experimental Replication (major),Open Data,Psychology Education (major)},
  annotation = {(US)}
}

@article{morey_fallacy_2016,
  title = {The Fallacy of Placing Confidence in Confidence Intervals},
  author = {Morey, Richard D. and Hoekstra, Rink and Rouder, Jeffrey N. and Lee, Michael D. and Wagenmakers, Eric-Jan},
  year = {2016},
  journal = {Psychonomic bulletin \& review},
  volume = {23},
  number = {1},
  pages = {103--123},
  urldate = {2016-07-03}
}

@misc{morey_power_2020,
  type = {Blog},
  title = {Power and Precision},
  author = {Morey, Richard D.},
  year = {2020},
  month = jun,
  abstract = {Why the push for replacing ``power'' with ``precision'' is misguided},
  howpublished = {https://medium.com/@richarddmorey/power-and-precision-47f644ddea5e}
}

@article{morey_preregistered_2021,
  title = {A Pre-Registered, Multi-Lab Non-Replication of the Action-Sentence Compatibility Effect ({{ACE}})},
  author = {Morey, Richard D. and Kaschak, Michael P. and {D{\'i}ez-{\'A}lamo}, Antonio M. and Glenberg, Arthur M. and Zwaan, Rolf A. and Lakens, Dani{\"e}l and Ib{\'a}{\~n}ez, Agust{\'i}n and Garc{\'i}a, Adolfo and Gianelli, Claudia and Jones, John L. and Madden, Julie and Alifano, Florencia and Bergen, Benjamin and Bloxsom, Nicholas G. and Bub, Daniel N. and Cai, Zhenguang G. and Chartier, Christopher R. and Chatterjee, Anjan and Conwell, Erin and Cook, Susan Wagner and Davis, Joshua D. and Evers, Ellen R. K. and Girard, Sandrine and Harter, Derek and Hartung, Franziska and Herrera, Eduar and Huettig, Falk and Humphries, Stacey and Juanchich, Marie and K{\"u}hne, Katharina and Lu, Shulan and Lynes, Tom and Masson, Michael E. J. and Ostarek, Markus and Pessers, Sebastiaan and Reglin, Rebecca and Steegen, Sara and Thiessen, Erik D. and Thomas, Laura E. and Trott, Sean and Vandekerckhove, Joachim and Vanpaemel, Wolf and Vlachou, Maria and Williams, Kristina and {Ziv-Crispel}, Noam},
  year = {2021},
  month = nov,
  journal = {Psychonomic Bulletin \& Review},
  issn = {1531-5320},
  doi = {10.3758/s13423-021-01927-8},
  urldate = {2022-01-10},
  abstract = {The Action-sentence Compatibility Effect (ACE) is a well-known demonstration of the role of motor activity in the comprehension of language. Participants are asked to make sensibility judgments on sentences by producing movements toward the body or away from the body. The ACE is the finding that movements are faster when the direction of the movement (e.g., toward) matches the direction of the action in the to-be-judged sentence (e.g., Art gave you the pen describes action toward you). We report on a pre-registered, multi-lab replication of one version of the ACE. The results show that none of the 18 labs involved in the study observed a reliable ACE, and that the meta-analytic estimate of the size of the ACE was essentially zero.},
  langid = {english}
}

@article{morey_why_2016,
  title = {Why Most of Psychology Is Statistically Unfalsifiable},
  author = {Morey, Richard D and Lakens, Dani{\"e}l},
  year = {2016},
  langid = {english}
}

@article{morris_using_2019,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = {2019},
  month = may,
  journal = {Statistics in Medicine},
  volume = {38},
  number = {11},
  eprint = {1712.03198},
  pages = {2074--2102},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.8086},
  urldate = {2019-08-06},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudorandom sampling. The key strength of simulation studies is the ability to understand the behaviour of statistical methods because some 'truth' (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analysed and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting and presentation. In particular, this tutorial provides: a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods and performance measures ('ADEMP'); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine that included at least one simulation study and identify areas for improvement.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology}
}

@article{morse_significance_1995,
  title = {The {{Significance}} of {{Saturation}}},
  author = {Morse, Janice M.},
  year = {1995},
  month = may,
  journal = {Qualitative Health Research},
  volume = {5},
  number = {2},
  pages = {147--149},
  publisher = {SAGE Publications Inc},
  issn = {1049-7323},
  doi = {10.1177/104973239500500201},
  urldate = {2021-01-02},
  langid = {english}
}

@incollection{moscovici_society_1972,
  title = {Society and Theory in Social Psychology.},
  shorttitle = {The Context of Social Psychology},
  booktitle = {Context of Social Psychology},
  author = {Moscovici, Serge},
  year = {1972},
  pages = {17--81}
}

@article{moshontz_psychological_2018,
  title = {The {{Psychological Science Accelerator}}: {{Advancing}} Psychology through a Distributed Collaborative Network},
  shorttitle = {The {{Psychological Science Accelerator}}},
  author = {Moshontz, Hannah and Campbell, Lorne and Ebersole, Charles R. and IJzerman, Hans and Urry, Heather L. and Forscher, Patrick S. and Grahe, Jon E. and McCarthy, Randy J. and Musser, Erica D. and Antfolk, Jan},
  year = {2018},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {501--515},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  doi = {10.1177/2515245918797607}
}

@article{motyl_state_2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  pages = {34--58},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315},
  doi = {10.1037/pspa0000084},
  abstract = {The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field's research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003--2004 and 2013--2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers' intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003--2004 may not be as bad as many feared, and (d) research published in 2013--2014 shows some improvement over research published in 2003--2004, a result that suggests the field is evolving in a positive direction. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Best Practices,Experimental Ethics,Experimental Replication,Experimentation,Personality Differences,Psychologist Attitudes,Sciences,Social Psychology,Social Sciences}
}

@article{mrozek_what_2002,
  title = {What Determines the Value of Life? A Meta-Analysis},
  shorttitle = {What Determines the Value of Life?},
  author = {Mrozek, Janusz R. and Taylor, Laura O.},
  year = {2002},
  journal = {Journal of Policy Analysis and Management},
  volume = {21},
  number = {2},
  pages = {253--270},
  issn = {1520-6688},
  doi = {10.1002/pam.10026},
  urldate = {2020-03-07},
  abstract = {A large literature has developed in which labor market contracts are used to estimate the value of a statistical life (VSL). Reported estimates of the VSL vary substantially, from less than \$100,000 to more than \$25 million. This research uses meta-analysis to quantitatively assess the VSL literature. Results from existing studies are pooled to identify the systematic relationships between VSL estimates and each study's particular features, such as the sample composition and research methods. This meta-analysis suggests that a VSL range of approximately \$1.5 million to \$2.5 million (in 1998 dollars) is what can be reasonably inferred from past labor-market studies when ``best practice'' assumptions are invoked. This range is considerably below many previous qualitative reviews of this literature. {\copyright} 2002 by the Association for Public Policy Analysis and Management.},
  langid = {english}
}

@article{mudge_setting_2012,
  title = {Setting an {{Optimal}} {$\alpha$} {{That Minimizes Errors}} in {{Null Hypothesis Significance Tests}}},
  author = {Mudge, Joseph F. and Baker, Leanne F. and Edge, Christopher B. and Houlahan, Jeff E.},
  year = {2012},
  month = feb,
  journal = {PLOS ONE},
  volume = {7},
  number = {2},
  pages = {e32734},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0032734},
  urldate = {2017-08-20},
  abstract = {Null hypothesis significance testing has been under attack in recent years, partly owing to the arbitrary nature of setting {$\alpha$} (the decision-making threshold and probability of Type I error) at a constant value, usually 0.05. If the goal of null hypothesis testing is to present conclusions in which we have the highest possible confidence, then the only logical decision-making threshold is the value that minimizes the probability (or occasionally, cost) of making errors. Setting {$\alpha$} to minimize the combination of Type I and Type II error at a critical effect size can easily be accomplished for traditional statistical tests by calculating the {$\alpha$} associated with the minimum average of {$\alpha$} and {$\beta$} at the critical effect size. This technique also has the flexibility to incorporate prior probabilities of null and alternate hypotheses and/or relative costs of Type I and Type II errors, if known. Using an optimal {$\alpha$} results in stronger scientific inferences because it estimates and minimizes both Type I errors and relevant Type II errors for a test. It also results in greater transparency concerning assumptions about relevant effect size(s) and the relative costs of Type I and II errors. By contrast, the use of {$\alpha$} = 0.05 results in arbitrary decisions about what effect sizes will likely be considered significant, if real, and results in arbitrary amounts of Type II error for meaningful potential effect sizes. We cannot identify a rationale for continuing to arbitrarily use {$\alpha$} = 0.05 for null hypothesis significance tests in any field, when it is possible to determine an optimal {$\alpha$}.},
  keywords = {Agricultural soil science,Decision making,Experimental design,Freshwater fish,Gene expression,Lakes,Research errors,Shores}
}

@article{mullan_town_1985,
  title = {The Town Meeting for Technology: {{The}} Maturation of Consensus Conferences},
  shorttitle = {The {{Town Meeting}} for {{Technology}}},
  author = {Mullan, Fitzhugh and Jacoby, Itzhak},
  year = {1985},
  month = aug,
  journal = {JAMA},
  volume = {254},
  number = {8},
  pages = {1068--1072},
  issn = {0098-7484},
  doi = {10.1001/jama.1985.03360080080035},
  urldate = {2021-04-12},
  abstract = {DURING the past 7 1/2 years, the National Institutes of Health (NIH) has sponsored 50 consensus development conferences assessing a wide diversity of important biomedical topics.The first five years of this new effort were a time of experimentation. Formats and approaches of these first-generation conferences changed to some extent with new topics. Gradually, however, a set of common principles emerged for conducting a consensus conference effectively. Starting in 1982, the second generation of consensus conferences were held in conformance with these principles. Careful assessment of these more recent experiences stimulated plans for a new approach to this effort, involving formal methods for data synthesis. At the onset of a new generation of consensus conferences, it is worthwhile to examine the evolution of the consensus development process to its current status and its potential for further growth as a part of the complex decision-making apparatus of our health care system.}
}

@article{mulligan_peer_2013,
  title = {Peer Review in a Changing World: {{An}} International Study Measuring the Attitudes of Researchers},
  shorttitle = {Peer Review in a Changing World},
  author = {Mulligan, Adrian and Hall, Louise and Raphael, Ellen},
  year = {2013},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {64},
  number = {1},
  pages = {132--161},
  issn = {1532-2890},
  doi = {10.1002/asi.22798},
  urldate = {2023-08-26},
  abstract = {This large-scale international study measures the attitudes of more than 4,000 researchers toward peer review. In 2009, 40,000 authors of research papers from across the globe were invited to complete an online survey. Researchers were asked to rate a number of general statements about peer review, and then a subset of respondents, who had themselves peer reviewed, rated a series of statements concerning their experience of peer review. The study found that the peer review process is highly regarded by the vast majority of researchers and considered by most to be essential to the communication of scholarly research. Nine out of 10 authors believe that peer review improved the last paper they published. Double-blind peer review is considered the most effective form of peer review. Nearly three quarters of researchers think that technological advances are making peer review more effective. Most researchers believe that although peer review should identify fraud, it is very difficult for it to do so. Reviewers are committed to conducting peer review in the future and believe that simple practical steps, such as training new reviewers would further improve peer review.},
  copyright = {{\copyright} 2012 ASIS\&T},
  langid = {english},
  keywords = {evaluation,publishing,questionnaires}
}

@book{murphy_statistical_2014,
  title = {Statistical Power Analysis: A Simple and General Model for Traditional and Modern Hypothesis Tests},
  shorttitle = {Statistical Power Analysis},
  author = {Murphy, Kevin R. and Myors, Brett and Wolach, Allen H.},
  year = {2014},
  edition = {Fourth edition},
  publisher = {Routledge, Taylor \& Francis Group},
  address = {New York},
  isbn = {978-1-84872-587-4 978-1-84872-588-1},
  lccn = {QA277 .M87 2014},
  keywords = {Statistical hypothesis testing,Statistical power analysis}
}

@article{murphy_testing_1999,
  title = {Testing the Hypothesis That Treatments Have Negligible Effects: {{Minimum-effect}} Tests in the General Linear Model.},
  shorttitle = {Testing the Hypothesis That Treatments Have Negligible Effects},
  author = {Murphy, Kevin R. and Myors, Brett},
  year = {1999},
  journal = {Journal of Applied Psychology},
  volume = {84},
  number = {2},
  pages = {234--248},
  doi = {10.1037/0021-9010.84.2.234},
  urldate = {2016-08-22}
}

@book{NAP12192,
  title = {On Being a Scientist: {{A}} Guide to Responsible Conduct in Research: {{Third}} Edition},
  author = {{National Academy of Sciences} and {National Academy of Engineering} and {Institute of Medicine}},
  year = {2009},
  publisher = {The National Academies Press},
  address = {Washington, DC},
  doi = {10.17226/12192},
  abstract = {The scientific research enterprise is built on a foundation of trust. Scientists trust that the results reported by others are valid. Society trusts that the results of research reflect an honest attempt by scientists to describe the world accurately and without bias. But this trust will endure only if the scientific community devotes itself to exemplifying and transmitting the values associated with ethical scientific conduct.Being a Scientist was designed to supplement the informal lessons in ethics provided by research supervisors and mentors. The book describes the ethical foundations of scientific practices and some of the personal and professional issues that researchers encounter in their work. It applies to all forms of research{\u 2}014whether in academic, industrial, or governmental settings-and to all scientific disciplines. third edition of On Being a Scientist reflects developments since the publication of the original edition in 1989 and a second edition in 1995. A continuing feature of this edition is the inclusion of a number of hypothetical scenarios offering guidance in thinking about and discussing these scenarios.Being a Scientist is aimed primarily at graduate students and beginning researchers, but its lessons apply to all scientists at all stages of their scientific careers.},
  isbn = {978-0-309-11970-2}
}

@article{neher_probability_1967,
  title = {Probability {{Pyramiding}}, {{Research Error}} and the {{Need}} for {{Independent Replication}}},
  author = {Neher, Andrew},
  year = {1967},
  month = apr,
  journal = {The Psychological Record},
  volume = {17},
  number = {2},
  pages = {257--262},
  issn = {2163-3452},
  doi = {10.1007/BF03393713},
  urldate = {2023-07-23},
  abstract = {Screening of results of behavioral science investigation occurs at several successive stages from the individual analysis to final publication, and selection is partly determined by a low probability that a result could have occurred by chance (i.e., a low p level). It is demonstrated that a relatively small degree of such selection is sufficient to pyramid the p level to many times its reported .01 or .05 size, resulting in a large proportion of fallacious ``findings'' in the behavioral science literature. This probability pyramid is one of a class of serious research errors which can be adequately reduced only through the practice of independent replication, the adoption of which is crucial to the behavioral sciences.},
  langid = {english}
}

@article{nemeth_devil_2001,
  title = {Devil's Advocate versus Authentic Dissent: Stimulating Quantity and Quality},
  shorttitle = {Devil's Advocate versus Authentic Dissent},
  author = {Nemeth, Charlan and Brown, Keith and Rogers, John},
  year = {2001},
  journal = {European Journal of Social Psychology},
  volume = {31},
  number = {6},
  pages = {707--720},
  issn = {1099-0992},
  doi = {10.1002/ejsp.58},
  urldate = {2020-04-23},
  abstract = {Given the relationship between uniformity of views, premature adoption of a preferred solution and poor decision making, many suggestions have been aimed at fostering dissent, including the usage of a `devil's advocate.' The hope is that such a mechanism will stimulate the kinds of reconsideration, better information processing and decision making as has been found to be stimulated by authentic dissent. In a prior study comparing these two processes, devil's advocate appeared to foster thinking that was primarily aimed at cognitive bolstering of the initial viewpoint rather than stimulate divergent thought. While that study left the actual position of the DA unknown, the present study compared conditions where the devil's advocate position was known (and consistent or inconsistent with the assigned position) or unknown. It further utilized quantity and quality of solutions as a dependent measure rather than simply cognitive activity. Results indicated that the authentic minority was superior to all three forms of `devil's advocate,' again underscoring the value and importance of authenticity and the difficulty in cloning such authenticity by role-playing techniques. Copyright {\copyright} 2001 John Wiley \& Sons, Ltd.},
  langid = {english}
}

@article{neyman_inductive_1957,
  title = {"{{Inductive Behavior}}" as a {{Basic Concept}} of {{Philosophy}} of {{Science}}},
  author = {Neyman, Jerzy},
  year = {1957},
  journal = {Revue de l'Institut International de Statistique / Review of the International Statistical Institute},
  volume = {25},
  number = {1/3},
  eprint = {1401671},
  eprinttype = {jstor},
  pages = {7},
  issn = {03731138},
  doi = {10.2307/1401671},
  urldate = {2015-11-30}
}

@article{neyman_problem_1933,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses.},
  author = {Neyman, Jerzy and Pearson, E. S.},
  year = {1933},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  volume = {231},
  number = {694-706},
  pages = {289--337},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.1933.0009},
  urldate = {2016-01-19},
  langid = {english}
}

@article{neyman_use_1928,
  title = {On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: {{Part I}}},
  author = {Neyman, Jerzy and Pearson, E. S.},
  year = {1928},
  month = dec,
  journal = {Biometrika},
  volume = {20A},
  number = {1-2},
  pages = {175--240},
  issn = {0006-3444},
  doi = {10.1093/biomet/20A.1-2.175},
  urldate = {2023-08-21}
}

@article{nickerson_confirmation_1998,
  title = {Confirmation Bias: {{A}} Ubiquitous Phenomenon in Many Guises},
  shorttitle = {Confirmation Bias},
  author = {Nickerson, Raymond S.},
  year = {1998},
  journal = {Review of general psychology},
  volume = {2},
  number = {2},
  pages = {175--220},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}

@article{nickerson_null_2000,
  title = {Null Hypothesis Significance Testing: {{A}} Review of an Old and Continuing Controversy.},
  shorttitle = {Null Hypothesis Significance Testing},
  author = {Nickerson, Raymond S.},
  year = {2000},
  journal = {Psychological Methods},
  volume = {5},
  number = {2},
  pages = {241--301},
  issn = {1082-989X},
  doi = {10.1037//1082-989X.5.2.241},
  urldate = {2015-12-03},
  langid = {english}
}

@book{niiniluoto_critical_1999,
  title = {Critical {{Scientific Realism}}},
  author = {Niiniluoto, Ilkka},
  year = {1999},
  publisher = {Oxford University Press},
  abstract = {Ilkka Niiniluoto comes to the rescue of scientific realism, showing that reports of its death have been greatly exaggerated. Philosophical realism holds that the aim of a particular discourse is to make true statements about its subject-matter. Niiniluoto surveys the different varieties ofrealism in ontology, semantics, epistemology, theory construction, and methodology. He then sets out his own original version, and defends it against competing theories in the philosophy of science. Niiniluoto's critical scientific realism is founded upon the notion of truth as correspondencebetween language and reality, and characterizes scientific progress in terms of increasing truthlikeness. This makes it possible not only to take seriously, but also to make precise, the troublesome idea that scientific theories typically are false but nevertheless close to the truth.},
  googlebooks = {Ng\_p\_3XCHxAC},
  isbn = {978-0-19-823833-1},
  langid = {english},
  keywords = {Philosophy / Metaphysics,Science / Philosophy & Social Aspects}
}

@article{niiniluoto_verisimilitude_1998,
  title = {Verisimilitude: {{The Third Period}}},
  author = {Niiniluoto, Ilkka},
  year = {1998},
  journal = {The British Journal for the Philosophy of Science},
  volume = {49},
  pages = {1--29},
  urldate = {2017-06-15}
}

@article{norman_truly_2004,
  title = {The Truly Remarkable Universality of Half a Standard Deviation: Confirmation through Another Look},
  shorttitle = {The Truly Remarkable Universality of Half a Standard Deviation},
  author = {Norman, Geoffrey R. and Sloan, Jeff A. and Wyrwich, Kathleen W.},
  year = {2004},
  journal = {Expert review of pharmacoeconomics \& outcomes research},
  volume = {4},
  number = {5},
  pages = {581--585}
}

@article{nosek_registered_2014,
  title = {Registered Reports: {{A}} Method to Increase the Credibility of Published Results},
  shorttitle = {Registered Reports},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {137--141},
  issn = {2151-2590(Electronic);1864-9335(Print)},
  doi = {10.1027/1864-9335/a000192},
  abstract = {The published journal article is the primary means of communicating scientific ideas, methods, and empirical data. Not all ideas and data get published. In the present scientific culture, novel and positive results are considered more publishable than replications and negative results. This creates incentives to avoid or ignore replications and negative results, even at the expense of accuracy (Giner-Sorolla, 2012; Nosek, Spies, \& Motyl, 2012). As a consequence, replications (Makel, Plucker, \& Hegarty, 2012) and negative results (Fanelli, 2010; Sterling, 1959) are rare in the published literature. This insight is not new, but the culture is resistant to change. This article introduces the first known journal issue in any discipline consisting exclusively of preregistered replication studies. It demonstrates that replications have substantial value, and that incentives can be changed.},
  copyright = {(c) 2014 APA, all rights reserved},
  keywords = {*Credibility,*Experimental Replication,*Scientific Communication,Sciences}
}

@article{nosek_what_2020,
  title = {What Is Replication?},
  author = {Nosek, Brian A. and Errington, Timothy M.},
  year = {2020},
  month = mar,
  journal = {PLOS Biology},
  volume = {18},
  number = {3},
  pages = {e3000691},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000691},
  urldate = {2024-06-16},
  abstract = {Credibility of scientific claims is established with evidence for their replicability using new data. According to common understanding, replication is repeating a study's procedure and observing whether the prior finding recurs. This definition is intuitive, easy to apply, and incorrect. We propose that replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research. This definition reduces emphasis on operational characteristics of the study and increases emphasis on the interpretation of possible outcomes. The purpose of replication is to advance theory by confronting existing understanding with new evidence. Ironically, the value of replication may be strongest when existing understanding is weakest. Successful replication provides evidence of generalizability across the conditions that inevitably differ from the original study; Unsuccessful replication indicates that the reliability of the finding may be more constrained than recognized previously. Defining replication as a confrontation of current theoretical expectations clarifies its important, exciting, and generative role in scientific progress.},
  langid = {english},
  keywords = {Acetylcholine,Elections,Frogs,Obesity,Open data,Philippines,Reproducibility,Social sciences}
}

@misc{nuijten_effectiveness_2023,
  title = {The Effectiveness of Implementing Statcheck in the Peer Review Process to Avoid Statistical Reporting Errors},
  author = {Nuijten, Mich{\`e}le B. and Wicherts, Jelte},
  year = {2023},
  month = jan,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/bxau9},
  urldate = {2023-08-07},
  abstract = {We investigated whether statistical reporting inconsistencies could be avoided if journals implement the tool statcheck in the peer review process. In a preregistered study covering over 7000 articles, we compared the inconsistency rates between two journals that implemented statcheck in their peer review process (Psychological Science and Journal of Experimental and Social Psychology) with two matched control journals (Journal of Experimental Psychology: General and Journal of Personality and Social Psychology, respectively), before and after statcheck was implemented. Preregistered multilevel logistic regression analyses showed that the decrease in both inconsistencies and decision inconsistencies around p = .05 is considerably steeper in statcheck journals than in control journals, offering support for the notion that statcheck can be a useful tool for journals to avoid statistical reporting inconsistencies in published articles. We discuss limitations and implications of these findings.},
  langid = {american},
  keywords = {journal policy,meta-science,Meta-science,p-values,peer review process,psychology,Social and Behavioral Sciences,statcheck,statistical errors,statistics}
}

@article{nuijten_prevalence_2015,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985--2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and {van Assen}, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2015},
  month = oct,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2},
  urldate = {2015-11-30},
  langid = {english}
}

@article{nunnally_place_1960,
  title = {The Place of Statistics in Psychology},
  author = {Nunnally, Jum},
  year = {1960},
  journal = {Educational and Psychological Measurement},
  volume = {20},
  number = {4},
  pages = {641--650},
  doi = {10.1177/001316446002000401},
  annotation = {00190}
}

@article{obels_analysis_2020,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Obels, Pepijn and Lakens, Dani{\"e}l and Coles, Nicholas A. and Gottfried, Jaroslav and Green, Seth A.},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {229--237},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920918872},
  urldate = {2020-07-19},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58\%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58\%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.},
  langid = {english}
}

@article{oddie_content_2013,
  title = {The Content, Consequence and Likeness Approaches to Verisimilitude: Compatibility, Trivialization, and Underdetermination},
  shorttitle = {The Content, Consequence and Likeness Approaches to Verisimilitude},
  author = {Oddie, Graham},
  year = {2013},
  month = jun,
  journal = {Synthese},
  volume = {190},
  number = {9},
  pages = {1647--1687},
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-011-9930-8},
  urldate = {2017-06-15},
  abstract = {Theories of verisimilitude have routinely been classified into two rival camps---the content approach and the likeness approach---and these appear to be motivated by very different sets of data and principles. The question thus naturally arises as to whether these approaches can be fruitfully combined. Recently Zwart and Franssen (Synthese 158(1):75--92, 2007) have offered precise analyses of the content and likeness approaches, and shown that given these analyses any attempt to meld content and likeness orderings violates some basic desiderata. Unfortunately their characterizations of the approaches do not embrace the paradigm examples of those approaches. I offer somewhat different characterizations of these two approaches, as well as of the consequence approach (Schurz and Weingartner (Synthese 172(3):415--436, 2010) which happily embrace their respective paradigms. Finally I prove that the three approaches are indeed compatible, but only just, and that the cost of combining them is too high. Any account which combines the strictures of what I call the strong likeness approach with the demands of either the content or the consequence approach suffers from precisely the same defect as Popper's---namely, it entails the trivialization of truthlikeness. The downside of eschewing the strong likeness constraints and embracing the content constraints alone is the underdetermination of the concept of truthlikeness.},
  langid = {english}
}

@article{odonnell_registered_2018,
  title = {Registered {{Replication Report}}: {{Dijksterhuis}} and van {{Knippenberg}} (1998)},
  shorttitle = {Registered {{Replication Report}}},
  author = {O'Donnell, Michael and Nelson, Leif D. and Ackermann, Evi and Aczel, Balazs and Akhtar, Athfah and Aldrovandi, Silvio and Alshaif, Nasseem and Andringa, Ronald and Aveyard, Mark and Babincak, Peter and Balatekin, Nursena and Baldwin, Scott A. and Banik, Gabriel and Baskin, Ernest and Bell, Raoul and Bia{\l}obrzeska, Olga and Birt, Angie R. and Boot, Walter R. and Braithwaite, Scott R. and Briggs, Jessie C. and Buchner, Axel and Budd, Desiree and Budzik, Kathryn and Bullens, Lottie and Bulley, Richard L. and Cannon, Peter R. and Cantarero, Katarzyna and Cesario, Joseph and Chambers, Stephanie and Chartier, Christopher R. and Chekroun, Peggy and Chong, Clara and Cleeremans, Axel and Coary, Sean P. and Coulthard, Jacob and Cramwinckel, Florien M. and Denson, Thomas F. and {D{\'i}az-Lago}, Marcos and DiDonato, Theresa E. and Drummond, Aaron and Eberlen, Julia and Ebersbach, Titus and Edlund, John E. and Finnigan, Katherine M. and Fisher, Justin and Frankowska, Natalia and {Garc{\'i}a-S{\'a}nchez}, Efra{\'i}n and Golom, Frank D. and Graves, Andrew J. and Greenberg, Kevin and Hanioti, Mando and Hansen, Heather A. and Harder, Jenna A. and Harrell, Erin R. and Hartanto, Andree and Inzlicht, Michael and Johnson, David J. and Karpinski, Andrew and Keller, Victor N. and Klein, Olivier and Koppel, Lina and Krahmer, Emiel and Lantian, Anthony and Larson, Michael J. and L{\'e}gal, Jean-Baptiste and Lucas, Richard E. and Lynott, Dermot and Magaldino, Corey M. and Massar, Karlijn and McBee, Matthew T. and McLatchie, Neil and Melia, Nadhilla and Mensink, Michael C. and Mieth, Laura and {Moore-Berg}, Samantha and Neeser, Geraldine and Newell, Ben R. and Noordewier, Marret K. and Ali {\"O}zdo{\u g}ru, Asil and Pantazi, Myrto and Parzuchowski, Micha{\l} and Peters, Kim and Philipp, Michael C. and Pollmann, Monique M. H. and Rentzelas, Panagiotis and {Rodr{\'i}guez-Bail{\'o}n}, Rosa and Philipp R{\"o}er, Jan and Ropovik, Ivan and Roque, Nelson A. and Rueda, Carolina and Rutjens, Bastiaan T. and Sackett, Katey and Salamon, Janos and {S{\'a}nchez-Rodr{\'i}guez}, {\'A}ngel and Saunders, Blair and Schaafsma, Juliette and {Schulte-Mecklenbeck}, Michael and Shanks, David R. and Sherman, Martin F. and Steele, Kenneth M. and Steffens, Niklas K. and Sun, Jessie and Susa, Kyle J. and Szaszi, Barnabas and Szollosi, Aba and Tamayo, Ricardo M. and Tingh{\"o}g, Gustav and Tong, Yuk-yue and Tweten, Carol and Vadillo, Miguel A. and Valcarcel, Deisy and {Van der Linden}, Nicolas and {van Elk}, Michiel and {van Harreveld}, Frenk and V{\"a}stfj{\"a}ll, Daniel and Vazire, Simine and Verduyn, Philippe and Williams, Matt N. and Willis, Guillermo B. and Wood, Sarah E. and Yang, Chunliang and Zerhouni, Oulmann and Zheng, Robert and Zrubka, Mark},
  year = {2018},
  month = mar,
  journal = {Perspectives on Psychological Science},
  volume = {13},
  number = {2},
  pages = {268--294},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691618755704},
  urldate = {2022-02-19},
  abstract = {Dijksterhuis and van Knippenberg (1998) reported that participants primed with a category associated with intelligence (``professor'') subsequently performed 13\% better on a trivia test than participants primed with a category associated with a lack of intelligence (``soccer hooligans''). In two unpublished replications of this study designed to verify the appropriate testing procedures, Dijksterhuis, van Knippenberg, and Holland observed a smaller difference between conditions (2\%--3\%) as well as a gender difference: Men showed the effect (9.3\% and 7.6\%), but women did not (0.3\% and -0.3\%). The procedure used in those replications served as the basis for this multilab Registered Replication Report. A total of 40 laboratories collected data for this project, and 23 of these laboratories met all inclusion criteria. Here we report the meta-analytic results for those 23 direct replications (total N = 4,493), which tested whether performance on a 30-item general-knowledge trivia task differed between these two priming conditions (results of supplementary analyses of the data from all 40 labs, N = 6,454, are also reported). We observed no overall difference in trivia performance between participants primed with the ``professor'' category and those primed with the ``hooligan'' category (0.14\%) and no moderation by gender.},
  langid = {english},
  keywords = {intelligence,priming,replication}
}

@article{okada_omega_2013,
  title = {Is {{Omega Squared Less Biased}}? A {{Comparison}} of {{Three Major Effect Size Indices}} in {{One-Way Anova}}},
  shorttitle = {Is {{Omega Squared Less Biased}}?},
  author = {Okada, Kensuke},
  year = {2013},
  month = jul,
  journal = {Behaviormetrika},
  volume = {40},
  number = {2},
  pages = {129--147},
  issn = {0385-7417, 1349-6964},
  doi = {10.2333/bhmk.40.129},
  urldate = {2019-12-23},
  langid = {english}
}

@article{olejnik_generalized_2003,
  title = {Generalized {{Eta}} and {{Omega Squared Statistics}}: {{Measures}} of {{Effect Size}} for {{Some Common Research Designs}}.},
  shorttitle = {Generalized {{Eta}} and {{Omega Squared Statistics}}},
  author = {Olejnik, Stephen and Algina, James},
  year = {2003},
  journal = {Psychological Methods},
  volume = {8},
  number = {4},
  pages = {434--447},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.8.4.434},
  urldate = {2015-12-01},
  langid = {english}
}

@article{olsson-collentine_heterogeneity_2020,
  title = {Heterogeneity in Direct Replications in Psychology and Its Association with Effect Size},
  author = {{Olsson-Collentine}, Anton and Wicherts, Jelte M. and {van Assen}, Marcel A. L. M.},
  year = {2020},
  journal = {Psychological Bulletin},
  volume = {146},
  number = {10},
  pages = {922--940},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/bul0000294},
  abstract = {We examined the evidence for heterogeneity (of effect sizes) when only minor changes to sample population and settings were made between studies and explored the association between heterogeneity and average effect size in a sample of 68 meta-analyses from 13 preregistered multilab direct replication projects in social and cognitive psychology. Among the many examined effects, examples include the Stroop effect, the ``verbal overshadowing'' effect, and various priming effects such as ``anchoring'' effects. We found limited heterogeneity; 48/68 (71\%) meta-analyses had nonsignificant heterogeneity, and most (49/68; 72\%) were most likely to have zero to small heterogeneity. Power to detect small heterogeneity (as defined by Higgins, Thompson, Deeks, \& Altman, 2003) was low for all projects (mean 43\%), but good to excellent for medium and large heterogeneity. Our findings thus show little evidence of widespread heterogeneity in direct replication studies in social and cognitive psychology, suggesting that minor changes in sample population and settings are unlikely to affect research outcomes in these fields of psychology. We also found strong correlations between observed average effect sizes (standardized mean differences and log odds ratios) and heterogeneity in our sample. Our results suggest that heterogeneity and moderation of effects is unlikely for a 0 average true effect size, but increasingly likely for larger average true effect size. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Cognitive Psychology,Experimental Laboratories,Experimental Replication,Homogeneity of Variance,Meta Analysis,Priming,Psychology,Social Psychology,Stroop Effect}
}

@article{olsson-collentine_heterogeneity_2020a,
  title = {Heterogeneity in Direct Replications in Psychology and Its Association with Effect Size.},
  author = {{Olsson-Collentine}, Anton and Wicherts, Jelte M. and {van Assen}, Marcel A. L. M.},
  year = {2020},
  month = oct,
  journal = {Psychological Bulletin},
  volume = {146},
  number = {10},
  pages = {922--940},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/bul0000294},
  urldate = {2022-03-13},
  langid = {english}
}

@article{opensciencecollaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  urldate = {2015-11-30},
  langid = {english}
}

@article{orben_crud_2020,
  title = {Crud ({{Re}}){{Defined}}},
  author = {Orben, Amy and Lakens, Dani{\"e}l},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {238--247},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920917961},
  urldate = {2021-02-18},
  abstract = {The idea that in behavioral research everything correlates with everything else was a niche area of the scientific literature for more than half a century. With the increasing availability of large data sets in psychology, the ``crud'' factor has, however, become more relevant than ever before. When referenced in empirical work, it is often used by researchers to discount minute---but statistically significant---effects that are deemed too small to be considered meaningful. This review tracks the history of the crud factor and examines how its use in the psychological- and behavioral-science literature has developed to this day. We highlight a common and deep-seated lack of understanding about what the crud factor is and discuss whether it can be proven to exist or estimated and how it should be interpreted. This lack of understanding makes the crud factor a convenient tool for psychologists to use to disregard unwanted results, even though the presence of a crud factor should be a large inconvenience for the discipline. To inspire a concerted effort to take the crud factor more seriously, we clarify the definitions of important concepts, highlight current pitfalls, and pose questions that need to be addressed to ultimately improve understanding of the crud factor. Such work will be necessary to develop the crud factor into a useful concept encouraging improved psychological research.},
  langid = {english},
  keywords = {crud factor,effect sizes,null-hypothesis significance testing,open data,open materials,review}
}

@article{parker_sample_2003,
  title = {Sample {{Size}}},
  author = {Parker, Robert A and Berman, Nancy G},
  year = {2003},
  month = aug,
  journal = {The American Statistician},
  volume = {57},
  number = {3},
  pages = {166--170},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/0003130031919},
  urldate = {2020-12-31},
  abstract = {Conventionally, sample size calculations are viewed as calculations determining the right number of subjects needed for a study. Such calculations follow the classical paradigm: ``for a difference X, I need sample size Y.'' We argue that the paradigm ``for a sample size Y, I get information Z'' is more appropriate for many studies and reflects the information needed by scientists when planning a study. This approach applies to both physiological studies and Phase I and II interventional studies. We provide actual examples from our own consulting work to demonstrate this. We conclude that sample size should be viewed not as a unique right number, but rather as a factor needed to assess the utility of a study.}
}

@article{parkhurst_statistical_2001,
  title = {Statistical Significance Tests: {{Equivalence}} and Reverse Tests Should Reduce Misinterpretation},
  shorttitle = {Statistical {{Significance Tests}}},
  author = {Parkhurst, David F.},
  year = {2001},
  journal = {Bioscience},
  volume = {51},
  number = {12},
  pages = {1051--1057},
  doi = {10.1641/0006-3568(2001)051[1051:SSTEAR]2.0.CO;2}
}

@article{parsons_psychological_2019,
  title = {Psychological {{Science Needs}} a {{Standard Practice}} of {{Reporting}} the {{Reliability}} of {{Cognitive-Behavioral Measurements}}},
  author = {Parsons, Sam and Kruijt, Anne-Wil and Fox, Elaine},
  year = {2019},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {4},
  pages = {378--395},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245919879695},
  urldate = {2021-01-03},
  abstract = {Psychological science relies on behavioral measures to assess cognitive processing; however, the field has not yet developed a tradition of routinely examining the reliability of these behavioral measures. Reliable measures are essential to draw robust inferences from statistical analyses, and subpar reliability has severe implications for measures' validity and interpretation. Without examining and reporting the reliability of measurements used in an analysis, it is nearly impossible to ascertain whether results are robust or have arisen largely from measurement error. In this article, we propose that researchers adopt a standard practice of estimating and reporting the reliability of behavioral assessments of cognitive processing. We illustrate the need for this practice using an example from experimental psychopathology, the dot-probe task, although we argue that reporting reliability is relevant across fields (e.g., social cognition and cognitive psychology). We explore several implications of low measurement reliability and the detrimental impact that failure to assess measurement reliability has on interpretability and comparison of results and therefore research quality. We argue that researchers in the field of cognition need to report measurement reliability as routine practice so that more reliable assessment tools can be developed. To provide some guidance on estimating and reporting reliability, we describe the use of bootstrapped split-half estimation and intraclass correlation coefficients to estimate internal consistency and test-retest reliability, respectively. For future researchers to build upon current results, it is imperative that all researchers provide psychometric information sufficient for estimating the accuracy of inferences and informing further development of cognitive-behavioral assessments.},
  langid = {english},
  keywords = {cognitive-behavioral tasks,estimating and reporting,open materials,psychometrics,reliability}
}

@book{pawitan_all_2001,
  title = {In All Likelihood: Statistical Modelling and Inference Using Likelihood},
  shorttitle = {In All Likelihood},
  author = {Pawitan, Yudi},
  year = {2001},
  publisher = {Clarendon Press ; Oxford University Press},
  address = {Oxford : New York},
  isbn = {978-0-19-850765-9},
  lccn = {QA276 .P286 2001},
  keywords = {Mathematical statistics}
}

@article{pemberton_text_2019,
  title = {Text Recycling: {{Views}} of {{North American}} Journal Editors from an Interview-Based Study},
  shorttitle = {Text Recycling},
  author = {Pemberton, Michael and Hall, Susanne and Moskovitz, Cary and Anson, Chris M.},
  year = {2019},
  journal = {Learned Publishing},
  volume = {32},
  number = {4},
  pages = {355--366},
  issn = {1741-4857},
  doi = {10.1002/leap.1259},
  urldate = {2022-09-27},
  abstract = {Over the past decade, text recycling (TR; AKA `self-plagiarism') has become a visible and somewhat contentious practice, particularly in the realm of journal articles. While growing numbers of publishers are writing editorials and formulating guidelines on TR, little is known about how editors view the practice or how they respond to it. We present results from an interview-based study of 21 North American journal editors from a broad range of academic disciplines. Our findings show that editors' beliefs and practices are quite individualized rather than being tied to disciplinary or other structural parameters. While none of our participants supported the use of large amounts of recycled material from one journal article to another, some editors were staunchly against any use of recycled material, while others were accepting of the practice in certain circumstances. Issues of originality, the challenges of rewriting text, the varied circulation of texts, and abiding by copyright law were prominent themes as editors discussed their approaches to TR. Overall, the interviews showed that many editors have not thought systematically about the practice of TR, and they sometimes have trouble aligning their beliefs and practices.},
  langid = {english}
}

@article{pereboom_fundamental_1971,
  title = {Some {{Fundamental Problems}} in {{Experimental Psychology}}: {{An Overview}}},
  shorttitle = {Some {{Fundamental Problems}} in {{Experimental Psychology}}},
  author = {Pereboom, A. C.},
  year = {1971},
  journal = {Psychological Reports},
  volume = {28},
  number = {2},
  doi = {10.2466/pr0.1971.28.2.439},
  urldate = {2023-07-23},
  abstract = {The application of the experimental approach to a multidimensional discipline presupposes that it will work, that control and analysis will generate explanations which will lead to a unified theory for a restricted behavior domain, and that there will be a fundamental basis for our concepts, scales, and methods which will justify the measurable generalization of that theory. Yet little of this appears to be happening. It is suggested that, in the name of basic research, more effort be directed toward fitting our empirical approach to the subject matter rather than attempting to do the reverse. This may mean less control, greater descriptive generality, and more tolerance for diverse theoretical positions.},
  langid = {english}
}

@article{perneger_what_1998,
  title = {What's Wrong with {{Bonferroni}} Adjustments},
  author = {Perneger, Thomas V.},
  year = {1998},
  journal = {Bmj},
  volume = {316},
  number = {7139},
  pages = {1236--1238},
  urldate = {2016-02-14}
}

@article{perugini_benefits_2025,
  title = {The Benefits of Reporting Critical Effect Size Values},
  author = {Perugini, Ambra and Toffalini, Enrico and Gambarota, Filippo and Lakens, Daniel and Pastore, Massimiliano and Finos, Livio and Alto{\`e}, Gianmarco},
  year = {2025},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.31234/osf.io/7qe92},
  urldate = {2025-03-12},
  abstract = {Critical effect size values represent the smallest detectable effect that can reach statistical significance given a specific sample size, alpha level and test statistic. It can be useful to calculate the critical effect size when designing a study, and evaluate whether such effects are plausible. Reporting critical effect size values may be useful when the sample size has not been planned a priori, or when there is uncertainty about the expected sample size that can be collected, when researchers plan to analyze the data with a statistical hypothesis test. To assist researchers in calculating critical effect size values we have developed an R package that allows researchers to report critical effect size values for group comparisons, correlations, linear regressions, and meta-analyses. Reflecting on critical effect size values could benefit researchers during the planning phase of the study by helping them to understand the limitations of their research design. Critical effect size values are also useful when evaluating studies performed by other researchers when a-priori power analyses were not performed, especially when non significant results are observed.},
  langid = {american}
}

@article{perugini_practical_2018,
  title = {A {{Practical Primer To Power Analysis}} for {{Simple Experimental Designs}}},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  year = {2018},
  month = jul,
  journal = {International Review of Social Psychology},
  volume = {31},
  number = {1},
  pages = {20},
  issn = {2397-8570},
  doi = {10.5334/irsp.181},
  urldate = {2019-04-07},
  abstract = {Power analysis is an important tool to use when planning studies. This contribution aims to remind readers what power analysis is, emphasize why it matters, and articulate when and how it should be used. The focus is on applications of power analysis for experimental designs often encountered in psychology, starting from simple two-group independent and paired groups and moving to one-way analysis of variance, factorial designs, contrast analysis, trend analysis, regression analysis, analysis of covariance, and mediation analysis. Special attention is given to the application of power analysis to moderation designs, considering both dichotomous and continuous predictors and moderators. Illustrative practical examples based on G*Power and R packages are provided throughout the article. Annotated code for the examples with R and dedicated computational tools are made freely available at a dedicated web page (https://github.com/mcfanda/primerPowerIRSP). Applications of power analysis for more complex designs are briefly mentioned, and some important general issues related to power analysis are discussed.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  keywords = {effect size,moderation,power analysis,sensitivity analysis,uncertainty}
}

@article{perugini_safeguard_2014,
  title = {Safeguard Power as a Protection against Imprecise Power Estimates},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  year = {2014},
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {3},
  pages = {319--332},
  doi = {10.1177/1745691614528519},
  urldate = {2017-05-10}
}

@article{peters_performance_2007,
  title = {Performance of the Trim and Fill Method in the Presence of Publication Bias and Between-Study Heterogeneity},
  author = {Peters, Jaime L. and Sutton, Alex J. and Jones, David R. and Abrams, Keith R. and Rushton, Lesley},
  year = {2007},
  month = nov,
  journal = {Statistics in Medicine},
  volume = {26},
  number = {25},
  pages = {4544--4562},
  issn = {0277-6715},
  doi = {10.1002/sim.2889},
  abstract = {The trim and fill method allows estimation of an adjusted meta-analysis estimate in the presence of publication bias. To date, the performance of the trim and fill method has had little assessment. In this paper, we provide a more comprehensive examination of different versions of the trim and fill method in a number of simulated meta-analysis scenarios, comparing results with those from usual unadjusted meta-analysis models and two simple alternatives, namely use of the estimate from: (i) the largest; or (ii) the most precise study in the meta-analysis. Findings suggest a great deal of variability in the performance of the different approaches. When there is large between-study heterogeneity the trim and fill method can underestimate the true positive effect when there is no publication bias. However, when publication bias is present the trim and fill method can give estimates that are less biased than the usual meta-analysis models. Although results suggest that the use of the estimate from the largest or most precise study seems a reasonable approach in the presence of publication bias, when between-study heterogeneity exists our simulations show that these estimates are quite biased. We conclude that in the presence of publication bias use of the trim and fill method can help to reduce the bias in pooled estimates, even though the performance of this method is not ideal. However, because we do not know whether funnel plot asymmetry is truly caused by publication bias, and because there is great variability in the performance of different trim and fill estimators and models in various meta-analysis scenarios, we recommend use of the trim and fill method as a form of sensitivity analysis as intended by the authors of the method.},
  langid = {english},
  pmid = {17476644},
  keywords = {Computer Simulation,Coronary Restenosis,Data Interpretation Statistical,Genotype,Humans,Meta-Analysis as Topic,Peptidyl-Dipeptidase A,Publication Bias}
}

@article{phillips_statistical_2001,
  title = {Statistical Significance of Sediment Toxicity Test Results: {{Threshold}} Values Derived by the Detectable Significance Approach},
  shorttitle = {Statistical Significance of Sediment Toxicity Test Results},
  author = {Phillips, Bryn M. and Hunt, John W. and Anderson, Brian S. and Puckett, H. Max and Fairey, Russell and Wilson, Craig J. and Tjeerdema, Ron},
  year = {2001},
  journal = {Environmental Toxicology and Chemistry},
  volume = {20},
  number = {2},
  pages = {371--373},
  issn = {1552-8618},
  doi = {10.1002/etc.5620200218},
  urldate = {2020-12-12},
  abstract = {A number of methods have been employed to determine the statistical significance of sediment toxicity test results. To allow consistency among comparisons, regardless of among-replicate variability, a protocol-specific approach has been used that considers protocol performance over a large number of comparisons. Ninetieth-percentile minimum significant difference (MSD) values were calculated to determine a critical threshold for statistically significant sample toxicity. Significant toxicity threshold values (as a percentage of laboratory control values) are presented for six species and nine endpoints based on data from as many as 720 stations. These threshold values are useful for interpreting sediment toxicity data from large studies and in eliminating cases where statistical significance is assigned in individual cases because among-replicate variability is small.},
  langid = {english},
  keywords = {Minimum significant difference,Sediment toxicity,Statistics}
}

@article{pickett_questionable_2017,
  title = {Questionable, {{Objectionable}} or {{Criminal}}? {{Public Opinion}} on {{Data Fraud}} and {{Selective Reporting}} in {{Science}}},
  shorttitle = {Questionable, {{Objectionable}} or {{Criminal}}?},
  author = {Pickett, Justin T. and Roche, Sean Patrick},
  year = {2017},
  month = mar,
  journal = {Science and Engineering Ethics},
  pages = {1--21},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-017-9886-2},
  urldate = {2018-01-11},
  abstract = {Data fraud and selective reporting both present serious threats to the credibility of science. However, there remains considerable disagreement among scientists about how best to sanction data fraud, and about the ethicality of selective reporting. The public is arguably the largest stakeholder in the reproducibility of science; research is primarily paid for with public funds, and flawed science threatens the public's welfare. Members of the public are able to make meaningful judgments about the morality of different behaviors using moral intuitions. Legal scholars emphasize that to maintain legitimacy, social control policies must be developed with some consideration given to the public's moral intuitions. Although there is a large literature on popular attitudes toward science, there is no existing evidence about public opinion on data fraud or selective reporting. We conducted two studies---a survey experiment with a nationwide convenience sample (N = 821), and a follow-up survey with a representative sample of US adults (N = 964)---to explore community members' judgments about the morality of data fraud and selective reporting in science. The findings show that community members make a moral distinction between data fraud and selective reporting, but overwhelmingly judge both behaviors to be immoral and deserving of punishment. Community members believe that scientists who commit data fraud or selective reporting should be fired and banned from receiving funding. For data fraud, most Americans support criminal penalties. Results from an ordered logistic regression analysis reveal few demographic and no significant partisan differences in punitiveness toward data fraud.},
  langid = {english}
}

@article{platt_strong_1964,
  title = {Strong {{Inference}}: {{Certain}} Systematic Methods of Scientific Thinking May Produce Much More Rapid Progress than Others},
  shorttitle = {Strong {{Inference}}},
  author = {Platt, John R.},
  year = {1964},
  month = oct,
  journal = {Science},
  volume = {146},
  number = {3642},
  pages = {347--353},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.146.3642.347},
  urldate = {2018-11-30},
  copyright = {{\copyright} 1964},
  langid = {english},
  pmid = {17739513},
  annotation = {00000}
}

@article{pocock_group_1977,
  title = {Group Sequential Methods in the Design and Analysis of Clinical Trials},
  author = {Pocock, Stuart J.},
  year = {1977},
  month = aug,
  journal = {Biometrika},
  volume = {64},
  number = {2},
  pages = {191--199},
  issn = {0006-3444},
  doi = {10.1093/biomet/64.2.191},
  urldate = {2021-01-12},
  abstract = {In clinical trials with sequential patient entry, fixed sample size designs are unjustified on ethical grounds and sequential designs are often impracticable. One solution is a group sequential design dividing patient entry into a number of equal-sized groups so that the decision to stop the trial or continue is based on repeated significance tests of the accumulated data after each group is evaluated. Exact results are obtained for a trial with two treatments and a normal response with known variance. The design problem of determining the required size and number of groups is also considered. Simulation shows that these normal results may be adapted to other types of response data. An example shows that group sequential designs can sometimes be statistically superior to standard sequential designs.}
}

@article{polanin_transparency_2020,
  title = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}: {{A Meta-Review}}},
  shorttitle = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}},
  author = {Polanin, Joshua R. and Hennessy, Emily A. and Tsuji, Sho},
  year = {2020},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {15},
  number = {4},
  pages = {1026--1041},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620906416},
  urldate = {2020-08-04},
  abstract = {Systematic review and meta-analysis are possible as viable research techniques only through transparent reporting of primary research; thus, one might expect meta-analysts to demonstrate best practice in their reporting of results and have a high degree of transparency leading to reproducibility of their work. This assumption has yet to be fully tested in the psychological sciences. We therefore aimed to assess the transparency and reproducibility of psychological meta-analyses. We conducted a meta-review by sampling 150 studies from Psychological Bulletin to extract information about each review's transparent and reproducible reporting practices. The results revealed that authors reported on average 55\% of criteria and that transparent reporting practices increased over the three decades studied (b = 1.09, SE = 0.24, t = 4.519, p {$<$} .001). Review authors consistently reported eligibility criteria, effect-size information, and synthesis techniques. Review authors, however, on average, did not report specific search results, screening and extraction procedures, and most importantly, effect-size and moderator information from each individual study. Far fewer studies provided statistical code required for complete analytical replication. We argue that the field of psychology and research synthesis in general should require review authors to report these elements in a transparent and reproducible manner.},
  langid = {english}
}

@book{popper_logic_2002,
  title = {{The logic of scientific discovery}},
  author = {Popper, Karl R},
  year = {2002},
  publisher = {Routledge},
  address = {London; New York},
  urldate = {2015-11-30},
  abstract = {When first published in 1959, this book revolutionized contemporary thinking about science and knowledge. It remains the one of the most widely read books about science to come out of the twentieth century.},
  isbn = {978-0-203-99462-7 978-0-415-27843-0 978-0-415-27844-7},
  langid = {Translated from the German.}
}

@article{primbs_are_2022,
  title = {Are {{Small Effects}} the {{Indispensable Foundation}} for a {{Cumulative Psychological Science}}? {{A Reply}} to {{G{\"o}tz}} et al. (2022)},
  shorttitle = {There Are No `{{Small}}' or `{{Large}}' {{Effects}}},
  author = {Primbs, Maximilian and Pennington, Charlotte Rebecca and Lakens, Dani{\"e}l and Silan, Miguel Alejandro and Lieck, Dwayne Sean Noah and Forscher, Patrick and Buchanan, Erin Michelle and Westwood, Samuel James},
  year = {2022},
  journal = {Perspectives on Psychological Science},
  doi = {10.31234/osf.io/6s8bj},
  urldate = {2022-01-10},
  abstract = {G{\"o}tz et al. (2021) argue that small effects are the indispensable foundation for a cumulative psychological science. Whilst we applaud their efforts to bring this important discussion to the forefront, we argue that their core arguments do not hold up under scrutiny, and if left uncorrected have the potential to undermine best practices in reporting and interpreting effect size estimates. Their article can be used as a convenient blanket defense to justify `small' effects as meaningful. In our reply, we first argue that comparisons between psychological science and genetics are fundamentally flawed because these disciplines have vastly different goals and methodology. Second, we argue that p-values, not effect sizes, are the main currency for publication in psychology, meaning that any biases in the literature are caused by this pressure to publish statistically significant results, not a pressure to publish large effects. Third, we contend that claims regarding small effects as important and consequential must be supported by empirical evidence, or at least require a falsifiable line of reasoning. Finally, we propose that researchers should evaluate effect sizes in relative, not absolute terms, and provide several approaches of how this can be achieved.}
}

@book{proschan_statistical_2006,
  title = {Statistical Monitoring of Clinical Trials: A Unified Approach},
  shorttitle = {Statistical Monitoring of Clinical Trials},
  author = {Proschan, Michael A. and Lan, K. K. Gordan and Wittes, Janet Turk},
  year = {2006},
  series = {Statistics for Biology and Health},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-0-387-30059-7},
  lccn = {R853.C55 .P76 2006},
  keywords = {Bayes Theorem,Clinical Trials,Data Interpretation Statistical,Drugs,Statistical methods,Statistics,statistics & numerical data,Testing}
}

@article{proschan_twostage_2005,
  title = {Two-{{Stage Sample Size Re-Estimation Based}} on a {{Nuisance Parameter}}: {{A Review}}},
  shorttitle = {Two-{{Stage Sample Size Re-Estimation Based}} on a {{Nuisance Parameter}}},
  author = {Proschan, Michael A.},
  year = {2005},
  month = jul,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {15},
  number = {4},
  pages = {559--574},
  publisher = {Taylor \& Francis},
  issn = {1054-3406},
  doi = {10.1081/BIP-200062852},
  urldate = {2021-01-10},
  abstract = {Sample size calculations are important and difficult in clinical trails because they depend on the nuisance parameter and treatment effect. Recently, much attention has been focused on two-stage methods whereby the first stage constitutes an internal pilot study used to estimate parameters and revise the final sample size. This paper reviews two-stage methods based on estimation of nuisance parameters in either a continuous or dichotomous outcome setting.},
  pmid = {16022163},
  keywords = {Adaptive methods,Ancillary statistic,Blinding,Clinical trials,Conditioning,Continuous outcome,Correlation,Dichotomous outcome,Independence,Internal pilot study,Lumping,Pooling,Power,Restricted design,Unrestricted design}
}

@book{psillos_scientific_1999,
  title = {Scientific Realism: How Science Tracks Truth},
  shorttitle = {Scientific Realism},
  author = {Psillos, Stathis},
  year = {1999},
  publisher = {Routledge},
  address = {London; New York},
  abstract = {Counter Scientific Realism is the optimistic view that modern science is on the right track: that the world really is the way our best scientific theories describe it to be. In his book, Stathis Psillos gives us a detailed and comprehensive study, which restores the intuitive plausibility of scientific realism. We see that throughout the twentieth century, scientific realism has been challenged by philosophical positions from all angles: from reductive empiricism, to instrumentalism and modern skeptical empiricism. Scientific Realism explains that the history of science does not undermine the notion of scientific realism, and instead makes it reasonable to accept scientific as the best philosophical account of science, its empirical success, its progress and its practice. Anyone wishing to gain a deeper understanding of the state of modern science and why scientific realism is plausible, should read this book.},
  isbn = {978-0-415-20818-5 978-0-415-20819-2 978-0-203-97964-8},
  langid = {english}
}

@article{quertemont_how_2011,
  title = {How to {{Statistically Show}} the {{Absence}} of an {{Effect}}},
  author = {Quertemont, Etienne},
  year = {2011},
  month = aug,
  journal = {Psychologica Belgica},
  volume = {51},
  number = {2},
  pages = {109--127},
  issn = {2054-670X, 0033-2879},
  doi = {10.5334/pb-51-2-109},
  urldate = {2016-05-17},
  annotation = {00015}
}

@article{rabelo_questionable_2020,
  title = {Questionable Research Practices among {{Brazilian}} Psychological Researchers: {{Results}} from a Replication Study and an International Comparison},
  shorttitle = {Questionable Research Practices among {{Brazilian}} Psychological Researchers},
  author = {Rabelo, Andr{\'e} L. A. and Farias, J{\'e}ssica E. M. and Sarmet, Maur{\'i}cio M. and Joaquim, Teresa C. R. and Hoersting, Raquel C. and Victorino, Luiz and Modesto, Jo{\~a}o G. N. and Pilati, Ronaldo},
  year = {2020},
  journal = {International Journal of Psychology},
  volume = {55},
  number = {4},
  pages = {674--683},
  issn = {1464-066X},
  doi = {10.1002/ijop.12632},
  urldate = {2022-09-18},
  abstract = {Research on scientific integrity is growing in psychology, and questionable research practices (QRPs) have received more attention due to its harmful effect on science. By replicating the procedures of previous research, the present study aimed at describing the use of QRPs among Brazilian psychological researchers and to make an international comparison with previous studies in other countries---the US and Italy. Two hundred and thirty-two Brazilian researchers in the field of psychology answered questions related to 10 different QRPs. Brazilian researchers indicated a lower tendency to engage in two QRPs (failing to report all of a study's dependent measures; deciding whether to collect more data after looking to see whether the results were significant) when compared to their Italian and North American counterparts, but indicated a higher tendency to engage in two other QRPs (selectively reporting studies that ``worked''; not reporting all of a study's conditions). Most of the sample did not admit integrity conflicts in their own research but indicated that others have integrity problems, as observed in previous studies. Those discrepancies could be attributed to contextual and systemic factors regarding different publication demands among the different nations. Further studies should focus on identifying the antecedents of QRPs.},
  langid = {english},
  keywords = {Bias,Meta-research,Questionable research practices,Replicability,Scientific integrity}
}

@article{radick_mendel_2022,
  title = {Mendel the Fraud? {{A}} Social History of Truth in Genetics},
  shorttitle = {Mendel the Fraud?},
  author = {Radick, Gregory},
  year = {2022},
  month = jun,
  journal = {Studies in History and Philosophy of Science},
  volume = {93},
  pages = {39--46},
  issn = {0039-3681},
  doi = {10.1016/j.shpsa.2021.12.012},
  urldate = {2023-08-24},
  abstract = {Two things about Gregor Mendel are common knowledge: first, that he was the ``monk in the garden'' whose experiments with peas in mid-nineteenth-century Moravia became the starting point for genetics; second, that, despite that exalted status, there is something fishy, maybe even fraudulent, about the data that Mendel reported. Although the notion that Mendel's numbers were, in statistical terms, too good to be true was well understood almost immediately after the famous ``rediscovery'' of his work in 1900, the problem became widely discussed and agonized over only from the 1960s, for reasons having as much to do with Cold War geopolitics as with traditional concerns about the objectivity of science. Appreciating the historical origins of the problem as we have inherited it can be a helpful step in shifting the discussion in more productive directions, scientific as well as historiographic.}
}

@article{reif_competitive_1961,
  title = {The {{Competitive World}} of the {{Pure Scientist}}},
  author = {Reif, F.},
  year = {1961},
  month = dec,
  journal = {Science},
  volume = {134},
  number = {3494},
  pages = {1957--1962},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.134.3494.1957},
  urldate = {2023-08-14}
}

@article{rice_heads_1994,
  title = {'{{Heads I}} Win, Tails You Lose': Testing Directional Alternative Hypotheses in Ecological and Evolutionary Research},
  shorttitle = {'{{Heads I}} Win, Tails You Lose'},
  author = {Rice, W. R. and Gaines, S. D.},
  year = {1994},
  month = jun,
  journal = {Trends in Ecology \& Evolution},
  volume = {9},
  number = {6},
  pages = {235--237},
  issn = {0169-5347},
  doi = {10.1016/0169-5347(94)90258-5},
  abstract = {Whenever experiments make a priori predictions about the direction of change in some parameter, one-tailed test statistics offer a potentially large gain in power over the corresponding two-tailed test. This gain is rarely used in ecology and evolution because of (1) the belief that one-tailed procedures are unavailable for most statistical tests and (2) an inherent dilemma in one-tailed tests: how do we handle large parameter changes in the unanticipated direction? The first problem is a misconception, whereas the second is easily resolved by recognizing that one- and two-tailed tests are simply extremes in a continuum of testing options.},
  langid = {english},
  pmid = {21236837},
  annotation = {00231}
}

@article{richard_one_2003,
  title = {One {{Hundred Years}} of {{Social Psychology Quantitatively Described}}.},
  author = {Richard, F. D. and Bond, Charles F. and {Stokes-Zoota}, Juli J.},
  year = {2003},
  journal = {Review of General Psychology},
  volume = {7},
  number = {4},
  pages = {331--363},
  issn = {1939-1552, 1089-2680},
  doi = {10.1037/1089-2680.7.4.331},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00683}
}

@article{richardson_eta_2011,
  title = {Eta Squared and Partial Eta Squared as Measures of Effect Size in Educational Research},
  author = {Richardson, John T.E.},
  year = {2011},
  month = jan,
  journal = {Educational Research Review},
  volume = {6},
  number = {2},
  pages = {135--147},
  issn = {1747938X},
  doi = {10.1016/j.edurev.2010.12.001},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00564}
}

@article{riesthuis_simulationbased_2024,
  title = {Simulation-{{Based Power Analyses}} for the {{Smallest Effect Size}} of {{Interest}}: {{A Confidence-Interval Approach}} for {{Minimum-Effect}} and {{Equivalence Testing}}},
  shorttitle = {Simulation-{{Based Power Analyses}} for the {{Smallest Effect Size}} of {{Interest}}},
  author = {Riesthuis, Paul},
  year = {2024},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {2},
  pages = {1--14},
  issn = {2515-2459},
  doi = {10.1177/25152459241240722},
  abstract = {Effect sizes are often used in psychology because they are crucial when determining the required sample size of a study and when interpreting the implications of a result. Recently, researchers have been encouraged to contextualize their effect sizes and determine what the smallest effect size is that yields theoretical or practical implications, also known as the "smallest effect size of interest" (SESOI). Having a SESOI will allow researchers to have more specific hypotheses, such as whether their findings are truly meaningful (i.e., minimum-effect testing) or whether no meaningful effect exists (i.e., equivalence testing). These types of hypotheses should be reflected in power analyses to accurately determine the required sample size. Through a confidence-interval-focused approach and simulations, I show how to conduct power analyses for minimum-effect and equivalence testing. Moreover, I show that conducting a power analysis for the SESOI might result in inconclusive results. This confidence-interval-focused simulation-based power analysis can be easily adopted to different types of research areas and designs. Last, I provide recommendations on how to conduct such simulation-based power analyses.},
  keywords = {COMPARATIVE BIOAVAILABILITY,eyewitness,memory,minimum-effect testing,PSYCHOLOGICAL-RESEARCH,STANDARDIZED REGRESSION-COEFFICIENTS}
}

@article{rijnsoever_can_2017,
  title = {({{I Can}}'t {{Get No}}) {{Saturation}}: {{A}} Simulation and Guidelines for Sample Sizes in Qualitative Research},
  shorttitle = {({{I Can}}'t {{Get No}}) {{Saturation}}},
  author = {van Rijnsoever, Frank J.},
  year = {2017},
  month = jul,
  journal = {PLOS ONE},
  volume = {12},
  number = {7},
  pages = {e0181689},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0181689},
  urldate = {2020-08-26},
  abstract = {I explore the sample size in qualitative research that is required to reach theoretical saturation. I conceptualize a population as consisting of sub-populations that contain different types of information sources that hold a number of codes. Theoretical saturation is reached after all the codes in the population have been observed once in the sample. I delineate three different scenarios to sample information sources: ``random chance,'' which is based on probability sampling, ``minimal information,'' which yields at least one new code per sampling step, and ``maximum information,'' which yields the largest number of new codes per sampling step. Next, I use simulations to assess the minimum sample size for each scenario for systematically varying hypothetical populations. I show that theoretical saturation is more dependent on the mean probability of observing codes than on the number of codes in a population. Moreover, the minimal and maximal information scenarios are significantly more efficient than random chance, but yield fewer repetitions per code to validate the findings. I formulate guidelines for purposive sampling and recommend that researchers follow a minimum information scenario.},
  langid = {english},
  keywords = {Computer and information sciences,Data management,Medicine and health sciences,Number theory,Probability theory,Qualitative studies,Simulation and modeling,Social sciences}
}

@article{rogers_how_1992,
  title = {How a Publicity Blitz Created the Myth of Subliminal Advertising},
  author = {Rogers, Stuart},
  year = {1992/1993},
  journal = {Public Relations Quarterly},
  volume = {37},
  number = {4},
  pages = {12},
  publisher = {Public Relations Quarterly},
  address = {Rhinebeck, United States},
  issn = {00333700},
  urldate = {2022-04-28},
  abstract = {In 1957, James M. Vicary, an unemployed market researcher, made a startling announcement based on research in high-speed photography, which was later popularized by Eastman Kodak Company. Vicary invented a new pseudoscience of subliminal advertising, and proceeded to convince chief executive officers, marketing directors, and advertising managers of multimillion-dollar corporations that consumers could comprehend information projected at 1/60,000th of a second, even though they could not actually "see" it. Vicary launched a media relations program intended to promote subliminal advertising as a way to sell a lot of product. Eventually, research disputed Vicary's claims. However, Vicary had signed contracts for retainer and consulting fees for which he reportedly collected millions of dollars. Vicary's scam appears to have lessons for today's practitioners.},
  copyright = {Copyright Public Relations Quarterly Winter 1992-1993},
  langid = {english},
  keywords = {Advertising,Advertising And Public Relations,Breweries,Experiments,Film studios,Fraud,History,Misrepresentation,Motion pictures,Psychology,Public relations,Reporters,Research}
}

@article{rogers_using_1993,
  title = {Using Significance Tests to Evaluate Equivalence between Two Experimental Groups.},
  author = {Rogers, James L. and Howard, Kenneth I. and Vessey, John T.},
  year = {1993},
  journal = {Psychological bulletin},
  volume = {113},
  number = {3},
  pages = {553--565},
  doi = {http://dx.doi.org/10.1037/0033-2909.113.3.553},
  urldate = {2016-05-15},
  annotation = {00477}
}

@article{roig_plagiarism_2009,
  title = {Plagiarism: Consider the Context},
  shorttitle = {Plagiarism},
  author = {Roig, Miguel},
  year = {2009},
  month = aug,
  journal = {Science (New York, N.Y.)},
  volume = {325},
  number = {5942},
  pages = {813--814},
  issn = {1095-9203},
  doi = {10.1126/science.325_813c},
  langid = {english},
  pmid = {19679793},
  keywords = {Language,Plagiarism,Publishing}
}

@article{ropovik_neglect_2021,
  title = {Neglect of Publication Bias Compromises Meta-Analyses of Educational Research},
  author = {Ropovik, Ivan and Adamkovic, Matus and Greger, David},
  year = {2021},
  month = jun,
  journal = {PLOS ONE},
  volume = {16},
  number = {6},
  pages = {e0252415},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0252415},
  urldate = {2022-04-06},
  abstract = {Because negative findings have less chance of getting published, available studies tend to be a biased sample. This leads to an inflation of effect size estimates to an unknown degree. To see how meta-analyses in education account for publication bias, we surveyed all meta-analyses published in the last five years in the Review of Educational Research and Educational Research Review. The results show that meta-analyses usually neglect publication bias adjustment. In the minority of meta-analyses adjusting for bias, mostly non-principled adjustment methods were used, and only rarely were the conclusions based on corrected estimates, rendering the adjustment inconsequential. It is argued that appropriate state-of-the-art adjustment (e.g., selection models) should be attempted by default, yet one needs to take into account the uncertainty inherent in any meta-analytic inference under bias. We conclude by providing practical recommendations on dealing with publication bias.},
  langid = {english},
  keywords = {Metaanalysis,Publication ethics,Research assessment,Research errors,Research reporting guidelines,Schools,Systematic reviews,Visual inspection}
}

@book{rosenthal_contrasts_2000,
  title = {Contrasts and Effect Sizes in Behavioral Research: A Correlational Approach},
  shorttitle = {Contrasts and Effect Sizes in Behavioral Research},
  author = {Rosenthal, Robert and Rosnow, Ralph L. and Rubin, Donald B.},
  year = {2000},
  publisher = {Cambridge University Press},
  address = {Cambridge, U.K. ; New York},
  isbn = {978-0-521-65258-2 978-0-521-65980-2},
  langid = {english},
  lccn = {BF39.2.A52 R67 2000},
  keywords = {Analysis of variance,Psychology,Psychometrics,Social sciences,Statistical methods}
}

@book{rosenthal_experimenter_1966,
  title = {Experimenter Effects in Behavioral Research},
  author = {Rosenthal, Robert},
  year = {1966},
  publisher = {Appleton-Century-Crofts},
  address = {New York},
  langid = {english},
  keywords = {Effect of experimenters on,Psychology,Research}
}

@article{rosnow_effect_2009,
  title = {Effect {{Sizes}}: {{Why}}, {{When}}, and {{How}} to {{Use Them}}},
  shorttitle = {Effect {{Sizes}}},
  author = {Rosnow, Ralph L. and Rosenthal, Robert},
  year = {2009},
  month = jan,
  journal = {Zeitschrift f{\"u}r Psychologie / Journal of Psychology},
  volume = {217},
  number = {1},
  pages = {6--14},
  issn = {0044-3409},
  doi = {10.1027/0044-3409.217.1.6},
  urldate = {2015-12-01},
  langid = {english},
  annotation = {00083}
}

@article{ross-hellauer_survey_2017,
  title = {Survey on Open Peer Review: {{Attitudes}} and Experience amongst Editors, Authors and Reviewers},
  shorttitle = {Survey on Open Peer Review},
  author = {{Ross-Hellauer}, Tony and Deppe, Arvid and Schmidt, Birgit},
  year = {2017},
  month = dec,
  journal = {PLOS ONE},
  volume = {12},
  number = {12},
  pages = {e0189311},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0189311},
  urldate = {2019-09-22},
  abstract = {Open peer review (OPR) is a cornerstone of the emergent Open Science agenda. Yet to date no large-scale survey of attitudes towards OPR amongst academic editors, authors, reviewers and publishers has been undertaken. This paper presents the findings of an online survey, conducted for the OpenAIRE2020 project during September and October 2016, that sought to bridge this information gap in order to aid the development of appropriate OPR approaches by providing evidence about attitudes towards and levels of experience with OPR. The results of this cross-disciplinary survey, which received 3,062 full responses, show the majority (60.3\%) of respondents to be believe that OPR as a general concept should be mainstream scholarly practice (although attitudes to individual traits varied, and open identities peer review was not generally favoured). Respondents were also in favour of other areas of Open Science, like Open Access (88.2\%) and Open Data (80.3\%). Among respondents we observed high levels of experience with OPR, with three out of four (76.2\%) reporting having taken part in an OPR process as author, reviewer or editor. There were also high levels of support for most of the traits of OPR, particularly open interaction, open reports and final-version commenting. Respondents were against opening reviewer identities to authors, however, with more than half believing it would make peer review worse. Overall satisfaction with the peer review system used by scholarly journals seems to strongly vary across disciplines. Taken together, these findings are very encouraging for OPR's prospects for moving mainstream but indicate that due care must be taken to avoid a ``one-size fits all'' solution and to tailor such systems to differing (especially disciplinary) contexts. OPR is an evolving phenomenon and hence future studies are to be encouraged, especially to further explore differences between disciplines and monitor the evolution of attitudes.},
  langid = {english},
  keywords = {Agriculture,Ecology and environmental sciences,Open access publishing,Open science,Peer review,Scientific publishing,Social sciences,Surveys}
}

@article{rouder_bayesian_2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  urldate = {2016-01-11},
  langid = {english},
  annotation = {01475}
}

@article{rouder_minimizing_2019,
  title = {Minimizing {{Mistakes}} in {{Psychological Science}}},
  author = {Rouder, Jeffrey N. and Haaf, Julia M. and Snyder, Hope K.},
  year = {2019},
  month = mar,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {1},
  pages = {3--11},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918801915},
  urldate = {2023-08-25},
  abstract = {Developing and implementing best practices in organizing a lab is challenging, especially in the face of new cultural norms, such as the open-science movement. Part of this challenge in today's landscape is using new technologies, including cloud storage and computer automation. In this article, we discuss a few practices designed to increase the reliability of scientific labs, focusing on ways to minimize common, ordinary mistakes. We borrow principles from the theory of high-reliability organizations, which has been used to characterize operational practices in high-risk environments, such as aviation and health care. Guided by these principles, we focus on five strategies: (a) implementing a lab culture focused on learning from mistakes, (b) using computer automation in data and metadata collection whenever possible, (c) standardizing organizational strategies, (d) using coded rather than menu-driven analyses, and (e) developing expanded documents that record how analyses were performed.},
  langid = {english}
}

@article{rouder_optional_2014,
  title = {Optional Stopping: {{No}} Problem for {{Bayesians}}},
  shorttitle = {Optional Stopping},
  author = {Rouder, Jeffrey N.},
  year = {2014},
  journal = {Psychonomic Bulletin \& Review},
  volume = {21},
  number = {2},
  pages = {301--308},
  urldate = {2016-07-07},
  annotation = {00096}
}

@book{royall_statistical_1997,
  title = {Statistical {{Evidence}}: {{A Likelihood Paradigm}}},
  shorttitle = {Statistical {{Evidence}}},
  author = {Royall, Richard},
  year = {1997},
  month = jun,
  publisher = {{Chapman and Hall/CRC}},
  address = {London ; New York},
  abstract = {Interpreting statistical data as evidence, Statistical Evidence: A Likelihood Paradigm focuses on the law of likelihood, fundamental to solving many of the problems associated with interpreting data in this way. Statistics has long neglected this principle, resulting in a seriously defective methodology. This book redresses the balance, explaining why science has clung to a defective methodology despite its well-known defects. After examining the strengths and weaknesses of the work of Neyman and Pearson and the Fisher paradigm, the author proposes an alternative paradigm which provides, in the law of likelihood, the explicit concept of evidence missing from the other paradigms. At the same time, this new paradigm retains the elements of objective measurement and control of the frequency of misleading results, features which made the old paradigms so important to science. The likelihood paradigm leads to statistical methods that have a compelling rationale and an elegant simplicity,  no longer forcing the reader to choose between frequentist and Bayesian statistics.},
  isbn = {978-0-412-04411-3},
  langid = {english}
}

@article{rozeboom_fallacy_1960,
  title = {The Fallacy of the Null-Hypothesis Significance Test.},
  author = {Rozeboom, William W.},
  year = {1960},
  journal = {Psychological bulletin},
  volume = {57},
  number = {5},
  pages = {416--428},
  doi = {10.1037/h0042040},
  annotation = {00807}
}

@article{rucker_undue_2008,
  title = {Undue Reliance on {{I}}(2) in Assessing Heterogeneity May Mislead},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido and Carpenter, James R. and Schumacher, Martin},
  year = {2008},
  month = nov,
  journal = {BMC medical research methodology},
  volume = {8},
  pages = {79},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-8-79},
  abstract = {BACKGROUND: The heterogeneity statistic I(2), interpreted as the percentage of variability due to heterogeneity between studies rather than sampling error, depends on precision, that is, the size of the studies included. METHODS: Based on a real meta-analysis, we simulate artificially 'inflating' the sample size under the random effects model. For a given inflation factor M = 1, 2, 3,... and for each trial i, we create a M-inflated trial by drawing a treatment effect estimate from the random effects model, using s(i)(2)/M as within-trial sampling variance. RESULTS: As precision increases, while estimates of the heterogeneity variance tau(2) remain unchanged on average, estimates of I(2) increase rapidly to nearly 100\%. A similar phenomenon is apparent in a sample of 157 meta-analyses. CONCLUSION: When deciding whether or not to pool treatment estimates in a meta-analysis, the yard-stick should be the clinical relevance of any heterogeneity present. tau(2), rather than I(2), is the appropriate measure for this purpose.},
  langid = {english},
  pmcid = {PMC2648991},
  pmid = {19036172},
  keywords = {Data Interpretation Statistical,Humans,Models Statistical,Randomized Controlled Trials as Topic,Reproducibility of Results,Research Design,Sample Size,Sensitivity and Specificity}
}

@article{samelson_watson_1980,
  title = {J {{B Watson}}'s {{Little Albert}}, {{Cyril Burt}}'s Twins, and the Need for a Critical Science},
  author = {Samelson, Franz},
  year = {1980},
  month = jul,
  journal = {American Psychologist},
  volume = {35},
  number = {7},
  pages = {619--625},
  publisher = {American Psychological Association},
  issn = {0003-066X},
  doi = {10.1037/0003-066X.35.7.619},
  urldate = {2023-07-23},
  abstract = {An examination of J. B. Watson's correspondence and publications illustrates the gradual development of his views on conditioning, leading to the famous experiment with 'Albert and the rat' in the winter of 1919--1920. It also raises some critical questions about the status of this experiment as a classic paradigm for human conditioning. When combined with the recent criticism of C. Burt's (1966) identical-twin data, this material seems to indicate at least occasional failures of 2 major mechanisms safeguarding scientific knowledge: critical analysis and replication. Further consideration of these issues points toward the need for a more critical science. (32 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Conditioning,Experimentation,need for critical analysis & replication,publications of J. B. Watson & C. Burt}
}

@article{sarafoglou_survey_2022,
  title = {A Survey on How Preregistration Affects the Research Workflow: Better Science but More Work},
  shorttitle = {A Survey on How Preregistration Affects the Research Workflow},
  author = {Sarafoglou, Alexandra and Kovacs, Marton and Bakos, Bence and Wagenmakers, Eric-Jan and Aczel, Balazs},
  year = {2022},
  journal = {Royal Society Open Science},
  volume = {9},
  number = {7},
  pages = {211997},
  publisher = {Royal Society},
  doi = {10.1098/rsos.211997},
  urldate = {2022-07-14},
  abstract = {The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioural sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the perceived benefits and challenges of preregistration from the researcher's perspective. To this end, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. While the benefits outweighed the challenges for the majority of researchers with preregistration experience, this was not the case for the majority of researchers without preregistration experience. The experienced advantages and disadvantages identified in our survey could inform future efforts to improve preregistration and thus help the methodology gain greater acceptance in the scientific community.},
  keywords = {meta-science,open science,replication crisis}
}

@article{scheel_excess_2021,
  title = {An {{Excess}} of {{Positive Results}}: {{Comparing}} the {{Standard Psychology Literature With Registered Reports}}},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Dani{\"e}l},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {25152459211007467},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459211007467},
  urldate = {2022-04-29},
  abstract = {Selectively publishing results that support the tested hypotheses (``positive'' results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  langid = {english},
  keywords = {hypothesis testing,open data,preregistered,publication bias,Registered Reports}
}

@article{scheel_why_2021,
  title = {Why {{Hypothesis Testers Should Spend Less Time Testing Hypotheses}}},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Dani{\"e}l},
  year = {2021},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {4},
  pages = {744--755},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620966795},
  urldate = {2022-01-10},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound ``derivation chain'' between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology's reform movement and help us to develop strong, testable theories, as Paul Meehl urged.},
  langid = {english},
  keywords = {exploratory research,hypothesis testing,replication crisis}
}

@misc{schiavone_consensusbased_2023,
  title = {A {{Consensus-Based Tool}} for {{Evaluating Threats}} to the {{Validity}} of {{Empirical Research}}},
  author = {Schiavone, Sarah R. and Quinn, Kimberly A. and Vazire, Simine},
  year = {2023},
  month = mar,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/fc8v3},
  urldate = {2023-03-22},
  abstract = {We introduce a tool to aid reviewers in identifying potential threats to the validity of empirical research. This tool was developed through consensus-based expert feedback. Reviewers can visit seaboat.io to identify relevant validity threats and generate reports to share alongside traditional peer review reports or in post-publication peer review.},
  langid = {american},
  keywords = {credibility revolution,four validities,Meta-science,metascience,peer review,Social and Behavioral Sciences,Theory and Philosophy of Science,validity}
}

@article{schimmack_ironic_2012,
  title = {The Ironic Effect of Significant Results on the Credibility of Multiple-Study Articles.},
  author = {Schimmack, Ulrich},
  year = {2012},
  journal = {Psychological Methods},
  volume = {17},
  number = {4},
  pages = {551--566},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0029487},
  urldate = {2018-05-24},
  abstract = {Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power has not improved. At the same time, the number of studies has increased from a single study to multiple studies within a single article. It has been overlooked that multiple study articles are severely underpowered because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple study articles undermine the credibility of the reported results and it is likely that questionable research practices contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple study articles is illustrated using Bem's (2011) article on extrasensory perception and Gailliot et al.'s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with non-significant results, if these studies had high power to replicate a published finding.},
  langid = {english},
  annotation = {00000}
}

@article{schmidt_shall_2009,
  title = {Shall We Really Do It Again? {{The}} Powerful Concept of Replication Is Neglected in the Social Sciences.},
  shorttitle = {Shall We Really Do It Again?},
  author = {Schmidt, Stefan},
  year = {2009},
  journal = {Review of General Psychology},
  volume = {13},
  number = {2},
  pages = {90--100},
  issn = {1939-1552, 1089-2680},
  doi = {10.1037/a0015108},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00000}
}

@article{schneider_something_2023,
  title = {Is Something Rotten in the State of {{Denmark}}? {{Cross-national}} Evidence for Widespread Involvement but Not Systematic Use of Questionable Research Practices across All Fields of Research.},
  shorttitle = {Is Something Rotten in the State of {{Denmark}}?},
  author = {Schneider, Jesper W. and Allum, Nick and Andersen, Jens Peter and Petersen, Michael Bang and Mejlgaard, Niels and Zachariae, Robert},
  year = {2023},
  month = sep,
  publisher = {OSF},
  doi = {10.31222/osf.io/r6j3z},
  urldate = {2024-03-17},
  abstract = {Questionable research practices (QRP) are believed to be widespread, but empirical assessments are generally restricted to few types of practices. Furthermore, conceptual confusion is rife with use and prevalence of QRPs often being confused as the same quantity. We present the hitherto most comprehensive study examining QRPs across countries and main scholarly fields. We survey perception, use, prevalence and predictors of QRP among 3,402 researchers in Denmark and 1,307 in the UK, USA, Croatia and Austria.  Results reveal remarkably similar response patterns among Danish and international respondents ({$\tau$} = 0.85). Self-reported use indicates whether respondents have used a QRP in recent publications. Overall, 9 out of 10 admitted having used at least one QRP. Median use is three out of nine QRP-items. Response patterns were also similar across fields, except for arts and humanities where rates were somewhat lower. Self-reported prevalence reflects the frequency of use. On average, prevalence rates were roughly three times lower compared to self-reported use. Findings indicated that self-report patterns were influenced by perceived social acceptability of QRPs. Results suggest that within a restricted time period most researchers use different types QRPs. The prevalence estimates, however, do not suggest outright systematic use of specific QRPs.  Perceived pressure was the strongest systemic predictor for prevalence. Conversely, more attention locally to research cultures, as well as academic age were negatively related to prevalence. Finally, the personality traits conscientiousness and agreeableness were also inversely associated with self-reported prevalence. Findings suggest that explanations of self-reported prevalence of QRPs is a complicated mixture of experience, systemic and individual factors, as well as motivated reasoning.},
  langid = {american}
}

@article{schnuerch_controlling_2020,
  title = {Controlling Decision Errors with Minimal Costs: {{The}} Sequential Probability Ratio t Test.},
  shorttitle = {Controlling Decision Errors with Minimal Costs},
  author = {Schnuerch, Martin and Erdfelder, Edgar},
  year = {2020},
  journal = {Psychological methods},
  volume = {25},
  number = {2},
  pages = {206--226},
  publisher = {American Psychological Association},
  doi = {10.1037/met0000234},
  abstract = {For several years, the public debate in psychological science has been dominated by what is referred to as the reproducibility crisis. This crisis has, inter alia, drawn attention to the need for proper control of statistical decision errors in testing psychological hypotheses. However, conventional methods of error probability control often require fairly large samples. Sequential statistical tests provide an attractive alternative: They can be applied repeatedly during the sampling process and terminate whenever there is sufficient evidence in the data for one of the hypotheses of interest. Thus, sequential tests may substantially reduce the required sample size without compromising predefined error probabilities. Herein, we discuss the most efficient sequential design, the sequential probability ratio test (SPRT), and show how it is easily implemented for a 2-sample t test using standard statistical software. We demonstrate, by means of simulations, that the SPRT not only reliably controls error probabilities but also typically requires substantially smaller samples than standard t tests and other common sequential designs. Moreover, we investigate the robustness of the SPRT against violations of its assumptions. Finally, we illustrate the sequential t test by applying it to an empirical example and provide recommendations on how psychologists can employ it in their own research to benefit from its desirable properties.}
}

@article{schoemann_determining_2017,
  title = {Determining {{Power}} and {{Sample Size}} for {{Simple}} and {{Complex Mediation Models}}},
  author = {Schoemann, Alexander M. and Boulton, Aaron J. and Short, Stephen D.},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {379--386},
  publisher = {SAGE Publications Inc},
  issn = {1948-5506},
  doi = {10.1177/1948550617715068},
  urldate = {2020-12-28},
  abstract = {Mediation analyses abound in social and personality psychology. Current recommendations for assessing power and sample size in mediation models include using a Monte Carlo power analysis simulation and testing the indirect effect with a bootstrapped confidence interval. Unfortunately, these methods have rarely been adopted by researchers due to limited software options and the computational time needed. We propose a new method and convenient tools for determining sample size and power in mediation models. We demonstrate our new method through an easy-to-use application that implements the method. These developments will allow researchers to quickly and easily determine power and sample size for simple and complex mediation models.},
  langid = {english},
  keywords = {mediation,power,R,sample size}
}

@article{schoenegger_social_2023,
  title = {Social Sciences in Crisis: On the Proposed Elimination of the Discussion Section},
  shorttitle = {Social Sciences in Crisis},
  author = {Schoenegger, Philipp and Pils, Raimund},
  year = {2023},
  month = aug,
  journal = {Synthese},
  volume = {202},
  number = {2},
  pages = {54},
  issn = {1573-0964},
  doi = {10.1007/s11229-023-04267-3},
  urldate = {2023-08-10},
  abstract = {The social sciences are facing numerous crises including those related to replication, theory, and applicability. We highlight that these crises imply epistemic malfunctions and affect science communication negatively. Several potential solutions have already been proposed, ranging from statistical improvements to changes in norms of scientific conduct. In this paper, we propose a structural solution: the elimination of the discussion section from social science research papers. We point out that discussion sections allow for an inappropriate narrativization of research that disguises actual results and enables the misstatement of true limitations. We go on to claim that removing this section and outsourcing it to other publications provides several epistemic advantages such as a division of academic labour, adversarial modes of progress, and a better alignment of the personal aims of scientists with the aims of science. After responding to several objections, we conclude that the potential benefits of moving away from the traditional model of academic papers outweigh the costs and have the potential to play a part in addressing the crises in the social sciences alongside other reforms. As such, we take our paper as proffering a further potential solution that should be applied complimentarily with other reform movements such as Open Science and hope that our paper can start a debate on this or similar proposals.},
  langid = {english},
  keywords = {Adversarial collaboration,Incentive structures,Open science,Philosophy of the social sciences,Science reform}
}

@article{schonbrodt_sequential_2017,
  title = {Sequential Hypothesis Testing with {{Bayes}} Factors: {{Efficiently}} Testing Mean Differences},
  shorttitle = {Sequential Hypothesis Testing with {{Bayes}} Factors},
  author = {Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
  year = {2017},
  month = jun,
  journal = {Psychological Methods},
  volume = {22},
  number = {2},
  pages = {322--339},
  issn = {1939-1463},
  doi = {10.1037/MET0000061},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference. (PsycINFO Database Record},
  langid = {english},
  pmid = {26651986},
  keywords = {Bayes Theorem,Data Interpretation Statistical,Humans,Probability,Research Design,Sample Size},
  annotation = {00070}
}

@article{schuirmann_comparison_1987,
  title = {A Comparison of the Two One-Sided Tests Procedure and the Power Approach for Assessing the Equivalence of Average Bioavailability},
  author = {Schuirmann, Donald J.},
  year = {1987},
  journal = {Journal of pharmacokinetics and biopharmaceutics},
  volume = {15},
  number = {6},
  pages = {657--680},
  urldate = {2016-07-22}
}

@article{schulz_sample_2005,
  title = {Sample Size Calculations in Randomised Trials: Mandatory and Mystical},
  shorttitle = {Sample Size Calculations in Randomised Trials},
  author = {Schulz, Kenneth F. and Grimes, David A.},
  year = {2005},
  journal = {The Lancet},
  volume = {365},
  number = {9467},
  pages = {1348--1353},
  doi = {10.1016/S0140-6736(05)61034-3},
  urldate = {2016-03-16}
}

@article{schumi_looking_2011,
  title = {Through the Looking Glass: Understanding Non-Inferiority},
  shorttitle = {Through the Looking Glass},
  author = {Schumi, Jennifer and Wittes, Janet T.},
  year = {2011},
  month = may,
  journal = {Trials},
  volume = {12},
  number = {1},
  pages = {106},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-12-106},
  urldate = {2022-03-05},
  abstract = {Non-inferiority trials test whether a new product is not unacceptably worse than a product already in use. This paper introduces concepts related to non-inferiority, and discusses the regulatory views of both the European Medicines Agency and the United States Food and Drug Administration.},
  langid = {english}
}

@book{schweder_confidence_2016,
  title = {Confidence, {{Likelihood}}, {{Probability}}: {{Statistical Inference}} with {{Confidence Distributions}}},
  shorttitle = {Confidence, {{Likelihood}}, {{Probability}}},
  author = {Schweder, Tore and Hjort, Nils Lid},
  year = {2016},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139046671},
  urldate = {2022-02-19},
  abstract = {This lively book lays out a methodology of confidence distributions and puts them through their paces. Among other merits, they lead to optimal combinations of confidence from different sources of information, and they can make complex models amenable to objective and indeed prior-free analysis for less subjectively inclined statisticians. The generous mixture of theory, illustrations, applications and exercises is suitable for statisticians at all levels of experience, as well as for data-oriented scientists. Some confidence distributions are less dispersed than their competitors. This concept leads to a theory of risk functions and comparisons for distributions of confidence. Neyman--Pearson type theorems leading to optimal confidence are developed and richly illustrated. Exact and optimal confidence distribution is the gold standard for inferred epistemic distributions. Confidence distributions and likelihood functions are intertwined, allowing prior distributions to be made part of the likelihood. Meta-analysis in likelihood terms is developed and taken beyond traditional methods, suiting it in particular to combining information across diverse data sources.},
  isbn = {978-0-521-86160-1}
}

@article{scull_rosenhan_2023,
  title = {Rosenhan Revisited: Successful Scientific Fraud},
  shorttitle = {Rosenhan Revisited},
  author = {Scull, Andrew},
  year = {2023},
  month = feb,
  journal = {History of Psychiatry},
  pages = {0957154X221150878},
  publisher = {SAGE Publications Ltd},
  issn = {0957-154X},
  doi = {10.1177/0957154X221150878},
  urldate = {2023-03-01},
  abstract = {The publication of David Rosenhan?s ?On being sane in insane places? in Science in 1973 played a crucial role in persuading the American Psychiatric Association to revise its diagnostic manual. The third edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-III) in its turn launched a revolution in American psychiatry whose reverberations continue to this day. Rosenhan?s paper continues to be cited hundreds of times a year, and its alleged findings are seen as crucial evidence of psychiatry?s failings. Yet based on the findings of an investigative journalist, Susannah Cahalan, and on records she shared with the author, we now know that this research is a spectacularly successful case of scientific fraud.},
  langid = {english}
}

@article{seaman_equivalence_1998,
  title = {Equivalence Confidence Intervals for Two-Group Comparisons of Means.},
  author = {Seaman, Michael A. and Serlin, Ronald C.},
  year = {1998},
  month = dec,
  journal = {Psychological Methods},
  volume = {3},
  number = {4},
  pages = {403--411},
  issn = {1082-989X},
  doi = {http://dx.doi.org.dianus.libr.tue.nl/10.1037/1082-989X.3.4.403},
  urldate = {2016-07-27},
  abstract = {Methods for determining whether 2 means are practically equivalent are discussed. Existing equivalency-testing procedures are reviewed and compared. An equivalence confidence interval is proposed and compared with the traditional confidence interval for a mean difference. The use of this new interval is described for both confirmatory and exploratory research and is shown to fit within the context of good-enough methods. Adaptations and extensions of the interval are proposed. (PsycINFO Database Record (c) 2013 APA, all rights reserved)(journal abstract)},
  copyright = {{\copyright} American Psychological Association 1998},
  langid = {english},
  keywords = {Confidence Limits (Statistics) (major),Mean (major),Statistical Analysis (major)}
}

@article{sedlmeier_studies_1989,
  title = {Do Studies of Statistical Power Have an Effect on the Power of Studies?},
  author = {Sedlmeier, Peter and Gigerenzer, Gerd},
  year = {1989},
  journal = {Psychological Bulletin},
  volume = {105},
  number = {2},
  pages = {309--316},
  doi = {10.1037/0033-2909.105.2.309},
  urldate = {2015-12-11}
}

@book{shadish_experimental_2001,
  title = {Experimental and Quasi-Experimental Designs for Generalized Causal Inference},
  author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald T.},
  year = {2001},
  publisher = {Houghton Mifflin},
  address = {Boston},
  isbn = {978-0-395-61556-0},
  langid = {english},
  lccn = {BD591 .S48 2001},
  keywords = {Causation,Experiments}
}

@book{shafer_mathematical_1976,
  title = {A Mathematical Theory of Evidence},
  author = {Shafer, Glenn},
  year = {1976},
  publisher = {Princeton University Press},
  address = {Princeton},
  abstract = {Both in science and in practical affairs we reason by combining facts only inconclusively supported by evidence. Building on an abstract understanding of this process of combination, this book constructs a theory of epistemic probability. It opens with a critique of the well-known Bayesian theory of epistemic probability.},
  isbn = {978-0-691-08175-5},
  langid = {english},
  keywords = {31.70 probability,31.73 mathematical statistics,Bewijstheorie,Stochastic processes,Theory of knowledge},
  annotation = {OCLC: 780440455}
}

@article{shmueli_explain_2010,
  ids = {shmueli_explain_2010-1},
  title = {To Explain or to Predict?},
  author = {Shmueli, Galit},
  year = {2010},
  journal = {Statistical science},
  volume = {25},
  number = {3},
  pages = {289--310}
}

@book{sidman_tactics_1960,
  title = {Tactics of {{Scientific Research}}: {{Evaluating Experimental Data}} in {{Psychology}}},
  shorttitle = {Tactics of {{Scientific Research}}},
  author = {Sidman, Murray},
  year = {1960},
  edition = {New edition},
  publisher = {Cambridge Center for Behavioral},
  abstract = {Discussing the major themes of replication, variability, and experimental design, Sidman describes the step-by-step planning of experiments, the need for constant attention to trends of incoming data, and the alteration of plan, method, or design that those trends sometimes make necessary. From the Author's Preface:The conception of experimental methodology that I advance here is neither revolutionary nor new. But I must caution the student not to expect a set of rules of experimental procedure, to be memorized in classic textbook fashion. The pursuit of science is an intensely personal affair. Experimenters cannot always tell us how or why they do what they do, and the fact that their conclusions are sound so much of the time remains a puzzle even to the many philosophers, logicians, and scientists who have devoted a major portion of their time and effort to this problem.},
  isbn = {978-0-9623311-0-7},
  langid = {english}
}

@article{simmons_falsepositive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  urldate = {2015-11-30},
  langid = {english}
}

@misc{simmons_life_2013,
  title = {Life after {{P-Hacking}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2013-01-17/2013-01-19},
  address = {New Orleans, LA}
}

@article{simons_constraints_2017,
  title = {Constraints on {{Generality}} ({{COG}}): {{A Proposed Addition}} to {{All Empirical Papers}}},
  shorttitle = {Constraints on {{Generality}} ({{COG}})},
  author = {Simons, Daniel J. and Shoda, Yuichi and Lindsay, D. Stephen},
  year = {2017},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {12},
  number = {6},
  pages = {1123--1128},
  issn = {1745-6916},
  doi = {10.1177/1745691617708630},
  urldate = {2020-01-18},
  abstract = {Psychological scientists draw inferences about populations based on samples---of people, situations, and stimuli---from those populations. Yet, few papers identify their target populations, and even fewer justify how or why the tested samples are representative of broader populations. A cumulative science depends on accurately characterizing the generality of findings, but current publishing standards do not require authors to constrain their inferences, leaving readers to assume the broadest possible generalizations. We propose that the discussion section of all primary research articles specify Constraints on Generality (i.e., a ``COG'' statement) that identify and justify target populations for the reported findings. Explicitly defining the target populations will help other researchers to sample from the same populations when conducting a direct replication, and it could encourage follow-up studies that test the boundary conditions of the original finding. Universal adoption of COG statements would change publishing incentives to favor a more cumulative science.},
  langid = {english},
  keywords = {generalizability,meta-science,open science,replication,reproducibility,science communication,transparency}
}

@article{simonsohn_pcurve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer.},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {534},
  urldate = {2016-02-02}
}

@article{simonsohn_small_2015,
  title = {Small Telescopes: {{Detectability}} and the Evaluation of Replication Results},
  author = {Simonsohn, Uri},
  year = {2015},
  month = may,
  journal = {Psychological Science},
  volume = {26},
  number = {5},
  pages = {559--569},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797614567341},
  urldate = {2016-08-08},
  abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ``unsuccessful'' replication attempts (i.e., studies yielding p {$>$} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ``protecting'' true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
  langid = {english},
  pmid = {25800521},
  keywords = {hypothesis testing,open materials,replication,statistical power}
}

@article{smaldino_natural_2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  year = {2016},
  month = sep,
  journal = {Royal Society Open Science},
  volume = {3},
  number = {9},
  pages = {160384},
  issn = {2054-5703},
  doi = {10.1098/rsos.160384},
  urldate = {2016-09-25},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing---no deliberate cheating nor loafing---by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  copyright = {{\copyright} 2016 The Authors.. http://creativecommons.org/licenses/by/4.0/Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
  langid = {english}
}

@article{smart_importance_1964,
  title = {The Importance of Negative Results in Psychological Research.},
  author = {Smart, Reginald G.},
  year = {1964},
  month = oct,
  journal = {Canadian Psychologist / Psychologie canadienne},
  volume = {5a},
  number = {4},
  pages = {225--232},
  issn = {0008-4832},
  doi = {10.1037/h0083036},
  urldate = {2023-08-18},
  langid = {english}
}

@article{smith_replication_1970,
  title = {Replication Studies: {{A}} Neglected Aspect of Psychological Research},
  shorttitle = {Replication Studies},
  author = {Smith, Nathaniel C.},
  year = {1970},
  journal = {American Psychologist},
  volume = {25},
  number = {10},
  pages = {970--975},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/h0029774},
  abstract = {A survey of the literature on replication and cross-validation research has revealed that psychologists have tended to ignore replication research. A review of the functions of and deterrents to replication studies is presented. Consideration of the factors influencing replication research suggests that the experimental method, as adopted from physics and chemistry, is invalid for investigating human behavior. (31 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Psychology}
}

@book{smithson_confidence_2003,
  title = {Confidence Intervals},
  author = {Smithson, Michael},
  year = {2003},
  series = {Sage University Papers. {{Quantitative}} Applications in the Social Sciences},
  number = {no. 07/140},
  publisher = {Sage Publications},
  address = {Thousand Oaks, Calif},
  isbn = {978-0-7619-2499-9},
  lccn = {HA31.2 .S59 2003},
  keywords = {Confidence intervals,Mathematics,Social sciences,Statistical methods}
}

@article{sotola_garbage_2022,
  title = {Garbage {{In}}, {{Garbage Out}}? {{Evaluating}} the {{Evidentiary Value}} of {{Published Meta-analyses Using Z-Curve Analysis}}},
  shorttitle = {Garbage {{In}}, {{Garbage Out}}?},
  author = {Sotola, Lukas K.},
  year = {2022},
  month = feb,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {32571},
  issn = {2474-7394},
  doi = {10.1525/collabra.32571},
  urldate = {2022-03-14},
  abstract = {The purpose of the current work was to examine the evidentiary value of the studies that have been included in published meta-analyses as a way of investigating the evidentiary value of the meta-analyses themselves. The studies included in 25 meta-analyses published in the last 10 years in Psychological Bulletin that investigated experimental mean differences were z-curved. Z-curve is a meta-analytic technique that allows one to estimate the predicted replicability, average power, publication bias, and false discovery rate of a population of studies. The results of the z-curves estimated a substantial file drawer in three-quarters of the meta-analyses; and in one-third of the meta-analyses, up to half of the studies are not expected to replicate and up to one-fifth of the studies included could be false positives. Possible reasons for these findings are discussed, and caution in interpreting published meta-analyses is recommended.}
}

@book{spanos_probability_1999,
  title = {Probability Theory and Statistical Inference: Econometric Modeling with Observational Data},
  shorttitle = {Probability Theory and Statistical Inference},
  author = {Spanos, Aris},
  year = {1999},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK ; New York, NY, USA},
  isbn = {978-0-521-41354-1},
  langid = {english},
  lccn = {HB139 .S66 1999},
  keywords = {Econometrics,Probabilities}
}

@article{spanos_who_2013,
  title = {Who Should Be Afraid of the {{Jeffreys-Lindley}} Paradox?},
  author = {Spanos, Aris},
  year = {2013},
  journal = {Philosophy of Science},
  volume = {80},
  number = {1},
  pages = {73--93},
  publisher = {University of Chicago Press Chicago, IL},
  doi = {10.1086/668875}
}

@article{spellman_short_2015,
  title = {A {{Short}} ({{Personal}}) {{Future History}} of {{Revolution}} 2.0},
  author = {Spellman, Barbara A.},
  year = {2015},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {10},
  number = {6},
  pages = {886--899},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691615609918},
  urldate = {2016-06-08},
  abstract = {Crisis of replicability is one term that psychological scientists use for the current introspective phase we are in---I argue instead that we are going through a revolution analogous to a political revolution. Revolution 2.0 is an uprising focused on how we should be doing science now (i.e., in a 2.0 world). The precipitating events of the revolution have already been well-documented: failures to replicate, questionable research practices, fraud, etc. And the fact that none of these events is new to our field has also been well-documented. I suggest four interconnected reasons as to why this time is different: changing technology, changing demographics of researchers, limited resources, and misaligned incentives. I then describe two reasons why the revolution is more likely to catch on this time: technology (as part of the solution) and the fact that these concerns cut across social and life sciences---that is, we are not alone. Neither side in the revolution has behaved well, and each has characterized the other in extreme terms (although, of course, each has had a few extreme actors). Some suggested reforms are already taking hold (e.g., journals asking for more transparency in methods and analysis decisions; journals publishing replications) but the feared tyrannical requirements have, of course, not taken root (e.g., few journals require open data; there is no ban on exploratory analyses). Still, we have not yet made needed advances in the ways in which we accumulate, connect, and extract conclusions from our aggregated research. However, we are now ready to move forward by adopting incremental changes and by acknowledging the multiplicity of goals within psychological science.},
  langid = {english},
  pmid = {26581743},
  keywords = {journal practices,Methodology,replication,scientific practices}
}

@article{spence_tempered_2024,
  title = {Tempered {{Expectations}}: {{A Tutorial}} for {{Calculating}} and {{Interpreting Prediction Intervals}} in the {{Context}} of {{Replications}}},
  shorttitle = {Tempered {{Expectations}}},
  author = {Spence, Jeffrey R. and Stanley, David J.},
  year = {2024},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {1},
  pages = {25152459231217932},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459231217932},
  urldate = {2024-06-17},
  abstract = {Over the last decade, replication research in the psychological sciences has become more visible. One way that replication research can be conducted is to compare the results of the replication study with the original study to look for consistency, that is to say, to evaluate whether the original study is ``replicable.'' Unfortunately, many popular and readily accessible methods for ascertaining replicability, such as comparing significance levels across studies or eyeballing confidence intervals, are generally ill suited to the task of comparing results across studies. To address this issue, we present the prediction interval as a statistic that is effective for determining whether a replication study is inconsistent with the original study. We review the statistical rationale for prediction intervals, demonstrate hand calculations, and provide a walkthrough using an R package for obtaining prediction intervals for means, d values, and correlations. To aid the effective adoption of prediction intervals, we provide guidance on the correct interpretation of results when using prediction intervals in replication research.},
  langid = {english}
}

@book{spiegelhalter_art_2019,
  title = {The {{Art}} of {{Statistics}}: {{How}} to {{Learn}} from {{Data}}},
  shorttitle = {The {{Art}} of {{Statistics}}},
  author = {Spiegelhalter, David},
  year = {2019},
  month = sep,
  edition = {Illustrated edition},
  publisher = {Basic Books},
  address = {New York},
  abstract = {The definitive guide to statistical thinkingStatistics are everywhere, as integral to science as they are to business, and in the popular media hundreds of times a day. In this age of big data, a basic grasp of statistical literacy is more important than ever if we want to separate the fact from the fiction, the ostentatious embellishments from the raw evidence -- and even more so if we hope to participate in the future, rather than being simple bystanders.In The Art of Statistics, world-renowned statistician David Spiegelhalter shows readers how to derive knowledge from raw data by focusing on the concepts and connections behind the math. Drawing on real world examples to introduce complex issues, he shows us how statistics can help us determine the luckiest passenger on the Titanic, whether a notorious serial killer could have been caught earlier, and if screening for ovarian cancer is beneficial. The Art of Statistics not only shows us how mathematicians have used statistical science to solve these problems -- it teaches us how we too can think like statisticians. We learn how to clarify our questions, assumptions, and expectations when approaching a problem, and -- perhaps even more importantly -- we learn how to responsibly interpret the answers we receive.Combining the incomparable insight of an expert with the playful enthusiasm of an aficionado, The Art of Statistics is the definitive guide to stats that every modern person needs.},
  isbn = {978-1-5416-1851-0},
  langid = {english}
}

@article{spiegelhalter_monitoring_1986,
  title = {Monitoring Clinical Trials: Conditional or Predictive Power?},
  shorttitle = {Monitoring Clinical Trials},
  author = {Spiegelhalter, David J. and Freedman, Laurence S. and Blackburn, Patrick R.},
  year = {1986},
  journal = {Controlled clinical trials},
  volume = {7},
  number = {1},
  pages = {8--17},
  publisher = {Elsevier},
  doi = {10.1016/0197-2456(86)90003-6}
}

@article{stanley_finding_2017,
  title = {Finding the Power to Reduce Publication Bias: {{Finding}} the Power to Reduce Publication Bias},
  shorttitle = {Finding the Power to Reduce Publication Bias},
  author = {Stanley, T. D. and Doucouliagos, Hristos and Ioannidis, John P. A.},
  year = {2017},
  journal = {Statistics in Medicine},
  issn = {02776715},
  doi = {10.1002/sim.7228},
  urldate = {2017-02-07},
  langid = {english}
}

@article{stanley_metaregression_2014,
  title = {Meta-Regression Approximations to Reduce Publication Selection Bias.},
  shorttitle = {Meta-Regression Approximations to Reduce Publication Selection Bias},
  author = {Stanley, T. D. and Doucouliagos, Hristos},
  year = {2014},
  month = mar,
  journal = {Research Synthesis Methods},
  volume = {5},
  number = {1},
  pages = {60--78},
  issn = {17592879},
  doi = {10.1002/jrsm.1095},
  urldate = {2018-05-24},
  abstract = {Publication selection bias represents a serious challenge to the integrity of all empirical sciences. We develop meta-regression approximations that are shown to reduce this bias and outperform conventional meta-analytic methods. Our approach is derived from Taylor polynomial approximations to the conditional mean of a truncated distribution. Monte Carlo simulations demonstrate how a new hybrid estimator provides a practical solution. These meta-regression methods are applied to several policy-relevant areas of research including: antidepressant effectiveness, the value of a statistical life and the employment effect of minimum wages and alter what we think we know.},
  langid = {english}
}

@article{steiger_test_2004,
  title = {Beyond the {{F Test}}: {{Effect Size Confidence Intervals}} and {{Tests}} of {{Close Fit}} in the {{Analysis}} of {{Variance}} and {{Contrast Analysis}}.},
  shorttitle = {Beyond the {{F Test}}},
  author = {Steiger, James H.},
  year = {2004},
  journal = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {164--182},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.164},
  urldate = {2016-01-11},
  langid = {english}
}

@article{sterling_publication_1959,
  title = {Publication {{Decisions}} and {{Their Possible Effects}} on {{Inferences Drawn}} from {{Tests}} of {{Significance--Or Vice Versa}}},
  author = {Sterling, Theodore D.},
  year = {1959},
  journal = {Journal of the American Statistical Association},
  volume = {54},
  number = {285},
  pages = {30--34},
  issn = {0162-1459},
  doi = {10.2307/2282137},
  urldate = {2019-12-12},
  abstract = {There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs-an "error of the first kind"-and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance.}
}

@article{stewart_ipd_2002,
  title = {To {{IPD}} or Not to {{IPD}}?: {{Advantages}} and {{Disadvantages}} of {{Systematic Reviews Using Individual Patient Data}}},
  shorttitle = {To {{IPD}} or Not to {{IPD}}?},
  author = {Stewart, Lesley A. and Tierney, Jayne F.},
  year = {2002},
  month = mar,
  journal = {Evaluation \& the Health Professions},
  volume = {25},
  number = {1},
  pages = {76--97},
  issn = {0163-2787, 1552-3918},
  doi = {10.1177/0163278702025001006},
  urldate = {2022-04-05},
  abstract = {Systematic reviews and meta-analyses that obtain original research data on individual participants enrolled in trials have been described as the gold standard of review. However, they may take longer and be more resource intensive than other types of review. The authors describe potential advantages and disadvantages of the individual patient data (IPD) approach, including benefits from improved data quality, benefits afforded by the type of analyses that can be done, and advantages in achieving consensus around results and interpretation by an international multi disciplinary team. Disadvantages and barriers relating to resource and expertise, negotiating collaboration, and software requirements are also discussed. At the outset, reviewers should consider the methodological factors likely to influence results in their particular review setting, together with time and resource constraints, so that an active decision can be made about whether to extract data frompublished reports, collect addi tional or replacement summary data from trialists, or collect IPD.},
  langid = {english}
}

@article{stodden_empirical_2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2584--2589},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708290115},
  urldate = {2019-05-19},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (               i               ) requesting data and code from authors and (               ii               ) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal               Science               after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy---author remission of data and code postpublication upon request---an improvement over no policy, but currently insufficient for reproducibility.},
  langid = {english}
}

@article{strand_error_2023,
  title = {Error Tight: {{Exercises}} for Lab Groups to Prevent Research Mistakes},
  shorttitle = {Error Tight},
  author = {Strand, Julia F.},
  year = {2023},
  journal = {Psychological Methods},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000547},
  abstract = {Scientists, being human, make mistakes. We transcribe things incorrectly, we make errors in our code, and we intend to do things and then forget. The consequences of errors in research may be as minor as wasted time and annoyance, but may be as severe as losing months of work or having to retract an article. The purpose of this tutorial is to help lab groups identify places in their research workflow where errors may occur and identify ways to avoid them. To do this, this article applies concepts from human factors research on how to create lab cultures and workflows that are intended to minimize errors. This article does not provide a one-size-fits-all set of guidelines for specific practices to use (e.g., one platform on which to backup data); instead, it gives examples of ways that mistakes can occur in research along with recommendations for systems that avoid and detect them. This tutorial is intended to be used as a discussion prompt prior to a lab meeting to help researchers reflect on their own processes and implement safeguards to avoid future errors. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Concepts,Data Sets,Errors,Experimental Laboratories,Forgetting,Methodology,Scientists}
}

@article{stroebe_alleged_2014,
  title = {The {{Alleged Crisis}} and the {{Illusion}} of {{Exact Replication}}},
  author = {Stroebe, W. and Strack, F.},
  year = {2014},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {1},
  pages = {59--71},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691613514450},
  urldate = {2015-11-30},
  langid = {english}
}

@article{stroop_studies_1935,
  title = {Studies of Interference in Serial Verbal Reactions.},
  author = {Stroop, J. Ridley},
  year = {1935},
  journal = {Journal of experimental psychology},
  volume = {18},
  number = {6},
  pages = {643--662},
  urldate = {2017-06-19}
}

@article{swift_questionable_2022,
  title = {Questionable Research Practices among Faculty and Students in {{APA-accredited}} Clinical and Counseling Psychology Doctoral Programs},
  author = {Swift, Joshua K. and link will open in a new window {Link to external site}, this and Christopherson, Cody D. and link will open in a new window {Link to external site}, this and Bird, Megan O. and link will open in a new window {Link to external site}, this and Z{\"o}ld, Amanda and link will open in a new window {Link to external site}, this and Goode, Jonathan and link will open in a new window {Link to external site}, this},
  year = {2022},
  month = aug,
  journal = {Training and Education in Professional Psychology},
  volume = {16},
  number = {3},
  pages = {299--305},
  publisher = {Educational Publishing Foundation},
  address = {Washington, US},
  issn = {1931-3918},
  doi = {10.1037/tep0000322},
  urldate = {2022-09-17},
  abstract = {This study examines self-reported engagement in questionable research practices (QRPs) by faculty (N = 164) and students (N = 110) in American Psychological Association--accredited clinical and counseling psychology doctoral programs. Both faculty and student participants were asked to report their own engagement as well as the engagement of their graduate school mentor in 12 QRPs. Nearly 65\% of the faculty participants and 50\% of the student participants reported engaging in at least one QRP. The most commonly reported QRP was selectively reporting findings that worked (35\% for faculty, 26\% for students), and the least commonly admitted was falsifying data (0\% for faculty, 1\% for students). Total number of QRPs engaged in was significantly predicted by knowledge of mentor engagement in QRPs (explaining 34\% of the variance for faculty and 19\% of the variance for students), but it was not predicted by degree year, number of publications, or self-reported researcher reputation. These results suggest that QRPs do occur in the field but perhaps at lower levels than had previously been thought. They also suggest that additional training in QRPs is needed. Training implications and future directions are discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved) (Source: journal abstract) Public Significance Statement---This study surveyed faculty and students in clinical and counseling psychology doctoral programs about their engagement in questionable research practices. The majority of participants reported engaging in at least one questionable research practice, and self-engagement was significantly associated with knowledge of mentor engagement. These results have important training implications for young scholars who aim to produce reliable and accurate research. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  copyright = {{\copyright} 2020, American Psychological Association},
  langid = {english},
  keywords = {Clinical Psychology Graduate Training (major),College Teachers (major),Counseling Psychology,Ethics,Experimental Ethics (major),Experimentation,Graduate Psychology Education (major),Graduate Students (major),Mentor,Self-Report,Test Construction,Training},
  annotation = {(US)}
}

@incollection{taper_philosophy_2011,
  title = {Philosophy of {{Statistics}}},
  booktitle = {Evidence, Evidence Functions, and Error Probabilities},
  author = {Taper, Mark L. and Lele, Subhash R.},
  editor = {Bandyophadhyay, P. S. and Forster, M. R.},
  year = {2011},
  pages = {513--531},
  publisher = {Elsevier, USA},
  urldate = {2017-06-17}
}

@article{taylor_bias_1996,
  title = {Bias in Linear Model Power and Sample Size Calculation Due to Estimating Noncentrality},
  author = {Taylor, Douglas J. and Muller, Keith E.},
  year = {1996},
  journal = {Communications in Statistics-Theory and Methods},
  volume = {25},
  number = {7},
  pages = {1595--1610},
  doi = {10.1080/03610929608831787},
  urldate = {2017-05-08}
}

@article{teare_sample_2014,
  title = {Sample Size Requirements to Estimate Key Design Parameters from External Pilot Randomised Controlled Trials: A Simulation Study},
  shorttitle = {Sample Size Requirements to Estimate Key Design Parameters from External Pilot Randomised Controlled Trials},
  author = {Teare, M. Dawn and Dimairo, Munyaradzi and Shephard, Neil and Hayman, Alex and Whitehead, Amy and Walters, Stephen J.},
  year = {2014},
  month = jul,
  journal = {Trials},
  volume = {15},
  number = {1},
  pages = {264},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-15-264},
  urldate = {2020-12-17},
  abstract = {External pilot or feasibility studies can be used to estimate key unknown parameters to inform the design of the definitive randomised controlled trial (RCT). However, there is little consensus on how large pilot studies need to be, and some suggest inflating estimates to adjust for the lack of precision when planning the definitive RCT.},
  langid = {english}
}

@article{tendeiro_diagnosing_2024,
  title = {Diagnosing the {{Misuse}} of the {{Bayes Factor}} in {{Applied Research}}},
  author = {Tendeiro, Jorge N. and Kiers, Henk A. L. and Hoekstra, Rink and Wong, Tsz Keung and Morey, Richard D.},
  year = {2024},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {1},
  pages = {25152459231213371},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459231213371},
  urldate = {2024-02-15},
  abstract = {Hypothesis testing is often used for inference in the social sciences. In particular, null hypothesis significance testing (NHST) and its p value have been ubiquitous in published research for decades. Much more recently, null hypothesis Bayesian testing (NHBT) and its Bayes factor have also started to become more commonplace in applied research. Following preliminary work by Wong and colleagues, we investigated how, and to what extent, researchers misapply the Bayes factor in applied psychological research by means of a literature study. Based on a final sample of 167 articles, our results indicate that, not unlike NHST and the  {$p$}  value, the use of NHBT and the Bayes factor also shows signs of misconceptions. We consider the root causes of the identified problems and provide suggestions to improve the current state of affairs. This article is aimed to assist researchers in drawing the best inferences possible while using NHBT and the Bayes factor in applied research.},
  langid = {english}
}

@article{tendeiro_review_2019,
  title = {A Review of Issues about Null Hypothesis {{Bayesian}} Testing},
  author = {Tendeiro, Jorge N. and Kiers, Henk A. L.},
  year = {2019},
  month = may,
  journal = {Psychological Methods},
  issn = {1939-1463},
  doi = {10.1037/met0000221},
  abstract = {Null hypothesis significance testing (NHST) has been under scrutiny for decades. The literature shows overwhelming evidence of a large range of problems affecting NHST. One of the proposed alternatives to NHST is using Bayes factors instead of p values. Here we denote the method of using Bayes factors to test point null models as "null hypothesis Bayesian testing" (NHBT). In this article we offer a wide overview of potential issues (limitations or sources of misinterpretation) with NHBT which is currently missing in the literature. We illustrate many of the shortcomings of NHBT by means of reproducible examples. The article concludes with a discussion of NHBT in particular and testing in general. In particular, we argue that posterior model probabilities should be given more emphasis than Bayes factors, because only the former provide direct answers to the most common research questions under consideration. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {31094544}
}

@article{terrin_adjusting_2003,
  title = {Adjusting for Publication Bias in the Presence of Heterogeneity},
  author = {Terrin, Norma and Schmid, Christopher H. and Lau, Joseph and Olkin, Ingram},
  year = {2003},
  month = jul,
  journal = {Statistics in Medicine},
  volume = {22},
  number = {13},
  pages = {2113--2126},
  issn = {0277-6715},
  doi = {10.1002/sim.1461},
  abstract = {It is known that the existence of publication bias can influence the conclusions of a meta-analysis. Some methods have been developed to deal with publication bias, but issues remain. One particular method called 'trim and fill' is designed to adjust for publication bias. The method, which is intuitively appealing and comprehensible by non-statisticians, is based on a simple and popular graphical tool called the funnel plot. We present a simulation study designed to evaluate the behaviour of this method. Our results indicate that when the studies are heterogeneous (that is, when they estimate different effects), trim and fill may inappropriately adjust for publication bias where none exists. We found that trim and fill may spuriously adjust for non-existent bias if (i) the variability among studies causes some precisely estimated studies to have effects far from the global mean or (ii) an inverse relationship between treatment efficacy and sample size is introduced by the studies' a priori power calculations. The results suggest that the funnel plot itself is inappropriate for heterogeneous meta-analyses. Selection modelling is an alternative method warranting further study. It performed better than trim and fill in our simulations, although its frequency of convergence varied, depending on the simulation parameters.},
  langid = {english},
  pmid = {12820277},
  keywords = {Computer Simulation,Data Interpretation Statistical,Meta-Analysis as Topic,Models Statistical,Odds Ratio,Publication Bias,Treatment Outcome}
}

@article{terschure_accumulation_2019,
  title = {Accumulation {{Bias}} in {{Meta-Analysis}}: {{The Need}} to {{Consider Time}} in {{Error Control}}},
  shorttitle = {Accumulation {{Bias}} in {{Meta-Analysis}}},
  author = {{ter Schure}, Judith and Gr{\"u}nwald, Peter D.},
  year = {2019},
  month = may,
  journal = {arXiv:1905.13494 [math, stat]},
  eprint = {1905.13494},
  primaryclass = {math, stat},
  urldate = {2019-07-13},
  abstract = {Studies accumulate over time and meta-analyses are mainly retrospective. These two characteristics introduce dependencies between the analysis time, at which a series of studies is up for meta-analysis, and results within the series. Dependencies introduce bias --- Accumulation Bias --- and invalidate the sampling distribution assumed for p-value tests, thus inflating type-I errors. But dependencies are also inevitable, since for science to accumulate efficiently, new research needs to be informed by past results. Here, we investigate various ways in which time influences error control in meta-analysis testing. We introduce an Accumulation Bias Framework that allows us to model a wide variety of practically occurring dependencies, including study series accumulation, meta-analysis timing, and approaches to multiple testing in living systematic reviews. The strength of this framework is that it shows how all dependencies affect p-value-based tests in a similar manner. This leads to two main conclusions. First, Accumulation Bias is inevitable, and even if it can be approximated and accounted for, no valid p-value tests can be constructed. Second, tests based on likelihood ratios withstand Accumulation Bias: they provide bounds on error probabilities that remain valid despite the bias. We leave the reader with a choice between two proposals to consider time in error control: either treat individual (primary) studies and meta-analyses as two separate worlds --- each with their own timing --- or integrate individual studies in the meta-analysis world. Taking up likelihood ratios in either approach allows for valid tests that relate well to the accumulating nature of scientific knowledge. Likelihood ratios can be interpreted as betting profits, earned in previous studies and invested in new ones, while the meta-analyst is allowed to cash out at any time and advise against future studies.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{thompson_effect_2007,
  title = {Effect Sizes, Confidence Intervals, and Confidence Intervals for Effect Sizes},
  author = {Thompson, Bruce},
  year = {2007},
  month = may,
  journal = {Psychology in the Schools},
  volume = {44},
  number = {5},
  pages = {423--432},
  issn = {00333085, 15206807},
  doi = {10.1002/pits.20234},
  urldate = {2015-11-30},
  langid = {english}
}

@article{tullock_publication_1959,
  title = {Publication {{Decisions}} and {{Tests}} of {{Significance}}---{{A Comment}}},
  author = {Tullock, Gordon},
  year = {1959},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {54},
  number = {287},
  pages = {593--593},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1959.10501522},
  urldate = {2022-04-04},
  abstract = {The custom of only publishing research when it reaches a certain degree of significance is likely to lead to errors, not through repetition of the same experiments, but over many different experiments.}
}

@article{tunc_falsificationist_2023,
  title = {A {{Falsificationist Treatment}} of {{Auxiliary Hypotheses}} in {{Social}} and {{Behavioral Sciences}}: {{Systematic Replications Framework}}},
  shorttitle = {A {{Falsificationist Treatment}} of {{Auxiliary Hypotheses}} in {{Social}} and {{Behavioral Sciences}}},
  author = {Tun{\c c}, Duygu Uygun and Tun{\c c}, Mehmet Necip},
  year = {2023},
  month = sep,
  journal = {Meta-Psychology},
  volume = {7},
  issn = {2003-2714},
  doi = {10.15626/MP.2021.2756},
  urldate = {2024-03-15},
  abstract = {Auxiliary hypotheses~AHs are indispensable in hypothesis-testing, because without them specification of testable predictions and consequently falsification is impossible. However, as AHs enter the test along with the main hypothesis, non-corroborative findings are ambiguous. Due to this ambiguity, AHs may also be employed to deflect falsification by providing ``alternative explanations'' of findings. This is not fatal to the extent that AHs are independently validated and safely relegated to background knowledge. But this is not always possible, especially in the so-called ``softer'' sciences where often theories are loosely organized, measurements are noisy, and constructs are vague. The Systematic Replications Framework (SRF) provides a methodological solution by disentangling the implications of the findings for the main hypothesis and the AHs through pre-planned series of systematically interlinked close and conceptual replications. SRF facilitates testing alternative explanations associated with different AHs and thereby increases test severity across a battery of tests. In this way, SRF assesses whether the corroboration of a hypothesis is conditional on particular AHs, and thus allows for a more objective evaluation of its empirical support and whether post hoc modifications to the theory are progressive or degenerative in the Lakatosian sense. Finally, SRF has several advantages over randomization-based systematic replication proposals, which generally assume a problematic neo-operationalist approach that prescribes exploration-oriented strategies in confirmatory contexts.},
  copyright = {Copyright (c) 2023 Duygu Uygun Tun{\c c}, Mehmet Necip Tun{\c c}},
  langid = {english},
  keywords = {Adversarial Collaboration,Auxiliary Hypotheses,Duhem-Quine Thesis,Empirical Underdetermination,Falsificationism}
}

@article{tversky_belief_1971,
  title = {Belief in the Law of Small Numbers},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1971},
  journal = {Psychological Bulletin},
  volume = {76},
  number = {2},
  pages = {105--110},
  issn = {1939-1455(Electronic);0033-2909(Print)},
  doi = {10.1037/h0031322},
  abstract = {Reports that people have erroneous intuitions about the laws of chance. In particular, they regard a sample randomly drawn from a population as highly representative, I.e., similar to the population in all essential characteristics. The prevalence of the belief and its unfortunate consequences for psychological research are illustrated by the responses of 84 professional psychologists to a questionnaire concerning research decisions.},
  copyright = {(c) 2015 APA, all rights reserved},
  keywords = {*Methodology,*Statistical Analysis,Consequence}
}

@article{tversky_features_1977,
  title = {Features of Similarity},
  author = {Tversky, Amos},
  year = {1977},
  journal = {Psychological review},
  volume = {84},
  number = {4},
  pages = {327--352},
  doi = {10.1037/0033-295X.84.4.327},
  urldate = {2017-06-18}
}

@article{ulrich_properties_2018,
  title = {Some Properties of P-Curves, with an Application to Gradual Publication Bias},
  author = {Ulrich, Rolf and Miller, Jeff},
  year = {2018},
  journal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {546--560},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000125},
  abstract = {p-curves provide a useful window for peeking into the file drawer in a way that might reveal p-hacking (Simonsohn, Nelson, \& Simmons, 2014a). The properties of p-curves are commonly investigated by computer simulations. On the basis of these simulations, it has been proposed that the skewness of this curve can be used as a diagnostic tool to decide whether the significant p values within a certain domain of research suggest the presence of p-hacking or actually demonstrate that there is a true effect. Here we introduce a rigorous mathematical approach that allows the properties of p-curves to be examined without simulations. This approach allows the computation of a p-curve for any statistic whose sampling distribution is known and thereby allows a thorough evaluation of its properties. For example, it shows under which conditions p-curves would exhibit the shape of a monotone decreasing function. In addition, we used weighted distribution functions to analyze how 2 different types of publication bias (i.e., cliff effects and gradual publication bias) influence the shapes of p-curves. The results of 2 survey experiments with more than 1,000 participants support the existence of a cliff effect at p = .05 and also suggest that researchers tend to be more likely to recommend submission of an article as the level of statistical significance increases beyond this p level. This gradual bias produces right-skewed p-curves mimicking the existence of real effects even when no such effects are actually present. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  keywords = {Computer Simulation,Effect Size (Statistical),Hypothesis Testing,Meta Analysis,Psychology,Scientific Communication,Statistical Significance,Statistical Tests}
}

@article{uyguntunc_epistemic_2023,
  title = {The Epistemic and Pragmatic Function of Dichotomous Claims Based on Statistical Hypothesis Tests},
  author = {Uygun Tun{\c c}, Duygu and Tun{\c c}, Mehmet Necip and Lakens, Dani{\"e}l},
  year = {2023},
  month = jun,
  journal = {Theory \& Psychology},
  volume = {33},
  number = {3},
  pages = {403--423},
  publisher = {SAGE Publications Ltd},
  issn = {0959-3543},
  doi = {10.1177/09593543231160112},
  urldate = {2024-05-20},
  abstract = {Researchers commonly make dichotomous claims based on continuous test statistics. Many have branded the practice as a misuse of statistics and criticize scientists for the widespread application of hypothesis tests to tentatively reject a hypothesis (or not) depending on whether a p-value is below or above an alpha level. Although dichotomous claims are rarely explicitly defended, we argue they play an important epistemological and pragmatic role in science. The epistemological function of dichotomous claims consists in transforming data into quasibasic statements, which are tentatively accepted singular facts that can corroborate or falsify theoretical claims. This transformation requires a prespecified methodological decision procedure such as Neyman-Pearson hypothesis tests. From the perspective of methodological falsificationism these decision procedures are necessary, as probabilistic statements (e.g., continuous test statistics) cannot function as falsifiers of substantive hypotheses. The pragmatic function of dichotomous claims is to facilitate scrutiny and criticism among peers by generating contestable claims, a process referred to by Popper as ``conjectures and refutations.'' We speculate about how the surprisingly widespread use of a 5\% alpha level might have facilitated this pragmatic function. Abandoning dichotomous claims, for example because researchers commonly misuse p-values, would sacrifice their crucial epistemic and pragmatic functions.},
  langid = {english}
}

@article{uyguntunc_falsificationist_2022,
  title = {A {{Falsificationist Treatment}} of {{Auxiliary Hypotheses}} in {{Social}} and {{Behavioral Sciences}}: {{Systematic Replications Framework}}},
  shorttitle = {A {{Falsificationist Treatment}} of {{Auxiliary Hypotheses}} in {{Social}} and {{Behavioral Sciences}}},
  author = {Uygun Tun{\c c}, Duygu and Tun{\c c}, Mehmet Necip},
  year = {2022},
  journal = {Meta-Psychology},
  doi = {10.31234/osf.io/pdm7y},
  urldate = {2022-02-09},
  abstract = {Auxiliary hypotheses (AHs) are indispensable in hypothesis-testing, because without them specification of testable predictions and consequently falsification is impossible. However, as AHs enter the test along with the main hypothesis, non-corroborative findings are ambiguous. Due to this ambiguity, AHs may also be employed to deflect falsification by providing ``alternative explanations'' of findings. This is not fatal to the extent that AHs are independently validated and safely relegated to background knowledge. But this is not always possible, especially in the so-called ``softer'' sciences where often theories are loosely organized, measurements are noisy, and constructs are vague. The Systematic Replications Framework (SRF) provides a methodological solution by disentangling the implications of the findings for the main hypothesis and the AHs through pre-planned series of systematically interlinked close and conceptual replications. SRF facilitates testing alternative explanations associated with different AHs and thereby increases test severity across a battery of tests. In this way, SRF assesses whether the corroboration of a hypothesis is conditional on particular AHs, and thus allows for a more objective evaluation of its empirical support and whether post hoc modifications to the theory are progressive or degenerative in the Lakatosian sense. Finally, SRF has several advantages over randomization-based systematic replication proposals, which generally assume a problematic neo-operationalist approach that prescribes exploration-oriented strategies in confirmatory contexts.},
  langid = {american},
  keywords = {adversarial collaboration,auxiliary hypotheses,Duhem-Quine Thesis,empirical underdetermination,falsificationism,Meta-Psychology,Quantitative Methods,replication,Social and Behavioral Sciences,Theory and Philosophy of Science}
}

@article{valentine_how_2010,
  title = {How {{Many Studies Do You Need}}?: {{A Primer}} on {{Statistical Power}} for {{Meta-Analysis}}},
  shorttitle = {How {{Many Studies Do You Need}}?},
  author = {Valentine, Jeffrey C. and Pigott, Therese D. and Rothstein, Hannah R.},
  year = {2010},
  month = apr,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {35},
  number = {2},
  pages = {215--247},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/1076998609346961},
  urldate = {2020-12-28},
  abstract = {In this article, the authors outline methods for using fixed and random effects power analysis in the context of meta-analysis. Like statistical power analysis for primary studies, power analysis for meta-analysis can be done either prospectively or retrospectively and requires assumptions about parameters that are unknown. The authors provide some suggestions for thinking about these parameters, in particular for the random effects variance component. The authors also show how the typically uninformative retrospective power analysis can be made more informative. The authors then discuss the value of confidence intervals, show how they could be used in addition to or instead of retrospective power analysis, and also demonstrate that confidence intervals can convey information more effectively in some situations than power analyses alone. Finally, the authors take up the question ``How many studies do you need to do a meta-analysis?'' and show that, given the need for a conclusion, the answer is ``two studies,'' because all other synthesis techniques are less transparent and/or are less likely to be valid. For systematic reviewers who choose not to conduct a quantitative synthesis, the authors provide suggestions for both highlighting the current limitations in the research base and for displaying the characteristics and results of studies that were found to meet inclusion criteria.},
  langid = {english},
  keywords = {meta-analysis,research methodology,statistics}
}

@article{vandeschoot_systematic_2017,
  title = {A Systematic Review of {{Bayesian}} Articles in Psychology: {{The}} Last 25 Years.},
  shorttitle = {A Systematic Review of {{Bayesian}} Articles in Psychology},
  author = {{van de Schoot}, Rens and Winter, Sonja D. and Ryan, Ois{\'i}n and {Zondervan-Zwijnenburg}, Mari{\"e}lle and Depaoli, Sarah},
  year = {2017},
  month = jun,
  journal = {Psychological Methods},
  volume = {22},
  number = {2},
  pages = {217--239},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000100},
  urldate = {2018-11-30},
  abstract = {Although the statistical tools most often used by researchers in the field of psychology over the last 25 years are based on frequentist statistics, it is often claimed that the alternative Bayesian approach to statistics is gaining in popularity. In the current article, we investigated this claim by performing the very first systematic review of Bayesian psychological articles published between 1990 and 2015 (n ϭ 1,579). We aim to provide a thorough presentation of the role Bayesian statistics plays in psychology. This historical assessment allows us to identify trends and see how Bayesian methods have been integrated into psychological research in the context of different statistical frameworks (e.g., hypothesis testing, cognitive models, IRT, SEM, etc.). We also describe take-home messages and provide ``big-picture'' recommendations to the field as Bayesian statistics becomes more popular. Our review indicated that Bayesian statistics is used in a variety of contexts across subfields of psychology and related disciplines. There are many different reasons why one might choose to use Bayes (e.g., the use of priors, estimating otherwise intractable models, modeling uncertainty, etc.). We found in this review that the use of Bayes has increased and broadened in the sense that this methodology can be used in a flexible manner to tackle many different forms of questions. We hope this presentation opens the door for a larger discussion regarding the current state of Bayesian statistics, as well as future trends.},
  langid = {english},
  annotation = {00047}
}

@article{vandeschoot_use_2021,
  title = {The {{Use}} of {{Questionable Research Practices}} to {{Survive}} in {{Academia Examined With Expert Elicitation}}, {{Prior-Data Conflicts}}, {{Bayes Factors}} for {{Replication Effects}}, and the {{Bayes Truth Serum}}},
  author = {{van de Schoot}, Rens and Winter, Sonja D. and Griffioen, Elian and Grimmelikhuijsen, Stephan and Arts, Ingrid and Veen, Duco and Grandfield, Elizabeth M. and Tummers, Lars G.},
  year = {2021},
  journal = {Frontiers in Psychology},
  volume = {12},
  issn = {1664-1078},
  urldate = {2022-01-27},
  abstract = {The popularity and use of Bayesian methods have increased across many research domains. The current article demonstrates how some less familiar Bayesian methods can be used. Specifically, we applied expert elicitation, testing for prior-data conflicts, the Bayesian Truth Serum, and testing for replication effects via Bayes Factors in a series of four studies investigating the use of questionable research practices (QRPs). Scientifically fraudulent or unethical research practices have caused quite a stir in academia and beyond. Improving science starts with educating Ph.D. candidates: the scholars of tomorrow. In four studies concerning 765 Ph.D. candidates, we investigate whether Ph.D. candidates can differentiate between ethical and unethical or even fraudulent research practices. We probed the Ph.D.s' willingness to publish research from such practices and tested whether this is influenced by (un)ethical behavior pressure from supervisors or peers. Furthermore, 36 academic leaders (deans, vice-deans, and heads of research) were interviewed and asked to predict what Ph.D.s would answer for different vignettes. Our study shows, and replicates, that some Ph.D. candidates are willing to publish results deriving from even blatant fraudulent behavior--data fabrication. Additionally, some academic leaders underestimated this behavior, which is alarming. Academic leaders have to keep in mind that Ph.D. candidates can be under more pressure than they realize and might be susceptible to using QRPs. As an inspiring example and to encourage others to make their Bayesian work reproducible, we published data, annotated scripts, and detailed output on the Open Science Framework (OSF).}
}

@book{vanfraassen_scientific_1980,
  title = {The Scientific Image},
  author = {Van Fraassen, Bas C.},
  year = {1980},
  series = {Clarendon Library of Logic and Philosophy},
  publisher = {Clarendon Press ; Oxford University Press},
  address = {Oxford : New York},
  isbn = {978-0-19-824424-0 978-0-19-824427-1},
  lccn = {Q175 .V335 1980},
  keywords = {Philosophy,Science}
}

@article{vantveer_preregistration_2016,
  title = {Pre-Registration in Social Psychology---{{A}} Discussion and Suggested Template},
  author = {{van 't Veer}, Anna Elisabeth and {Giner-Sorolla}, Roger},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {2--12},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2016.03.004},
  urldate = {2018-09-26},
  abstract = {Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied---reviewed and unreviewed pre-registration---and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.},
  keywords = {Pre-registration,Research methods,Reviewed pre-registration (RPR),Solid science,Unreviewed pre-registration (UPR)},
  annotation = {00000}
}

@article{varkey_principles_2021,
  title = {Principles of {{Clinical Ethics}} and {{Their Application}} to {{Practice}}},
  author = {Varkey, Basil},
  year = {2021},
  journal = {Medical Principles and Practice: International Journal of the Kuwait University, Health Science Centre},
  volume = {30},
  number = {1},
  pages = {17--28},
  issn = {1423-0151},
  doi = {10.1159/000509119},
  abstract = {An overview of ethics and clinical ethics is presented in this review. The 4 main ethical principles, that is beneficence, nonmaleficence, autonomy, and justice, are defined and explained. Informed consent, truth-telling, and confidentiality spring from the principle of autonomy, and each of them is discussed. In patient care situations, not infrequently, there are conflicts between ethical principles (especially between beneficence and autonomy). A four-pronged systematic approach to ethical problem-solving and several illustrative cases of conflicts are presented. Comments following the cases highlight the ethical principles involved and clarify the resolution of these conflicts. A model for patient care, with caring as its central element, that integrates ethical aspects (intertwined with professionalism) with clinical and technical expertise desired of a physician is illustrated.},
  langid = {english},
  pmcid = {PMC7923912},
  pmid = {32498071},
  keywords = {Autonomy,Beneficence,Confidentiality,Ethics,Ethics Clinical,Humans,Informed consent,Informed Consent,Integrated patient care model,Morals,Negotiating,Patient-Centered Care,Personal Autonomy,Problem Solving,Professionalism,Social Justice,Truth Disclosure}
}

@article{vazire_credibility_2022,
  title = {Credibility {{Beyond Replicability}}: {{Improving}} the {{Four Validities}} in {{Psychological Science}}},
  shorttitle = {Credibility {{Beyond Replicability}}},
  author = {Vazire, Simine and Schiavone, Sarah R. and Bottesini, Julia G.},
  year = {2022},
  month = apr,
  journal = {Current Directions in Psychological Science},
  volume = {31},
  number = {2},
  pages = {162--168},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1177/09637214211067779},
  urldate = {2022-04-09},
  abstract = {Psychological science's ``credibility revolution'' has produced an explosion of metascientific work on improving research practices. Although much attention has been paid to replicability (reducing false positives), improving credibility depends on addressing a wide range of problems afflicting psychological science, beyond simply making psychology research more replicable. Here we focus on the ``four validities'' and highlight recent developments---many of which have been led by early-career researchers---aimed at improving these four validities in psychology research. We propose that the credibility revolution in psychology, which has its roots in replicability, can be harnessed to improve psychology's validity more broadly.},
  langid = {english},
  keywords = {credibility revolution,four validities,metascience,replication crisis}
}

@article{vazire_quality_2017,
  title = {Quality {{Uncertainty Erodes Trust}} in {{Science}}},
  author = {Vazire, Simine},
  year = {2017},
  month = feb,
  journal = {Collabra: Psychology},
  volume = {3},
  number = {1},
  pages = {1},
  issn = {2474-7394},
  doi = {10.1525/collabra.74},
  urldate = {2019-12-14},
  abstract = {Article: Quality Uncertainty Erodes Trust in Science},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english}
}

@article{vazire_where_2022,
  title = {Where {{Are}} the {{Self-Correcting Mechanisms}} in {{Science}}?},
  author = {Vazire, Simine and Holcombe, Alex O.},
  year = {2022},
  month = jun,
  journal = {Review of General Psychology},
  volume = {26},
  number = {2},
  pages = {212--223},
  publisher = {SAGE Publications Inc},
  issn = {1089-2680},
  doi = {10.1177/10892680211033912},
  urldate = {2023-08-26},
  abstract = {It is often said that science is self-correcting, but the replication crisis suggests that self-correction mechanisms have fallen short. How can we know whether a particular scientific field has effective self-correction mechanisms, that is, whether its findings are credible? The usual processes that supposedly provide mechanisms for scientific self-correction, such as journal-based peer review and institutional committees, have been inadequate. We describe more verifiable indicators of a field's commitment to self-correction. These fall under the broad headings of 1) transparency, which is already the subject of many reform efforts and 2) critical appraisal, which has received less attention and which we focus on here. Only by obtaining Observable Self-Correction Indicators (OSCIs) can we begin to evaluate the claim that ``science is self-correcting.'' We expect that the veracity of this claim varies across fields and subfields, and suggest that some fields, such as psychology and biomedicine, fall far short of an appropriate level of transparency and, especially, critical appraisal. Fields without robust, verifiable mechanisms for transparency and critical appraisal cannot reasonably be said to be self-correcting, and thus do not warrant the credibility often imputed to science as a whole.},
  langid = {english}
}

@article{verschuere_registered_2018,
  title = {Registered {{Replication Report}} on {{Mazar}}, {{Amir}}, and {{Ariely}} (2008)},
  author = {Verschuere, Bruno and Meijer, Ewout H. and Jim, Ariane and Hoogesteyn, Katherine and Orthey, Robin and McCarthy, Randy J. and Skowronski, John J. and Acar, Oguz A. and Aczel, Balazs and Bakos, Bence E. and Barbosa, Fernando and Baskin, Ernest and B{\`e}gue, Laurent and {Ben-Shakhar}, Gershon and Birt, Angie R. and Blatz, Lisa and Charman, Steve D. and Claesen, Aline and Clay, Samuel L. and Coary, Sean P. and Crusius, Jan and Evans, Jacqueline R. and Feldman, Noa and {Ferreira-Santos}, Fernando and Gamer, Matthias and Gomes, Sara and {Gonz{\'a}lez-Iraizoz}, Marta and Holzmeister, Felix and Huber, Juergen and Isoni, Andrea and Jessup, Ryan K. and Kirchler, Michael and {klein Selle}, Nathalie and Koppel, Lina and Kovacs, Marton and Laine, Tei and Lentz, Frank and Loschelder, David D. and Ludvig, Elliot A. and Lynn, Monty L. and Martin, Scott D. and McLatchie, Neil M. and Mechtel, Mario and Nahari, Galit and {\"O}zdo{\u g}ru, Asil Ali and Pasion, Rita and Pennington, Charlotte R. and Roets, Arne and Rozmann, Nir and Scopelliti, Irene and Spiegelman, Eli and Suchotzki, Kristina and Sutan, Angela and Szecsi, Peter and Tingh{\"o}g, Gustav and Tisserand, Jean-Christian and Tran, Ulrich S. and Van Hiel, Alain and Vanpaemel, Wolf and V{\"a}stfj{\"a}ll, Daniel and Verliefde, Thomas and Vezirian, K{\'e}vin and Voracek, Martin and Warmelink, Lara and Wick, Katherine and Wiggins, Bradford J. and Wylie, Keith and Y{\i}ld{\i}z, Ezgi},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {299--317},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918781032},
  urldate = {2022-02-19},
  abstract = {The self-concept maintenance theory holds that many people will cheat in order to maximize self-profit, but only to the extent that they can do so while maintaining a positive self-concept. Mazar, Amir, and Ariely (2008, Experiment 1) gave participants an opportunity and incentive to cheat on a problem-solving task. Prior to that task, participants either recalled the Ten Commandments (a moral reminder) or recalled 10 books they had read in high school (a neutral task). Results were consistent with the self-concept maintenance theory. When given the opportunity to cheat, participants given the moral-reminder priming task reported solving 1.45 fewer matrices than did those given a neutral prime (Cohen's d = 0.48); moral reminders reduced cheating. Mazar et al.'s article is among the most cited in deception research, but their Experiment 1 has not been replicated directly. This Registered Replication Report describes the aggregated result of 25 direct replications (total N = 5,786), all of which followed the same preregistered protocol. In the primary meta-analysis (19 replications, total n = 4,674), participants who were given an opportunity to cheat reported solving 0.11 more matrices if they were given a moral reminder than if they were given a neutral reminder (95\% confidence interval = [-0.09, 0.31]). This small effect was numerically in the opposite direction of the effect observed in the original study (Cohen's d = -0.04).},
  langid = {english},
  keywords = {cheating,honesty,Many Labs,morality,open data,open materials,preregistered,replication}
}

@article{viamonte_costbenefit_2006,
  title = {A {{Cost-Benefit Analysis}} of {{Risk-Reduction Strategies Targeted}} at {{Older Drivers}}},
  author = {Viamonte, Sarah M. and Ball, Karlene K. and Kilgore, Meredith},
  year = {2006},
  month = dec,
  journal = {Traffic Injury Prevention},
  volume = {7},
  number = {4},
  pages = {352--359},
  publisher = {Taylor \& Francis},
  issn = {1538-9588},
  doi = {10.1080/15389580600791362},
  urldate = {2020-03-08},
  abstract = {Objective. The risk of motor-vehicle collisions increases as driving-related functional abilities decline. These declines can accompany normal or pathological aging and can be identified through driving-related functional screening exams upon license renewal. The objective of this cost-benefit analysis was to determine the utility of four functional screening procedures used to identify drivers at risk for motor-vehicle collisions, as well as an intervention designed to maintain or improve functional abilities. Additionally, this study sought to determine the expected cost per driver if an intervention was designed to target only those drivers who failed the functional ability-based driving screen, versus the expected cost per driver if the intervention was distributed en masse to all drivers 75 years and older. Improving functional abilities in older adults has potential far-reaching health and financial impacts which are broader than their impact of maintaining mobility. Methods. A decision tree was constructed to evaluate the expected costs and benefits of (a) screening all drivers and intervening when indicated (several screening batteries of varying length were considered), (b) no screening, but intervening with all drivers of older age, or (c) neither screening nor intervening (i.e., re-licensing per usual). Test characteristics and risk probabilities were based on a cohort of drivers aged 75 and older from a previous study (Ball et al., 2006). Relevant sensitivity analyses were conducted. Results. Providing all drivers with the speed-of-processing intervention is the most cost-beneficial option (expected cost per driver = \$493.30), even if the cost of the intervention doubles. Sensitivity analysis indicated the effectiveness of the intervention could drop from 86\% to 25\% and the preventative approach of intervening with all drivers remains the most cost-beneficial strategy. The least cost-beneficial option is almost always re-licensing per usual (expected cost per driver = \$1,562.84). Conclusion. Screening drivers upon license renewal is not currently beneficial because the available technology cannot consistently identify drivers at risk for a collision. However, the speed-of-processing intervention has demonstrated efficacy in improving driving competence (Roenker et al., 2003) and is a non-invasive, moderate-cost intervention that has the potential to protect the safety and mobility, as well as the financial interests, of older drivers and the community at large.},
  pmid = {17114092},
  keywords = {Cost-Benefit Analysis,Driver Screen,Intervention,Senior Driver}
}

@article{viechtbauer_conducting_2010,
  title = {Conducting Meta-Analyses in {{R}} with the Metafor Package},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  journal = {J Stat Softw},
  volume = {36},
  number = {3},
  pages = {1--48},
  doi = {http://dx.doi.org/10.18637/jss.v036.i03},
  urldate = {2016-08-07}
}

@article{vohs_multisite_2021,
  title = {A {{Multisite Preregistered Paradigmatic Test}} of the {{Ego-Depletion Effect}}},
  author = {Vohs, Kathleen D. and Schmeichel, Brandon J. and Lohmann, Sophie and Gronau, Quentin F. and Finley, Anna J. and Ainsworth, Sarah E. and Alquist, Jessica L. and Baker, Michael D. and Brizi, Ambra and Bunyi, Angelica and Butschek, Grant J. and Campbell, Collier and Capaldi, Jonathan and Cau, Chuting and Chambers, Heather and Chatzisarantis, Nikos L. D. and Christensen, Weston J. and Clay, Samuel L. and Curtis, Jessica and De Cristofaro, Valeria and {del Rosario}, Kareena and Diel, Katharina and Do{\u g}ruol, Yasemin and Doi, Megan and Donaldson, Tina L. and Eder, Andreas B. and Ersoff, Mia and Eyink, Julie R. and Falkenstein, Angelica and Fennis, Bob M. and Findley, Matthew B. and Finkel, Eli J. and Forgea, Victoria and Friese, Malte and Fuglestad, Paul and {Garcia-Willingham}, Natasha E. and Geraedts, Lea F. and Gervais, Will M. and Giacomantonio, Mauro and Gibson, Bryan and Gieseler, Karolin and Gineikiene, Justina and Gloger, Elana M. and Gobes, Carina M. and Grande, Maria and Hagger, Martin S. and Hartsell, Bethany and Hermann, Anthony D. and Hidding, Jasper J. and Hirt, Edward R. and Hodge, Josh and Hofmann, Wilhelm and Howell, Jennifer L. and Hutton, Robert D. and Inzlicht, Michael and James, Lily and Johnson, Emily and Johnson, Hannah L. and Joyce, Sarah M. and Joye, Yannick and Kaben, Jan Helge and Kammrath, Lara K. and Kelly, Caitlin N. and Kissell, Brian L. and Koole, Sander L. and Krishna, Anand and Lam, Christine and Lee, Kelemen T. and Lee, Nick and Leighton, Dana C. and Loschelder, David D. and Maranges, Heather M. and Masicampo, E. J. and Mazara, Kennedy and McCarthy, Samantha and McGregor, Ian and Mead, Nicole L. and Mendes, Wendy B. and Meslot, Carine and Michalak, Nicholas M. and Milyavskaya, Marina and Miyake, Akira and {Moeini-Jazani}, Mehrad and Muraven, Mark and Nakahara, Erin and Patel, Krishna and Petrocelli, John V. and Pollak, Katja M. and Price, Mindi M. and Ramsey, Haley J. and Rath, Maximilian and Robertson, Jacob A. and Rockwell, Rachael and Russ, Isabella F. and Salvati, Marco and Saunders, Blair and Scherer, Anne and Sch{\"u}tz, Astrid and Schmitt, Kristin N. and Segerstrom, Suzanne C. and Serenka, Benjamin and Sharpinskyi, Konstantyn and Shaw, Meaghan and Sherman, Janelle and Song, Yu and Sosa, Nicholas and Spillane, Kaitlyn and Stapels, Julia and Stinnett, Alec J. and Strawser, Hannah R. and Sweeny, Kate and Theodore, Dominic and Tonnu, Karine and {van Oldenbeuving}, Yasmijn and {vanDellen}, Michelle R. and Vergara, Raiza C. and Walker, Jasmine S. and Waugh, Christian E. and Weise, Feline and Werner, Kaitlyn M. and Wheeler, Craig and White, Rachel A. and Wichman, Aaron L. and Wiggins, Bradford J. and Wills, Julian A. and Wilson, Janie H. and Wagenmakers, Eric-Jan and Albarrac{\'i}n, Dolores},
  year = {2021},
  month = oct,
  journal = {Psychological Science},
  volume = {32},
  number = {10},
  pages = {1566--1581},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797621989733},
  urldate = {2022-03-16},
  abstract = {We conducted a preregistered multilaboratory project (k = 36; N = 3,531) to assess the size and robustness of ego-depletion effects using a novel replication method, termed the paradigmatic replication approach. Each laboratory implemented one of two procedures that was intended to manipulate self-control and tested performance on a subsequent measure of self-control. Confirmatory tests found a nonsignificant result (d = 0.06). Confirmatory Bayesian meta-analyses using an informed-prior hypothesis ({$\delta$} = 0.30, SD = 0.15) found that the data were 4 times more likely under the null than the alternative hypothesis. Hence, preregistered analyses did not find evidence for a depletion effect. Exploratory analyses on the full sample (i.e., ignoring exclusion criteria) found a statistically significant effect (d = 0.08); Bayesian analyses showed that the data were about equally likely under the null and informed-prior hypotheses. Exploratory moderator tests suggested that the depletion effect was larger for participants who reported more fatigue but was not moderated by trait self-control, willpower beliefs, or action orientation.},
  langid = {english},
  keywords = {ego depletion,open data,open materials,preregistered,registered replication,self-control}
}

@article{vosgerau_99_2019,
  title = {99\% Impossible: {{A}} Valid, or Falsifiable, Internal Meta-Analysis},
  shorttitle = {99\% Impossible},
  author = {Vosgerau, Joachim and Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2019},
  month = sep,
  journal = {Journal of Experimental Psychology. General},
  volume = {148},
  number = {9},
  pages = {1628--1639},
  issn = {1939-2222},
  doi = {10.1037/xge0000663},
  abstract = {Several researchers have relied on, or advocated for, internal meta-analysis, which involves statistically aggregating multiple studies in a paper to assess their overall evidential value. Advocates of internal meta-analysis argue that it provides an efficient approach to increasing statistical power and solving the file-drawer problem. Here we show that the validity of internal meta-analysis rests on the assumption that no studies or analyses were selectively reported. That is, the technique is only valid if (a) all conducted studies were included (i.e., an empty file drawer), and (b) for each included study, exactly one analysis was attempted (i.e., there was no p-hacking). We show that even very small doses of selective reporting invalidate internal meta-analysis. For example, the kind of minimal p-hacking that increases the false-positive rate of 1 study to just 8\% increases the false-positive rate of a 10-study internal meta-analysis to 83\%. If selective reporting is approximately zero, but not exactly zero, then internal meta-analysis is invalid. To be valid, (a) an internal meta-analysis would need to contain exclusively studies that were properly preregistered, (b) those preregistrations would have to be followed in all essential aspects, and (c) the decision of whether to include a given study in an internal meta-analysis would have to be made before any of those studies are run. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {31464485},
  keywords = {Humans,Meta-Analysis as Topic,Publication Bias,Reproducibility of Results}
}

@article{vuorre_curating_2018,
  title = {Curating {{Research Assets}}: {{A Tutorial}} on the {{Git Version Control System}}},
  shorttitle = {Curating {{Research Assets}}},
  author = {Vuorre, Matti and Curley, James P.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {219--236},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918754826},
  urldate = {2022-03-17},
  abstract = {Recent calls for improving reproducibility have increased attention to the ways in which researchers curate, share, and collaborate on their research assets. In this Tutorial, we explain how version control systems, such as the popular Git program, support these functions and then show how to use Git with a graphical interface in the RStudio program. This Tutorial is written for researchers with no previous experience using version control systems and covers both single-user and collaborative workflows. The online Supplemental Material provides information on advanced Git command-line functions. Git presents an elegant solution to specific challenges to curating, sharing, and collaborating on research assets and can be implemented in common workflows with little extra effort.},
  langid = {english},
  keywords = {Git,open materials,open science,reproducibility,research methods,version control}
}

@article{wacholder_assessing_2004,
  title = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}: {{An Approach}} for {{Molecular Epidemiology Studies}}},
  shorttitle = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}},
  author = {Wacholder, S. and Chanock, S. and {Garcia-Closas}, M. and {El ghormli}, L. and Rothman, N.},
  year = {2004},
  month = mar,
  journal = {JNCI Journal of the National Cancer Institute},
  volume = {96},
  number = {6},
  pages = {434--442},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/djh075},
  urldate = {2015-11-30},
  langid = {english}
}

@article{wagenmakers_practical_2007,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Wagenmakers, Eric-Jan},
  year = {2007},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1069-9384},
  doi = {10.3758/BF03194105},
  abstract = {In the field of psychology, the practice of p value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of the p value procedure. In particular, p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover, p values do not quantify statistical evidence. This article reviews these p value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to these p value problems is to adopt a model selection perspective and use the Bayesian information criterion (BIC) for statistical inference (Raftery, 1995). The BIC provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from SPSS output.},
  langid = {english},
  pmid = {18087943},
  keywords = {Bayes Theorem,Humans,Models Psychological,Psychology}
}

@article{wagenmakers_registered_2016,
  title = {Registered {{Replication Report}}: {{Strack}}, {{Martin}}, \& {{Stepper}} (1988)},
  shorttitle = {Registered {{Replication Report}}},
  author = {Wagenmakers, Eric-Jan and Beek, T. and Dijkhoff, L. and Gronau, Q. F. and Acosta, A. and Adams, R. B. and Albohn, D. N. and Allard, E. S. and Benning, S. D. and {Blouin-Hudon}, E.-M. and Bulnes, L. C. and Caldwell, T. L. and {Calin-Jageman}, R. J. and Capaldi, C. A. and Carfagno, N. S. and Chasten, K. T. and Cleeremans, A. and Connell, L. and DeCicco, J. M. and Dijkstra, K. and Fischer, A. H. and Foroni, F. and Hess, U. and Holmes, K. J. and Jones, J. L. H. and Klein, O. and Koch, C. and Korb, S. and Lewinski, P. and Liao, J. D. and Lund, S. and Lupianez, J. and Lynott, D. and Nance, C. N. and Oosterwijk, S. and Ozdo{\u g}ru, A. A. and {Pacheco-Unguetti}, A. P. and Pearson, B. and Powis, C. and Riding, S. and Roberts, T.-A. and Rumiati, R. I. and Senden, M. and {Shea-Shumsky}, N. B. and Sobocko, K. and Soto, J. A. and Steiner, T. G. and Talarico, J. M. and {van Allen}, Z. M. and Vandekerckhove, M. and Wainwright, B. and Wayand, J. F. and Zeelenberg, R. and Zetzer, E. E. and Zwaan, R. A.},
  year = {2016},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {6},
  pages = {917--928},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691616674458},
  urldate = {2019-08-16},
  langid = {english},
  pmid = {27784749},
  keywords = {facial feedback hypothesis,many-labs,preregistration,replication}
}

@article{wagenmakers_why_2011,
  title = {Why Psychologists Must Change the Way They Analyze Their Data: The Case of Psi: Comment on {{Bem}} (2011)},
  shorttitle = {Why Psychologists Must Change the Way They Analyze Their Data},
  author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and {van der Maas}, Han L. J.},
  year = {2011},
  month = mar,
  journal = {Journal of Personality and Social Psychology},
  volume = {100},
  number = {3},
  pages = {426--432},
  issn = {1939-1315},
  doi = {10.1037/a0022790},
  abstract = {Does psi exist? D. J. Bem (2011) conducted 9 studies with over 1,000 participants in an attempt to demonstrate that future events retroactively affect people's responses. Here we discuss several limitations of Bem's experiments on psi; in particular, we show that the data analysis was partly exploratory and that one-sided p values may overstate the statistical evidence against the null hypothesis. We reanalyze Bem's data with a default Bayesian t test and show that the evidence for psi is weak to nonexistent. We argue that in order to convince a skeptical audience of a controversial claim, one needs to conduct strictly confirmatory studies and analyze the results with statistical tests that are conservative rather than liberal. We conclude that Bem's p values do not indicate evidence in favor of precognition; instead, they indicate that experimental psychologists need to change the way they conduct their experiments and analyze their data.},
  langid = {english},
  pmid = {21280965},
  keywords = {Bayes Theorem,Behavioral Research,Cognition,Data Interpretation Statistical,Guidelines as Topic,Humans,Parapsychology,Psychology}
}

@article{wald_sequential_1945,
  title = {Sequential Tests of Statistical Hypotheses},
  author = {Wald, Abraham},
  year = {1945},
  journal = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  eprint = {2235829},
  eprinttype = {jstor},
  pages = {117--186},
  doi = {https://www.jstor.org/stable/2240273},
  urldate = {2015-12-03}
}

@article{waldron_not_2022,
  title = {Not All Pre-Registrations Are Equal},
  author = {Waldron, Sophie and Allen, Christopher},
  year = {2022},
  month = dec,
  journal = {Neuropsychopharmacology},
  volume = {47},
  number = {13},
  pages = {2181--2183},
  publisher = {Nature Publishing Group},
  issn = {1740-634X},
  doi = {10.1038/s41386-022-01418-x},
  urldate = {2022-11-11},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Neuroscience,Psychology}
}

@article{wang_pvalue_2019,
  title = {The P-Value and Model Specification in Statistics},
  author = {Wang, Bokai and Zhou, Zhirou and Wang, Hongyue and Tu, Xin M. and Feng, Changyong},
  year = {2019},
  month = jul,
  journal = {General Psychiatry},
  volume = {32},
  number = {3},
  pages = {e100081},
  publisher = {BMJ Publishing Group Ltd},
  issn = {2517-729X},
  doi = {10.1136/gpsych-2019-100081},
  urldate = {2023-07-07},
  abstract = {The p value has been widely used as a way to summarise the significance in data analysis. However, misuse and misinterpretation of the p value is common in practice. Our result shows that if the model specification is wrong, the distribution of the p value may be inappropriate, which makes the decision based on the p value invalid.},
  chapter = {Biostatistical methods in psychiatry},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See:~http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  keywords = {asymptotic distribution,hypothesis testing,linear regression}
}

@article{wason_failure_1960,
  title = {On the Failure to Eliminate Hypotheses in a Conceptual Task},
  author = {Wason, P. C.},
  year = {1960},
  month = jul,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {12},
  number = {3},
  pages = {129--140},
  issn = {0033-555X},
  doi = {10.1080/17470216008416717},
  urldate = {2015-12-30},
  langid = {english}
}

@book{wassmer_group_2016,
  title = {Group {{Sequential}} and {{Confirmatory Adaptive Designs}} in {{Clinical Trials}}},
  author = {Wassmer, Gernot and Brannath, Werner},
  year = {2016},
  series = {Springer {{Series}} in {{Pharmaceutical Statistics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-32562-0},
  urldate = {2020-03-16},
  isbn = {978-3-319-32560-6 978-3-319-32562-0},
  langid = {english}
}

@misc{wassmer_rpact_2019,
  title = {Rpact: {{Confirmatory}} Adaptive Clinical Trial Design and Analysis},
  author = {Wassmer, Gernot and Pahlke, Friedrich},
  year = {2019}
}

@article{weinshall-margel_overlooked_2011,
  title = {Overlooked Factors in the Analysis of Parole Decisions},
  author = {{Weinshall-Margel}, Keren and Shapard, John},
  year = {2011},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {42},
  pages = {E833-E833},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1110910108},
  urldate = {2022-02-14},
  abstract = {Danziger et al. (1) concluded that meal breaks taken by Israeli parole boards influence the boards' decisions. This conclusion depends on the order of cases being random or at least exogenous to the timing of meal breaks. We examined data provided by the authors and obtained additional data from 12 hearing days ( n = 227 decisions).* We also interviewed three attorneys, a parole panel judge, and five personnel at Israeli Prison Services and Court Management, learning that case ordering is not random and that several factors contribute to the downward trend in prisoner success between meal breaks. The most important is that the board tries to complete all cases from one prison before it takes a break and to start with another {\dots}  [↵][1]1To whom correspondence should be addressed. E-mail: johnshapard\{at\}gmail.com.  [1]: \#xref-corresp-1-1},
  chapter = {Letter},
  langid = {english},
  pmid = {21987788}
}

@book{wellek_testing_2010,
  title = {Testing Statistical Hypotheses of Equivalence and Noninferiority},
  author = {Wellek, Stefan},
  year = {2010},
  edition = {2nd ed},
  publisher = {CRC Press},
  address = {Boca Raton},
  isbn = {978-1-4398-0818-4},
  lccn = {QA277 .W46 2010},
  keywords = {Statistical hypothesis testing}
}

@article{westberg_combining_1985,
  title = {Combining {{Independent Statistical Tests}}},
  author = {Westberg, Margareta},
  year = {1985},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {34},
  number = {3},
  eprint = {2987655},
  eprinttype = {jstor},
  pages = {287--296},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0039-0526},
  doi = {10.2307/2987655},
  urldate = {2020-12-30},
  abstract = {In the present study two well-known combination methods, Fisher's and Tippett's, are compared according to their power. The calculations are made for normally and chi-square distributed test statistics. None of the two procedures is uniformly better than the other according to the power but sometimes the power curves cross each other. The calculated power-graphs give guidelines for when to use Fisher's method and when to use Tippett's.}
}

@article{westfall_statistical_2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {5},
  pages = {2020--2045},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Sampling (Experimental),Statistical Power,Stimulus Parameters}
}

@article{westlake_use_1972,
  title = {Use of {{Confidence Intervals}} in {{Analysis}} of {{Comparative Bioavailability Trials}}},
  author = {Westlake, Wilfred J.},
  year = {1972},
  month = aug,
  journal = {Journal of Pharmaceutical Sciences},
  volume = {61},
  number = {8},
  pages = {1340--1341},
  issn = {0022-3549},
  doi = {10.1002/JPS.2600610845},
  urldate = {2018-06-16},
  abstract = {Journal of Pharmaceutical Sciences, pharmacokinetics, biopharmaceutics, pharmacodynamics, drug development, protein-peptide chemistry, drug delivery},
  langid = {english},
  pmid = {5050398},
  keywords = {Biopharmaceutics,Drug Compounding,Humans,Pharmaceutical Preparations,Statistics as Topic},
  annotation = {00297}
}

@book{whitney_balanced_2016,
  title = {Balanced {{Ethics Review}}},
  author = {Whitney, Simon N.},
  year = {2016},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-20705-6},
  urldate = {2021-11-19},
  isbn = {978-3-319-20704-9 978-3-319-20705-6},
  langid = {english}
}

@article{wicherts_degrees_2016,
  title = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}: {{A Checklist}} to {{Avoid}} p-{{Hacking}}},
  shorttitle = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}},
  author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and Aert, Van and M, Robbie C. and Assen, Van and M, Marcel A. L.},
  year = {2016},
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01832},
  urldate = {2017-03-21},
  abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
  langid = {english},
  keywords = {BIAS,Experimental design (study designs),p-hacking,questionable research practices,Research methods education,significance chasing,significance testing}
}

@article{wicherts_psychology_2011,
  title = {Psychology Must Learn a Lesson from Fraud Case},
  author = {Wicherts, Jelte M.},
  year = {2011},
  month = dec,
  journal = {Nature},
  volume = {480},
  number = {7375},
  pages = {7--7},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/480007a},
  urldate = {2023-08-25},
  abstract = {Sharing data could help to avert scandals like the Diederik Stapel revelations, and improve the quality of research, says Jelte M. Wicherts.},
  copyright = {2011 Springer Nature Limited},
  langid = {english},
  keywords = {Ethics,Psychology}
}

@article{wiebels_leveraging_2021,
  title = {Leveraging {{Containers}} for {{Reproducible Psychological Research}}},
  author = {Wiebels, Kristina and Moreau, David},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {25152459211017853},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459211017853},
  urldate = {2022-03-17},
  abstract = {Containers have become increasingly popular in computing and software engineering and are gaining traction in scientific research. They allow packaging up all code and dependencies to ensure that analyses run reliably across a range of operating systems and software versions. Despite being a crucial component for reproducible science, containerization has yet to become mainstream in psychology. In this tutorial, we describe the logic behind containers, what they are, and the practical problems they can solve. We walk the reader through the implementation of containerization within a research workflow with examples using Docker and R. Specifically, we describe how to use existing containers, build personalized containers, and share containers alongside publications. We provide a worked example that includes all steps required to set up a container for a research project and can easily be adapted and extended. We conclude with a discussion of the possibilities afforded by the large-scale adoption of containerization, especially in the context of cumulative, open science, toward a more efficient and inclusive research ecosystem.},
  langid = {english},
  keywords = {containerization,open materials,replication,reproducibility,research workflow,software}
}

@article{wigboldus_encourage_2016,
  title = {Encourage {{Playing}} with {{Data}} and {{Discourage Questionable Reporting Practices}}},
  author = {Wigboldus, Daniel H. J. and Dotsch, Ron},
  year = {2016},
  month = mar,
  journal = {Psychometrika},
  volume = {81},
  number = {1},
  pages = {27--32},
  issn = {1860-0980},
  doi = {10.1007/s11336-015-9445-1},
  urldate = {2019-08-03},
  langid = {english},
  keywords = {Confirmatory Analysis,Data Analysis Phase,Data Analysis Plan,Reporting Practice,Research Practice}
}

@article{williams_impact_1995,
  title = {Impact of {{Measurement Error}} on {{Statistical Power}}: {{Review}} of an {{Old Paradox}}},
  shorttitle = {Impact of {{Measurement Error}} on {{Statistical Power}}},
  author = {Williams, Richard H. and Zimmerman, Donald W. and Zumbo, Bruno D.},
  year = {1995},
  month = jul,
  journal = {The Journal of Experimental Education},
  volume = {63},
  number = {4},
  pages = {363--370},
  publisher = {Routledge},
  issn = {0022-0973},
  doi = {10.1080/00220973.1995.9943470},
  urldate = {2020-12-28},
  abstract = {The relation between test reliability and statistical power has been a controversial issue, perhaps due in part to a 1975 publication in the Psychological Bulletin by Overall and Woodward, ``Unreliability of Difference Scores: A Paradox for the Measurement of Change'', in which they demonstrated that a Student t test based on pretest-posttest differences can attain its greatest power when the difference score reliability is zero. In the present article, the authors attempt to explain this paradox by demonstrating in several ways that power is not a mathematical function of reliability unless either true score variance or error score variance is constant.}
}

@article{wilson_practical_2015,
  title = {A {{Practical Guide}} to {{Value}} of {{Information Analysis}}},
  author = {Wilson, Edward C. F.},
  year = {2015},
  month = feb,
  journal = {PharmacoEconomics},
  volume = {33},
  number = {2},
  pages = {105--121},
  issn = {1179-2027},
  doi = {10.1007/s40273-014-0219-x},
  urldate = {2020-11-10},
  abstract = {Value of information analysis is a quantitative method to estimate the return on investment in proposed research projects. It can be used in a number of ways. Funders of research may find it useful to rank projects in terms of the expected return on investment from a variety of competing projects. Alternatively, trialists can use the principles to identify the efficient sample size of a proposed study as an alternative to traditional power calculations, and finally, a value of information analysis can be conducted alongside an economic evaluation as a quantitative adjunct to the `future research' or `next steps' section of a study write up. The purpose of this paper is to present a brief introduction to the methods, a step-by-step guide to calculation and a discussion of issues that arise in their application to healthcare decision making. Worked examples are provided in the accompanying online appendices as Microsoft Excel spreadsheets.},
  langid = {english}
}

@article{wilsonvanvoorhis_understanding_2007,
  title = {Understanding Power and Rules of Thumb for Determining Sample Sizes},
  author = {Wilson VanVoorhis, Carmen R. and Morgan, Betsy L.},
  year = {2007},
  journal = {Tutorials in quantitative methods for psychology},
  volume = {3},
  number = {2},
  pages = {43--50},
  doi = {10.20982/tqmp.03.2.p043}
}

@book{winer_statistical_1962,
  title = {Statistical Principles in Experimental Design},
  author = {Winer, B. J},
  year = {1962},
  publisher = {New York : McGraw-Hill},
  urldate = {2019-06-03},
  abstract = {"Written primarily for students and research workers in the area of the behavioral sciences, this book is meant to provide a text and comprehensive reference source on statistical principles underlying experimental design. Particular emphasis is given to those designs that are likely to prove useful in research in the behavioral sciences. The book primarily emphasizes the logical basis of principles underlying designs for experiments rather than mathematical derivations associated with relevant sampling distributions. The topics selected for inclusion are those covered in courses taught by the author during the past several years. Students in these courses have widely varying backgrounds in mathematics and come primarily from the fields of psychology, education, economics, sociology, and industrial engineering. It has been the intention of the author to keep the book at a readability level appropriate for students having a mathematical background equivalent to freshman college algebra. From experience with those sections of the book which have been used as text material in dittoed form, there is evidence to indicate that, in large measure, the desired readability level has been attained. Admittedly, however, there are some sections in the book where this readability goal has not been achieved. The first course in design, as taught by the author, has as a prerequisite a basic course in statistical inference. The contents of Chaps. 1 and 2 review the highlights of what is included in the prerequisite material. These chapters are not meant to provide the reader with a first exposure to these topics. They are intended to provide a review of terminology and notation for the concepts which are more fully developed in later chapters. By no means is all the material included in the book covered in a one semester course. In a course of this length, the author has included Chaps. 3, 4, parts of 5, 6, parts of 7, parts of 10, and parts of 11. Chapters 8 through 11 were written to be somewhat independent of each other. Hence one may read, with understanding, in these chapters without undue reference to material in the others. In general, the discussion of principles, interpretations of illustrative examples, and computational procedures are included in successive sections within the same chapter. However, to facilitate the use of the book as a reference source, this procedure is not followed in Chaps. 5 and 6. Basic principles associated with a large class of designs for factorial experiments are discussed in Chap. 5. Detailed illustrative examples of these designs are presented in Chap. 6. For teaching purposes, the author includes relevant material from Chap. 6 with the corresponding material in Chap. 5. Selected topics from Chaps. 7 through 11 have formed the basis for a second course in experimental design. Relatively complete tables for sampling distributions of statistics used in the analysis of experimental designs are included in the Appendix. Ample references to source materials having mathematical proofs for the principles stated in the text are provided"--Preface. (PsycINFO Database Record (c) 2008 APA, all rights reserved).  x, 672 p. : ill. ; 24 cm. Social sciences -- Statistical methods. Psychology -- Statistical methods. Research. Plan d'exp{\'e}rience. Experimentelle Psychologie. Statistik. Versuchsplanung. Psychology -- Research. Social sciences -- Research. Experimenteel ontwerp. Variantieanalyse. Experimental design. Statistics as Topic.},
  langid = {english}
}

@article{wingen_no_2020,
  title = {No {{Replication}}, {{No Trust}}? {{How Low Replicability Influences Trust}} in {{Psychology}}},
  shorttitle = {No {{Replication}}, {{No Trust}}?},
  author = {Wingen, Tobias and Berkessel, Jana B. and Englich, Birte},
  year = {2020},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {11},
  number = {4},
  pages = {454--463},
  publisher = {SAGE Publications Inc},
  issn = {1948-5506},
  doi = {10.1177/1948550619877412},
  urldate = {2021-11-27},
  abstract = {In the current psychological debate, low replicability of psychological findings is a central topic. While the discussion about the replication crisis has a huge impact on psychological research, we know less about how it impacts public trust in psychology. In this article, we examine whether low replicability damages public trust and how this damage can be repaired. Studies 1--3 provide correlational and experimental evidence that low replicability reduces public trust in psychology. Additionally, Studies 3--5 evaluate the effectiveness of commonly used trust-repair strategies such as information about increased transparency (Study 3), explanations for low replicability (Study 4), or recovered replicability (Study 5). We found no evidence that these strategies significantly repair trust. However, it remains possible that they have small but potentially meaningful effects, which could be detected with larger samples. Overall, our studies highlight the importance of replicability for public trust in psychology.},
  langid = {english},
  keywords = {open science,public trust,replicability,replication crisis}
}

@article{wiseman_registered_2019,
  title = {Registered Reports: An Early Example and Analysis},
  shorttitle = {Registered Reports},
  author = {Wiseman, Richard and Watt, Caroline and Kornbrot, Diana},
  year = {2019},
  month = jan,
  journal = {PeerJ},
  volume = {7},
  pages = {e6232},
  issn = {2167-8359},
  doi = {10.7717/peerj.6232},
  urldate = {2019-08-02},
  abstract = {The recent `replication crisis' in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting `Registered Reports', wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology (EJP). A retrospective and observational comparison of Registered and non-Registered Reports published in the EJP during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson's pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.},
  langid = {english}
}

@article{wittes_role_1990,
  title = {The Role of Internal Pilot Studies in Increasing the Efficiency of Clinical Trials},
  author = {Wittes, Janet and Brittain, Erica},
  year = {1990},
  journal = {Statistics in Medicine},
  volume = {9},
  number = {1-2},
  pages = {65--72},
  issn = {1097-0258},
  doi = {10.1002/sim.4780090113},
  urldate = {2020-12-31},
  abstract = {Investigators often design clinical trials without knowing precisely the values of such necessary parameters as the variances or the event rates in the control group. In order to determine reasonable values for such parameters, they may design a small pilot study external to the main trial. In this paper we propose designs, which we term internal pilot studies, that designate a portion of the main trial as a pilot phase. At the end of the internal pilot study, the investigators recompute preselected parameters and recalculate required sample size. The study then proceeds with the modifications dictated by the internal pilot. Final analyses of the results incorporate all data, disregarding the fact that part of the data came from a pilot phase. As one example of this type of design, we consider a study to compare two normally distributed means. By simulation, we show a numerical example for which the effect of the procedure on the {$\alpha$}-level is negligible, but the potential gain in power considerable. We urge considering a similar approach for a number of types of endpoints.},
  copyright = {Copyright {\copyright} 1990 John Wiley \& Sons, Ltd.},
  langid = {english}
}

@article{wong_potential_2022,
  title = {On the {{Potential Mismatch Between}} the {{Function}} of the {{Bayes Factor}} and {{Researchers}}' {{Expectations}}},
  author = {Wong, Tsz Keung and Kiers, Henk and Tendeiro, Jorge},
  year = {2022},
  month = jun,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {36357},
  issn = {2474-7394},
  doi = {10.1525/collabra.36357},
  urldate = {2022-11-08},
  abstract = {The aim of this study is to investigate whether there is a potential mismatch between the usability of a statistical tool and psychology researchers' expectation of it. Bayesian statistics is often promoted as an ideal substitute for frequentists statistics since it coincides better with researchers' expectations and needs. A particular incidence of this is the proposal of replacing Null Hypothesis Significance Testing (NHST) by Null Hypothesis Bayesian Testing (NHBT) using the Bayes factor. In this paper, it is studied to what extent the usability and expectations of NHBT match well. First, a study of the reporting practices in 73 psychological publications was carried out. It was found that eight Questionable Reporting and Interpreting Practices (QRIPs) occur more than once among the practitioners when doing NHBT. Specifically, our analysis provides insight into possible mismatches and their occurrence frequencies. A follow-up survey study has been conducted to assess such mismatches. The sample (N = 108) consisted of psychology researchers, experts in methodology (and/or statistics), and applied researchers in fields other than psychology. The data show that discrepancies exist among the participants. Interpreting the Bayes Factor as posterior odds and not acknowledging the notion of relative evidence in the Bayes Factor are arguably the most concerning ones. The results of the paper suggest that a shift of statistical paradigm cannot solve the problem of misinterpretation altogether if the users are not well acquainted with the tools.}
}

@article{wynants_prediction_2020,
  title = {Prediction Models for Diagnosis and Prognosis of Covid-19: Systematic Review and Critical Appraisal},
  shorttitle = {Prediction Models for Diagnosis and Prognosis of Covid-19},
  author = {Wynants, Laure and Calster, Ben Van and Collins, Gary S. and Riley, Richard D. and Heinze, Georg and Schuit, Ewoud and Bonten, Marc M. J. and Dahly, Darren L. and Damen, Johanna A. and Debray, Thomas P. A. and de Jong, Valentijn M. T. and Vos, Maarten De and Dhiman, Paula and Haller, Maria C. and Harhay, Michael O. and Henckaerts, Liesbet and Heus, Pauline and Kammer, Michael and Kreuzberger, Nina and Lohmann, Anna and Luijken, Kim and Ma, Jie and Martin, Glen P. and McLernon, David J. and Navarro, Constanza L. Andaur and Reitsma, Johannes B. and Sergeant, Jamie C. and Shi, Chunhu and Skoetz, Nicole and Smits, Luc J. M. and Snell, Kym I. E. and Sperrin, Matthew and Spijker, Ren{\'e} and Steyerberg, Ewout W. and Takada, Toshihiko and Tzoulaki, Ioanna and van Kuijk, Sander M. J. and van Bussel, Bas C. T. and van der Horst, Iwan C. C. and van Royen, Florien S. and Verbakel, Jan Y. and Wallisch, Christine and Wilkinson, Jack and Wolff, Robert and Hooft, Lotty and Moons, Karel G. M. and van Smeden, Maarten},
  year = {2020},
  month = apr,
  journal = {BMJ},
  volume = {369},
  pages = {m1328},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj.m1328},
  urldate = {2022-02-09},
  abstract = {Objective To review and appraise the validity and usefulness of published and preprint reports of prediction models for diagnosing coronavirus disease 2019 (covid-19) in patients with suspected infection, for prognosis of patients with covid-19, and for detecting people in the general population at increased risk of covid-19 infection or being admitted to hospital with the disease. Design Living systematic review and critical appraisal by the COVID-PRECISE (Precise Risk Estimation to optimise covid-19 Care for Infected or Suspected patients in diverse sEttings) group. Data sources PubMed and Embase through Ovid, up to 1 July 2020, supplemented with arXiv, medRxiv, and bioRxiv up to 5 May 2020. Study selection Studies that developed or validated a multivariable covid-19 related prediction model. Data extraction At least two authors independently extracted data using the CHARMS (critical appraisal and data extraction for systematic reviews of prediction modelling studies) checklist; risk of bias was assessed using PROBAST (prediction model risk of bias assessment tool). Results 37 421 titles were screened, and 169 studies describing 232 prediction models were included. The review identified seven models for identifying people at risk in the general population; 118 diagnostic models for detecting covid-19 (75 were based on medical imaging, 10 to diagnose disease severity); and 107 prognostic models for predicting mortality risk, progression to severe disease, intensive care unit admission, ventilation, intubation, or length of hospital stay. The most frequent types of predictors included in the covid-19 prediction models are vital signs, age, comorbidities, and image features. Flu-like symptoms are frequently predictive in diagnostic models, while sex, C reactive protein, and lymphocyte counts are frequent prognostic factors. Reported C index estimates from the strongest form of validation available per model ranged from 0.71 to 0.99 in prediction models for the general population, from 0.65 to more than 0.99 in diagnostic models, and from 0.54 to 0.99 in prognostic models. All models were rated at high or unclear risk of bias, mostly because of non-representative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, high risk of model overfitting, and unclear reporting. Many models did not include a description of the target population (n=27, 12\%) or care setting (n=75, 32\%), and only 11 (5\%) were externally validated by a calibration plot. The Jehi diagnostic model and the 4C mortality score were identified as promising models. Conclusion Prediction models for covid-19 are quickly entering the academic literature to support medical decision making at a time when they are urgently needed. This review indicates that almost all pubished prediction models are poorly reported, and at high risk of bias such that their reported predictive performance is probably optimistic. However, we have identified two (one diagnostic and one prognostic) promising models that should soon be validated in multiple cohorts, preferably through collaborative efforts and data sharing to also allow an investigation of the stability and heterogeneity in their performance across populations and settings. Details on all reviewed models are publicly available at https://www.covprecise.org/. Methodological guidance as provided in this paper should be followed because unreliable predictions could cause more harm than benefit in guiding clinical decisions. Finally, prediction model authors should adhere to the TRIPOD (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) reporting guideline. Systematic review registration Protocol https://osf.io/ehc47/, registration https://osf.io/wy245. Readers' note This article is a living systematic review that will be updated to reflect emerging evidence. Updates may occur for up to two years from the date of original publication. This version is update 3 of the original article published on 7 April 2020 (BMJ 2020;369:m1328). Previous updates can be found as data supplements (https://www.bmj.com/content/369/bmj.m1328/related\#datasupp). When citing this paper please consider adding the update number and date of access for clarity.},
  chapter = {Research},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC                 BY. No commercial re-use. See rights and permissions. Published by                 BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {32265220}
}

@article{yarkoni_choosing_2017,
  title = {Choosing {{Prediction Over Explanation}} in {{Psychology}}: {{Lessons From Machine Learning}}},
  shorttitle = {Choosing {{Prediction Over Explanation}} in {{Psychology}}},
  author = {Yarkoni, Tal and Westfall, Jacob},
  year = {2017},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {12},
  number = {6},
  pages = {1100--1122},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691617693393},
  urldate = {2018-09-26},
  abstract = {Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology's near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.},
  langid = {english},
  annotation = {00102}
}

@article{yuan_post_2005,
  title = {On the {{Post Hoc Power}} in {{Testing Mean Differences}}},
  author = {Yuan, Ke-Hai and Maxwell, Scott},
  year = {2005},
  month = jun,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {30},
  number = {2},
  pages = {141--167},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/10769986030002141},
  urldate = {2020-07-14},
  abstract = {Retrospective or post hoc power analysis is recommended by reviewers and editors of many journals. Little literature has been found that gave a serious study of the post hoc power. When the sample size is large, the observed effect size is a good estimator of the true effect size. One would hope that the post hoc power is also a good estimator of the true power. This article studies whether such a power estimator provides valuable information about the true power., Using analytical, numerical, and Monte Carlo approaches, our results show that the estimated power does not provide useful information when the true power is small. It is almost always a biased estimator of the true power. The bias can be negative or positive. Large sample size alone does not guarantee the post hoc power to be a good estimator of the true power. Actually, when the population variance is known, the cumulative distribution function of the post hoc power is solely a function of the population power. This distribution is uniform when the true power equals 0.5 and highly skewed when the true power is near 0 or 1. When the population variance is unknown, the post hoc power behaves essentially the same as when the variance is known.},
  langid = {english}
}

@article{zabell_fisher_1992,
  title = {R. {{A}}. {{Fisher}} and {{Fiducial Argument}}},
  author = {Zabell, S. L.},
  year = {1992},
  month = aug,
  journal = {Statistical Science},
  volume = {7},
  number = {3},
  pages = {369--387},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177011233},
  urldate = {2022-02-19},
  abstract = {The fiducial argument arose from Fisher's desire to create an inferential alternative to inverse methods. Fisher discovered such an alternative in 1930, when he realized that pivotal quantities permit the derivation of probability statements concerning an unknown parameter independent of any assumption concerning its a priori distribution. The original fiducial argument was virtually indistinguishable from the confidence approach of Neyman, although Fisher thought its application should be restricted in ways reflecting his view of inductive reasoning, thereby blending an inferential and a behaviorist viewpoint. After Fisher attempted to extend the fiducial argument to the multiparameter setting, this conflict surfaced, and he then abandoned the unconditional sampling approach of his earlier papers for the conditional approach of his later work. Initially unable to justify his intuition about the passage from a probability assertion about a statistic (conditional on a parameter) to a probability assertion about a parameter (conditional on a statistic), Fisher thought in 1956 that he had finally discovered the way out of this enigma with his concept of recognizable subset. But the crucial argument for the relevance of this concept was founded on yet another intuition--one which, now clearly stated, was later demonstrated to be false by Buehler and Feddersen in 1963.},
  keywords = {Behrens-Fisher problem,Fiducial inference,Jerzy Neyman,Maurice Bartlett,R. A. Fisher,recognize subsets}
}

@book{zenko_red_2015,
  title = {Red {{Team}}: {{How}} to {{Succeed By Thinking Like}} the {{Enemy}}},
  shorttitle = {Red {{Team}}},
  author = {Zenko, Micah},
  year = {2015},
  month = nov,
  edition = {1st edition},
  publisher = {Basic Books},
  address = {New York},
  abstract = {Essential reading for business leaders and policymakers, an in-depth investigation of red teaming, the practice of inhabiting the perspective of potential competitors to gain a strategic advantage Red teaming. The concept is as old as the Devil's Advocate, the eleventh-century Vatican official charged with discrediting candidates for sainthood. Today, red teams are used widely in both the public and the private sector by those seeking to better understand the interests, intentions, and capabilities of institutional rivals. In the right circumstances, red teams can yield impressive results, giving businesses an edge over their competition, poking holes in vital intelligence estimates, and troubleshooting dangerous military missions long before boots are on the ground. But not all red teams are created equal; indeed, some cause more damage than they prevent. Drawing on a fascinating range of case studies, Red Team shows not only how to create and empower red teams, but also what to do with the information they produce. In this vivid, deeply-informed account, national security expert Micah Zenko provides the definitive book on this important strategy -- full of vital insights for decision makers of all kinds.},
  isbn = {978-0-465-04894-6},
  langid = {english}
}

@article{zumbo_note_1998,
  title = {A Note on Misconceptions Concerning Prospective and Retrospective Power},
  author = {Zumbo, Bruno D. and Hubley, Anita M.},
  year = {1998},
  journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  volume = {47},
  number = {2},
  pages = {385--388},
  issn = {1467-9884},
  doi = {10.1111/1467-9884.00139},
  urldate = {2022-01-21},
  abstract = {This paper addresses some misconceptions concerning prospective and retrospective power when journal editors, reviewers and readers want to know the observed power of statistical tests within a completed study. In addition, a practical solution to both the problem of computing the power after a study has been conducted and the critical evaluation of null findings is suggested},
  langid = {english},
  keywords = {Effect size,Research design,Statistical power,Statistics}
}
