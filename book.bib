
@article{abelson_value_2003,
  title = {The {{Value}} of {{Life}} and {{Health}} for {{Public Policy}}},
  author = {Abelson, Peter},
  year = {2003},
  month = jun,
  journal = {Economic Record},
  volume = {79},
  pages = {S2-S13},
  issn = {00130249, 14754932},
  doi = {10.1111/1475-4932.00087},
  langid = {english}
}

@book{aberson_applied_2019,
  title = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}},
  shorttitle = {Applied {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Aberson, Christopher L.},
  year = {2019},
  month = feb,
  edition = {Second},
  publisher = {{Routledge}},
  address = {{New York}},
  abstract = {Applied Power Analysis for the Behavioral Sciences is a practical "how-to" guide to conducting statistical power analyses for psychology and related fields. The book provides a guide to conducting analyses that is appropriate for researchers and students, including those with limited quantitative backgrounds. With practical use in mind, the text provides detailed coverage of topics such as how to estimate expected effect sizes and power analyses for complex designs. The topical coverage of the text, an applied approach, in-depth coverage of popular statistical procedures, and a focus on conducting analyses using R make the text a unique contribution to the power literature. To facilitate application and usability, the text includes ready-to-use R code developed for the text. An accompanying R package called pwr2ppl (available at https://github.com/chrisaberson/pwr2ppl) provides tools for conducting power analyses across each topic covered in the text.},
  isbn = {978-1-138-04456-2},
  langid = {english},
  annotation = {00000}
}

@misc{aert_correcting_2018,
  title = {Correcting for {{Publication Bias}} in a {{Meta-Analysis}} with the {{P-uniform}}* {{Method}}},
  author = {van Aert, Robbie C. M. and van Assen, Marcel A. L. M.},
  year = {2018},
  month = oct,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/zqjr9},
  abstract = {Publication bias is a major threat to the validity of a meta-analysis resulting in overestimated effect sizes. We propose an extension and improvement of the publication bias method p-uniform called p-uniform*. P-uniform* improves upon p-uniform in three ways, as it (i) entails a more efficient estimator, (ii) eliminates the overestimation of effect size caused by between-study variance in true effect size, and (iii) enables estimating and testing for the presence of the between-study variance. We compared the statistical properties of p-uniform* with p-uniform, the selection model approach of Hedges (1992), and the random-effects model. Statistical properties of p-uniform* and the selection model approach were comparable and generally outperformed p-uniform and the random-effects model if publication bias was present. We demonstrate that p-uniform* and the selection model approach estimate average effect size and between-study variance rather well with ten or more studies in the meta-analysis when publication bias is not extreme. P-uniform* generally provides more accurate estimates of the between-study variance in meta-analyses containing many studies (e.g., 60 or more) and if publication bias is present. We offer recommendations for applied researchers, provide an R package as well as an easy-to-use web application for applying p-uniform*.},
  langid = {american},
  keywords = {meta-analysis,Other Statistics and Probability,p-uniform,Physical Sciences and Mathematics,Psychology,publication bias,Quantitative Psychology,selection model approach,Social and Behavioral Sciences,Statistics and Probability}
}

@article{albers_credible_2018,
  title = {Credible {{Confidence}}: {{A Pragmatic View}} on the {{Frequentist}} vs {{Bayesian Debate}}},
  shorttitle = {Credible {{Confidence}}},
  author = {Albers, Casper J. and Kiers, Henk A. L. and van Ravenzwaaij, Don},
  year = {2018},
  month = aug,
  journal = {Collabra: Psychology},
  volume = {4},
  number = {1},
  pages = {31},
  publisher = {{The Regents of the University of California}},
  issn = {2474-7394},
  doi = {10.1525/collabra.149},
  abstract = {Article: Credible Confidence: A Pragmatic View on the Frequentist vs Bayesian Debate},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english}
}

@article{albers_when_2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper J. and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis}
}

@article{albers_when_2018-1,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper J. and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.09.004},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, {$\omega$}2 and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis}
}

@article{aldrich_r_1997,
  title = {R.{{A}}. {{Fisher}} and the Making of Maximum Likelihood 1912-1922},
  author = {Aldrich, John},
  year = {1997},
  month = sep,
  journal = {Statistical Science},
  volume = {12},
  number = {3},
  pages = {162--176},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1030037906},
  abstract = {In 1922 R. A. Fisher introduced the method of maximum likelihood. He first presented the numerical procedure in 1912. This paper considers Fisher's changing justifications for the method, the concepts he developed around it (including likelihood, sufficiency, efficiency and information) and the approaches he discarded (including inverse probability).},
  keywords = {Bayes's postulate,efficiency,Fisher,Information,inverse probability,maximum likelihood,Pearson,student,sufficiency}
}

@article{allison_power_1997,
  title = {Power and Money: {{Designing}} Statistically Powerful Studies While Minimizing Financial Costs},
  shorttitle = {Power and Money},
  author = {Allison, David B. and Allison, Ronald L. and Faith, Myles S. and Paultre, Furcy and {Pi-Sunyer}, F. Xavier},
  year = {1997},
  journal = {Psychological Methods},
  volume = {2},
  number = {1},
  pages = {20--33},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/1082-989X.2.1.20},
  abstract = {Adequate statistical power is increasingly demanded in research designs. However, obtaining adequate research funding is increasingly difficult. This places researchers in a difficult position. In response, the authors advocate an approach to designing studies that considers statistical power and financial concerns simultaneously. Their purpose is twofold: (a) to introduce the general paradigm of cost optimization in the context of power analysis and (b) to present techniques for such optimization. Techniques are presented in the context of a randomized clinical trial. The authors consider (a) selecting optimal cutpoints for subject screening tests; (b) optimally allocating subjects to different treatment conditions; (c) choosing between obtaining more subjects or taking more replicate measurements; and (d) using prerandomization covariates. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cost Containment,Experimentation,Experimenters,Statistical Power}
}

@article{altman_statistics_1995,
  title = {Statistics Notes: {{Absence}} of Evidence Is Not Evidence of Absence},
  shorttitle = {Statistics Notes},
  author = {Altman, Douglas G. and Bland, J. Martin},
  year = {1995},
  month = aug,
  journal = {BMJ},
  volume = {311},
  number = {7003},
  pages = {485},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.311.7003.485},
  abstract = {The non-equivalence of statistical significance and clinical importance has long been recognised, but this error of interpretation remains common. Although a significant result in a large study may sometimes not be clinically important, a far greater problem arises from misinterpretation of non-significant findings. By convention a P value greater than 5\% (P{$>$}0.05) is called ``not significant.'' Randomised controlled clinical trials that do not show a significant difference between the treatments being compared are often called ``negative.'' This term wrongly implies that the study has shown that there is no difference, whereas usually all that has been shown is an absence of evidence of a difference. These are quite different statements. The sample size of controlled trials is generally inadequate, with a consequent lack of power to detect real, and clinically worthwhile, differences in treatment. Freiman et al1 found that only \ldots},
  copyright = {\textcopyright{} 1995 BMJ Publishing Group Ltd.},
  langid = {english},
  pmid = {7647644}
}

@article{anderson_addressing_2017,
  title = {Addressing the ``{{Replication Crisis}}'': {{Using Original Studies}} to {{Design Replication Studies}} with {{Appropriate Statistical Power}}},
  shorttitle = {Addressing the ``{{Replication Crisis}}''},
  author = {Anderson, Samantha F. and Maxwell, Scott E.},
  year = {2017},
  month = mar,
  journal = {Multivariate Behavioral Research},
  pages = {1--20},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171.2017.1289361},
  langid = {english}
}

@incollection{anderson_group_2014,
  title = {Group {{Sequential Design}} in {{R}}},
  booktitle = {Clinical {{Trial Biostatistics}} and {{Biopharmaceutical Applications}}},
  author = {Anderson, Keaven M.},
  year = {2014},
  pages = {179--209},
  publisher = {{CRC Press}},
  address = {{New York}},
  isbn = {978-1-4822-1218-1}
}

@article{anderson_sample-size_2017,
  title = {Sample-Size Planning for More Accurate Statistical Power: {{A}} Method Adjusting Sample Effect Sizes for Publication Bias and Uncertainty},
  shorttitle = {Sample-Size Planning for More Accurate Statistical Power},
  author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
  year = {2017},
  journal = {Psychological science},
  volume = {28},
  number = {11},
  pages = {1547--1562},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}},
  doi = {10.1177/0956797617723724}
}

@article{anderson_theres_2016,
  title = {There's More than One Way to Conduct a Replication Study: {{Beyond}} Statistical Significance.},
  shorttitle = {There's More than One Way to Conduct a Replication Study},
  author = {Anderson, Samantha F. and Maxwell, Scott E.},
  year = {2016},
  journal = {Psychological Methods},
  volume = {21},
  number = {1},
  pages = {1--12},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000051},
  langid = {english}
}

@article{anvari_not_2021,
  title = {Not All Effects Are Indispensable: {{Psychological}} Science Requires Verifiable Lines of Reasoning for Whether an Effect Matters.},
  shorttitle = {Not All Effects Are Indispensable},
  author = {Anvari, Farid and Kievit, Rogier and Lakens, Daniel and Pennington, Charlotte Rebecca and Przybylski, Andrew K. and Tiokhin, Leo and Wiernik, Brenton M. and Orben, Amy},
  year = {2021},
  month = jun,
  journal = {Perspectives on Psychological Science},
  doi = {10.31234/osf.io/g3vtr},
  abstract = {Psychological researchers currently lack guidance for how to make claims about and evaluate the practical relevance and significance of observed effect sizes, i.e. whether a finding will have impact when translated to a different context of application. Although psychologists have recently highlighted theoretical justifications for why small effect sizes might be practically relevant, such justifications fail to provide the information necessary for evaluation and falsification. Claims about whether an observed effect size is practically relevant need to consider both the mechanisms amplifying and counteracting practical relevance, as well as the assumptions underlying each mechanism at play. To provide guidance for making claims about whether an observed effect size is practically relevant in such a way that the claims can be systematically evaluated, we present examples of widely applicable mechanisms and the key assumptions needed for justifying whether an observed effect size can be expected to generalize to different contexts. Routine use of these mechanisms to justify claims about practical relevance has the potential to make researchers' claims about generalizability substantially more transparent. This transparency can help move psychological science towards a more rigorous assessment of when psychological findings can be applied in the world.},
  langid = {american},
  keywords = {benchmarks,effect size,evaluation,Meta-science,practical significance,Social and Behavioral Sciences}
}

@article{anvari_using_2021,
  title = {Using Anchor-Based Methods to Determine the Smallest Effect Size of Interest},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2021},
  month = sep,
  journal = {Journal of Experimental Social Psychology},
  volume = {96},
  pages = {104159},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2021.104159},
  abstract = {Effect sizes are an important outcome of quantitative research, but few guidelines exist that explain how researchers can determine which effect sizes are meaningful. Psychologists often want to study effects that are large enough to make a difference to people's subjective experience. Thus, subjective experience is one way to gauge the meaningfulness of an effect. We propose and illustrate one method for how to quantify the smallest subjectively experienced difference\textemdash the smallest change in an outcome measure that individuals consider to be meaningful enough in their subjective experience such that they are willing to rate themselves as feeling different\textemdash using an anchor-based method with a global rating of change question applied to the positive and negative affect scale. We provide a step-by-step guide for the questions that researchers need to consider in deciding whether and how to use the anchor-based method, and we make explicit the assumptions of the method that future research can examine. For researchers interested in people's subjective experiences, this anchor-based method provides one way to specify a smallest effect size of interest, which allows researchers to interpret observed results in terms of their theoretical and practical significance.},
  langid = {english},
  keywords = {Minimum important difference,Negative affect,Positive affect,Practical significance,Smallest effect size of interest,Smallest subjectively experienced difference,Subjectively experienced difference}
}

@article{appelbaum_journal_2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  shorttitle = {Journal Article Reporting Standards for Quantitative Research in Psychology},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and {Mayo-Wilson}, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  year = {2018},
  month = jan,
  journal = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3},
  publisher = {{US: American Psychological Association}},
  issn = {1935-990X},
  doi = {10.1037/amp0000191}
}

@article{armitage_repeated_1969,
  title = {Repeated Significance Tests on Accumulating Data},
  author = {Armitage, Peter and McPherson, C. K. and Rowe, B. C.},
  year = {1969},
  journal = {Journal of the Royal Statistical Society: Series A (General)},
  volume = {132},
  number = {2},
  pages = {235--244},
  publisher = {{Wiley Online Library}}
}

@article{arslan_how_2019,
  title = {How to {{Automatically Document Data With}} the Codebook {{Package}} to {{Facilitate Data Reuse}}},
  author = {Arslan, Ruben C.},
  year = {2019},
  month = may,
  journal = {Advances in Methods and Practices in Psychological Science},
  pages = {2515245919838783},
  issn = {2515-2459},
  doi = {10.1177/2515245919838783},
  abstract = {Data documentation in psychology lags behind not only many other disciplines, but also basic standards of usefulness. Psychological scientists often prefer to invest the time and effort that would be necessary to document existing data well in other duties, such as writing and collecting more data. Codebooks therefore tend to be unstandardized and stored in proprietary formats, and they are rarely properly indexed in search engines. This means that rich data sets are sometimes used only once\textemdash by their creators\textemdash and left to disappear into oblivion. Even if they can find an existing data set, researchers are unlikely to publish analyses based on it if they cannot be confident that they understand it well enough. My codebook package makes it easier to generate rich metadata in human- and machine-readable codebooks. It uses metadata from existing sources and automates some tedious tasks, such as documenting psychological scales and reliabilities, summarizing descriptive statistics, and identifying patterns of missingness. The codebook R package and Web app make it possible to generate a rich codebook in a few minutes and just three clicks. Over time, its use could lead to psychological data becoming findable, accessible, interoperable, and reusable, thereby reducing research waste and benefiting both its users and the scientific community as a whole.},
  langid = {english}
}

@book{babbage_reflections_1830,
  title = {Reflections on the {{Decline}} of {{Science}} in {{England}}: {{And}} on {{Some}} of {{Its Causes}}},
  shorttitle = {Reflections on the {{Decline}} of {{Science}} in {{England}}},
  author = {Babbage, Charles},
  year = {1830},
  publisher = {{B. Fellowes}},
  abstract = {Book digitized by Google and uploaded to the Internet Archive by user tpb.},
  collaborator = {{unknown library}},
  langid = {english}
}

@article{bacchetti_current_2010,
  title = {Current Sample Size Conventions: {{Flaws}}, Harms, and Alternatives},
  shorttitle = {Current Sample Size Conventions},
  author = {Bacchetti, Peter},
  year = {2010},
  month = mar,
  journal = {BMC Medicine},
  volume = {8},
  number = {1},
  pages = {17},
  issn = {1741-7015},
  doi = {10.1186/1741-7015-8-17},
  abstract = {The belief remains widespread that medical research studies must have statistical power of at least 80\% in order to be scientifically sound, and peer reviewers often question whether power is high enough.},
  keywords = {Current Convention,Inadequate Sample Size,Information Method,Marginal Return,Sample Size Planning}
}

@article{bacchetti_simple_2008,
  title = {Simple, {{Defensible Sample Sizes Based}} on {{Cost Efficiency}}},
  author = {Bacchetti, Peter and McCulloch, Charles E. and Segal, Mark R.},
  year = {2008},
  journal = {Biometrics},
  volume = {64},
  number = {2},
  pages = {577--585},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2008.01004_1.x},
  abstract = {The conventional approach of choosing sample size to provide 80\% or greater power ignores the cost implications of different sample size choices. Costs, however, are often impossible for investigators and funders to ignore in actual practice. Here, we propose and justify a new approach for choosing sample size based on cost efficiency, the ratio of a study's projected scientific and/or practical value to its total cost. By showing that a study's projected value exhibits diminishing marginal returns as a function of increasing sample size for a wide variety of definitions of study value, we are able to develop two simple choices that can be defended as more cost efficient than any larger sample size. The first is to choose the sample size that minimizes the average cost per subject. The second is to choose sample size to minimize total cost divided by the square root of sample size. This latter method is theoretically more justifiable for innovative studies, but also performs reasonably well and has some justification in other cases. For example, if projected study value is assumed to be proportional to power at a specific alternative and total cost is a linear function of sample size, then this approach is guaranteed either to produce more than 90\% power or to be more cost efficient than any sample size that does. These methods are easy to implement, based on reliable inputs, and well justified, so they should be regarded as acceptable alternatives to current conventional approaches.},
  langid = {english},
  keywords = {Innovation,Peer review,Power,Research funding,Study design},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2008.01004\_1.x}
}

@book{baguley_serious_2012,
  title = {Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences},
  shorttitle = {Serious Stats},
  author = {Baguley, Thom},
  year = {2012},
  publisher = {{Palgrave Macmillan}},
  address = {{Houndmills, Basingstoke, Hampshire [England] ; New York}},
  isbn = {978-0-230-57717-6 978-0-230-57718-3},
  lccn = {BF39 .B3175 2012},
  keywords = {Psychology,Psychometrics,Social sciences,Statistical methods}
}

@article{baguley_standardized_2009,
  title = {Standardized or Simple Effect Size: {{What}} Should Be Reported?},
  shorttitle = {Standardized or Simple Effect Size},
  author = {Baguley, Thom},
  year = {2009},
  month = aug,
  journal = {British Journal of Psychology},
  volume = {100},
  number = {3},
  pages = {603--617},
  issn = {2044-8295},
  doi = {10.1348/000712608X377117},
  abstract = {It is regarded as best practice for psychologists to report effect size when disseminating quantitative research findings. Reporting of effect size in the psychological literature is patchy \textendash{} though this may be changing \textendash{} and when reported it is far from clear that appropriate effect size statistics are employed. This paper considers the practice of reporting point estimates of standardized effect size and explores factors such as reliability, range restriction and differences in design that distort standardized effect size unless suitable corrections are employed. For most purposes simple (unstandardized) effect size is more robust and versatile than standardized effect size. Guidelines for deciding what effect size metric to use and how to report it are outlined. Foremost among these are: (i) a preference for simple effect size over standardized effect size, and (ii) the use of confidence intervals to indicate a plausible range of values the effect might take. Deciding on the appropriate effect size statistic to report always requires careful thought and should be influenced by the goals of the researcher, the context of the research and the potential needs of readers.},
  langid = {english}
}

@article{baguley_understanding_2004,
  title = {Understanding Statistical Power in the Context of Applied Research},
  author = {Baguley, Thom},
  year = {2004},
  month = mar,
  journal = {Applied Ergonomics},
  volume = {35},
  number = {2},
  pages = {73--80},
  issn = {0003-6870},
  doi = {10.1016/j.apergo.2004.01.002},
  abstract = {Estimates of statistical power are widely used in applied research for purposes such as sample size calculations. This paper reviews the benefits of power and sample size estimation and considers several problems with the use of power calculations in applied research that result from misunderstandings or misapplications of statistical power. These problems include the use of retrospective power calculations and standardized measures of effect size. Methods of increasing the power of proposed research that do not involve merely increasing sample size (such as reduction in measurement error, increasing `dose' of the independent variable and optimizing the design) are noted. It is concluded that applied researchers should consider a broader range of factors (other than sample size) that influence statistical power, and that the use of standardized measures of effect size should be avoided (except as intermediate stages in prospective power or sample size calculations).},
  langid = {english},
  keywords = {Applied research,Experimental design,Statistical power}
}

@article{bakan_test_1966,
  title = {The Test of Significance in Psychological Research.},
  author = {Bakan, David},
  year = {1966},
  journal = {Psychological bulletin},
  volume = {66},
  number = {6},
  pages = {423--437},
  doi = {10.1037/h0020412}
}

@article{bakker_recommendations_2020,
  title = {Recommendations in Pre-Registrations and Internal Review Board Proposals Promote Formal Power Analyses but Do Not Increase Sample Size},
  author = {Bakker, Marjan and Veldkamp, Coosje L. S. and van den Akker, Olmo R. and van Assen, Marcel A. L. M. and Crompvoets, Elise and Ong, How Hwee and Wicherts, Jelte M.},
  year = {2020},
  month = jul,
  journal = {PLOS ONE},
  volume = {15},
  number = {7},
  pages = {e0236079},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0236079},
  abstract = {In this preregistered study, we investigated whether the statistical power of a study is higher when researchers are asked to make a formal power analysis before collecting data. We compared the sample size descriptions from two sources: (i) a sample of pre-registrations created according to the guidelines for the Center for Open Science Preregistration Challenge (PCRs) and a sample of institutional review board (IRB) proposals from Tilburg School of Behavior and Social Sciences, which both include a recommendation to do a formal power analysis, and (ii) a sample of pre-registrations created according to the guidelines for Open Science Framework Standard Pre-Data Collection Registrations (SPRs) in which no guidance on sample size planning is given. We found that PCRs and IRBs (72\%) more often included sample size decisions based on power analyses than the SPRs (45\%). However, this did not result in larger planned sample sizes. The determined sample size of the PCRs and IRB proposals (Md = 90.50) was not higher than the determined sample size of the SPRs (Md = 126.00; W = 3389.5, p = 0.936). Typically, power analyses in the registrations were conducted with G*power, assuming a medium effect size, {$\alpha$} = .05 and a power of .80. Only 20\% of the power analyses contained enough information to fully reproduce the results and only 62\% of these power analyses pertained to the main hypothesis test in the pre-registration. Therefore, we see ample room for improvements in the quality of the registrations and we offer several recommendations to do so.},
  langid = {english},
  keywords = {Analysis of variance,Computer software,Linear regression analysis,Metaanalysis,Open science,Psychology,Research ethics,Social sciences}
}

@article{ball_effects_2002,
  title = {Effects of Cognitive Training Interventions with Older Adults: A Randomized Controlled Trial},
  shorttitle = {Effects of Cognitive Training Interventions with Older Adults},
  author = {Ball, Karlene and Berch, Daniel B. and Helmers, Karin F. and Jobe, Jared B. and Leveck, Mary D. and Marsiske, Michael and Morris, John N. and Rebok, George W. and Smith, David M. and Tennstedt, Sharon L.},
  year = {2002},
  journal = {Jama},
  volume = {288},
  number = {18},
  pages = {2271--2281},
  publisher = {{American Medical Association}}
}

@book{barber_pitfalls_1976,
  title = {Pitfalls in {{Human Research}}: {{Ten Pivotal Points}}},
  shorttitle = {Pitfalls in {{Human Research}}},
  author = {Barber, Theodore Xenophon},
  year = {1976},
  publisher = {{Pergamon Press}},
  googlebooks = {UBN9AAAAMAAJ},
  isbn = {978-0-08-020935-7},
  langid = {english},
  keywords = {Psychology / General}
}

@article{bartos_z-curve20_2020,
  title = {Z-{{Curve}}.2.0: {{Estimating Replication Rates}} and {{Discovery Rates}}},
  shorttitle = {Z-{{Curve}}.2.0},
  author = {Barto{\v s}, Franti{\v s}ek and Schimmack, Ulrich},
  year = {2020},
  month = jan,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/urgtn},
  abstract = {This article introduces z-curve.2.0 as a method that estimates the expected replication rate (ERR) and the expected discovery rate (EDR) based on the test-statistics of studies selected for significance. Z-curve.2.0 extends the work by Brunner and Schimmack (2019) in several ways. First, we show that a new estimation method using expectation-maximization outperforms the kernel-density approach of z-curve.1.0. Second, we examine the coverage of bootstrapped confidence intervals to provide information about the uncertainty in z-curve estimates. Third, we extended z-curve to estimate the number of all studies that were conducted, including studies with non-significant results that may not have been reported, solely on the basis of significant results. This allows us to estimate the EDR; that is, the percentage of significant results that were obtained in all studies. EDR can be used to assess the size of the file-drawer, estimate the maximum number of false positive results, and may provide a better estimate of the success rate in actual replication studies than the ERR because exact replications are impossible.}
}

@article{bauer_unifying_1996,
  title = {A Unifying Approach for Confidence Intervals and Testing of Equivalence and Difference},
  author = {Bauer, Peter and Kieser, Meinhard},
  year = {1996},
  journal = {Biometrika},
  volume = {83},
  number = {4},
  pages = {934--937}
}

@book{bausell_power_2002,
  title = {Power {{Analysis}} for {{Experimental Research}}: {{A Practical Guide}} for the {{Biological}}, {{Medical}} and {{Social Sciences}}},
  shorttitle = {Power {{Analysis}} for {{Experimental Research}}},
  author = {Bausell, R. Barker and Li, Yu-Fang},
  year = {2002},
  month = sep,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  abstract = {Power analysis is an essential tool for determining whether a statistically significant result can be expected in a scientific experiment prior to the experiment being performed. Many funding agencies and institutional review boards now require power analyses to be carried out before they will approve experiments, particularly where they involve the use of human subjects. This comprehensive, yet accessible, book provides practising researchers with step-by-step instructions for conducting power/sample size analyses, assuming only basic prior knowledge of summary statistics and the normal distribution. It contains a unified approach to statistical power analysis, with numerous easy-to-use tables to guide the reader without the need for further calculations or statistical expertise. This will be an indispensable text for researchers and graduates in the medical and biological sciences needing to apply power analysis in the design of their experiments.},
  langid = {english}
}

@book{bausell_power_2002-1,
  title = {Power Analysis for Experimental Research: A Practical Guide for the Biological, Medical and Social Sciences},
  shorttitle = {Power Analysis for Experimental Research},
  author = {Bausell, R. Barker and Li, Yu-Fang},
  year = {2002},
  publisher = {{Cambridge University Press}}
}

@article{bayarri_rejection_2016,
  title = {Rejection Odds and Rejection Ratios: {{A}} Proposal for Statistical Practice in Testing Hypotheses},
  shorttitle = {Rejection Odds and Rejection Ratios},
  author = {Bayarri, M.J. and Benjamin, Daniel J. and Berger, James O. and Sellke, Thomas M.},
  year = {2016},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  volume = {72},
  pages = {90--103},
  issn = {00222496},
  doi = {10.1016/j.jmp.2015.12.007},
  langid = {english},
  keywords = {Bayes factors,Bayesian,Frequentist,Odds}
}

@incollection{becker_failsafe_2005,
  title = {Failsafe {{N}} or {{File-Drawer Number}}},
  booktitle = {Publication {{Bias}} in {{Meta-Analysis}}},
  author = {Becker, Betsy Jane},
  year = {2005},
  pages = {111--125},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/0470870168.ch7},
  abstract = {This chapter contains sections titled: Introduction Definition of the Failsafe N Examples Assumptions of the Failsafe N Variations on the Failsafe N Summary of the Examples Applications of the Failsafe N Conclusions Acknowledgement References},
  chapter = {7},
  isbn = {978-0-470-87016-7},
  langid = {english},
  keywords = {'file-drawer' analysis,‘offset publication bias’,augmented Fisher test statistic,failsafe N or file-drawer number,file-drawer effect,Raudenbush's teacher expectancy data set,Stouffer tests},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0470870168.ch7}
}

@article{bem_feeling_2011,
  title = {Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect},
  shorttitle = {Feeling the Future},
  author = {Bem, Daryl J.},
  year = {2011},
  month = mar,
  journal = {Journal of Personality and Social Psychology},
  volume = {100},
  number = {3},
  pages = {407--425},
  issn = {1939-1315},
  doi = {10.1037/a0021524},
  abstract = {The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by "time-reversing" well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.},
  langid = {english},
  pmid = {21280961},
  keywords = {Affect,Awareness,Boredom,Cognition,Erotica,Escape Reaction,Female,Habituation; Psychophysiologic,Humans,Male,Mental Recall,Parapsychology,Subliminal Stimulation,Time Factors}
}

@article{bem_feeling_2015,
  title = {Feeling the Future: {{A}} Meta-Analysis of 90 Experiments on the Anomalous Anticipation of Random Future Events},
  shorttitle = {Feeling the Future},
  author = {Bem, Daryl and Tressoldi, Patrizio and Rabeyron, Thomas and Duggan, Michael},
  year = {2015},
  month = oct,
  journal = {F1000Research},
  issn = {2046-1402},
  doi = {10.12688/f1000research.7177.1},
  langid = {english}
}

@article{bem_must_2011,
  title = {Must Psychologists Change the Way They Analyze Their Data?},
  author = {Bem, Daryl J. and Utts, Jessica and Johnson, Wesley O.},
  year = {2011},
  month = oct,
  journal = {Journal of Personality and Social Psychology},
  volume = {101},
  number = {4},
  pages = {716--719},
  issn = {1939-1315},
  doi = {10.1037/a0024777},
  abstract = {Wagenmakers, Wetzels, Borsboom, and van der Maas (2011) argued that psychologists should replace the familiar "frequentist" statistical analyses of their data with bayesian analyses. To illustrate their argument, they reanalyzed a set of psi experiments published recently in this journal by Bem (2011), maintaining that, contrary to his conclusion, his data do not yield evidence in favor of the psi hypothesis. We argue that they have incorrectly selected an unrealistic prior distribution for their analysis and that a bayesian analysis using a more reasonable distribution yields strong evidence in favor of the psi hypothesis. More generally, we argue that there are advantages to bayesian analyses that merit their increased use in the future. However, as Wagenmakers et al.'s analysis inadvertently revealed, they contain hidden traps that must be better understood before being more widely substituted for the familiar frequentist analyses currently employed by most research psychologists.},
  langid = {english},
  pmid = {21928916},
  keywords = {Data Interpretation; Statistical,Humans,Psychology}
}

@article{ben-shachar_effectsize_2020,
  title = {Effectsize: {{Estimation}} of {{Effect Size Indices}} and {{Standardized Parameters}}},
  shorttitle = {Effectsize},
  author = {{Ben-Shachar}, Mattan S. and L{\"u}decke, Daniel and Makowski, Dominique},
  year = {2020},
  month = dec,
  journal = {Journal of Open Source Software},
  volume = {5},
  number = {56},
  pages = {2815},
  issn = {2475-9066},
  doi = {10.21105/joss.02815},
  abstract = {Ben-Shachar et al., (2020). effectsize: Estimation of Effect Size Indices and Standardized Parameters. Journal of Open Source Software, 5(56), 2815, https://doi.org/10.21105/joss.02815},
  langid = {english}
}

@article{benjamini_controlling_1995,
  title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
  shorttitle = {Controlling the False Discovery Rate},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  year = {1995},
  journal = {Journal of the royal statistical society. Series B (Methodological)},
  pages = {289--300}
}

@article{benjamini_its_2016,
  title = {It's {{Not}} the p-Values' {{Fault}}},
  author = {Benjamini, Yoav},
  year = {2016},
  journal = {The American Statistician: Supplemental Material to the ASA Statement on P-Values and Statistical Significance},
  volume = {70},
  pages = {1--2}
}

@article{berger_interplay_2004,
  title = {The {{Interplay}} of {{Bayesian}} and {{Frequentist Analysis}}},
  author = {Berger, J. O. and Bayarri, M. J.},
  year = {2004},
  month = feb,
  journal = {Statistical Science},
  volume = {19},
  number = {1},
  pages = {58--80},
  issn = {0883-4237},
  doi = {10.1214/088342304000000116},
  langid = {english}
}

@book{berkeley_defence_1735,
  title = {A Defence of Free-Thinking in Mathematics, in Answer to a Pamphlet of {{Philalethes Cantabrigiensis}} Entitled {{Geometry No Friend}} to {{Infidelity}}. {{Also}} an Appendix Concerning Mr. {{Walton}}'s {{Vindication}} of the Principles of Fluxions against the Objections Contained in {{The}} Analyst. {{By}} the Author of {{The}} Minute Philosopher},
  author = {Berkeley, George},
  year = {1735},
  volume = {3}
}

@book{berkeley_defence_1735-1,
  title = {A Defence of Free-Thinking in Mathematics, in Answer to a Pamphlet of {{Philalethes Cantabrigiensis}} Entitled {{Geometry No Friend}} to {{Infidelity}}. {{Also}} an Appendix Concerning Mr. {{Walton}}'s {{Vindication}} of the Principles of Fluxions against the Objections Contained in {{The}} Analyst. {{By}} the Author of {{The}} Minute Philosopher},
  author = {Berkeley, George},
  year = {1735},
  volume = {3}
}

@article{bigby_understanding_2014,
  title = {Understanding and Evaluating Systematic Reviews and Meta-Analyses},
  author = {Bigby, Michael},
  year = {2014},
  month = mar,
  journal = {Indian Journal of Dermatology},
  volume = {59},
  number = {2},
  pages = {134},
  issn = {0019-5154},
  doi = {10.4103/0019-5154.127671},
  abstract = {A systematic review is a summary of existing evidence that answers a specific clinical question, contains a thorough, unbiased search of the relevant literature, explicit criteria for assessing studies and structured presentation of the results. A systematic review that incorporates quantitative pooling of similar studies to produce an overall summary of treatment effects is a meta-analysis. A systematic review should have clear, focused clinical objectives containing four elements expressed through the acronym PICO ( \textbf{P}atient, group of \textbf{p}atients, or \textbf{p}roblem, an \textbf{I}ntervention, a \textbf{C}omparison intervention and specific \textbf{O} utcomes). Explicit and thorough search of the literature is a pre-requisite of any good systematic review. Reviews should have pre-defined explicit criteria for what studies would be included and the analysis should include only those studies that fit the inclusion criteria. The quality (risk of bias) of the primary studies should be critically appraised. Particularly the role of publication and language bias should be acknowledged and addressed by the review, whenever possible. Structured reporting of the results with quantitative pooling of the data must be attempted, whenever appropriate. The review should include interpretation of the data, including implications for clinical practice and further research. Overall, the current quality of reporting of systematic reviews remains highly variable.},
  langid = {english},
  pmid = {24700930}
}

@article{bishop_fallibility_2018,
  title = {Fallibility in {{Science}}: {{Responding}} to {{Errors}} in the {{Work}} of {{Oneself}} and {{Others}}},
  shorttitle = {Fallibility in {{Science}}},
  author = {Bishop, D. V. M.},
  year = {2018},
  month = jul,
  journal = {Advances in Methods and Practices in Psychological Science},
  pages = {2515245918776632},
  issn = {2515-2459},
  doi = {10.1177/2515245918776632},
  langid = {english},
  annotation = {00000}
}

@book{bland_introduction_2015,
  title = {An Introduction to Medical Statistics},
  author = {Bland, Martin},
  year = {2015},
  series = {Oxford Medical Publications},
  edition = {Fourth edition},
  publisher = {{Oxford University Press}},
  address = {{Oxford}},
  isbn = {978-0-19-958992-0},
  langid = {english},
  lccn = {RA409 .B55 2015},
  keywords = {Medical statistics}
}

@incollection{blume_likelihood_2011,
  title = {Likelihood and Its {{Evidential Framework}}},
  booktitle = {Philosophy of {{Statistics}}},
  author = {Blume, Jeffrey D.},
  year = {2011},
  pages = {493--511},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-51862-0.50014-9},
  isbn = {978-0-444-51862-0},
  langid = {english}
}

@book{borenstein_introduction_2009,
  title = {Introduction to Meta-Analysis},
  editor = {Borenstein, Michael},
  year = {2009},
  publisher = {{John Wiley \& Sons}},
  address = {{Chichester, U.K}},
  abstract = {This text provides a concise and clearly presented discussion of all the elements in a meta-analysis. It is illustrated with worked examples throughout, with visual explanations, using screenshots from Excel spreadsheets and computer programs such as Comprehensive Meta-Analysis (CMA) or Strata},
  isbn = {978-0-470-05724-7},
  lccn = {R853.M48 I58 2009},
  keywords = {Meta-analysis,Meta-Analysis as Topic}
}

@article{bosco_correlational_2015,
  title = {Correlational Effect Size Benchmarks},
  author = {Bosco, Frank A. and Aguinis, Herman and Singh, Kulraj and Field, James G. and Pierce, Charles A.},
  year = {2015},
  month = mar,
  journal = {The Journal of Applied Psychology},
  volume = {100},
  number = {2},
  pages = {431--449},
  issn = {1939-1854},
  doi = {10.1037/a0038047},
  abstract = {Effect size information is essential for the scientific enterprise and plays an increasingly central role in the scientific process. We extracted 147,328 correlations and developed a hierarchical taxonomy of variables reported in Journal of Applied Psychology and Personnel Psychology from 1980 to 2010 to produce empirical effect size benchmarks at the omnibus level, for 20 common research domains, and for an even finer grained level of generality. Results indicate that the usual interpretation and classification of effect sizes as small, medium, and large bear almost no resemblance to findings in the field, because distributions of effect sizes exhibit tertile partitions at values approximately one-half to one-third those intuited by Cohen (1988). Our results offer information that can be used for research planning and design purposes, such as producing better informed non-nil hypotheses and estimating statistical power and planning sample size accordingly. We also offer information useful for understanding the relative importance of the effect sizes found in a particular study in relationship to others and which research domains have advanced more or less, given that larger effect sizes indicate a better understanding of a phenomenon. Also, our study offers information about research domains for which the investigation of moderating effects may be more fruitful and provide information that is likely to facilitate the implementation of Bayesian analysis. Finally, our study offers information that practitioners can use to evaluate the relative effectiveness of various types of interventions.},
  langid = {english},
  pmid = {25314367},
  keywords = {Behavioral Research,Benchmarking,Data Interpretation; Statistical,Humans}
}

@article{brown_errors_1983,
  title = {Errors, {{Types I}} and {{II}}},
  author = {Brown, George W.},
  year = {1983},
  month = jun,
  journal = {American Journal of Diseases of Children},
  volume = {137},
  number = {6},
  pages = {586--591},
  issn = {0002-922X},
  doi = {10.1001/archpedi.1983.02140320062014},
  abstract = {\textbullet{} The practicing physician and the clinical investigator regularly confront therapeutic trials, diagnostic tests, and other hypothesis-testing situations. The clinical literature increasingly displays statistical notations and concepts related to decision making in medicine. For these reasons, the physician is obligated to have some familiarity with the principles behind the null hypothesis, Type I and II errors, statistical power, and related elements of hypothesis testing.(Am J Dis Child 1983;137:586-591)}
}

@article{brown_grim_2017,
  title = {The {{GRIM Test}}: {{A Simple Technique Detects Numerous Anomalies}} in the {{Reporting}} of {{Results}} in {{Psychology}}},
  shorttitle = {The {{GRIM Test}}},
  author = {Brown, Nicholas J. L. and Heathers, James A. J.},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {363--369},
  issn = {1948-5506},
  doi = {10.1177/1948550616673876},
  abstract = {We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20\% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.},
  langid = {english}
}

@article{brunner_estimating_2020,
  title = {Estimating {{Population Mean Power Under Conditions}} of {{Heterogeneity}} and {{Selection}} for {{Significance}}},
  author = {Brunner, Jerry and Schimmack, Ulrich},
  year = {2020},
  month = may,
  journal = {Meta-Psychology},
  volume = {4},
  issn = {2003-2714},
  doi = {10.15626/MP.2018.874},
  abstract = {In scientific fields that use significance tests, statistical power is important for successful replications of significant results because it is the long-run success rate in a series of exact replication studies. For any population of significant results, there is a population of power values of the statistical tests on which conclusions are based. We give exact theoretical results showing how selection for significance affects the distribution of statistical power in a heterogeneous population of significance tests. In a set of large-scale simulation studies, we compare four methods for estimating population mean power of a set of studies selected for significance (a maximum likelihood model, extensions of p-curve and p-uniform, \&amp; z-curve). The p-uniform and p-curve methods performed well with a fixed effects size and varying sample sizes. However, when there was substantial variability in effect sizes as well as sample sizes, both methods systematically overestimate mean power. With heterogeneity in effect sizes, the maximum likelihood model produced the most accurate estimates when the distribution of effect sizes matched the assumptions of the model, but z-curve produced more accurate estimates when the assumptions of the maximum likelihood model were not met. We recommend the use of z-curve to estimate the typical power of significant results, which has implications for the replicability of significant results in psychology journals.},
  copyright = {Copyright (c) 2020 Jerry Brunner, Ulrich Schimmack},
  langid = {english},
  keywords = {Effect size,Maximum likelihood,Meta-analysis,P-curve,P-uniform,Post-hoc power analysis,Power estimation,Publication bias,Replicability,Z-curve}
}

@article{bryan_behavioural_2021,
  title = {Behavioural Science Is Unlikely to Change the World without a Heterogeneity Revolution},
  author = {Bryan, Christopher J. and Tipton, Elizabeth and Yeager, David S.},
  year = {2021},
  month = jul,
  journal = {Nature Human Behaviour},
  pages = {1--10},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01143-3},
  abstract = {In the past decade, behavioural science has gained influence in policymaking but suffered a crisis of confidence in the replicability of its findings. Here, we describe a nascent heterogeneity revolution that we believe these twin historical trends have triggered. This revolution will be defined by the recognition that most treatment effects are heterogeneous, so the variation in effect estimates across studies that defines the replication crisis is to be expected as long as heterogeneous effects are studied without a systematic approach to sampling and moderation. When studied systematically, heterogeneity can be leveraged to build more complete theories of causal mechanism that could inform nuanced and dependable guidance to policymakers. We recommend investment in shared research infrastructure to make it feasible to study behavioural interventions in heterogeneous and generalizable samples, and suggest low-cost steps researchers can take immediately to avoid being misled by heterogeneity and begin to learn from it instead.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Human behaviour;Policy;Research management;Science, technology and society Subject\_term\_id: human-behaviour;policy;research-management;science-technology-and-society}
}

@article{brysbaert_how_2019,
  title = {How Many Participants Do We Have to Include in Properly Powered Experiments? {{A}} Tutorial of Power Analysis with Reference Tables},
  shorttitle = {How Many Participants Do We Have to Include in Properly Powered Experiments?},
  author = {Brysbaert, Marc},
  year = {2019},
  month = jul,
  journal = {Journal of Cognition},
  volume = {2},
  number = {1},
  pages = {16},
  issn = {2514-4820},
  doi = {10.5334/joc.72},
  abstract = {Article: How many participants do we have to include in properly powered experiments?  A tutorial of power analysis with reference tables},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english}
}

@article{brysbaert_how_2019-1,
  title = {How Many Words Do We Read per Minute? {{A}} Review and Meta-Analysis of Reading Rate},
  shorttitle = {How Many Words Do We Read per Minute?},
  author = {Brysbaert, Marc},
  year = {2019},
  journal = {Journal of Memory and Language},
  volume = {109},
  pages = {104047},
  publisher = {{Elsevier}}
}

@article{brysbaert_power_2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  abstract = {Article: Power Analysis and Effect Size in Mixed Effects Models: A Tutorial},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  annotation = {00012}
}

@misc{buchanan_mote_2017,
  title = {{{MOTE}}: {{Effect Size}} and {{Confidence Interval Calculator}}.},
  author = {Buchanan, Erin M. and Scofield, J and Valentine, K. D.},
  year = {2017}
}

@article{bulus_bound_2021,
  title = {Bound {{Constrained Optimization}} of {{Sample Sizes Subject}} to {{Monetary Restrictions}} in {{Planning Multilevel Randomized Trials}} and {{Regression Discontinuity Studies}}},
  author = {Bulus, Metin and Dong, Nianbo},
  year = {2021},
  month = apr,
  journal = {The Journal of Experimental Education},
  volume = {89},
  number = {2},
  pages = {379--401},
  publisher = {{Routledge}},
  issn = {0022-0973},
  doi = {10.1080/00220973.2019.1636197},
  abstract = {Sample size determination in multilevel randomized trials (MRTs) and multilevel regression discontinuity designs (MRDDs) can be complicated due to multilevel structure, monetary restrictions, differing marginal costs per treatment and control units, and range restrictions in sample size at one or more levels. These issues have sparked a set of studies under optimal design literature where scholars consider sample size determination as an allocation problem. The literature on optimal design of MRTs and MRDDs and their implementation in software packages has been scarce, scattered, and incomplete. This study unifies optimal design literature and extends currently available software under bound constrained optimal sample allocation (BCOSA) framework via bound constrained optimization technique. The BCOSA framework, introduction to the cosa R library, and an illustration that replicates and extends minimum required sample size determination for an evaluation report is provided.},
  keywords = {bound constrained optimal sample allocation,conditional optimal design,multilevel randomized trials,multilevel regression discontinuity designs},
  annotation = {\_eprint: https://doi.org/10.1080/00220973.2019.1636197}
}

@article{burriss_changes_2015,
  title = {Changes in Women's Facial Skin Color over the Ovulatory Cycle Are Not Detectable by the Human Visual System},
  author = {Burriss, Robert P. and Troscianko, Jolyon and Lovell, P. George and Fulford, Anthony J. C. and Stevens, Martin and Quigley, Rachael and Payne, Jenny and Saxton, Tamsin K. and Rowland, Hannah M.},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130093},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130093},
  abstract = {Human ovulation is not advertised, as it is in several primate species, by conspicuous sexual swellings. However, there is increasing evidence that the attractiveness of women's body odor, voice, and facial appearance peak during the fertile phase of their ovulatory cycle. Cycle effects on facial attractiveness may be underpinned by changes in facial skin color, but it is not clear if skin color varies cyclically in humans or if any changes are detectable. To test these questions we photographed women daily for at least one cycle. Changes in facial skin redness and luminance were then quantified by mapping the digital images to human long, medium, and shortwave visual receptors. We find cyclic variation in skin redness, but not luminance. Redness decreases rapidly after menstrual onset, increases in the days before ovulation, and remains high through the luteal phase. However, we also show that this variation is unlikely to be detectable by the human visual system. We conclude that changes in skin color are not responsible for the effects of the ovulatory cycle on women's attractiveness.},
  keywords = {Cameras,Color vision,Estradiol,Estrogens,Face,Luminance,Ovulation,Visual system}
}

@article{button_minimal_2015,
  title = {Minimal Clinically Important Difference on the {{Beck Depression Inventory}} - {{II}} According to the Patient's Perspective},
  author = {Button, K. S. and Kounali, D. and Thomas, L. and Wiles, N. J. and Peters, T. J. and Welton, N. J. and Ades, A. E. and Lewis, G.},
  year = {2015},
  month = nov,
  journal = {Psychological Medicine},
  volume = {45},
  number = {15},
  pages = {3269--3279},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291715001270},
  abstract = {Background The Beck Depression Inventory, 2nd edition (BDI-II) is widely used in research on depression. However, the minimal clinically important difference (MCID) is unknown. MCID can be estimated in several ways. Here we take a patient-centred approach, anchoring the change on the BDI-II to the patient's global report of improvement. Method  We used data collected (n = 1039) from three randomized controlled trials for the management of depression. Improvement on a `global rating of change' question was compared with changes in BDI-II scores using general linear modelling to explore baseline dependency, assessing whether MCID is best measured in absolute terms (i.e. difference) or as percent reduction in scores from baseline (i.e. ratio), and receiver operator characteristics (ROC) to estimate MCID according to the optimal threshold above which individuals report feeling `better'. Results Improvement in BDI-II scores associated with reporting feeling `better' depended on initial depression severity, and statistical modelling indicated that MCID is best measured on a ratio scale as a percentage reduction of score. We estimated a MCID of a 17.5\% reduction in scores from baseline from ROC analyses. The corresponding estimate for individuals with longer duration depression who had not responded to antidepressants was higher at 32\%. Conclusions MCID on the BDI-II is dependent on baseline severity, is best measured on a ratio scale, and the MCID for treatment-resistant depression is larger than that for more typical depression. This has important implications for clinical trials and practice.},
  keywords = {2nd edition (BDI-II),Beck Depression Inventory,depression,minimal clinically important difference,outcome assessment,primary care}
}

@article{button_power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, K. S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = apr,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475}
}

@article{carter_correcting_2019,
  title = {Correcting for {{Bias}} in {{Psychology}}: {{A Comparison}} of {{Meta-Analytic Methods}}},
  shorttitle = {Correcting for {{Bias}} in {{Psychology}}},
  author = {Carter, Evan C. and Sch{\"o}nbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
  year = {2019},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {115--144},
  issn = {2515-2459},
  doi = {10.1177/2515245919847196},
  abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses\textemdash that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
  langid = {english}
}

@article{carter_publication_2014,
  title = {Publication Bias and the Limited Strength Model of Self-Control: Has the Evidence for Ego Depletion Been Overestimated?},
  shorttitle = {Publication Bias and the Limited Strength Model of Self-Control},
  author = {Carter, Evan C. and McCullough, Michael E.},
  year = {2014},
  month = jul,
  journal = {Frontiers in Psychology},
  volume = {5},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00823}
}

@article{cascio_open_1983,
  title = {Open a {{New Window}} in {{Rational Research Planning}}: {{Adjust Alpha}} to {{Maximize Statistical Power}}},
  shorttitle = {Open a {{New Window}} in {{Rational Research Planning}}},
  author = {Cascio, Wayne F. and Zedeck, Sheldon},
  year = {1983},
  journal = {Personnel Psychology},
  volume = {36},
  number = {3},
  pages = {517--526},
  issn = {1744-6570},
  doi = {10.1111/j.1744-6570.1983.tb02233.x},
  abstract = {Alternative strategies for optimizing statistical power in applied psychological research are considered. Increasing sample size and combining predictors in order to yield a useful effect size are well-known tactics for increasing power. A third approach, increasing alpha, is rarely used because of zealous adherence to convention. There are two related aspects in setting the alpha level. First, the relative seriousness of Type I and Type II errors must be considered. This assessment must then be qualified and redetermined after taking into account the prior probability that an effect exists. Procedures that make these processes objective are demonstrated. When sample size and effect size are both fixed, increasing alpha may be the only feasible strategy for maximizing power. It is concluded that a priori power analysis should be a major consideration in any test of an hypothesis, and that alpha level adjustment should be viewed as a useful strategy for increasing power.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1744-6570.1983.tb02233.x}
}

@book{chang_adaptive_2016,
  title = {Adaptive {{Design Theory}} and {{Implementation Using SAS}} and {{R}}},
  author = {Chang, Mark},
  year = {2016},
  month = oct,
  edition = {2nd edition},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Get Up to Speed on Many Types of Adaptive Designs  Since the publication of the first edition, there have been remarkable advances in the methodology and application of adaptive trials. Incorporating many of these new developments, Adaptive Design Theory and Implementation Using SAS and R, Second Edition offers a detailed framework to understand the use of various adaptive design methods in clinical trials.  New to the Second Edition  Twelve new chapters covering blinded and semi-blinded sample size reestimation design, pick-the-winners design, biomarker-informed adaptive design, Bayesian designs, adaptive multiregional trial design, SAS and R for group sequential design, and much more More analytical methods for K-stage adaptive designs, multiple-endpoint adaptive design, survival modeling, and adaptive treatment switching New material on sequential parallel designs with rerandomization and the skeleton approach in adaptive dose-escalation trials Twenty new SAS macros and R functions Enhanced end-of-chapter problems that give readers hands-on practice addressing issues encountered in designing real-life adaptive trials  Covering even more adaptive designs, this book provides biostatisticians, clinical scientists, and regulatory reviewers with up-to-date details on this innovative area in pharmaceutical research and development. Practitioners will be able to improve the efficiency of their trial design, thereby reducing the time and cost of drug development.},
  isbn = {978-1-138-03423-5},
  langid = {english}
}

@techreport{chatziathanasiou_beware_2022,
  type = {{{SSRN Scholarly Paper}}},
  title = {Beware the {{Lure}} of {{Narratives}}: `{{Hungry Judges}}' {{Should}} Not {{Motivate}} the {{Use}} of `{{Artificial Intelligence}}' in {{Law}}},
  shorttitle = {Beware the {{Lure}} of {{Narratives}}},
  author = {Chatziathanasiou, Konstantin},
  year = {2022},
  month = jan,
  number = {ID 4011603},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.4011603},
  abstract = {The `hungry judge' effect, as presented by a famous study, is a common point of reference to underline human bias in judicial decision-making. This is particularly pronounced in the literature on `artificial intelligence' (AI) in law. Here, the effect is invoked to counter concerns about bias in automated decision-aids and to motivate their use. However, the validity of the `hungry judge' effect is doubtful. In our context, this is problematic for, at least, two reasons. First, shaky evidence leads to a misconstruction of the problem that may warrant an AI intervention. Second, painting the justice system worse than it actually is, is a dangerous argumentative strategy as it undermines institu-tional trust. Against this background, this article revisits the original `hungry judge' study and argues that it cannot be relied on as an argument in the AI discourse or beyond. The case of `hungry judges' demonstrates the lure of narratives, the dangers of `problem gerrymandering', and ultimately the need for a careful reception of social science.},
  langid = {english},
  keywords = {extra-legal influences,hungry judge,judicial decision-making,social science in law}
}

@article{chin_questionable_2021,
  title = {Questionable {{Research Practices}} and {{Open Science}} in {{Quantitative Criminology}}},
  author = {Chin, Jason M. and Pickett, Justin T. and Vazire, Simine and Holcombe, Alex O.},
  year = {2021},
  month = aug,
  journal = {Journal of Quantitative Criminology},
  issn = {1573-7799},
  doi = {10.1007/s10940-021-09525-6},
  abstract = {Questionable research practices (QRPs) lead to incorrect research results and contribute to irreproducibility in science. Researchers and institutions have proposed open science practices (OSPs) to improve the detectability of QRPs and the credibility of science. We examine the prevalence of QRPs and OSPs in criminology, and researchers' opinions of those practices.},
  langid = {english}
}

@article{cho_is_2013,
  title = {Is Two-Tailed Testing for Directional Research Hypotheses Tests Legitimate?},
  author = {Cho, Hyun-Chul and Abe, Shuzo},
  year = {2013},
  month = sep,
  journal = {Journal of Business Research},
  series = {Advancing {{Research Methods}} in {{Marketing}}},
  volume = {66},
  number = {9},
  pages = {1261--1266},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2012.02.023},
  abstract = {This paper demonstrates that there is currently a widespread misuse of two-tailed testing for directional research hypotheses tests. One probable reason for this overuse of two-tailed testing is the seemingly valid beliefs that two-tailed testing is more conservative and safer than one-tailed testing. However, the authors examine the legitimacy of this notion and find it to be flawed. A second and more fundamental cause of the current problem is the pervasive oversight in making a clear distinction between the research hypothesis and the statistical hypothesis. Based upon the explicated, sound relationship between the research and statistical hypotheses, the authors propose a new scheme of hypothesis classification to facilitate and clarify the proper use of statistical hypothesis testing in empirical research.},
  keywords = {hypothesis testing,one-tailed testing,Research hypothesis in existential form,Research hypothesis in non-existential form,Statistical hypothesis,two-tailed testing}
}

@article{cohen_earth_1994,
  title = {The Earth Is Round (p {$<$} .05).},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/0003-066X.49.12.997},
  langid = {english}
}

@article{cohen_earth_1995,
  title = {The Earth Is Round ( p\hspace{0.6em}{$<$}\hspace{0.6em}.05): {{Rejoinder}}},
  shorttitle = {The Earth Is Round ( p\hspace{0.6em}{$<$}\hspace{0.6em}.05)},
  author = {Cohen, Jacob},
  year = {1995},
  month = dec,
  journal = {American Psychologist},
  volume = {50},
  number = {12},
  pages = {1103},
  issn = {0003-066X},
  doi = {http://dx.doi.org/10.1037/0003-066X.50.12.1103},
  copyright = {\textcopyright{} 1995, American Psychological Association},
  langid = {english},
  keywords = {Null Hypothesis Testing (major)}
}

@article{cohen_statistical_1965,
  title = {Some Statistical Issues in Psychological Research},
  author = {Cohen, Jacob},
  year = {1965},
  journal = {Handbook of clinical psychology},
  pages = {95--121}
}

@book{cohen_statistical_1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {2nd ed},
  publisher = {{L. Erlbaum Associates}},
  address = {{Hillsdale, N.J}},
  isbn = {978-0-8058-0283-2},
  lccn = {HA29 .C66 1988},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis}
}

@article{cohen_things_1990,
  title = {Things {{I}} Have Learned (so Far)},
  author = {Cohen, Jacob},
  year = {1990},
  journal = {American Psychologist},
  volume = {45},
  number = {12},
  pages = {1304--1312},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.45.12.1304},
  abstract = {This is an account of what I have learned (so far) about the application of statistics to psychology and the other sociobiomedical sciences. It includes the principles "less is more" (fewer variables, more highly targeted issues, sharp rounding off), "simple is better" (graphic representation, unit weighting for linear composites), and "some things you learn aren't so." I have learned to avoid the many misconceptions that surround Fisherian null hypothesis testing. I have also learned the importance of power analysis and the determination of just how big (rather than how statistically significant) are the effects that we study. Finally, I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data, and that things take time. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Psychology,Social Sciences,Statistics}
}

@article{colling_registered_2020,
  title = {Registered {{Replication Report}} on {{Fischer}}, {{Castel}}, {{Dodd}}, and {{Pratt}} (2003)},
  author = {Colling, Lincoln J. and Sz{\H u}cs, D{\'e}nes and De Marco, Damiano and Cipora, Krzysztof and Ulrich, Rolf and Nuerk, Hans-Christoph and Soltanlou, Mojtaba and Bryce, Donna and Chen, Sau-Chin and Schroeder, Philipp Alexander and Henare, Dion T. and Chrystall, Christine K. and Corballis, Paul M. and Ansari, Daniel and Goffin, Celia and Sokolowski, H. Moriah and Hancock, Peter J. B. and Millen, Ailsa E. and Langton, Stephen R. H. and Holmes, Kevin J. and Saviano, Mark S. and Tummino, Tia A. and Lindemann, Oliver and Zwaan, Rolf A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Beckov{\'a}, Ad{\'e}la and Vranka, Marek A. and Cutini, Simone and Mammarella, Irene Cristina and Mulatti, Claudio and Bell, Raoul and Buchner, Axel and Mieth, Laura and R{\"o}er, Jan Philipp and Klein, Elise and Huber, Stefan and Moeller, Korbinian and Ocampo, Brenda and Lupi{\'a}{\~n}ez, Juan and {Ortiz-Tudela}, Javier and {de la Fuente}, Juanma and Santiago, Julio and Ouellet, Marc and Hubbard, Edward M. and Toomarian, Elizabeth Y. and Job, Remo and Treccani, Barbara and McShane, Blakeley B.},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {143--162},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920903079},
  abstract = {The attentional spatial-numerical association of response codes (Att-SNARC) effect (Fischer, Castel, Dodd, \& Pratt, 2003)?the finding that participants are quicker to detect left-side targets when the targets are preceded by small numbers and quicker to detect right-side targets when they are preceded by large numbers?has been used as evidence for embodied number representations and to support strong claims about the link between number and space (e.g., a mental number line). We attempted to replicate Experiment 2 of Fischer et al. by collecting data from 1,105 participants at 17 labs. Across all 1,105 participants and four interstimulus-interval conditions, the proportion of times the effect we observed was positive (i.e., directionally consistent with the original effect) was .50. Further, the effects we observed both within and across labs were minuscule and incompatible with those observed by Fischer et al. Given this, we conclude that we failed to replicate the effect reported by Fischer et al. In addition, our analysis of several participant-level moderators (finger-counting habits, reading and writing direction, handedness, and mathematics fluency and mathematics anxiety) revealed no substantial moderating effects. Our results indicate that the Att-SNARC effect cannot be used as evidence to support strong claims about the link between number and space.}
}

@article{colquhoun_false_2019,
  title = {The {{False Positive Risk}}: {{A Proposal Concerning What}} to {{Do About}} p-{{Values}}},
  shorttitle = {The {{False Positive Risk}}},
  author = {Colquhoun, David},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {192--201},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1529622},
  abstract = {It is widely acknowledged that the biomedical literature suffers from a surfeit of false positive results. Part of the reason for this is the persistence of the myth that observation of p {$<$} 0.05 is sufficient justification to claim that you have made a discovery. It is hopeless to expect users to change their reliance on p-values unless they are offered an alternative way of judging the reliability of their conclusions. If the alternative method is to have a chance of being adopted widely, it will have to be easy to understand and to calculate. One such proposal is based on calculation of false positive risk(FPR). It is suggested that p-values and confidence intervals should continue to be given, but that they should be supplemented by a single additional number that conveys the strength of the evidence better than the p-value. This number could be the minimum FPR (that calculated on the assumption of a prior probability of 0.5, the largest value that can be assumed in the absence of hard prior data). Alternatively one could specify the prior probability that it would be necessary to believe in order to achieve an FPR of, say, 0.05.},
  keywords = {Bayes,False positive,False positive report probability,False positive risk,FPR,Likelihood ratio,Point null,Positive predictive value}
}

@article{colquhoun_reproducibility_2017,
  title = {The {{Reproducibility Of Research And The Misinterpretation Of P Values}}},
  author = {Colquhoun, David},
  year = {2017},
  month = aug,
  journal = {bioRxiv},
  pages = {144337},
  doi = {10.1101/144337},
  abstract = {We wish to answer this question If you observe a "significant" P value after doing a single unbiased experiment, what is the probability that your result is a false positive?. The weak evidence provided by P values between 0.01 and 0.05 is explored by exact calculations of false positive risks. When you observe P = 0.05, the odds in favour of there being a real effect (given by the likelihood ratio) are about 3:1. This is far weaker evidence than the odds of 19 to 1 that might, wrongly, be inferred from the P value. And if you want to limit the false positive risk to 5 \%, you would have to assume that you were 87\% sure that there was a real effect before the experiment was done. If you observe P = 0.001 in a well-powered experiment, it gives a likelihood ratio of almost 100:1 odds on there being a real effect. That would usually be regarded as conclusive, But the false positive risk would still be 8\% if the prior probability of a real effect was only 0.1. And, in this case, if you wanted to achieve a false positive risk of 5\% you would need to observe P = 0.00045. It is recommended that the terms "significant" and "non-significant" should never be used. Rather, P values should be supplemented by specifying the prior probability that would be needed to produce a specified (e.g. 5\%) false positive risk. It may also be helpful to specify the minimum false positive risk associated with the observed P value. Despite decades of warnings, many areas of science still insist on labelling a result of P {$<$} 0.05 as "significant". This practice must account for a substantial part of the lack of reproducibility in some areas of science. And this is before you get to the many other well-known problems, like multiple comparisons, lack of randomisation and P-hacking. Science is endangered by statistical misunderstanding, and by university presidents and research funders who impose perverse incentives on scientists.},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english}
}

@article{cook_assessing_2014,
  title = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial: {{DELTA}} ({{Difference ELicitation}} in {{TriAls}}) Review},
  shorttitle = {Assessing Methods to Specify the Target Difference for a Randomised Controlled Trial},
  author = {Cook, Jonathan and Hislop, Jennifer and Adewuyi, Temitope and Harrild, Kirsten and Altman, Douglas and Ramsay, Craig and Fraser, Cynthia and Buckley, Brian and Fayers, Peter and Harvey, Ian and Briggs, Andrew and Norrie, John and Fergusson, Dean and Ford, Ian and Vale, Luke},
  year = {2014},
  month = jan,
  journal = {Health Technology Assessment},
  volume = {18},
  number = {28},
  issn = {1366-5278, 2046-4924},
  doi = {10.3310/hta18280},
  langid = {english}
}

@article{cook_choosing_2017,
  title = {Choosing the Target Difference ('effect Size') for a Randomised Controlled Trial - {{DELTA2}} Guidance Protocol},
  author = {Cook, Jonathan A. and Julious, Steven A. and Sones, William and Rothwell, Joanne C. and Ramsay, Craig R. and Hampson, Lisa V. and Emsley, Richard and Walters, Stephen J. and Hewitt, Catherine and Bland, Martin and Fergusson, Dean A. and Berlin, Jesse A. and Altman, Doug and Vale, Luke D.},
  year = {2017},
  month = jun,
  journal = {Trials},
  volume = {18},
  number = {1},
  pages = {271},
  issn = {1745-6215},
  doi = {10.1186/s13063-017-1969-5},
  abstract = {BACKGROUND: A key step in the design of a randomised controlled trial (RCT) is the estimation of the number of participants needed. By far the most common approach is to specify a target difference and then estimate the corresponding sample size; this sample size is chosen to provide reassurance that the trial will have high statistical power to detect such a difference between the randomised groups (at the planned statistical significance level). The sample size has many implications for the conduct of the study, as well as carrying scientific and ethical aspects to its choice. Despite the critical role of the target difference for the primary outcome in the design of an RCT, the manner in which it is determined has received little attention. This article reports the protocol of the Difference ELicitation in TriAls (DELTA2) project, which will produce guidance on the specification and reporting of the target difference for the primary outcome in a sample size calculation for RCTs. METHODS/DESIGN: The DELTA2 project has five components: systematic literature reviews of recent methodological developments (stage 1) and existing funder guidance (stage 2); a Delphi study (stage 3); a 2-day consensus meeting bringing together researchers, funders and patient representatives, as well as one-off engagement sessions at relevant stakeholder meetings (stage 4); and the preparation and dissemination of a guidance document (stage 5). DISCUSSION: Specification of the target difference for the primary outcome is a key component of the design of an RCT. There is a need for better guidance for researchers and funders regarding specification and reporting of this aspect of trial design. The aim of this project is to produce consensus based guidance for researchers and funders.},
  langid = {english},
  pmcid = {PMC5469157},
  pmid = {28606102},
  keywords = {Clinically important difference,Consensus,Delphi Technique,Effect size,Endpoint Determination,Guidance,Humans,Pilot study,Randomised controlled trial,Randomized Controlled Trials as Topic,Research Design,Sample size,Sample Size,Target difference}
}

@article{cook_p-value_2002,
  title = {P-{{Value Adjustment}} in {{Sequential Clinical Trials}}},
  author = {Cook, Thomas D.},
  year = {2002},
  journal = {Biometrics},
  volume = {58},
  number = {4},
  pages = {1005--1011},
  publisher = {{Wiley Online Library}}
}

@book{cooper_handbook_2009,
  title = {The Handbook of Research Synthesis and Meta-Analysis},
  editor = {Cooper, Harris M. and Hedges, Larry V. and Valentine, Jeff C.},
  year = {2009},
  edition = {2nd ed},
  publisher = {{Russell Sage Foundation}},
  address = {{New York}},
  isbn = {978-0-87154-163-5},
  lccn = {Q180.55.M4 H35 2009},
  keywords = {Information storage and retrieval systems,Methodology,Research,Statistical methods}
}

@book{cooper_reporting_2020,
  title = {Reporting Quantitative Research in Psychology: {{How}} to Meet {{APA Style Journal Article Reporting Standards}} (2nd Ed.).},
  shorttitle = {Reporting Quantitative Research in Psychology},
  author = {Cooper, Harris},
  year = {2020},
  publisher = {{American Psychological Association}},
  address = {{Washington}},
  doi = {10.1037/0000178-000},
  isbn = {978-1-4338-3283-3 978-1-4338-3342-7},
  langid = {english}
}

@article{copay_understanding_2007,
  title = {Understanding the Minimum Clinically Important Difference: A Review of Concepts and Methods},
  shorttitle = {Understanding the Minimum Clinically Important Difference},
  author = {Copay, Anne G. and Subach, Brian R. and Glassman, Steven D. and Polly, David W. and Schuler, Thomas C.},
  year = {2007},
  journal = {The Spine Journal},
  volume = {7},
  number = {5},
  pages = {541--546},
  doi = {10.1016/j.spinee.2007.01.008}
}

@article{correll_avoid_2020,
  title = {Avoid {{Cohen}}'s `{{Small}}', `{{Medium}}', and `{{Large}}' for {{Power Analysis}}},
  author = {Correll, Joshua and Mellinger, Christopher and McClelland, Gary H. and Judd, Charles M.},
  year = {2020},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {3},
  pages = {200--207},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.12.009},
  abstract = {One of the most difficult and important decisions in power analysis involves specifying an effect size. Researchers frequently employ definitions of small, medium, and large that were proposed by Jacob Cohen. These definitions are problematic for two reasons. First, they are arbitrary, based on non-scientific criteria. Second, they are inconsistent, changing dramatically and illogically as a function of the statistical test a researcher plans to use (e.g., t-test versus regression). These problems may be unknown to many researchers, but they have a huge impact on power analyses. Estimates of the required n may be inappropriately doubled or cut in half. For power analyses to have any meaning, these definitions of effect size should be avoided.},
  langid = {english},
  keywords = {effect size,research design,research methods}
}

@manual{cousineau_superb_2019,
  type = {Manual},
  title = {Superb: {{Computes}} Standard Error and Confidence Interval of Means under Various Designs and Sampling Schemes},
  author = {Cousineau, Denis and Chiasson, Felix},
  year = {2019}
}

@article{cowles_origins_1982,
  title = {On the Origins of the. 05 Level of Statistical Significance.},
  author = {Cowles, Michael and Davis, Caroline},
  year = {1982},
  journal = {American Psychologist},
  volume = {37},
  number = {5},
  pages = {553}
}

@article{cox_problems_1958,
  title = {Some {{Problems Connected}} with {{Statistical Inference}}},
  author = {Cox, D. R.},
  year = {1958},
  month = jun,
  journal = {Annals of Mathematical Statistics},
  volume = {29},
  number = {2},
  pages = {357--372},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177706618},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  mrnumber = {MR94890},
  zmnumber = {0088.11702}
}

@article{cribbie_recommendations_2004,
  title = {Recommendations for Applying Tests of Equivalence},
  author = {Cribbie, Robert A. and Gruman, Jamie A. and {Arpin-Cribbie}, Chantal A.},
  year = {2004},
  journal = {Journal of clinical psychology},
  volume = {60},
  number = {1},
  pages = {1--10},
  publisher = {{Wiley Online Library}}
}

@article{cumming_confidence_2006,
  title = {Confidence Intervals and Replication: {{Where}} Will the next Mean Fall?},
  shorttitle = {Confidence Intervals and Replication},
  author = {Cumming, Geoff and Maillardet, Robert},
  year = {2006},
  journal = {Psychological Methods},
  volume = {11},
  number = {3},
  pages = {217--227},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.3.217},
  langid = {english}
}

@book{cumming_introduction_2016,
  title = {Introduction to the {{New Statistics}}: {{Estimation}}, {{Open Science}}, and {{Beyond}}},
  shorttitle = {Introduction to the {{New Statistics}}},
  author = {Cumming, Geoff and {Calin-Jageman}, Robert},
  year = {2016},
  month = oct,
  publisher = {{Routledge}},
  abstract = {This is the first introductory statistics text to use an estimation approach from the start to help readers understand effect sizes, confidence intervals (CIs), and meta-analysis (`the new statistics'). It is also the first text to explain the new and exciting Open Science practices, which encourage replication and enhance the trustworthiness of research. In addition, the book explains NHST fully so students can understand published research. Numerous real research examples are used throughout. The book uses today's most effective learning strategies and promotes critical thinking, comprehension, and retention, to deepen users' understanding of statistics and modern research methods. The free ESCI (Exploratory Software for Confidence Intervals) software makes concepts visually vivid, and provides calculation and graphing facilities. The book can be used with or without ESCI.  Other highlights include: - Coverage of both estimation and NHST approaches, and how to easily translate between the two.  - Some exercises use ESCI to analyze data and create graphs including CIs, for best understanding of estimation methods.  -Videos of the authors describing key concepts and demonstrating use of ESCI provide an engaging learning tool for traditional or flipped classrooms. -In-chapter exercises and quizzes with related commentary allow students to learn by doing, and to monitor their progress. -End-of-chapter exercises and commentary, many using real data, give practice for using the new statistics to analyze data, as well as for applying research judgment in realistic contexts.  -Don't fool yourself tips help students avoid common errors.  -Red Flags highlight the meaning of "significance" and what p values actually mean.  -Chapter outlines, defined key terms, sidebars of key points, and summarized take-home messages provide a study tool at exam time.  -http://www.routledge.com/cw/cumming offers for students: ESCI downloads; data sets; key term flashcards; tips for using SPSS for analyzing data; and videos. For instructors it offers: tips for teaching the new statistics and Open Science; additional homework exercises; assessment items; answer keys for homework and assessment items; and downloadable text images; and PowerPoint lecture slides.  Intended for introduction to statistics, data analysis, or quantitative methods courses in psychology, education, and other social and health sciences, researchers interested in understanding the new statistics will also appreciate this book. No familiarity with introductory statistics is assumed.},
  googlebooks = {KR8xDQAAQBAJ},
  isbn = {978-1-317-48337-3},
  langid = {english},
  keywords = {Business \& Economics / Statistics,Education / Statistics,Medical / Biostatistics,Psychology / Statistics}
}

@article{cumming_new_2014,
  title = {The {{New Statistics}}: {{Why}} and {{How}}},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, Geoff},
  year = {2014},
  month = jan,
  journal = {Psychological Science},
  volume = {25},
  number = {1},
  pages = {7--29},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797613504966},
  langid = {english}
}

@article{cumming_replication_2008,
  title = {Replication and {\emph{p}} {{Intervals}}: {\emph{p}} {{Values Predict}} the {{Future Only Vaguely}}, but {{Confidence Intervals Do Much Better}}},
  shorttitle = {Replication and {\emph{p}} {{Intervals}}},
  author = {Cumming, Geoff},
  year = {2008},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {3},
  number = {4},
  pages = {286--300},
  issn = {17456916, 17456924},
  doi = {10.1111/j.1745-6924.2008.00079.x},
  langid = {english}
}

@book{cumming_understanding_2013,
  title = {Understanding the New Statistics: {{Effect}} Sizes, Confidence Intervals, and Meta-Analysis},
  shorttitle = {Understanding the New Statistics},
  author = {Cumming, Geoff},
  year = {2013},
  publisher = {{Routledge}}
}

@article{danziger_extraneous_2011,
  title = {Extraneous Factors in Judicial Decisions},
  author = {Danziger, S. and Levav, J. and {Avnaim-Pesso}, L.},
  year = {2011},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {17},
  pages = {6889--6892},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/PNAS.1018033108},
  langid = {english},
  annotation = {00711}
}

@book{de_groot_methodology_1969,
  title = {Methodology},
  author = {{de Groot}, Adrianus Dingeman},
  year = {1969},
  volume = {6},
  publisher = {{Mouton \& Co.}},
  address = {{The Hague}}
}

@misc{de_vrieze_meta-analyses_2018,
  title = {Meta-Analyses Were Supposed to End Scientific Debates. {{Often}}, They Only Cause More Controversy},
  author = {{de Vrieze}, Jop and {2018} and Pm, 4:15},
  year = {2018},
  month = sep,
  journal = {Science | AAAS},
  abstract = {Compiling the evidence from dozens of studies doesn't always bring clarity},
  howpublished = {https://www.sciencemag.org/news/2018/09/meta-analyses-were-supposed-end-scientific-debates-often-they-only-cause-more},
  langid = {english}
}

@article{debruine_understanding_2021,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa M. and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  keywords = {lme4,mixed-effects models,open materials,power,R,simulation}
}

@article{delacre_why_2017,
  title = {Why {{Psychologists Should}} by {{Default Use Welch}}'s {\emph{t}}-Test {{Instead}} of {{Student}}'s {\emph{t}}-Test},
  author = {Delacre, Marie and Lakens, Dani{\"e}l and Leys, Christophe},
  year = {2017},
  journal = {International Review of Social Psychology},
  volume = {30},
  number = {1},
  issn = {2119-4130},
  doi = {10.5334/irsp.82},
  abstract = {When comparing two independent groups, psychology researchers commonly use Student's t-tests. Assumptions of normality and homogeneity of variance underlie this test. More often than not, when these conditions are not met, Student's t-test can be severely biased and lead to invalid statistical inferences. Moreover, we argue that the assumption of equal variances will seldom hold in psychological research, and choosing between Student's t-test and Welch's t-test based on the outcomes of a test of the equality of variances often fails to provide an appropriate answer. We show that the Welch's t-test provides a better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student's t-test when the assumptions are met. We argue that Welch's t-test should be used as a default strategy.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  keywords = {homogeneity of variance,Homoscedasticity,Levene’s test,statistical power,Student’s t-test,type 1 error,type 2 error,Welch’s t-test}
}

@misc{delacre_why_2021,
  title = {Why {{Hedges}}' G*s Based on the Non-Pooled Standard Deviation Should Be Reported with {{Welch}}'s t-Test},
  author = {Delacre, Marie and Lakens, Daniel and Ley, Christophe and Liu, Limin and Leys, Christophe},
  year = {2021},
  month = may,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/tu6mp},
  abstract = {Researchers are generally required to report and interpret effect sizes and associated confidence intervals. When comparing two independent groups, the most commonly used estimator of effect size is Cohen's ds where sample mean difference is divided by the pooled standard deviation. However, computing the pooled error term is not valid when both groups do not share common population variances. Furthermore, the assumption of equal population variances is unlikely in many psychological fields. Consequently, researchers shift to the use of Welch's t-test over Student's t-test in the context of hypothesis testing. Meanwhile, the question which effect size to report when equal variances are not assumed remains open. Based on Monte Carlo simulations, we compare Hedges' gs (i.e. Cohen's ds with correction for bias) to Glass's gs, Shieh's gs and Hedges' g\_s\^*. Comparisons are made under normality as well as under realistic deviations from the assumptions of normality and equal variances. Although it is not directly related with Welch's t-test (unlike Shieh's gs), we recommend the use of Hedges' g\_s\^* because it shows better properties than all other estimators. Practical recommendations, R package and Shiny App in order to compute effect size estimators and confidence intervals are provided.},
  keywords = {Effect size,Monte Carlo Simulations,Parametric Assumptions,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods}
}

@article{detsky_using_1990,
  title = {Using Cost-Effectiveness Analysis to Improve the Efficiency of Allocating Funds to Clinical Trials},
  author = {Detsky, Allan S.},
  year = {1990},
  journal = {Statistics in Medicine},
  volume = {9},
  number = {1-2},
  pages = {173--184},
  issn = {1097-0258},
  doi = {10.1002/sim.4780090124},
  abstract = {This study applied a cost-effectiveness model to seven randomized trials. The model demonstrates the effect of design choices made in the planning stages of a clinical trial on the costs and benefits derived from conducting the trial. The study focused on one parameter used to calculate sample size: the minimum clinically important difference in event rates between control and experimental therapies. The study shows that the model can be operationalized to these trials. A computerized software package and manual has been developed to simplify the calculations. While there was some variation in the incremental cost-effectiveness ratios across the seven trials in this study, all ratios may be below the funding threshold. This analytical technique can be used to demonstrate explicitly the resource consequences of the design of randomized trials and perhaps to set funding priorities.},
  copyright = {Copyright \textcopyright{} 1990 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780090124}
}

@book{dienes_understanding_2008,
  title = {Understanding Psychology as a Science: {{An}} Introduction to Scientific and Statistical Inference},
  shorttitle = {Understanding Psychology as a Science},
  author = {Dienes, Zoltan},
  year = {2008},
  publisher = {{Palgrave Macmillan}}
}

@article{dienes_using_2014,
  title = {Using {{Bayes}} to Get the Most out of Non-Significant Results},
  author = {Dienes, Zoltan},
  year = {2014},
  journal = {Frontiers in Psychology},
  volume = {5},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00781},
  abstract = {No scientific conclusion follows automatically from a statistically non-significant result, yet people routinely use non-significant results to guide conclusions about the status of theories (or the effectiveness of practices). To know whether a non-significant result counts against a theory, or if it just indicates data insensitivity, researchers must use one of: power, intervals (such as confidence or credibility intervals), or else an indicator of the relative evidence for one theory over another, such as a Bayes factor. I argue Bayes factors allow theory to be linked to data in a way that overcomes the weaknesses of the other approaches. Specifically, Bayes factors use the data themselves to determine their sensitivity in distinguishing theories (unlike power), and they make use of those aspects of a theory's predictions that are often easiest to specify (unlike power and intervals, which require specifying the minimal interesting value in order to address theory). Bayes factors provide a coherent approach to determining whether non-significant results support a null hypothesis over a theory, or whether the data are just insensitive. They allow accepting and rejecting the null hypothesis to be put on an equal footing. Concrete examples are provided to indicate the range of application of a simple online Bayes calculator, which reveal both the strengths and weaknesses of Bayes factors.},
  langid = {english},
  keywords = {Bayes factor,confidence interval,credibility interval,Significance testing,statistical inference}
}

@article{dmitrienko_traditional_2013,
  title = {Traditional Multiplicity Adjustment Methods in Clinical Trials},
  author = {Dmitrienko, Alex and D'Agostino Sr, Ralph},
  year = {2013},
  journal = {Statistics in Medicine},
  volume = {32},
  number = {29},
  pages = {5172--5218},
  issn = {1097-0258},
  doi = {10.1002/sim.5990},
  abstract = {This tutorial discusses important statistical problems arising in clinical trials with multiple clinical objectives based on different clinical variables, evaluation of several doses or regiments of a new treatment, analysis of multiple patient subgroups, etc. Simultaneous assessment of several objectives in a single trial gives rise to multiplicity. If unaddressed, problems of multiplicity can undermine integrity of statistical inferences. The tutorial reviews key concepts in multiple hypothesis testing and introduces main classes of methods for addressing multiplicity in a clinical trial setting. General guidelines for the development of relevant and efficient multiple testing procedures are presented on the basis of application-specific clinical and statistical information. Case studies with common multiplicity problems are used to motivate and illustrate the statistical methods presented in the tutorial, and software implementation of the multiplicity adjustment methods is discussed. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {clinical trials,multiple testing procedures,multiplicity adjustments,multiplicity problems,type I error rate},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5990}
}

@article{dodge_method_1929,
  title = {A {{Method}} of {{Sampling Inspection}}},
  author = {Dodge, H. F. and Romig, H. G.},
  year = {1929},
  month = oct,
  journal = {Bell System Technical Journal},
  volume = {8},
  number = {4},
  pages = {613--631},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1929.tb01240.x},
  abstract = {This paper outlines some of the general considerations which must be taken into account in setting up any practical sampling inspection plan. An economical method of inspection is developed in detail for the case where the purpose of the inspection is to determine the acceptability of discrete lots of a product submitted by a producer. By employing probability theory, the method places a definite barrier in the path of material of defective quality and gives this protection to the consumer with a minimum of inspection expense.},
  langid = {english}
}

@article{dongen_multiple_2019,
  title = {Multiple {{Perspectives}} on {{Inference}} for {{Two Simple Statistical Scenarios}}},
  author = {van Dongen, Noah N. N. and van Doorn, Johnny B. and Gronau, Quentin F. and van Ravenzwaaij, Don and Hoekstra, Rink and Haucke, Matthias N. and Lakens, Dani{\"e}l and Hennig, Christian and Morey, Richard D. and Homer, Saskia and Gelman, Andrew and Sprenger, Jan and Wagenmakers, Eric-Jan},
  year = {2019},
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {328--339},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1565553},
  abstract = {When data analysts operate within different statistical frameworks (e.g., frequentist versus Bayesian, emphasis on estimation versus emphasis on testing), how does this impact the qualitative conclusions that are drawn for real data? To study this question empirically we selected from the literature two simple scenarios\textemdash involving a comparison of two proportions and a Pearson correlation\textemdash and asked four teams of statisticians to provide a concise analysis and a qualitative interpretation of the outcome. The results showed considerable overall agreement; nevertheless, this agreement did not appear to diminish the intensity of the subsequent debate over which statistical framework is more appropriate to address the questions at hand.},
  keywords = {Frequentist or Bayesian,Multilab analysis,Statistical paradigms,Testing or estimation}
}

@book{dubin_theory_1969,
  title = {Theory Building},
  author = {Dubin, Robert},
  year = {1969},
  publisher = {{Free Press}},
  address = {{New York}},
  langid = {english},
  annotation = {OCLC: 609596212}
}

@article{dunn_multiple_1961,
  title = {Multiple {{Comparisons}} among {{Means}}},
  author = {Dunn, Olive Jean},
  year = {1961},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {56},
  number = {293},
  pages = {52--64},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1961.10482090},
  abstract = {Methods for constructing simultaneous confidence intervals for all possible linear contrasts among several means of normally distributed variables have been given by Scheff\'e and Tukey. In this paper the possibility is considered of picking in advance a number (say m) of linear contrasts among k means, and then estimating these m linear contrasts by confidence intervals based on a Student t statistic, in such a way that the overall confidence level for the m intervals is greater than or equal to a preassigned value. It is found that for some values of k, and for m not too large, intervals obtained in this way are shorter than those using the F distribution or the Studentized range. When this is so, the experimenter may be willing to select the linear combinations in advance which he wishes to estimate in order to have m shorter intervals instead of an infinite number of longer intervals.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1961.10482090}
}

@article{dupont_sequential_1983,
  title = {Sequential Stopping Rules and Sequentially Adjusted {{P}} Values: {{Does}} One Require the Other?},
  shorttitle = {Sequential Stopping Rules and Sequentially Adjusted {{P}} Values},
  author = {Dupont, William D.},
  year = {1983},
  month = jan,
  journal = {Controlled Clinical Trials},
  volume = {4},
  number = {1},
  pages = {3--10},
  issn = {0197-2456},
  doi = {10.1016/S0197-2456(83)80003-8},
  abstract = {During the course of a clinical trial it is normally necessary to conduct periodic reviews of the data in order to determine whether the trial should be terminated. Since these reviews affect the probability of the final outcome, many statisticians recommend that the P values quoted for a clinical trial be sequentially adjusted to account for the possibility of premature termination. In this article it is argued that the sequentially adjusted P value is an inappropriate measure of the strength of evidence justified by a clinical trial. This is because the size of sequentially adjusted P values will vary according to actions that might have been taken if the trial had gone differently than it in fact did. Although such contingencies will effect the frequency of occurrence of certain events in hypothetical sequence of trial replications, it is hard to see why decisions that would have been made in response to outcomes that did not occur should have any bearing on the strength of evidence that can be attributed to the results that were actually observed. The credibility merited by a clinical trial depends not only on the implausibility of the observed results under the null hypothesis, but also on factors such as the medical plausibility of hypothesis well supported by the data, and the extent to which observed results have been predicted in advance. It is argued that publishing these factors along with fixed sample P values is the best way to indicate the degree of certainty that should be attributed to the conclusions of a clinical trial.},
  langid = {english},
  keywords = {Clinical trials,foundations of statistical inference,likelihood principle,Sequential stopping rules}
}

@article{ebersole_many_2016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  shorttitle = {Many {{Labs}} 3},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and {Joy-Gaba}, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and {van Allen}, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences\textemdash conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  langid = {english},
  keywords = {Cognitive psychology,Individual differences,Participant pool,Replication,Sampling effects,Situational effects,Social psychology}
}

@article{eckermann_value_2010,
  title = {The {{Value}} of {{Value}} of {{Information}}},
  author = {Eckermann, Simon and Karnon, Jon and Willan, Andrew R.},
  year = {2010},
  month = sep,
  journal = {PharmacoEconomics},
  volume = {28},
  number = {9},
  pages = {699--709},
  issn = {1179-2027},
  doi = {10.2165/11537370-000000000-00000},
  abstract = {Value of information (VOI) methods have been proposed as a systematic approach to inform optimal research design and prioritization. Four related questions arise that VOI methods could address. (i) Is further research for a health technology assessment (HTA) potentially worthwhile? (ii) Is the cost of a given research design less than its expected value? (iii) What is the optimal research design for an HTA? (iv) How can research funding be best prioritized across alternative HTAs?},
  langid = {english}
}

@article{elson_press_2014,
  title = {Press {{CRTT}} to Measure Aggressive Behavior: The Unstandardized Use of the Competitive Reaction Time Task in Aggression Research},
  shorttitle = {Press {{CRTT}} to Measure Aggressive Behavior},
  author = {Elson, Malte and Mohseni, M. Rohangis and Breuer, Johannes and Scharkow, Michael and Quandt, Thorsten},
  year = {2014},
  month = jun,
  journal = {Psychological Assessment},
  volume = {26},
  number = {2},
  pages = {419--432},
  issn = {1939-134X},
  doi = {10.1037/a0035569},
  abstract = {The competitive reaction time task (CRTT) is the measure of aggressive behavior most commonly used in laboratory research. However, the test has been criticized for issues in standardization because there are many different test procedures and at least 13 variants to calculate a score for aggressive behavior. We compared the different published analyses of the CRTT using data from 3 different studies to scrutinize whether it would yield the same results. The comparisons revealed large differences in significance levels and effect sizes between analysis procedures, suggesting that the unstandardized use and analysis of the CRTT have substantial impacts on the results obtained, as well as their interpretations. Based on the outcome of our comparisons, we provide suggestions on how to address some of the issues associated with the CRTT, as well as a guideline for researchers studying aggressive behavior in the laboratory.},
  langid = {english},
  pmid = {24447279},
  keywords = {Adult,Aggression,Competitive Behavior,Female,Humans,Male,Reaction Time,Social Behavior,Task Performance and Analysis,Young Adult}
}

@article{erdfelder_gpower_1996,
  title = {{{GPOWER}}: {{A}} General Power Analysis Program},
  shorttitle = {{{GPOWER}}},
  author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  year = {1996},
  month = mar,
  journal = {Behavior Research Methods, Instruments, \& Computers},
  volume = {28},
  number = {1},
  pages = {1--11},
  issn = {1532-5970},
  doi = {10.3758/BF03203630},
  abstract = {GPOWER is a completely interactive, menu-driven program for IBM-compatible and Apple Macintosh personal computers. It performs high-precision statistical power analyses for the most common statistical tests in behavioral research, that is,t tests,F tests, and{$\chi$}2 tests. GPOWER computes (1) power values for given sample sizes, effect sizes and{$\alpha$} levels (post hoc power analyses); (2) sample sizes for given effect sizes,{$\alpha$} levels, and power values (a priori power analyses); and (3){$\alpha$} and{$\beta$} values for given sample sizes, effect sizes, and{$\beta$}/{$\alpha$} ratios (compromise power analyses). The program may be used to display graphically the relation between any two of the relevant variables, and it offers the opportunity to compute the effect size measures from basic parameters defining the alternative hypothesis. This article delineates reasons for the development of GPOWER and describes the program's capabilities and handling.},
  langid = {english}
}

@article{evers_revisiting_2014,
  ids = {evers_revisiting_2014-1},
  title = {Revisiting {{Tversky}}'s Diagnosticity Principle},
  author = {Evers, Ellen R. K. and Lakens, Dani{\"e}l},
  year = {2014},
  journal = {Frontiers in Psychology},
  volume = {5},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00875}
}

@article{eysenck_exercise_1978,
  title = {An Exercise in Mega-Silliness},
  author = {Eysenck, H. J.},
  year = {1978},
  journal = {American Psychologist},
  volume = {33},
  number = {5},
  pages = {517--517},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.33.5.517.a},
  abstract = {Disputes the methodology and conclusions of M. L. Smith and G. V. Glass (see record 1978-10341-001) in their meta-analysis of psychotherapy outcome studies. Smith and Glass's use of a compilation of studies, mostly of poor design, is an abandonment of scholarship. There remains no acceptable evidence for the efficacy of psychotherapy. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Meta Analysis,Psychotherapeutic Outcomes,Psychotherapeutic Techniques,Psychotherapy}
}

@article{fanelli_how_2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta-Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  year = {2009},
  month = may,
  journal = {PLOS ONE},
  volume = {4},
  number = {5},
  pages = {e5738},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005738},
  abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, ``cooking'' of data, etc\ldots{} Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86\textendash 4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once \textendash a serious form of misconduct by any standard\textendash{} and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91\textendash 19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words ``falsification'' or ``fabrication'', and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
  langid = {english},
  keywords = {Deception,Medical journals,Medicine and health sciences,Metaanalysis,Scientific misconduct,Scientists,Social research,Surveys}
}

@article{fanelli_positive_2010,
  title = {``{{Positive}}'' {{Results Increase Down}} the {{Hierarchy}} of the {{Sciences}}},
  author = {Fanelli, Daniele},
  year = {2010},
  month = apr,
  journal = {PLoS ONE},
  volume = {5},
  number = {4},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the ``hardness'' of scientific research\textemdash i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors\textemdash is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a ``positive'' (full or partial) or ``negative'' support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in ``softer'' sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  pmcid = {PMC2850928},
  pmid = {20383332}
}

@article{faul_gpower_2007,
  title = {{{GPower}} 3: {{A}} Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences},
  shorttitle = {G*{{Power}} 3},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  year = {2007},
  month = may,
  journal = {Behavior Research Methods},
  volume = {39},
  number = {2},
  pages = {175--191},
  issn = {1554-351X, 1554-3528},
  doi = {10.3758/BF03193146},
  langid = {english}
}

@article{ferguson_comment_2014,
  title = {Comment: {{Why}} Meta-Analyses Rarely Resolve Ideological Debates},
  shorttitle = {Comment},
  author = {Ferguson, Christopher J.},
  year = {2014},
  journal = {Emotion Review},
  volume = {6},
  number = {3},
  pages = {251--252}
}

@article{ferguson_providing_2021,
  title = {Providing a Lower-Bound Estimate for Psychology's ``Crud Factor'': {{The}} Case of Aggression},
  shorttitle = {Providing a Lower-Bound Estimate for Psychology's ``Crud Factor''},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2021},
  month = dec,
  journal = {Professional Psychology: Research and Practice},
  volume = {52},
  number = {6},
  pages = {620--626},
  publisher = {{American Psychological Association}},
  address = {{Washington, US}},
  issn = {0735-7028},
  doi = {http://dx.doi.org/10.1037/pro0000386},
  abstract = {When conducting research on large data sets, statistically significant findings having only trivial interpretive meaning may appear. Little consensus exists whether such small effects can be meaningfully interpreted. The current analysis examines the possibility that trivial effects may emerge in large datasets, but that some such effects may lack interpretive value. When such results match an investigator's hypothesis, they may be over-interpreted. The current study examines this issue as related to aggression research in two large samples. Specifically, in the first study, the National Longitudinal Study of Adolescent to Adult Health (AddHeath) dataset was used. Fifteen variables with little theoretical relevance to aggression were selected, then correlated with self-reported delinquency. For the second study, the Understanding Society database was used. As with Study 1, 14 nonsensical variables were correlated with conduct problems. Many variables achieved ``statistical significance'' and some effect sizes approached or exceeded r = .10, despite little theoretical relevance between the variables. It is recommended that effect sizes below r = .10 should not be interpreted as hypothesis supportive. (PsycInfo Database Record (c) 2021 APA, all rights reserved) (Source: journal abstract)},
  copyright = {\textcopyright{} 2021, American Psychological Association},
  langid = {english},
  keywords = {Aggressive Behavior (major),Behavior Problems (major),Data Sets,Effect Size (Statistical) (major),Self-Report (major),Statistical Significance (major)},
  annotation = {(US)}
}

@article{ferguson_vast_2012,
  title = {A Vast Graveyard of Undead Theories Publication Bias and Psychological Science's Aversion to the Null},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2012},
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {555--561}
}

@article{ferron_power_1996,
  title = {The {{Power}} of {{Randomization Tests}} for {{Single-Case Phase Designs}}},
  author = {Ferron, John and Onghena, Patrick},
  year = {1996},
  month = apr,
  journal = {The Journal of Experimental Education},
  volume = {64},
  number = {3},
  pages = {231--239},
  publisher = {{Routledge}},
  issn = {0022-0973},
  doi = {10.1080/00220973.1996.9943805},
  abstract = {Monte Carlo methods were used to estimate the power of randomization tests used with single-case designs involving the random assignment of treatments to phases. The design studied involved 2 treatments and 6 phases. The power was studied for 6 standardized effect sizes (0, .2, .5, .8, 1.1, and 1.4), 4 levels of autocorrelation (1st order autocorrelation coefficients of -.3, 0, .3, and .6), and 5 different phase lengths (4, 5, 6, 7, and 8 observations). Power was estimated for each condition by simulating 10,000 experiments. The results showed an adequate level of power ({$>$} .80) when effect sizes were large (1.1 and 1.4), phase lengths exceeded 5, and autocorrelation was not negative.},
  annotation = {\_eprint: https://doi.org/10.1080/00220973.1996.9943805}
}

@book{feyerabend_against_1993,
  title = {Against Method},
  author = {Feyerabend, Paul},
  year = {1993},
  edition = {3rd ed},
  publisher = {{Verso}},
  address = {{London ; New York}},
  isbn = {978-0-86091-481-5 978-0-86091-646-8},
  lccn = {Q175 .F42 1993},
  keywords = {Methodology,Philosophy,Rationalism,Science}
}

@article{fiedler_questionable_2015,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2015},
  month = oct,
  journal = {Social Psychological and Personality Science},
  pages = {1948550615612150},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615612150},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english},
  keywords = {ethics/morality,language,research methods,research practices,survey methodology}
}

@article{fiedler_tools_2004,
  title = {Tools, Toys, Truisms, and Theories: {{Some}} Thoughts on the Creative Cycle of Theory Formation},
  shorttitle = {Tools, Toys, Truisms, and Theories},
  author = {Fiedler, Klaus},
  year = {2004},
  journal = {Personality and Social Psychology Review},
  volume = {8},
  number = {2},
  pages = {123--131},
  doi = {10.1207/s15327957pspr0802_5}
}

@article{fiedler_tools_2004-1,
  title = {Tools, Toys, Truisms, and Theories: {{Some}} Thoughts on the Creative Cycle of Theory Formation},
  shorttitle = {Tools, Toys, Truisms, and Theories},
  author = {Fiedler, Klaus},
  year = {2004},
  journal = {Personality and Social Psychology Review},
  volume = {8},
  number = {2},
  pages = {123--131}
}

@article{field_minimizing_2004,
  title = {Minimizing the Cost of Environmental Management Decisions by Optimizing Statistical Thresholds},
  author = {Field, Scott A. and Tyre, Andrew J. and Jonz{\'e}n, Niclas and Rhodes, Jonathan R. and Possingham, Hugh P.},
  year = {2004},
  month = aug,
  journal = {Ecology Letters},
  volume = {7},
  number = {8},
  pages = {669--675},
  issn = {1461-0248},
  doi = {10.1111/j.1461-0248.2004.00625.x},
  abstract = {Environmental management decisions are prone to expensive mistakes if they are triggered by hypothesis tests using the conventional Type I error rate ({$\alpha$}) of 0.05. We derive optimal {$\alpha$}-levels for decision-making by minimizing a cost function that specifies the overall cost of monitoring and management. When managing an economically valuable koala population, it shows that a decision based on {$\alpha~$}=~0.05 carries an expected cost over \$5~million greater than the optimal decision. For a species of such value, there is never any benefit in guarding against the spurious detection of declines and therefore management should proceed directly to recovery action. This result holds in most circumstances where the species' value substantially exceeds its recovery costs. For species of lower economic value, we show that the conventional {$\alpha$}-level of 0.05 rarely approximates the optimal decision-making threshold. This analysis supports calls for reversing the statistical `burden of proof' in environmental decision-making when the cost of Type II errors is relatively high.},
  langid = {english},
  keywords = {Koala,management,optimal monitoring,statistical power,Statistical Significance,type I error,type II error}
}

@book{fisher_design_1935,
  title = {The Design of Experiments},
  author = {Fisher, Ronald Aylmer},
  year = {1935},
  publisher = {{Oliver And Boyd; Edinburgh; London}}
}

@book{fisher_statistical_1956,
  title = {Statistical Methods and Scientific Inference},
  author = {Fisher, Ronald A.},
  year = {1956},
  volume = {viii},
  publisher = {{Hafner Publishing Co.}},
  address = {{Oxford, England}},
  abstract = {An explicit statement of the logical nature of statistical reasoning that has been implicitly required in the development and use of statistical techniques in the making of uncertain inferences and in the design of experiments. Included is a consideration of the concept of mathematical probability; a comparison of fiducial and confidence intervals; a comparison of the logic of tests of significance with the acceptance decision approach; and a discussion of the principles of prediction and estimation.},
  copyright = {(c) 2016 APA, all rights reserved}
}

@article{fortin_big_2013,
  title = {Big Science vs. Little Science: How Scientific Impact Scales with Funding},
  shorttitle = {Big Science vs. Little Science},
  author = {Fortin, Jean-Michel and Currie, David J.},
  year = {2013}
}

@article{fraley_n-pact_2014,
  title = {The {{N-Pact Factor}}: {{Evaluating}} the {{Quality}} of {{Empirical Journals}} with {{Respect}} to {{Sample Size}} and {{Statistical Power}}},
  shorttitle = {The {{N-Pact Factor}}},
  author = {Fraley, R. Chris and Vazire, Simine},
  year = {2014},
  month = oct,
  journal = {PLOS ONE},
  volume = {9},
  number = {10},
  pages = {e109019},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0109019},
  abstract = {The authors evaluate the quality of research reported in major journals in social-personality psychology by ranking those journals with respect to their  N -pact Factors (NF)\textemdash the statistical power of the empirical studies they publish to detect typical effect sizes. Power is a particularly important attribute for evaluating research quality because, relative to studies that have low power, studies that have high power are more likely to (a) to provide accurate estimates of effects, (b) to produce literatures with low false positive rates, and (c) to lead to replicable findings. The authors show that the average sample size in social-personality research is 104 and that the power to detect the typical effect size in the field is approximately 50\%. Moreover, they show that there is considerable variation among journals in sample sizes and power of the studies they publish, with some journals consistently publishing higher power studies than others. The authors hope that these rankings will be of use to authors who are choosing where to submit their best work, provide hiring and promotion committees with a superior way of quantifying journal quality, and encourage competition among journals to improve their NF rankings.},
  keywords = {Personality,Personality differences,Psychology,Research Design,Research integrity,Research quality assessment,Social psychology,Social research}
}

@article{francis_frequency_2014,
  title = {The Frequency of Excess Success for Articles in {{Psychological Science}}},
  author = {Francis, Gregory},
  year = {2014},
  month = mar,
  journal = {Psychonomic Bulletin \& Review},
  volume = {21},
  number = {5},
  pages = {1180--1187},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-014-0601-x},
  abstract = {Recent controversies have questioned the quality of scientific practice in the field of psychology, but these concerns are often based on anecdotes and seemingly isolated cases. To gain a broader perspective, this article applies an objective test for excess success to a large set of articles published in the journal Psychological Science between 2009 and 2012. When empirical studies succeed at a rate much higher than is appropriate for the estimated effects and sample sizes, readers should suspect that unsuccessful findings have been suppressed, the experiments or analyses were improper, or the theory does not properly account for the data. In total, problems appeared for 82 \% (36 out of 44) of the articles in Psychological Science that had four or more experiments and could be analyzed.},
  langid = {english},
  keywords = {Cognitive psychology,Probabilistic reasoning,Statistical inference,Statistics}
}

@article{francis_too_2012,
  title = {Too Good to Be True: {{Publication}} Bias in Two Prominent Studies from Experimental Psychology},
  shorttitle = {Too Good to Be True},
  author = {Francis, Gregory},
  year = {2012},
  journal = {Psychonomic bulletin \& review},
  volume = {19},
  number = {2},
  pages = {151--156}
}

@article{franco_publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  year = {2014},
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  doi = {10.1126/SCIENCE.1255484},
  annotation = {00377}
}

@article{freiman_importance_1978,
  title = {The Importance of Beta, the Type {{II}} Error and Sample Size in the Design and Interpretation of the Randomized Control Trial. {{Survey}} of 71 "Negative" Trials},
  author = {Freiman, J. A. and Chalmers, T. C. and Smith, H. and Kuebler, R. R.},
  year = {1978},
  month = sep,
  journal = {The New England Journal of Medicine},
  volume = {299},
  number = {13},
  pages = {690--694},
  issn = {0028-4793},
  doi = {10.1056/NEJM197809282991304},
  abstract = {Seventy-one "negative" randomized control trials were re-examined to determine if the investigators had studied large enough samples to give a high probability (greater than 0.90) of detecting a 25 per cent and 50 per cent therapeutic improvement in the response. Sixty-seven of the trials had a greater than 10 per cent risk of missing a true 25 per cent therapeutic improvement, and with the same risk, 50 of the trials could have missed a 50 per cent improvement. Estimates of 90 per cent confidence intervals for the true improvement in each trial showed that in 57 of these "negative" trials, a potential 25 per cent improvement was possible, and 34 of the trials showed a potential 50 per cent improvement. Many of the therapies labeled as "no different from control" in trials using inadequate samples have not received a fair test. Concern for the probability of missing an important therapeutic improvement because of small sample sizes deserves more attention in the planning of clinical trials.},
  langid = {english},
  pmid = {355881},
  keywords = {Clinical Trials as Topic,Humans,Probability,Research,Research Design,Therapeutics}
}

@article{frick_appropriate_1996,
  title = {The Appropriate Use of Null Hypothesis Testing.},
  author = {Frick, Robert W.},
  year = {1996},
  journal = {Psychological Methods},
  volume = {1},
  number = {4},
  pages = {379--390},
  doi = {10.1037/1082-989X.1.4.379}
}

@article{fricker_assessing_2019,
  title = {Assessing the {{Statistical Analyses Used}} in {{Basic}} and {{Applied Social Psychology After Their}} P-{{Value Ban}}},
  author = {Fricker, Ronald D. and Burke, Katherine and Han, Xiaoyan and Woodall, William H.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {374--384},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537892},
  abstract = {In this article, we assess the 31 articles published in Basic and Applied Social Psychology (BASP) in 2016, which is one full year after the BASP editors banned the use of inferential statistics. We discuss how the authors collected their data, how they reported and summarized their data, and how they used their data to reach conclusions. We found multiple instances of authors overstating conclusions beyond what the data would support if statistical significance had been considered. Readers would be largely unable to recognize this because the necessary information to do so was not readily available.},
  keywords = {Effect size,Inference ban,NHST,Psychology,Statistical significance}
}

@article{fried_method_1993,
  title = {A Method for Achieving Consensus on Rheumatoid Arthritis Outcome Measures: The {{OMERACT}} Conference Process.},
  shorttitle = {A Method for Achieving Consensus on Rheumatoid Arthritis Outcome Measures},
  author = {Fried, B. J. and Boers, M. and Baker, P. R.},
  year = {1993},
  journal = {The Journal of rheumatology},
  volume = {20},
  number = {3},
  pages = {548--551}
}

@article{friede_sample_2006,
  title = {Sample Size Recalculation in Internal Pilot Study Designs: A Review},
  shorttitle = {Sample Size Recalculation in Internal Pilot Study Designs},
  author = {Friede, Tim and Kieser, Meinhard},
  year = {2006},
  journal = {Biometrical Journal: Journal of Mathematical Methods in Biosciences},
  volume = {48},
  number = {4},
  pages = {537--555},
  publisher = {{Wiley Online Library}},
  doi = {10.1002/bimj.200510238}
}

@article{fugard_supporting_2015,
  title = {Supporting Thinking on Sample Sizes for Thematic Analyses: A Quantitative Tool},
  shorttitle = {Supporting Thinking on Sample Sizes for Thematic Analyses},
  author = {Fugard, Andrew J. B. and Potts, Henry W. W.},
  year = {2015},
  month = nov,
  journal = {International Journal of Social Research Methodology},
  volume = {18},
  number = {6},
  pages = {669--684},
  publisher = {{Routledge}},
  issn = {1364-5579},
  doi = {10.1080/13645579.2015.1005453},
  abstract = {Thematic analysis is frequently used to analyse qualitative data in psychology, healthcare, social research and beyond. An important stage in planning a study is determining how large a sample size may be required, however current guidelines for thematic analysis are varied, ranging from around 2 to over 400 and it is unclear how to choose a value from the space in between. Some guidance can also not be applied prospectively. This paper introduces a tool to help users think about what would be a useful sample size for their particular context when investigating patterns across participants. The calculation depends on (a) the expected population theme prevalence of the least prevalent theme, derived either from prior knowledge or based on the prevalence of the rarest themes considered worth uncovering, e.g. 1 in 10, 1 in 100; (b) the number of desired instances of the theme; and (c) the power of the study. An adequately powered study will have a high likelihood of finding sufficient themes of the desired prevalence. This calculation can then be used alongside other considerations. We illustrate how to use the method to calculate sample size before starting a study and achieved power given a sample size, providing tables of answers and code for use in the free software, R. Sample sizes are comparable to those found in the literature, for example to have 80\% power to detect two instances of a theme with 10\% prevalence, 29 participants are required. Increasing power, increasing the number of instances or decreasing prevalence increases the sample size needed. We do not propose this as a ritualistic requirement for study design, but rather as a pragmatic supporting tool to help plan studies using thematic analysis.},
  keywords = {Corrigendum,power analysis,sample size determination,thematic analysis},
  annotation = {\_eprint: https://doi.org/10.1080/13645579.2015.1005453}
}

@article{funder_evaluating_2019,
  title = {Evaluating Effect Size in Psychological Research: {{Sense}} and Nonsense},
  shorttitle = {Evaluating Effect Size in Psychological Research},
  author = {Funder, David C. and Ozer, Daniel J.},
  year = {2019},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {156--168},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}},
  doi = {10.1177/2515245919847202}
}

@article{gerring_mere_2012,
  title = {Mere {{Description}}},
  author = {Gerring, John},
  year = {2012},
  month = oct,
  journal = {British Journal of Political Science},
  volume = {42},
  number = {4},
  pages = {721--746},
  publisher = {{Cambridge University Press}},
  issn = {0007-1234, 1469-2112},
  doi = {10.1017/S0007123412000130},
  abstract = {This article attempts to reformulate and resuscitate the seemingly prosaic methodological task of description, which is often derided in favour of causal analysis. First, the problem of definition is addressed: what does this category of analysis (`description') refer to? Secondly, a taxonomy of descriptive arguments is offered, emphasizing the diversity contained within this genre of empirical analysis. Thirdly, the demise of description within political science is charted over the past century, with comparisons to other disciplines. Fourthly, it is argued that the task of description ought to be approached independently, not merely as a handmaiden of causal theories. Fifthly, the methodological difficulties of descriptive inference are addressed. Finally, fruitful research areas within the rubric of description are reviewed.},
  langid = {english}
}

@article{glockner_irrational_2016,
  title = {The Irrational Hungry Judge Effect Revisited: {{Simulations}} Reveal That the Magnitude of the Effect Is Overestimated},
  shorttitle = {The Irrational Hungry Judge Effect Revisited},
  author = {Gl{\"o}ckner, Andreas},
  year = {2016},
  journal = {Judgment and Decision Making},
  volume = {11},
  number = {6},
  pages = {601--610}
}

@article{glover_likelihood_2004,
  title = {Likelihood Ratios: {{A}} Simple and Flexible Statistic for Empirical Psychologists},
  shorttitle = {Likelihood Ratios},
  author = {Glover, Scott and Dixon, Peter},
  year = {2004},
  journal = {Psychonomic Bulletin \& Review},
  volume = {11},
  number = {5},
  pages = {791--806},
  publisher = {{Springer}}
}

@article{good_bayesnon-bayes_1992,
  title = {The {{Bayes}}/{{Non-Bayes}} Compromise: {{A}} Brief Review},
  shorttitle = {The {{Bayes}}/{{Non-Bayes Compromise}}},
  author = {Good, I. J.},
  year = {1992},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {87},
  number = {419},
  pages = {597--606},
  issn = {01621459},
  doi = {10.2307/2290192}
}

@article{good_c140_1982,
  title = {C140. {{Standardized}} Tail-Area Probabilities},
  author = {Good, I. J.},
  year = {1982},
  month = dec,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {16},
  number = {1},
  pages = {65--66},
  issn = {0094-9655},
  doi = {10.1080/00949658208810607}
}

@article{goodyear-smith_analysis_2012,
  title = {Analysis of Decisions Made in Meta-Analyses of Depression Screening and the Risk of Confirmation Bias: {{A}} Case Study},
  shorttitle = {Analysis of Decisions Made in Meta-Analyses of Depression Screening and the Risk of Confirmation Bias},
  author = {{Goodyear-Smith}, Felicity A and {van Driel}, Mieke L and Arroll, Bruce and Del Mar, Chris},
  year = {2012},
  month = jun,
  journal = {BMC Medical Research Methodology},
  volume = {12},
  pages = {76},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-76},
  abstract = {Background Depression is common in primary care and clinicians are encouraged to screen their patients. Meta-analyses have evaluated the effectiveness of screening, but two author groups consistently reached completely opposite conclusions. Methods We identified five systematic reviews on depression screening conducted between 2001 and 2009, three by Gilbody and colleagues and two by the United States Preventive Task Force. The two author groups consistently reached completely opposite conclusions. We analyzed two contemporaneous systematic reviews, applying a stepwise approach to unravel their methods. Decision points were identified, and discrepancies between systematic reviews authors' justification of choices made were recorded. Results Two systematic reviews each addressing three research questions included 26 randomized controlled trials with different combinations in each review. For the outcome depression screening resulting in treatment, both reviews undertook meta-analyses of imperfectly overlapping studies. Two in particular, pooled each by only one of the reviews, influenced the recommendations in opposite directions. Justification for inclusion or exclusion of studies was obtuse. Conclusion Systematic reviews may be less objective than assumed. Based on this analysis of two meta-analyses we hypothesise that strongly held prior beliefs (confirmation bias) may have influenced inclusion and exclusion criteria of studies, and their interpretation. Authors should be required to declare a priori any strongly held prior beliefs within their hypotheses, before embarking on systematic reviews.},
  pmcid = {PMC3464667},
  pmid = {22691262}
}

@techreport{gosset_application_1904,
  title = {The {{Application}} of the "{{Law}} of {{Error}}" to the {{Work}} of the {{Brewery}}},
  author = {Gosset, W. S.},
  year = {1904},
  month = mar,
  number = {1 vol 8},
  pages = {3--16},
  institution = {{Arthur Guinness \& Son, Ltd.}}
}

@article{green_how_1991,
  title = {How {{Many Subjects Does It Take To Do A Regression Analysis}}},
  author = {Green, S. B.},
  year = {1991},
  month = jul,
  journal = {Multivariate Behavioral Research},
  volume = {26},
  number = {3},
  pages = {499--510},
  issn = {0027-3171},
  doi = {10.1207/s15327906mbr2603_7},
  abstract = {Numerous rules-of-thumb have been suggested for determining the minimum number of subjects required to conduct multiple regression analyses. These rules-of-thumb are evaluated by comparing their results against those based on power analyses for tests of hypotheses of multiple and partial correlations. The results did not support the use of rules-of-thumb that simply specify some constant (e.g., 100 subjects) as the minimum number of subjects or a minimum ratio of number of subjects (N) to number of predictors (m). Some support was obtained for a rule-of-thumb that N {$\geq$} 50 + 8 m for the multiple correlation and N {$\geq$}104 + m for the partial correlation. However, the rule-of-thumb for the multiple correlation yields values too large for N when m {$\geq$} 7, and both rules-of-thumb assume all studies have a medium-size relationship between criterion and predictors. Accordingly, a slightly more complex rule-of thumb is introduced that estimates minimum sample size as function of effect size as well as the number of predictors. It is argued that researchers should use methods to determine sample size that incorporate effect size.},
  langid = {english},
  pmid = {26776715}
}

@article{green_simr_2016,
  title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  shorttitle = {{{SIMR}}},
  author = {Green, Peter and MacLeod, Catriona J.},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12504},
  abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  copyright = {\textcopyright{} 2015 The Authors. Methods in Ecology and Evolution \textcopyright{} 2015 British Ecological Society},
  langid = {english},
  keywords = {experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
  annotation = {00148}
}

@article{greenland_statistical_2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  issn = {0393-2990, 1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  langid = {english}
}

@article{greenwald_consequences_1975,
  title = {Consequences of Prejudice against the Null Hypothesis.},
  author = {Greenwald, Anthony G.},
  year = {1975},
  journal = {Psychological Bulletin},
  volume = {82},
  number = {1},
  pages = {1}
}

@article{grunwald_safe_2019,
  title = {Safe {{Testing}}},
  author = {Gr{\"u}nwald, Peter and {de Heide}, Rianne and Koolen, Wouter},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.07801 [cs, math, stat]},
  eprint = {1906.07801},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We present a new theory of hypothesis testing. The main concept is the S-value, a notion of evidence which, unlike p-values, allows for effortlessly combining evidence from several tests, even in the common scenario where the decision to perform a new test depends on the previous test outcome: safe tests based on S-values generally preserve Type-I error guarantees under such "optional continuation". S-values exist for completely general testing problems with composite null and alternatives. Their prime interpretation is in terms of gambling or investing, each S-value corresponding to a particular investment. Surprisingly, optimal "GROW" S-values, which lead to fastest capital growth, are fully characterized by the joint information projection (JIPr) between the set of all Bayes marginal distributions on H0 and H1. Thus, optimal S-values also have an interpretation as Bayes factors, with priors given by the JIPr. We illustrate the theory using two classical testing scenarios: the one-sample t-test and the 2x2 contingency table. In the t-test setting, GROW s-values correspond to adopting the right Haar prior on the variance, like in Jeffreys' Bayesian t-test. However, unlike Jeffreys', the "default" safe t-test puts a discrete 2-point prior on the effect size, leading to better behavior in terms of statistical power. Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, S-values and safe tests may provide a methodology acceptable to adherents of all three schools.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{gupta_intention_2011,
  title = {Intention-to-Treat Concept: {{A}} Review},
  shorttitle = {Intention-to-Treat Concept},
  author = {Gupta, Sandeep K.},
  year = {2011},
  journal = {Perspectives in Clinical Research},
  volume = {2},
  number = {3},
  pages = {109--112},
  issn = {2229-3485},
  doi = {10.4103/2229-3485.83221},
  abstract = {Randomized controlled trials often suffer from two major complications, i.e., noncompliance and missing outcomes. One potential solution to this problem is a statistical concept called intention-to-treat (ITT) analysis. ITT analysis includes every subject who is randomized according to randomized treatment assignment. It ignores noncompliance, protocol deviations, withdrawal, and anything that happens after randomization. ITT analysis maintains prognostic balance generated from the original random treatment allocation. In ITT analysis, estimate of treatment effect is generally conservative. A better application of the ITT approach is possible if complete outcome data are available for all randomized subjects. Per-protocol population is defined as a subset of the ITT population who completed the study without any major protocol violations.},
  pmcid = {PMC3159210},
  pmid = {21897887}
}

@article{hacking_experimentation_1982,
  title = {Experimentation and {{Scientific Realism}}},
  author = {Hacking, Ian},
  year = {1982},
  journal = {Philosophical Topics},
  volume = {13},
  number = {1},
  pages = {71--87},
  issn = {0276-2080},
  doi = {10/fz8ftm},
  annotation = {00312}
}

@book{hacking_logic_1965,
  title = {Logic of {{Statistical Inference}}},
  author = {Hacking, Ian},
  year = {1965},
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  abstract = {One of Ian Hacking's earliest publications, this book showcases his early ideas on the central concepts and questions surrounding statistical reasoning. He explores the basic principles of statistical reasoning and tests them, both at a philosophical level and in terms of their practical consequences for statisticians. Presented in a fresh twenty-first-century series livery, and including a specially commissioned preface written by Jan-Willem Romeijn, illuminating its enduring importance and relevance to philosophical enquiry, Hacking's influential and original work has been revived for a new generation of readers.},
  isbn = {978-1-316-50814-5},
  langid = {english},
  annotation = {01160}
}

@article{hagger_multilab_2016,
  title = {A {{Multilab Preregistered Replication}} of the {{Ego-Depletion Effect}}},
  author = {Hagger, M. S. and Chatzisarantis, N. L. D. and Alberts, H. and Anggono, C. O. and Batailler, C. and Birt, A. R. and Brand, R. and Brandt, M. J. and Brewer, G. and Bruyneel, S. and Calvillo, D. P. and Campbell, W. K. and Cannon, P. R. and Carlucci, M. and Carruth, N. P. and Cheung, T. and Crowell, A. and De Ridder, D. T. D. and Dewitte, S. and Elson, M. and Evans, J. R. and Fay, B. A. and Fennis, B. M. and Finley, A. and Francis, Z. and Heise, E. and Hoemann, H. and Inzlicht, M. and Koole, S. L. and Koppel, L. and Kroese, F. and Lange, F. and Lau, K. and Lynch, B. P. and Martijn, C. and Merckelbach, H. and Mills, N. V. and Michirev, A. and Miyake, A. and Mosser, A. E. and Muise, M. and Muller, D. and Muzi, M. and Nalis, D. and Nurwanti, R. and Otgaar, H. and Philipp, M. C. and Primoceri, P. and Rentzsch, K. and Ringos, L. and Schlinkert, C. and Schmeichel, B. J. and Schoch, S. F. and Schrama, M. and Sch{\"u}tz, A. and Stamos, A. and Tingh{\"o}g, G. and Ullrich, J. and {vanDellen}, M. and Wimbarti, S. and Wolff, W. and Yusainy, C. and Zerhouni, O. and Zwienenberg, M.},
  year = {2016},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {4},
  pages = {546--573},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691616652873},
  langid = {english},
  pmid = {27474142},
  keywords = {Adult,energy model,Humans,Meta-analysis,Meta-Analysis as Topic,Reproducibility of Results,Research Design,resource depletion,Self-control,self-regulation,strength model,Task Performance and Analysis,Young Adult}
}

@article{hallahan_statistical_1996,
  title = {Statistical Power: {{Concepts}}, Procedures, and Applications},
  shorttitle = {Statistical Power},
  author = {Hallahan, Mark and Rosenthal, Robert},
  year = {1996},
  month = may,
  journal = {Behaviour Research and Therapy},
  volume = {34},
  number = {5},
  pages = {489--499},
  issn = {0005-7967},
  doi = {10.1016/0005-7967(95)00082-8},
  abstract = {This paper discusses the concept of statistical power and its application to psychological research. Power, the probability that a significance test will produce a significant result when the null hypothesis is false, often is neglected with potentially serious consequences. The concept of power should be considered as part of planning and interpreting research. This article provides explication of the concept of power and suggestions for researchers to increase the power of their investigations.},
  langid = {english}
}

@article{halpern_continuing_2002,
  title = {The Continuing Unethical Conduct of Underpowered Clinical Trials},
  author = {Halpern, Scott D. and Karlawish, Jason HT and Berlin, Jesse A.},
  year = {2002},
  journal = {Jama},
  volume = {288},
  number = {3},
  pages = {358--362},
  publisher = {{American Medical Association}},
  doi = {doi:10.1001/jama.288.3.358}
}

@article{halpern_sample_2001,
  title = {The Sample Size for a Clinical Trial: {{A Bayesian}} Decision Theoretic Approach},
  shorttitle = {The Sample Size for a Clinical Trial},
  author = {Halpern, Jerry and Brown Jr, Byron Wm and Hornberger, John},
  year = {2001},
  journal = {Statistics in Medicine},
  volume = {20},
  number = {6},
  pages = {841--858},
  publisher = {{Wiley Online Library}},
  doi = {10.1002/sim.703}
}

@article{hand_deconstructing_1994,
  title = {Deconstructing {{Statistical Questions}}},
  author = {Hand, David J.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {157},
  number = {3},
  pages = {317--356},
  issn = {09641998},
  doi = {10.2307/2983526},
  annotation = {00153}
}

@article{harms_making_2018,
  title = {Making 'null Effects' Informative: Statistical Techniques and Inferential Frameworks},
  shorttitle = {Making 'null Effects' Informative},
  author = {Harms, Christopher and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Clinical and Translational Research},
  number = {3},
  pages = {382--393},
  issn = {2424810X},
  doi = {10.18053/jctres.03.2017S2.007},
  abstract = {Being able to interpret `null effects' is important for cumulative knowledge generation in science. To draw informative conclusions from null-effects, researchers need to move beyond the incorrect interpretation of a non-significant result in a null-hypothesis significance test as evidence of the absence of an effect. We explain how to statistically evaluate null-results using equivalence tests, Bayesian estimation, and Bayes factors. A worked example demonstrates how to apply these statistical tools and interpret the results. Finally, we explain how no statistical approach can actually prove that the null-hypothesis is true, and briefly discuss the philosophical differences between statistical approaches to examine null-effects. The increasing availability of easy-to-use software and online tools to perform equivalence tests, Bayesian estimation, and calculate Bayes factors make it timely and feasible to complement or move beyond traditional null-hypothesis tests, and allow researchers to draw more informative conclusions about null-effects.},
  langid = {english}
}

@book{harrer_doing_2021,
  title = {Doing {{Meta-Analysis}} with {{R}}: {{A Hands-On Guide}}},
  shorttitle = {Doing {{Meta-Analysis}} with {{R}}},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi A. and Ebert, David D.},
  year = {2021},
  month = sep,
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  doi = {10.1201/9781003107347},
  abstract = {Doing Meta-Analysis with R: A Hands-On Guide serves as an accessible introduction on how meta-analyses can be conducted in R. Essential steps for meta-analysis are covered, including calculation and pooling of outcome measures, forest plots, heterogeneity diagnostics, subgroup analyses, meta-regression, methods to control for publication bias, risk of bias assessments and plotting tools. Advanced but highly relevant topics such as network meta-analysis, multi-three-level meta-analyses, Bayesian meta-analysis approaches and SEM meta-analysis are also covered. A companion R package, dmetar, is introduced at the beginning of the guide. It contains data sets and several helper functions for the meta and metafor package used in the guide.  The programming and statistical background covered in the book are kept at a non-expert level, making the book widely accessible.  Features\textbullet{} Contains two introductory chapters on how to set up an R environment and do basic imports/manipulations of meta-analysis data, including exercises\textbullet{} Describes statistical concepts clearly and concisely before applying them in R\textbullet{} Includes step-by-step guidance through the coding required to perform meta-analyses, and a companion R package for the book},
  isbn = {978-1-00-310734-7}
}

@article{hauck_new_1984,
  title = {A New Statistical Procedure for Testing Equivalence in Two-Group Comparative Bioavailability Trials},
  author = {Hauck, Dr Walter W. and Anderson, Sharon},
  year = {1984},
  month = feb,
  journal = {Journal of Pharmacokinetics and Biopharmaceutics},
  volume = {12},
  number = {1},
  pages = {83--91},
  issn = {0090-466X},
  doi = {10.1007/BF01063612},
  abstract = {The clinical problem of testing for equivalence in comparative bioavailability trials is restated in terms of the proper statistical hypotheses. A simple t-test procedure for these hypotheses has been devloped that is more powerful than the methods based on usual (shortest) and symmetric confidence intervals. In this note, this new procedure is explained and an example is given, including the method for sample size determination.},
  langid = {english},
  keywords = {bioavailability,Biochemistry; general,bioequivalence,Biomedical Engineering,hypothesis tests,Pharmacology/Toxicology,Pharmacy,sample size determination,Veterinary Medicine}
}

@article{heath_calculating_2020,
  title = {Calculating the {{Expected Value}} of {{Sample Information}} in {{Practice}}: {{Considerations}} from 3 {{Case Studies}}:},
  shorttitle = {Calculating the {{Expected Value}} of {{Sample Information}} in {{Practice}}},
  author = {Heath, Anna and Kunst, Natalia and Jackson, Christopher and Strong, Mark and {Alarid-Escudero}, Fernando and {Goldhaber-Fiebert}, Jeremy D. and Baio, Gianluca and Menzies, Nicolas A. and Jalal, Hawre},
  year = {2020},
  month = apr,
  journal = {Medical Decision Making},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/0272989X20912402},
  abstract = {Background. Investing efficiently in future research to improve policy decisions is an important goal. Expected value of sample information (EVSI) can be used t...},
  copyright = {\textcopyright{} The Author(s) 2020},
  langid = {english}
}

@article{hedges_power_2001,
  title = {The Power of Statistical Tests in Meta-Analysis.},
  author = {Hedges, Larry V. and Pigott, Therese D.},
  year = {2001},
  journal = {Psychological methods},
  volume = {6},
  number = {3},
  pages = {203--217},
  publisher = {{American Psychological Association}},
  doi = {10.1037/1082-989X.6.3.203}
}

@misc{heino_legacy_2016,
  title = {The Legacy of Social Psychology},
  author = {Heino, Matti TJ},
  year = {2016},
  month = nov,
  journal = {Data punk | K\"aytt\"aytymisarkkitehtuuri},
  abstract = {What can we learn re-examining the classic cognitive dissonance experiment?},
  langid = {english}
}

@article{hilgard_maximal_2021,
  title = {Maximal Positive Controls: {{A}} Method for Estimating the Largest Plausible Effect Size},
  shorttitle = {Maximal Positive Controls},
  author = {Hilgard, Joseph},
  year = {2021},
  month = mar,
  journal = {Journal of Experimental Social Psychology},
  volume = {93},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2020.104082},
  abstract = {Effect sizes in social psychology are generally not large and are limited by error variance in manipulation and measurement. Effect sizes exceeding these limits are implausible and should be viewed with skepticism. Maximal positive controls, experimental conditions that should show an obvious and predictable effect, can provide estimates of the upper limits of plausible effect sizes on a measure. In this work, maximal positive controls are conducted for three measures of aggressive cognition, and the effect sizes obtained are compared to studies found through systematic review. Questions are raised regarding the plausibility of certain reports with effect sizes comparable to, or in excess of, the effect sizes found in maximal positive controls. Maximal positive controls may provide a means to identify implausible study results at lower cost than direct replication.},
  langid = {english},
  keywords = {Aggression,Aggressive thought,Positive controls,Scientific self-correction,Violent video games}
}

@article{hill_empirical_2008,
  title = {Empirical {{Benchmarks}} for {{Interpreting Effect Sizes}} in {{Research}}},
  author = {Hill, Carolyn J. and Bloom, Howard S. and Black, Alison Rebeck and Lipsey, Mark W.},
  year = {2008},
  journal = {Child Development Perspectives},
  volume = {2},
  number = {3},
  pages = {172--177},
  issn = {1750-8606},
  doi = {10.1111/j.1750-8606.2008.00061.x},
  abstract = {ABSTRACT\textemdash{} There is no universal guideline or rule of thumb for judging the practical importance or substantive significance of a standardized effect size estimate for an intervention. Instead, one must develop empirical benchmarks of comparison that reflect the nature of the intervention being evaluated, its target population, and the outcome measure or measures being used. This approach is applied to the assessment of effect size measures for educational interventions designed to improve student academic achievement. Three types of empirical benchmarks are illustrated: (a) normative expectations for growth over time in student achievement, (b) policy-relevant gaps in student achievement by demographic group or school performance, and (c) effect size results from past research for similar interventions and target populations. The findings can be used to help assess educational interventions, and the process of doing so can provide guidelines for how to develop and use such benchmarks in other fields.},
  copyright = {\textcopyright{} 2008, Copyright the Author(s); Journal Compilation \textcopyright{} 2008, Society for Research in Child Development with Exclusive License to Print by MDRC},
  langid = {english},
  keywords = {educational evaluation,effect size,student performance},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1750-8606.2008.00061.x}
}

@article{hodges_testing_1954,
  title = {Testing the {{Approximate Validity}} of {{Statistical Hypotheses}}},
  author = {Hodges, J. L. and Lehmann, E. L.},
  year = {1954},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {16},
  number = {2},
  pages = {261--268},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1954.tb00169.x},
  abstract = {[The distinction between statistical significance and material significance in hypotheses testing is discussed. Modifications of the customary tests, in order to test for the absence of material significance, are derived for several parametric problems, for the chi-square test of goodness of fit, and for Student's hypothesis. The latter permits one to test the hypothesis that the means of two normal populations of equal variance, do not differ by more than a stated amount.]}
}

@article{hoenig_abuse_2001,
  title = {The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis},
  shorttitle = {The Abuse of Power},
  author = {Hoenig, John M. and Heisey, Dennis M.},
  year = {2001},
  journal = {The American Statistician},
  volume = {55},
  number = {1},
  pages = {19--24},
  doi = {10.1198/000313001300339897}
}

@article{huedo-medina_assessing_2006,
  title = {Assessing Heterogeneity in Meta-Analysis: {{Q}} Statistic or {{I}}\$\^2\$ Index?},
  shorttitle = {Assessing Heterogeneity in Meta-Analysis},
  author = {{Huedo-Medina}, Tania B. and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio and Botella, Juan},
  year = {2006},
  journal = {Psychological methods},
  volume = {11},
  number = {2},
  pages = {193},
  annotation = {00000}
}

@article{hung_behavior_1997,
  title = {The {{Behavior}} of the {{P-Value When}} the {{Alternative Hypothesis}} Is {{True}}},
  author = {Hung, H. M. James and O'Neill, Robert T. and Bauer, Peter and Kohne, Karl},
  year = {1997},
  journal = {Biometrics},
  volume = {53},
  number = {1},
  pages = {11--22},
  issn = {0006-341X},
  doi = {10.2307/2533093},
  abstract = {The P-value is a random variable derived from the distribution of the test statistic used to analyze a data set and to test a null hypothesis. Under the null hypothesis, the P-value based on a continuous test statistic has a uniform distribution over the interval [0, 1], regardless of the sample size of the experiment. In contrast, the distribution of the P-value under the alternative hypothesis is a function of both sample size and the true value or range of true values of the tested parameter. The characteristics, such as mean and percentiles, of the P-value distribution can give valuable insight into how the P-value behaves for a variety of parameter values and sample sizes. Potential applications of the P-value distribution under the alternative hypothesis to the design, analysis, and interpretation of results of clinical trials are considered.},
  annotation = {00148}
}

@article{ioannidis_exploratory_2007,
  title = {An Exploratory Test for an Excess of Significant Findings},
  author = {Ioannidis, John P. A. and Trikalinos, T. A},
  year = {2007},
  month = jun,
  journal = {Clinical Trials},
  volume = {4},
  number = {3},
  pages = {245--253},
  issn = {1740-7745},
  doi = {10.1177/1740774507079441},
  langid = {english},
  annotation = {00322}
}

@article{ioannidis_why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLoS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  langid = {english},
  annotation = {05886}
}

@article{isager_deciding_2021,
  title = {Deciding What to Replicate: {{A}} Formal Definition of ``Replication Value'' and a Decision Model for Replication Study Selection.},
  shorttitle = {Deciding What to Replicate},
  author = {Isager, Peder M. and van Aert, Robbie C. M. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Brandt, Mark and DeSoto, K. Andrew and {Giner-Sorolla}, Roger and Krueger, Joachim and Perugini, Marco and Ropovik, Ivan and van 't Veer, Anna and Vranka, Marek A. and Lakens, Dani{\"e}l},
  year = {2021},
  journal = {Psychological Methods},
  doi = {10.31222/osf.io/2gurz},
  abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, researchers who conduct replication studies face a difficult problem; there are many more studies in need of replication than there are funds available for replicating. To select studies for replication efficiently, we need to understand which studies are the most in need of replication. In other words, we need to understand which replication efforts have the highest expected utility. In this article we propose a general rule for study selection in replication research based on the replication value of the claims considered for replication. The replication value of a claim is defined as the maximum expected utility we could gain by replicating the claim, and is a function of (1) the value of being certain about the claim, and (2) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using replication value as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of replication value.},
  keywords = {Design of Experiments and Sample Surveys,expected utility,Physical Sciences and Mathematics,replication,replication value,Statistics and Probability,study selection}
}

@article{iyengar_selection_1988,
  title = {Selection {{Models}} and the {{File Drawer Problem}}},
  author = {Iyengar, Satish and Greenhouse, Joel B.},
  year = {1988},
  journal = {Statistical Science},
  volume = {3},
  number = {1},
  pages = {109--117},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.}
}

@article{jaeschke_measurement_1989,
  title = {Measurement of Health Status: {{Ascertaining}} the Minimal Clinically Important Difference},
  shorttitle = {Measurement of Health Status},
  author = {Jaeschke, Roman and Singer, Joel and Guyatt, Gordon H.},
  year = {1989},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {10},
  number = {4},
  pages = {407--415},
  issn = {0197-2456},
  doi = {10.1016/0197-2456(89)90005-6},
  langid = {english},
  pmid = {2691207},
  keywords = {Measurement,quality of life,responsiveness},
  annotation = {02971}
}

@book{jeffreys_theory_1939,
  title = {Theory of Probability},
  author = {Jeffreys, Harold},
  year = {1939},
  series = {The {{International}} Series of Monographs on Physics},
  edition = {1st ed},
  publisher = {{Oxford University Press}},
  address = {{Oxford [Oxfordshire]: New York}},
  isbn = {978-0-19-853193-7},
  lccn = {QA273 .J4 1983},
  keywords = {Probabilities}
}

@book{jennison_group_2000,
  title = {Group Sequential Methods with Applications to Clinical Trials},
  author = {Jennison, Christopher and Turnbull, Bruce W.},
  year = {2000},
  publisher = {{Chapman \& Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-0-8493-0316-6},
  lccn = {R853.C55 J46 2000},
  keywords = {Clinical Trials,Decision Theory,methods,Models; Statistical,Statistical methods,Statistics}
}

@article{john_measuring_2012,
  title = {Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  journal = {Psychological science},
  volume = {23},
  number = {5},
  pages = {524--532},
  annotation = {00795}
}

@article{johnson_revised_2013,
  title = {Revised Standards for Statistical Evidence},
  author = {Johnson, V. E.},
  year = {2013},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {48},
  pages = {19313--19317},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1313476110},
  langid = {english},
  annotation = {00466}
}

@article{johnson_revised_2013-1,
  title = {Revised Standards for Statistical Evidence},
  author = {Johnson, V. E.},
  year = {2013},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {48},
  pages = {19313--19317},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1313476110},
  langid = {english},
  annotation = {00466}
}

@article{jones_test_1952,
  title = {Test of Hypotheses: One-Sided vs. Two-Sided Alternatives.},
  shorttitle = {Test of Hypotheses},
  author = {Jones, L. V.},
  year = {1952},
  month = jan,
  journal = {Psychological Bulletin},
  volume = {49},
  number = {1},
  pages = {43--46},
  issn = {0033-2909},
  doi = {http://dx.doi.org/10.1037/h0056832},
  abstract = {"The failure, among psychologists, to utilize the one-tailed statistical test, where it is appropriate, very likely is due to the propagation of the two-tailed model by writers of text-books in psychological statistics. It is typical, in such texts, to find little or no attention given to one-tailed tests. Since the test of the null hypothesis against a one-sided alternative is the most powerful test for all directional hypotheses, it is strongly recommended that the one-tailed model be adopted wherever its use is appropriate." (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  copyright = {\textcopyright{} American Psychological Association 1952},
  langid = {english},
  keywords = {Hypothesis Testing (major),Psychologists,Statistical Tests (major),Statistics (major)},
  annotation = {00095}
}

@article{joseph_manipulation_2020,
  title = {The Manipulation of Affect: {{A}} Meta-Analysis of Affect Induction Procedures},
  shorttitle = {The Manipulation of Affect},
  author = {Joseph, Dana L. and Chan, Micaela Y. and Heintzelman, Samantha J. and Tay, Louis and Diener, Ed and Scotney, Victoria S.},
  year = {2020},
  journal = {Psychological Bulletin},
  volume = {146},
  number = {4},
  pages = {355--375},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/bul0000224},
  abstract = {Affect inductions have become essential for testing theories of affect and for conducting experimental research on the effects of mood and emotion. The current review takes stock of the vast body of existing literature on affect induction procedures (AIPs; also referred to as mood inductions) to evaluate the effectiveness of affect inductions as research tools and to test theories of affect (e.g., the bipolarity hypothesis, negativity bias, positivity offset, and theories of emotionality and gender) using meta-analytic data. In doing so, we seek to address whether AIPs are effective for inducing affective states, what conditions maximize their effectiveness, for which emotions they are most effective, for whom they are most effective, and whether affect induction findings can provide insight into theories of affect. A meta-analysis of 874 samples and 53,509 participants suggests that affect inductions are effective on average ({$\delta$} = 1.32), but this effectiveness varies with the type of affect induction, the emotion being induced, and the gender of the participants. Further, results indicate coupled activation where the induction of positive (negative) emotions leads to a corresponding reduction in negative (positive) emotions, which provides support for the bipolar continuum of positive and negative affect. Results also revealed a negativity bias in which individuals display stronger reactions to negative stimuli than positive stimuli. A practical guide in the choice of affect induction procedures for researchers is presented and implications for emotion theory are discussed. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Emotional States,Emotionality (Personality),Emotions,Human Sex Differences,Negative Emotions,Negativism,Positive Emotions,Positivism,Theories}
}

@article{jostmann_short_2016,
  title = {A Short History of the Weight-Importance Effect and a Recommendation for Pre-Testing: {{Commentary}} on {{Ebersole}} et al. (2016)},
  shorttitle = {A Short History of the Weight-Importance Effect and a Recommendation for Pre-Testing},
  author = {Jostmann, Nils B. and Lakens, Dani{\"e}l and Schubert, Thomas W.},
  year = {2016},
  journal = {Journal of Experimental Social Psychology},
  volume = {67},
  pages = {93--94},
  issn = {00221031},
  doi = {10.1016/j.jesp.2015.12.001},
  langid = {english}
}

@article{jostmann_weight_2009,
  title = {Weight as an {{Embodiment}} of {{Importance}}},
  author = {Jostmann, Nils B. and Lakens, Dani{\"e}l and Schubert, Thomas W.},
  year = {2009},
  journal = {Psychological Science},
  volume = {20},
  number = {9},
  pages = {1169--1174},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2009.02426.x},
  abstract = {Four studies show that the abstract concept of importance is grounded in bodily experiences of weight. Participants provided judgments of importance while they held either a heavy or a light clipboard. Holding a heavy clipboard increased judgments of monetary value (Study 1) and made participants consider fair decision-making procedures to be more important (Study 2). It also caused more elaborate thinking, as indicated by higher consistency between related judgments (Study 3) and by greater polarization of agreement ratings for strong versus weak arguments (Study 4). In line with an embodied perspective on cognition, these findings suggest that, much as weight makes people invest more physical effort in dealing with concrete objects, it also makes people invest more cognitive effort in dealing with abstract issues.},
  langid = {english},
  annotation = {00397}
}

@article{julious_sample_2004,
  title = {Sample Sizes for Clinical Trials with Normal Data},
  author = {Julious, Steven A.},
  year = {2004},
  month = jun,
  journal = {Statistics in Medicine},
  volume = {23},
  number = {12},
  pages = {1921--1986},
  issn = {0277-6715},
  doi = {10.1002/sim.1783},
  abstract = {This article gives an overview of sample size calculations for parallel group and cross-over studies with Normal data. Sample size derivation is given for trials where the objective is to demonstrate: superiority, equivalence, non-inferiority, bioequivalence and estimation to a given precision, for different types I and II errors. It is demonstrated how the different trial objectives influence the null and alternative hypotheses of the trials and how these hypotheses influence the calculations. Sample size tables for the different types of trials and worked examples are given.},
  langid = {english},
  pmid = {15195324},
  keywords = {Biometry,Cross-Over Studies,Humans,Randomized Controlled Trials as Topic,Research Design,Sample Size,Therapeutic equivalency},
  annotation = {00317}
}

@article{julious_sample_2005,
  title = {Sample Size of 12 per Group Rule of Thumb for a Pilot Study},
  author = {Julious, Steven A.},
  year = {2005},
  journal = {Pharmaceutical Statistics},
  volume = {4},
  number = {4},
  pages = {287--291},
  issn = {1539-1612},
  doi = {10.1002/pst.185},
  abstract = {When designing a clinical trial an appropriate justification for the sample size should be provided in the protocol. However, there are a number of settings when undertaking a pilot trial when there is no prior information to base a sample size on. For such pilot studies the recommendation is a sample size of 12 per group. The justifications for this sample size are based on rationale about feasibility; precision about the mean and variance; and regulatory considerations. The context of the justifications are that future studies will use the information from the pilot in their design. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {pilot study,sample size},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.185}
}

@article{kass_bayes_1995,
  title = {Bayes Factors},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  year = {1995},
  journal = {Journal of the american statistical association},
  volume = {90},
  number = {430},
  pages = {773--795},
  doi = {10.1080/01621459.1995.10476572},
  annotation = {11249}
}

@article{keefe_defining_2013,
  title = {Defining a {{Clinically Meaningful Effect}} for the {{Design}} and {{Interpretation}} of {{Randomized Controlled Trials}}},
  author = {Keefe, Richard S. E. and Kraemer, Helena C. and Epstein, Robert S. and Frank, Ellen and Haynes, Ginger and Laughren, Thomas P. and Mcnulty, James and Reed, Shelby D. and Sanchez, Juan and Leon, Andrew C.},
  year = {2013},
  journal = {Innovations in Clinical Neuroscience},
  volume = {10},
  number = {5-6 Suppl A},
  pages = {4S-19S},
  issn = {2158-8333},
  abstract = {Objective: This article captures the proceedings of a meeting aimed at defining clinically meaningful effects for use in randomized controlled trials for psychopharmacological agents., Design: Experts from a variety of disciplines defined clinically meaningful effects from their perspectives along with viewpoints about how to design and interpret randomized controlled trials., Setting: The article offers relevant, practical, and sometimes anecdotal information about clinically meaningful effects and how to interpret them., Participants: The concept for this session was the work of co-chairs Richard Keefe and the late Andy Leon. Faculty included Richard Keefe, PhD; James McNulty, AbScB; Robert S. Epstein, MD, MS; Shelby D. Reed, PhD; Juan Sanchez, MD; Ginger Haynes, PhD; Andrew C. Leon, PhD; Helena Chmura Kraemer, PhD; Ellen Frank, PhD, and Kenneth L. Davis, MD., Results: The term clinically meaningful effect is an important aspect of designing and interpreting randomized controlled trials but can be particularly difficult in the setting of psychopharmacology where effect size may be modest, particularly over the short term, because of a strong response to placebo. Payers, regulators, patients, and clinicians have different concerns about clinically meaningful effects and may describe these terms differently. The use of moderators in success rate differences may help better delineate clinically meaningful effects., Conclusion: There is no clear consensus on a single definition for clinically meaningful differences in randomized controlled trials, and investigators must be sensitive to specific concerns of stakeholders in psychopharmacology in order to design and execute appropriate clinical trials.},
  pmcid = {PMC3719483},
  pmid = {23882433},
  annotation = {00022}
}

@article{kelley_confidence_2007,
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  author = {Kelley, Ken},
  year = {2007},
  journal = {Journal of Statistical Software},
  volume = {20},
  number = {8},
  issn = {1548-7660},
  doi = {10.18637/JSS.V020.I08},
  abstract = {The behavioral, educational, and social sciences are undergoing a paradigmatic shift in methodology, from disciplines that focus on the dichotomous outcome of null hypothesis significance tests to disciplines that report and interpret effect sizes and their corresponding confidence intervals. Due to the arbitrariness of many measurement instruments used in the behavioral, educational, and social sciences, some of the most widely reported effect sizes are standardized. Although forming confidence intervals for standardized effect sizes can be very beneficial, such confidence interval procedures are generally difficult to implement because they depend on noncentral t, F , and {$\chi$}2 distributions. At present, no main-stream statistical package provides exact confidence intervals for standardized effects without the use of specialized programming scripts. Methods for the Behavioral, Educational, and Social Sciences (MBESS) is an R package that has routines for calculating confidence intervals for noncentral t, F , and {$\chi$}2 distributions, which are then used in the calculation of exact confidence intervals for standardized effect sizes by using the confidence interval transformation and inversion principles. The present article discusses the way in which confidence intervals are formed for standardized effect sizes and illustrates how such confidence intervals can be easily formed using MBESS in R.},
  langid = {english},
  annotation = {00163}
}

@article{kelley_effect_2012,
  title = {On Effect Size},
  author = {Kelley, Ken and Preacher, Kristopher J.},
  year = {2012},
  journal = {Psychological methods},
  volume = {17},
  number = {2},
  pages = {137--152},
  publisher = {{American Psychological Association}},
  doi = {10.1037/a0028086}
}

@article{kelley_sample_2006,
  title = {Sample Size Planning for the Standardized Mean Difference: Accuracy in Parameter Estimation via Narrow Confidence Intervals.},
  shorttitle = {Sample Size Planning for the Standardized Mean Difference},
  author = {Kelley, Ken and Rausch, Joseph R.},
  year = {2006},
  journal = {Psychological methods},
  volume = {11},
  number = {4},
  pages = {363--385},
  doi = {10.1037},
  annotation = {00083}
}

@book{kenett_information_2016,
  title = {Information {{Quality}}: {{The Potential}} of {{Data}} and {{Analytics}} to {{Generate Knowledge}}},
  shorttitle = {Information {{Quality}}},
  author = {Kenett, Ron S. and Shmueli, Galit and Kenett, Ron},
  year = {2016},
  month = dec,
  edition = {1st edition},
  publisher = {{Wiley}},
  address = {{Chichester, West Sussex}},
  isbn = {978-1-118-87444-8},
  langid = {english}
}

@article{kennedy-shaffer_before_2019,
  title = {Before p {$<$} 0.05 to {{Beyond}} p {$<$} 0.05: {{Using History}} to {{Contextualize}} p-{{Values}} and {{Significance Testing}}},
  shorttitle = {Before p {$<$} 0.05 to {{Beyond}} p {$<$} 0.05},
  author = {{Kennedy-Shaffer}, Lee},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {82--90},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537891},
  abstract = {As statisticians and scientists consider a world beyond p {$<$} 0.05, it is important to not lose sight of how we got to this point. Although significance testing and p-values are often presented as prescriptive procedures, they came about through a process of refinement and extension to other disciplines. Ronald A. Fisher and his contemporaries formalized these methods in the early twentieth century and Fisher's 1925 Statistical Methods for Research Workers brought the techniques to experimentalists in a variety of disciplines. Understanding how these methods arose, spread, and were argued over since then illuminates how p {$<$} 0.05 came to be a standard for scientific inference, the advantage it offered at the time, and how it was interpreted. This historical perspective can inform the work of statisticians today by encouraging thoughtful consideration of how their work, including proposed alternatives to the p-value, will be perceived and used by scientists. And it can engage students more fully and encourage critical thinking rather than rote applications of formulae. Incorporating history enables students, practitioners, and statisticians to treat the discipline as an ongoing endeavor, crafted by fallible humans, and provides a deeper understanding of the subject and its consequences for science and society.},
  pmid = {31413381},
  keywords = {Education,Foundational issues,Hypothesis testing,Inference,Probability},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1537891}
}

@article{kenny_unappreciated_2019,
  title = {The Unappreciated Heterogeneity of Effect Sizes: {{Implications}} for Power, Precision, Planning of Research, and Replication},
  shorttitle = {The Unappreciated Heterogeneity of Effect Sizes},
  author = {Kenny, David A. and Judd, Charles M.},
  year = {2019},
  journal = {Psychological Methods},
  volume = {24},
  number = {5},
  pages = {578--589},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/met0000209},
  abstract = {Repeated investigations of the same phenomenon typically yield effect sizes that vary more than one would expect from sampling error alone. Such variation is even found in exact replication studies, suggesting that it is not only because of identifiable moderators but also to subtler random variation across studies. Such heterogeneity of effect sizes is typically ignored, with unfortunate consequences. We consider its implications for power analyses, the precision of estimated effects, and the planning of original and replication research. With heterogeneity and an interest in generalizing to a population of studies, the usual power calculations and confidence intervals are likely misleading, and the preference for single definitive large-N studies is misguided. Researchers and methodologists need to recognize that effects are often heterogeneous and plan accordingly. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Experimental Replication,Population (Statistics),Prediction,Statistical Power}
}

@article{kerr_harking_1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  issn = {1088-8683, 1532-7957},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  langid = {english},
  pmid = {15647155},
  annotation = {00553}
}

@article{king_point_2011,
  title = {A Point of Minimal Important Difference ({{MID}}): A Critique of Terminology and Methods},
  shorttitle = {A Point of Minimal Important Difference ({{MID}})},
  author = {King, Madeleine T.},
  year = {2011},
  month = apr,
  journal = {Expert Review of Pharmacoeconomics \& Outcomes Research},
  volume = {11},
  number = {2},
  pages = {171--184},
  issn = {1473-7167},
  doi = {10.1586/erp.11.9},
  abstract = {The minimal important difference (MID) is a phrase with instant appeal in a field struggling to interpret health-related quality of life and other patient-reported outcomes. The terminology can be confusing, with several terms differing only slightly in definition (e.g., minimal clinically important difference, clinically important difference, minimally detectable difference, the subjectively significant difference), and others that seem similar despite having quite different meanings (minimally detectable difference versus minimum detectable change). Often, nuances of definition are of little consequence in the way that these quantities are estimated and used. Four methods are commonly employed to estimate MIDs: patient rating of change (global transition items); clinical anchors; standard error of measurement; and effect size. These are described and critiqued in this article. There is no universal MID, despite the appeal of the notion. Indeed, for a particular patient-reported outcome instrument or scale, the MID is not an immutable characteristic, but may vary by population and context. At both the group and individual level, the MID may depend on the clinical context and decision at hand, the baseline from which the patient starts, and whether they are improving or deteriorating. Specific estimates of MIDs should therefore not be overinterpreted. For a given health-related quality-of-life scale, all available MID estimates (and their confidence intervals) should be considered, amalgamated into general guidelines and applied judiciously to any particular clinical or research context.},
  pmid = {21476819},
  keywords = {clinical significance,health-related quality of life,HRQOL,interpretation,MCID,MID,minimal clinically important difference,minimal important difference,patient-reported outcome,PRO},
  annotation = {00223}
}

@article{kirk_practical_1996,
  title = {Practical Significance: {{A}} Concept Whose Time Has Come},
  shorttitle = {Practical Significance},
  author = {Kirk, Roger E.},
  year = {1996},
  journal = {Educational and psychological measurement},
  volume = {56},
  number = {5},
  pages = {746--759},
  publisher = {{Sage Publications Sage CA: Thousand Oaks, CA}}
}

@article{kirk_practical_1996-1,
  title = {Practical {{Significance}}: {{A Concept Whose Time Has Come}}},
  shorttitle = {Practical {{Significance}}},
  author = {Kirk, R. E.},
  year = {1996},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {56},
  number = {5},
  pages = {746--759},
  issn = {0013-1644},
  doi = {10.1177/0013164496056005002},
  langid = {english},
  annotation = {01567}
}

@book{kish_survey_1965,
  title = {Survey {{Sampling}}},
  author = {Kish, Leslie},
  year = {1965},
  publisher = {{Wiley}},
  address = {{New York}}
}

@book{kitcher_advancement_1993,
  title = {The Advancement of Science: Science without Legend, Objectivity without Illusions},
  shorttitle = {The Advancement of Science},
  author = {Kitcher, Philip},
  year = {1993},
  publisher = {{Oxford University Press}},
  address = {{New York}},
  isbn = {978-0-19-504628-1},
  lccn = {Q175 .K533 1993},
  keywords = {History,Philosophy,Science}
}

@article{kraft_interpreting_2020,
  title = {Interpreting Effect Sizes of Education Interventions},
  author = {Kraft, Matthew A.},
  year = {2020},
  journal = {Educational Researcher},
  volume = {49},
  number = {4},
  pages = {241--253},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}},
  doi = {10.3102/0013189X20912798}
}

@article{kruschke_bayesian_2011,
  title = {Bayesian Assessment of Null Values via Parameter Estimation and Model Comparison},
  author = {Kruschke, John K.},
  year = {2011},
  journal = {Perspectives on Psychological Science},
  volume = {6},
  number = {3},
  pages = {299--312},
  annotation = {00203}
}

@article{kruschke_bayesian_2013,
  title = {Bayesian Estimation Supersedes the t Test.},
  author = {Kruschke, John K.},
  year = {2013},
  journal = {Journal of Experimental Psychology: General},
  volume = {142},
  number = {2},
  pages = {573--603},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0029146},
  langid = {english},
  annotation = {00483}
}

@article{kruschke_bayesian_2017,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {Kruschke, John K. and Liddell, Torrin M.},
  year = {2017},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  langid = {english},
  annotation = {00059}
}

@book{kruschke_doing_2014,
  title = {Doing {{Bayesian Data Analysis}}, {{Second Edition}}: {{A Tutorial}} with {{R}}, {{JAGS}}, and {{Stan}}},
  shorttitle = {Doing {{Bayesian Data Analysis}}, {{Second Edition}}},
  author = {Kruschke, John K.},
  year = {2014},
  month = nov,
  edition = {2 edition},
  publisher = {{Academic Press}},
  address = {{Boston}},
  abstract = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition provides an accessible approach for conducting Bayesian data analysis, as material is explained clearly with concrete examples. Included are step-by-step instructions on how to carry out Bayesian data analyses in the popular and free software R and WinBugs, as well as new programs in JAGS and Stan. The new programs are designed to be much easier to use than the scripts in the first edition. In particular, there are now compact high-level scripts that make it easy to run the programs on your own data sets. The book is divided into three parts and begins with the basics: models, probability, Bayes' rule, and the R programming language. The discussion then moves to the fundamentals applied to inferring a binomial probability, before concluding with chapters on the generalized linear model. Topics include metric-predicted variable on one or two groups; metric-predicted variable with one metric predictor; metric-predicted variable with multiple metric predictors; metric-predicted variable with one nominal predictor; and metric-predicted variable with multiple nominal predictors. The exercises found in the text have explicit purposes and guidelines for accomplishment. This book is intended for first-year graduate students or advanced undergraduates in statistics, data analysis, psychology, cognitive science, social sciences, clinical sciences, and consumer sciences in business. Accessible, including the basics of essential concepts of probability and random samplingExamples with R programming language and JAGS softwareComprehensive coverage of all scenarios addressed by non-Bayesian textbooks: t-tests, analysis of variance (ANOVA) and comparisons in ANOVA, multiple regression, and chi-square (contingency table analysis)Coverage of experiment planningR and JAGS computer programming code on websiteExercises have explicit purposes and guidelines for accomplishment Provides step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs},
  isbn = {978-0-12-405888-0},
  langid = {english}
}

@article{kruschke_rejecting_2018,
  title = {Rejecting or {{Accepting Parameter Values}} in {{Bayesian Estimation}}},
  author = {Kruschke, John K.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {270--280},
  issn = {2515-2459},
  doi = {10.1177/2515245918771304},
  abstract = {This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.},
  langid = {english}
}

@article{kruschke_rejecting_2018-1,
  title = {Rejecting or {{Accepting Parameter Values}} in {{Bayesian Estimation}}},
  author = {Kruschke, John K.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {270--280},
  issn = {2515-2459},
  doi = {10.1177/2515245918771304},
  abstract = {This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.},
  langid = {english}
}

@article{kvarven_comparing_2020,
  title = {Comparing Meta-Analyses and Preregistered Multiple-Laboratory Replication Projects},
  author = {Kvarven, Amanda and Str{\o}mland, Eirik and Johannesson, Magnus},
  year = {2020},
  month = apr,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {4},
  pages = {423--434},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0787-z},
  abstract = {Many researchers rely on meta-analysis to summarize research evidence. However, there is a concern that publication bias and selective reporting may lead to biased meta-analytic effect sizes. We compare the results of meta-analyses to large-scale preregistered replications in psychology carried out at multiple laboratories. The multiple-laboratory replications provide precisely estimated effect sizes that do not suffer from publication bias or selective reporting. We searched the literature and identified 15\,meta-analyses on the same topics as multiple-laboratory replications. We find that meta-analytic effect sizes are significantly different from replication effect sizes for 12 out of the 15\,meta-replication pairs. These differences are systematic and, on average, meta-analytic effect sizes are almost three times as large as replication effect sizes. We also implement three methods of correcting meta-analysis for bias, but these methods do not substantively improve the meta-analytic results.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english}
}

@book{lakatos_methodology_1978,
  title = {The Methodology of Scientific Research Programmes: {{Volume}} 1: {{Philosophical}} Papers},
  shorttitle = {The Methodology of Scientific Research Programmes},
  author = {Lakatos, Imre},
  year = {1978},
  volume = {1},
  publisher = {{Cambridge University Press}}
}

@article{lakens_calculating_2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  author = {Lakens, Dani{\"e}l},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00863},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  langid = {english},
  keywords = {Cohen's d,effect sizes,eta-squared,power analysis,sample size planning}
}

@article{lakens_challenges_2015,
  title = {On the Challenges of Drawing Conclusions from {\emph{p}} -Values Just below 0.05},
  author = {Lakens, Dani{\"e}l},
  year = {2015},
  journal = {PeerJ},
  volume = {3},
  pages = {e1142},
  issn = {2167-8359},
  doi = {10.7717/peerj.1142},
  langid = {english}
}

@article{lakens_equivalence_2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta-Analyses}}},
  shorttitle = {Equivalence {{Tests}}},
  author = {Lakens, Dani{\"e}l},
  year = {2017},
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {355--362},
  issn = {1948-5506},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  langid = {english}
}

@article{lakens_equivalence_2018,
  title = {Equivalence Testing for Psychological Research: {{A}} Tutorial},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english}
}

@article{lakens_improving_2020,
  title = {Improving {{Inferences About Null Effects With Bayes Factors}} and {{Equivalence Tests}}},
  author = {Lakens, Dani{\"e}l and McLatchie, Neil and Isager, Peder M and Scheel, Anne M and Dienes, Zoltan},
  year = {2020},
  month = jan,
  journal = {The Journals of Gerontology: Series B},
  volume = {75},
  number = {1},
  pages = {45--57},
  issn = {1079-5014},
  doi = {10.1093/geronb/gby065},
  abstract = {Researchers often conclude an effect is absent when a null-hypothesis significance test yields a nonsignificant p value. However, it is neither logically nor statistically correct to conclude an effect is absent when a hypothesis test is not significant. We present two methods to evaluate the presence or absence of effects: Equivalence testing (based on frequentist statistics) and Bayes factors (based on Bayesian statistics). In four examples from the gerontology literature, we illustrate different ways to specify alternative models that can be used to reject the presence of a meaningful or predicted effect in hypothesis tests. We provide detailed explanations of how to calculate, report, and interpret Bayes factors and equivalence tests. We also discuss how to design informative studies that can provide support for a null model or for the absence of a meaningful effect. The conceptual differences between Bayes factors and equivalence tests are discussed, and we also note when and why they might lead to similar or different inferences in practice. It is important that researchers are able to falsify predictions or can quantify the support for predicted null effects. Bayes factors and equivalence tests provide useful statistical tools to improve inferences about null effects.}
}

@article{lakens_improving_2020-2,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigour}} by {{Making Hypothesis Tests Machine Readable}}},
  author = {Lakens, Dani{\"e}l and DeBruine, Lisa},
  year = {2020},
  month = jan,
  doi = {10.31234/osf.io/5xcda},
  abstract = {Making scientific information machine-readable greatly facilitates its re-use. Many scientific articles have the goal to test a hypothesis, and making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine readable. We believe there are two benefits to specifying a hypothesis test in a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis test will become more transparent, falsifiable, and rigorous. Second, scientists will benefit if information related to hypothesis tests in scientific articles is easily findable and re-usable, for example when performing meta-analyses, during peer review, and when examining meta-scientific research questions. We examine what a machine readable hypothesis test should looks like, and demonstrate the feasibility of machine readable hypothesis tests in a real-life example.}
}

@article{lakens_justify_2018,
  title = {Justify Your Alpha},
  author = {Lakens, Dani{\"e}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and Oliveira, Cilene Lino and Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = feb,
  journal = {Nature Human Behaviour},
  volume = {2},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  copyright = {2018 The Publisher},
  langid = {english}
}

@article{lakens_performing_2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: {{Sequential}} Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  langid = {english}
}

@article{lakens_practical_2021,
  title = {The Practical Alternative to the p Value Is the Correctly Used p Value},
  author = {Lakens, Dani{\"e}l},
  year = {2021},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {3},
  pages = {639--648},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620958012},
  abstract = {Because of the strong overreliance on p values in the scientific literature, some researchers have argued that we need to move beyond p values and embrace practical alternatives. When proposing alternatives to p values statisticians often commit the ``statistician's fallacy,'' whereby they declare which statistic researchers really ``want to know.'' Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p value. As long as null-hypothesis tests have been criticized, researchers have suggested including minimum-effect tests and equivalence tests in our statistical toolbox, and these tests have the potential to greatly improve the questions researchers ask. If anyone believes p values affect the quality of scientific research, preventing the misinterpretation of p values by developing better evidence-based education and user-centered statistical software should be a top priority. Polarized discussions about which statistic scientists should use has distracted us from examining more important questions, such as asking researchers what they want to know when they conduct scientific research. Before we can improve our statistical inferences, we need to improve our statistical questions.},
  langid = {english},
  keywords = {equivalence tests,null-hypothesis testing,p values,statistical inferences}
}

@article{lakens_reproducibility_2016,
  title = {On the Reproducibility of Meta-Analyses: Six Practical Recommendations},
  shorttitle = {On the Reproducibility of Meta-Analyses},
  author = {Lakens, Dani{\"e}l and Hilgard, Joe and Staaks, Janneke},
  year = {2016},
  journal = {BMC Psychology},
  volume = {4},
  pages = {24},
  issn = {2050-7283},
  doi = {10.1186/s40359-016-0126-3},
  abstract = {Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions.},
  keywords = {Meta-analysis,Open science,Reporting guidelines,Reproducibility}
}

@article{lakens_sailing_2014,
  title = {Sailing from the Seas of Chaos into the Corridor of Stability: {{Practical}} Recommendations to Increase the Informational Value of Studies},
  shorttitle = {Sailing {{From}} the {{Seas}} of {{Chaos Into}} the {{Corridor}} of {{Stability}}},
  author = {Lakens, Dani{\"e}l and Evers, E. R. K.},
  year = {2014},
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {3},
  pages = {278--292},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614528520},
  langid = {english}
}

@article{lakens_sample_2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  journal = {Collabra: Psychology},
  doi = {10.31234/osf.io/9d3yf},
  abstract = {An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  keywords = {Experimental Design and Sample Surveys,power analysis,Quantitative Methods,sample size justification,Social and Behavioral Sciences,study design,value of information}
}

@article{lakens_simulation-based_2021,
  title = {Simulation-{{Based Power Analysis}} for {{Factorial Analysis}} of {{Variance Designs}}},
  author = {Lakens, Dani{\"e}l and Caldwell, Aaron R.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920951503},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920951503},
  abstract = {Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure that a study is adequately powered to yield informative results with an ANOVA, researchers can perform an a priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not allow power analyses for complex designs with several within-participants factors. Moreover, power analyses often need {$\eta$}2{$\mathsl{p}\eta$}p2{$<$}math display="inline" id="math1-2515245920951503" overflow="scroll" altimg="eq-00001.gif"{$><$}mrow{$><$}msubsup{$><$}mi mathvariant="normal"{$>\eta<$}/mi{$><$}mi{$>$}p{$<$}/mi{$><$}mn{$>$}2{$<$}/mn{$><$}/msubsup{$><$}/mrow{$><$}/math{$>$} or Cohen's f as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package Superpower and online Shiny apps to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-participants factors. Predicted effects are entered by specifying means, standard deviations, and, for within-participants factors, the correlations. The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons. The software can plot power across a range of sample sizes, can control for multiple comparisons, and can compute power when the homogeneity or sphericity assumption is violated. This Tutorial demonstrates how to perform a priori power analysis to design informative studies for main effects, interactions, and individual comparisons and highlights important factors that determine the statistical power for factorial ANOVA designs.},
  langid = {english},
  keywords = {ANOVA,hypothesis test,open materials,power analysis,sample-size justification}
}

@article{lakens_too_2017,
  title = {Too {{True}} to Be {{Bad}}: {{When Sets}} of {{Studies With Significant}} and {{Nonsignificant Findings Are Probably True}}},
  shorttitle = {Too {{True}} to Be {{Bad}}},
  author = {Lakens, Dani{\"e}l and Etz, Alexander J.},
  year = {2017},
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {8},
  pages = {875--881},
  issn = {1948-5506},
  doi = {10.1177/1948550617693058},
  abstract = {Psychology journals rarely publish nonsignificant results. At the same time, it is often very unlikely (or ``too good to be true'') that a set of studies yields exclusively significant results. Here, we use likelihood ratios to explain when sets of studies that contain a mix of significant and nonsignificant results are likely to be true or ``too true to be bad.'' As we show, mixed results are not only likely to be observed in lines of research but also, when observed, often provide evidence for the alternative hypothesis, given reasonable levels of statistical power and an adequately controlled low Type 1 error rate. Researchers should feel comfortable submitting such lines of research with an internal meta-analysis for publication. A better understanding of probabilities, accompanied by more realistic expectations of what real sets of studies look like, might be an important step in mitigating publication bias in the scientific literature.},
  langid = {english}
}

@article{lakens_value_2019,
  title = {The Value of Preregistration for Psychological Science: {{A}} Conceptual Analysis},
  shorttitle = {The Value of Preregistration for Psychological Science},
  author = {Lakens, Dani{\"e}l},
  year = {2019},
  journal = {Japanese Psychological Review},
  volume = {62},
  number = {3},
  pages = {221--230},
  doi = {10.24602/sjpr.62.3_221},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.},
  keywords = {hypothesis testing,meta-science,preregistration,registered reports,severity}
}

@article{lakens_value_2020,
  title = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}: {{A Conceptual Analysis}}},
  shorttitle = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}},
  author = {Lakens, Dani{\"e}l},
  year = {2020},
  journal = {Japanese Psychological Review},
  doi = {10.31234/osf.io/jbh4w},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.}
}

@article{lakens_why_2022,
  title = {Why {{P}} Values Are Not Measures of Evidence},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = jan,
  journal = {Trends in Ecology \& Evolution},
  issn = {0169-5347},
  doi = {10.1016/j.tree.2021.12.006},
  langid = {english}
}

@article{lan_discrete_1983,
  title = {Discrete {{Sequential Boundaries}} for {{Clinical Trials}}},
  author = {Lan, K. K. Gordon and DeMets, David L.},
  year = {1983},
  month = dec,
  journal = {Biometrika},
  volume = {70},
  number = {3},
  pages = {659},
  issn = {00063444},
  doi = {10.2307/2336502},
  annotation = {01888}
}

@book{leamer_specification_1978,
  title = {Specification {{Searches}}: {{Ad Hoc Inference}} with {{Nonexperimental Data}}},
  shorttitle = {Specification {{Searches}}},
  author = {Leamer, Edward E.},
  year = {1978},
  month = apr,
  edition = {1 edition},
  publisher = {{Wiley}},
  address = {{New York usw.}},
  abstract = {Offers a radically new approach to inference with nonexperimental data when the statistical model is ambiguously defined. Examines the process of model searching and its implications for inference. Identifies six different varieties of specification searches, discussing the inferential consequences of each in detail.},
  isbn = {978-0-471-01520-8},
  langid = {english}
}

@book{leamer_specification_1978-1,
  title = {Specification {{Searches}}: {{Ad Hoc Inference}} with {{Nonexperimental Data}}},
  shorttitle = {Specification {{Searches}}},
  author = {Leamer, Edward E.},
  year = {1978},
  month = apr,
  edition = {1 edition},
  publisher = {{Wiley}},
  address = {{New York usw.}},
  abstract = {Offers a radically new approach to inference with nonexperimental data when the statistical model is ambiguously defined. Examines the process of model searching and its implications for inference. Identifies six different varieties of specification searches, discussing the inferential consequences of each in detail.},
  isbn = {978-0-471-01520-8},
  langid = {english}
}

@book{lehmann_testing_2005,
  title = {Testing Statistical Hypotheses},
  author = {Lehmann, E. L. and Romano, Joseph P.},
  year = {2005},
  series = {Springer Texts in Statistics},
  edition = {3rd ed},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-98864-1},
  lccn = {QA277 .L425 2005},
  keywords = {Statistical hypothesis testing}
}

@article{lenth_post_2007,
  title = {Post Hoc Power: Tables and Commentary},
  shorttitle = {Post Hoc Power},
  author = {Lenth, Russell V.},
  year = {2007},
  journal = {Iowa City: Department of Statistics and Actuarial Science, University of Iowa}
}

@article{lenth_practical_2001,
  title = {Some Practical Guidelines for Effective Sample Size Determination},
  author = {Lenth, Russell V.},
  year = {2001},
  journal = {The American Statistician},
  volume = {55},
  number = {3},
  pages = {187--193},
  doi = {10.1198/000313001317098149}
}

@article{leon_role_2011,
  title = {The {{Role}} and {{Interpretation}} of {{Pilot Studies}} in {{Clinical Research}}},
  author = {Leon, Andrew C. and Davis, Lori L. and Kraemer, Helena C.},
  year = {2011},
  month = may,
  journal = {Journal of psychiatric research},
  volume = {45},
  number = {5},
  pages = {626--629},
  issn = {0022-3956},
  doi = {10.1016/j.jpsychires.2010.10.008},
  abstract = {Pilot studies represent a fundamental phase of the research process. The purpose of conducting a pilot study is to examine the feasibility of an approach that is intended to be used in a larger scale study. The roles and limitations of pilot studies are described here using a clinical trial as an example. A pilot study can be used to evaluate the feasibility of recruitment, randomization, retention, assessment procedures, new methods, and implementation of the novel intervention., A pilot study is not a hypothesis testing study. Safety, efficacy and effectiveness are not evaluated in a pilot. Contrary to tradition, a pilot study does not provide a meaningful effect size estimate for planning subsequent studies due to the imprecision inherent in data from small samples. Feasibility results do not necessarily generalize beyond the inclusion and exclusion criteria of the pilot design., A pilot study is a requisite initial step in exploring a novel intervention or an innovative application of an intervention. Pilot results can inform feasibility and identify modifications needed in the design of a larger, ensuing hypothesis testing study. Investigators should be forthright in stating these objectives of a pilot study. Grant reviewers and other stakeholders should expect no more.},
  pmcid = {PMC3081994},
  pmid = {21035130}
}

@article{levine_communication_2008,
  title = {A Communication Researchers' Guide to Null Hypothesis Significance Testing and Alternatives},
  author = {Levine, Timothy R. and Weber, Ren{\'e} and Park, Hee Sun and Hullett, Craig R.},
  year = {2008},
  journal = {Human Communication Research},
  volume = {34},
  number = {2},
  pages = {188--209}
}

@article{leys_how_2019,
  title = {How to {{Classify}}, {{Detect}}, and {{Manage Univariate}} and {{Multivariate Outliers}}, {{With Emphasis}} on {{Pre-Registration}}},
  author = {Leys, Christophe and Delacre, Marie and Mora, Youri L. and Lakens, Dani{\"e}l and Ley, Christophe},
  year = {2019},
  month = apr,
  journal = {International Review of Social Psychology},
  volume = {32},
  number = {1},
  pages = {5},
  issn = {2397-8570},
  doi = {10.5334/irsp.289},
  abstract = {Researchers often lack knowledge about how to deal with outliers when analyzing their data. Even more frequently, researchers do not pre-specify how they plan to manage outliers. In this paper we aim to improve research practices by outlining what you need to know about outliers. We start by providing a functional definition of outliers. We then lay down an appropriate nomenclature/classification of outliers.~This nomenclature is used to understand what kinds of outliers can be encountered and serves as a guideline to make appropriate decisions regarding the conservation, deletion, or recoding of outliers. These decisions might impact the validity of statistical inferences as well as the reproducibility of our experiments. To be able to make informed decisions about outliers you first need proper detection tools. We remind readers why the most common outlier detection methods are problematic and recommend the use of the median absolute deviation to detect univariate outliers, and of the Mahalanobis-MCD distance to detect multivariate outliers. An R package was created that can be used to easily perform these detection tests. Finally, we promote the use of pre-registration to avoid flexibility in data analysis when handling outliers. ~ Publishers note: due to a typesetting error, this paper was originally published with incorrect table numbering, where tables 2, 3, and 4 were incorrectly labelled. This was corrected soon after publication.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  keywords = {Malahanobis distance,median absolute deviation,minimum covariance determinant,outliers,preregistration,robust detection}
}

@article{linden_heterogeneity_2021,
  title = {Heterogeneity of {{Research Results}}: {{A New Perspective From Which}} to {{Assess}} and {{Promote Progress}} in {{Psychological Science}}},
  shorttitle = {Heterogeneity of {{Research Results}}},
  author = {Linden, Audrey Helen and H{\"o}nekopp, Johannes},
  year = {2021},
  month = mar,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {2},
  pages = {358--376},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620964193},
  abstract = {Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.},
  langid = {english},
  keywords = {heterogeneity,meta-analysis,philosophy of science,psychological research,replication,statistical power}
}

@article{lindley_statistical_1957,
  title = {A Statistical Paradox},
  author = {Lindley, Dennis V.},
  year = {1957},
  journal = {Biometrika},
  volume = {44},
  number = {1/2},
  pages = {187--192}
}

@article{lindsay_replication_2015,
  title = {Replication in {{Psychological Science}}},
  author = {Lindsay, D. Stephen},
  year = {2015},
  month = dec,
  journal = {Psychological Science},
  volume = {26},
  number = {12},
  pages = {1827--1832},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797615616374},
  langid = {english},
  pmid = {26553013}
}

@article{lovakov_empirically_2017,
  title = {Empirically {{Derived Guidelines}} for {{Interpreting Effect Size}} in {{Social Psychology}}},
  author = {Lovakov, Andrey and Agadullina, Elena},
  year = {2017},
  month = nov,
  journal = {PsyArXiv},
  doi = {10.17605/OSF.IO/2EPC4},
  abstract = {A number of recent research publications have shown that commonly used guidelines for interpreting effect sizes suggested by Cohen (1988) do not fit well with the empirical distribution of those effect sizes, and tend to overestimate them in many research areas. This study proposes empirically derived guidelines for interpreting effect sizes for research in social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis was carried out on the empirical distribution of 9884 correlation coefficients and 3580 Hedges' g statistics extracted from studies included in 98 published meta-analyses. The analysis reveals that the 25th, 50th, and 75th percentiles corresponded to correlation coefficients values of 0.12, 0.25, and 0.42 and to Hedges' g values of 0.15, 0.38, and 0.69, respectively. This suggests that Cohen's guidelines tend to overestimate medium and large effect sizes. It is recommended that correlation coefficients of 0.10, 0.25, and 0.40 and Hedges' g of 0.15, 0.40, and 0.70 should be interpreted as small, medium, and large effects for studies in social psychology. The analysis also shows that more than half of all studies lack sufficient sample size to detect a medium effect. This paper reports the sample sizes required to achieve appropriate statistical power for the identification of small, medium, and large effects. This can be used for performing appropriately powered future studies when information about exact effect size is not available.}
}

@article{mahoney_publication_1977,
  title = {Publication Prejudices: {{An}} Experimental Study of Confirmatory Bias in the Peer Review System},
  shorttitle = {Publication Prejudices},
  author = {Mahoney, Michael J.},
  year = {1977},
  month = jun,
  journal = {Cognitive Therapy and Research},
  volume = {1},
  number = {2},
  pages = {161--175},
  issn = {1573-2819},
  doi = {10.1007/BF01173636},
  abstract = {Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.},
  langid = {english},
  keywords = {Clinical Research,Cognitive Psychology,Experimental Study,Review System,Theoretical Perspective}
}

@article{maier_justify_2022,
  title = {Justify Your Alpha: {{A}} Primer on Two Practical Approaches},
  shorttitle = {Justify {{Your Alpha}}},
  author = {Maier, Maximilian and Lakens, Dani{\"e}l},
  year = {2022},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.31234/osf.io/ts4r6},
  abstract = {The default use of an alpha level of 0.05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power p-values lower than the alpha level can be more likely when the null hypothesis is true, than when the alternative hypothesis is true (i.e., Lindley's paradox). This manuscript explains two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of 0.05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley's paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors), but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that have a better justification should improve statistical inferences and can increase the efficiency and informativeness of scientific research.},
  langid = {american},
  keywords = {Hypothesis Testing,Meta-science,Quantitative Methods,Social and Behavioral Sciences,Statistical Power,Type 1 Error,Type 2 Error}
}

@article{makel_both_2021,
  title = {Both {{Questionable}} and {{Open Research Practices Are Prevalent}} in {{Education Research}}},
  author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan A.},
  year = {2021},
  month = nov,
  journal = {Educational Researcher},
  volume = {50},
  number = {8},
  pages = {493--504},
  publisher = {{American Educational Research Association}},
  issn = {0013-189X},
  doi = {10.3102/0013189X211001356},
  abstract = {Concerns about the conduct of research are pervasive in many fields, including education. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and five open research practices. We asked them to estimate the prevalence of the practices in the field, to self-report their own use of such practices, and to estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are used by many education researchers. This baseline information will be useful as education researchers seek to understand existing social norms and grapple with whether and how to improve research practices.},
  langid = {english},
  keywords = {ethics,globalization,open science,psychology,questionable research practices,replication,research methodology,survey research}
}

@article{marshall_does_2013,
  title = {Does {{Sample Size Matter}} in {{Qualitative Research}}?: {{A Review}} of {{Qualitative Interviews}} in Is {{Research}}},
  shorttitle = {Does {{Sample Size Matter}} in {{Qualitative Research}}?},
  author = {Marshall, Bryan and Cardon, Peter and Poddar, Amit and Fontenot, Renee},
  year = {2013},
  month = sep,
  journal = {Journal of Computer Information Systems},
  volume = {54},
  number = {1},
  pages = {11--22},
  publisher = {{Taylor \& Francis}},
  issn = {0887-4417},
  doi = {10.1080/08874417.2013.11645667},
  abstract = {This study examines 83 IS qualitative studies in leading IS journals for the following purposes: (a) identifying the extent to which IS qualitative studies employ best practices of justifying sample size; (b) identifying optimal ranges of interviews for various types of qualitative research; and (c) identifying the extent to which cultural factors (such as journal of publication, number of authors, world region) impact sample size of interviews. Little or no rigor for justifying sample size was shown for virtually all of the IS studies in this dataset. Furthermore, the number of interviews conducted for qualitative studies is correlated with cultural factors, implying the subjective nature of sample size in qualitative IS studies. Recommendations are provided for minimally acceptable practices of justifying sample size of interviews in qualitative IS studies.},
  keywords = {data saturation,qualitative interviews,qualitative methodology,sample size},
  annotation = {\_eprint: https://doi.org/10.1080/08874417.2013.11645667}
}

@book{maxwell_designing_2004,
  title = {Designing Experiments and Analyzing Data: A Model Comparison Perspective},
  shorttitle = {Designing Experiments and Analyzing Data},
  author = {Maxwell, Scott E. and Delaney, Harold D.},
  year = {2004},
  edition = {2nd ed},
  publisher = {{Lawrence Erlbaum Associates}},
  address = {{Mahwah, N.J}},
  isbn = {978-0-8058-3718-6},
  lccn = {QA279 .M384 2004},
  keywords = {Experimental design}
}

@book{maxwell_designing_2017,
  title = {Designing {{Experiments}} and {{Analyzing Data}}: {{A Model Comparison Perspective}}, {{Third Edition}}},
  shorttitle = {Designing {{Experiments}} and {{Analyzing Data}}},
  author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
  year = {2017},
  month = aug,
  edition = {3 edition},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  abstract = {Designing Experiments and Analyzing Data: A Model Comparison Perspective (3rd edition) offers an integrative conceptual framework for understanding experimental design and data analysis. Maxwell, Delaney, and Kelley first apply fundamental principles to simple experimental designs followed by an application of the same principles to more complicated designs. Their integrative conceptual framework better prepares readers to understand the logic behind a general strategy of data analysis that is appropriate for a wide variety of designs, which allows for the introduction of more complex topics that are generally omitted from other books. Numerous pedagogical features further facilitate understanding: examples of published research demonstrate the applicability of each chapter's content; flowcharts assist in choosing the most appropriate procedure; end-of-chapter lists of important formulas highlight key ideas and assist readers in locating the initial presentation of equations; useful programming code and tips are provided throughout the book and in associated resources available online, and extensive sets of exercises help develop a deeper understanding of the subject. Detailed solutions for some of the exercises and realistic data sets are included on the website (DesigningExperiments.com). The pedagogical approach used throughout the book enables readers to gain an overview of experimental design, from conceptualization of the research question to analysis of the data. The book and its companion website with web apps, tutorials, and detailed code are ideal for students and researchers seeking the optimal way to design their studies and analyze the resulting data.},
  isbn = {978-1-138-89228-6},
  langid = {english},
  annotation = {00000}
}

@incollection{maxwell_ethics_2011,
  title = {Ethics and Sample Size Planning},
  booktitle = {Handbook of Ethics in Quantitative Methodology},
  author = {Maxwell, Scott E. and Kelley, Ken},
  year = {2011},
  pages = {179--204},
  publisher = {{Routledge}}
}

@article{maxwell_sample_2008,
  title = {Sample {{Size Planning}} for {{Statistical Power}} and {{Accuracy}} in {{Parameter Estimation}}},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  year = {2008},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {59},
  number = {1},
  pages = {537--563},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.59.103006.093735},
  langid = {english}
}

@book{mayo_statistical_2018,
  title = {Statistical Inference as Severe Testing: How to Get beyond the Statistics Wars},
  shorttitle = {Statistical Inference as Severe Testing},
  author = {Mayo, Deborah G.},
  year = {2018},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-1-107-05413-4},
  langid = {english},
  lccn = {QA276 .M3755 2018},
  keywords = {Deviation (Mathematics),Error analysis (Mathematics),Fallacies (Logic),Inference,Mathematical statistics},
  annotation = {00004}
}

@article{mazzolari_myths_2022,
  title = {Myths and Methodologies: {{The}} Use of Equivalence and Non-Inferiority Tests for Interventional Studies in Exercise Physiology and Sport Science},
  shorttitle = {Myths and Methodologies},
  author = {Mazzolari, Raffaele and Porcelli, Simone and Bishop, David J. and Lakens, Dani{\"e}l},
  year = {2022},
  journal = {Experimental Physiology},
  volume = {107},
  number = {3},
  pages = {201--212},
  issn = {1469-445X},
  doi = {10.1113/EP090171},
  abstract = {Exercise physiology and sport science have traditionally made use of the null hypothesis of no difference to make decisions about experimental interventions. In this article, we aim to review current statistical approaches typically used by exercise physiologists and sport scientists for the design and analysis of experimental interventions and to highlight the importance of including equivalence and non-inferiority studies, which address different research questions from deciding whether an effect is present. Initially, we briefly describe the most common approaches, along with their rationale, to investigate the effects of different interventions. We then discuss the main steps involved in the design and analysis of equivalence and non-inferiority studies, commonly performed in other research fields, with worked examples from exercise physiology and sport science scenarios. Finally, we provide recommendations to exercise physiologists and sport scientists who would like to apply the different approaches in future research. We hope this work will promote the correct use of equivalence and non-inferiority designs in exercise physiology and sport science whenever the research context, conditions, applications, researchers' interests or reasonable beliefs justify these approaches.},
  langid = {english},
  keywords = {intervention efficacy,methodology,statistical review},
  annotation = {\_eprint: https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/EP090171}
}

@article{mccarthy_registered_2018,
  title = {Registered {{Replication Report}} on {{Srull}} and {{Wyer}} (1979)},
  author = {McCarthy, Randy J. and Skowronski, John J. and Verschuere, Bruno and Meijer, Ewout H. and Jim, Ariane and Hoogesteyn, Katherine and Orthey, Robin and Acar, Oguz A. and Aczel, Balazs and Bakos, Bence E. and Barbosa, Fernando and Baskin, Ernest and B{\`e}gue, Laurent and {Ben-Shakhar}, Gershon and Birt, Angie R. and Blatz, Lisa and Charman, Steve D. and Claesen, Aline and Clay, Samuel L. and Coary, Sean P. and Crusius, Jan and Evans, Jacqueline R. and Feldman, Noa and {Ferreira-Santos}, Fernando and Gamer, Matthias and Gerlsma, Coby and Gomes, Sara and {Gonz{\'a}lez-Iraizoz}, Marta and Holzmeister, Felix and Huber, Juergen and Huntjens, Rafaele J. C. and Isoni, Andrea and Jessup, Ryan K. and Kirchler, Michael and {klein Selle}, Nathalie and Koppel, Lina and Kovacs, Marton and Laine, Tei and Lentz, Frank and Loschelder, David D. and Ludvig, Elliot A. and Lynn, Monty L. and Martin, Scott D. and McLatchie, Neil M. and Mechtel, Mario and Nahari, Galit and {\"O}zdo{\u g}ru, Asil Ali and Pasion, Rita and Pennington, Charlotte R. and Roets, Arne and Rozmann, Nir and Scopelliti, Irene and Spiegelman, Eli and Suchotzki, Kristina and Sutan, Angela and Szecsi, Peter and Tingh{\"o}g, Gustav and Tisserand, Jean-Christian and Tran, Ulrich S. and Van Hiel, Alain and Vanpaemel, Wolf and V{\"a}stfj{\"a}ll, Daniel and Verliefde, Thomas and Vezirian, K{\'e}vin and Voracek, Martin and Warmelink, Lara and Wick, Katherine and Wiggins, Bradford J. and Wylie, Keith and Y{\i}ld{\i}z, Ezgi},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {321--336},
  issn = {2515-2459},
  doi = {10.1177/2515245918777487},
  abstract = {Srull and Wyer (1979) demonstrated that exposing participants to more hostility-related stimuli caused them subsequently to interpret ambiguous behaviors as more hostile. In their Experiment 1, participants descrambled sets of words to form sentences. In one condition, 80\% of the descrambled sentences described hostile behaviors, and in another condition, 20\% described hostile behaviors. Following the descrambling task, all participants read a vignette about a man named Donald who behaved in an ambiguously hostile manner and then rated him on a set of personality traits. Next, participants rated the hostility of various ambiguously hostile behaviors (all ratings on scales from 0 to 10). Participants who descrambled mostly hostile sentences rated Donald and the ambiguous behaviors as approximately 3 scale points more hostile than did those who descrambled mostly neutral sentences. This Registered Replication Report describes the results of 26 independent replications (N = 7,373 in the total sample; k = 22 labs and N = 5,610 in the primary analyses) of Srull and Wyer's Experiment 1, each of which followed a preregistered and vetted protocol. A random-effects meta-analysis showed that the protagonist was seen as 0.08 scale points more hostile when participants were primed with 80\% hostile sentences than when they were primed with 20\% hostile sentences (95\% confidence interval, CI = [0.004, 0.16]). The ambiguously hostile behaviors were seen as 0.08 points less hostile when participants were primed with 80\% hostile sentences than when they were primed with 20\% hostile sentences (95\% CI = [-0.18, 0.01]). Although the confidence interval for one outcome excluded zero and the observed effect was in the predicted direction, these results suggest that the currently used methods do not produce an assimilative priming effect that is practically and routinely detectable.},
  langid = {english},
  keywords = {hostility,impression formation,Many Labs,open data,open materials,preregistered,priming,replication}
}

@book{mcelreath_statistical_2016,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2016},
  volume = {122},
  publisher = {{CRC Press}}
}

@article{mcgrath_when_2006,
  title = {When Effect Sizes Disagree: {{The}} Case of r and d.},
  shorttitle = {When Effect Sizes Disagree},
  author = {McGrath, Robert E. and Meyer, Gregory J.},
  year = {2006},
  journal = {Psychological Methods},
  volume = {11},
  number = {4},
  pages = {386--401},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.4.386},
  langid = {english}
}

@article{mcgraw_common_1992,
  title = {A Common Language Effect Size Statistic},
  author = {McGraw, Kenneth O. and Wong, S. P.},
  year = {1992},
  journal = {Psychological Bulletin},
  volume = {111},
  number = {2},
  pages = {361--365},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.111.2.361},
  abstract = {Some of the shortcomings in interpretability and generalizability of the effect size statistics currently available to researchers can be overcome by a statistic that expresses how often a score sampled from one distribution will be greater than a score sampled from another distribution. The statistic, the common language effect size indicator, is easily calculated from sample means and variances (or from proportions in the case of nominal-level data). It can be used for expressing the effect observed in both independent and related sample designs and in both 2-group and n-group designs. Empirical tests show it to be robust to violations of the normality assumption, particularly when the variances in the 2 parent distributions are equal. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Effect Size (Statistical)}
}

@misc{mcintosh_power_2020,
  title = {Power Calculations in Single Case Neuropsychology},
  author = {McIntosh, Robert D. and Rittmo, Jonathan {\"O}},
  year = {2020},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/fxz49},
  abstract = {Researchers and clinicians in neuropsychology often compare individual patients against healthy control samples, to quantify evidence for cognitive-behavioural deficits and dissociations. Statistical methods for these comparisons have been developed that control Type I (false positive) errors effectively. However, remarkably little attention has been given to the power of these tests. In this practical primer, we describe, in minimally technical terms, the origins and limits of power for case-control comparisons. We argue that power calculations can play useful roles in single-case study design and interpretation, and we make suggestions for optimising power in practice. As well as providing figures, tables and tools for estimating the power of case-control comparisons, we hope to assist researchers in setting realistic expectations for what such tests can achieve in general.},
  keywords = {Behavioral Neuroscience,Clinical Neuroscience,Cognitive Neuroscience,deficit,dissociation,Neuroscience,power,single-case,statistical methods}
}

@article{mcshane_adjusting_2016,
  title = {Adjusting for {{Publication Bias}} in {{Meta-Analysis}}: {{An Evaluation}} of {{Selection Methods}} and {{Some Cautionary Notes}}},
  shorttitle = {Adjusting for {{Publication Bias}} in {{Meta-Analysis}}},
  author = {McShane, Blakeley B. and B{\"o}ckenholt, Ulf and Hansen, Karsten T.},
  year = {2016},
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {5},
  pages = {730--749},
  keywords = {Effect size,Meta-analysis,p-curve,p-uniform,selection methods}
}

@article{meehl_appraising_1990,
  title = {Appraising and Amending Theories: {{The}} Strategy of {{Lakatosian}} Defense and Two Principles That Warrant It},
  shorttitle = {Appraising and Amending Theories},
  author = {Meehl, Paul E.},
  year = {1990},
  journal = {Psychological Inquiry},
  volume = {1},
  number = {2},
  pages = {108--141},
  doi = {10.1207/s15327965pli0102_1}
}

@article{meehl_theoretical_1978,
  title = {Theoretical {{Risks}} and {{Tabular Asterisks}}: {{Sir Karl}}, {{Sir Ronald}}, and the {{Slow Progress}} of {{Soft Psychology}}},
  shorttitle = {Theoretical {{Risks}} and {{Tabular Asterisks}}},
  author = {Meehl, Paul E.},
  year = {1978},
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {46},
  number = {4},
  pages = {806--834},
  doi = {10.1037/0022-006X.46.4.806}
}

@article{meehl_theory-testing_1967,
  title = {Theory-Testing in Psychology and Physics: {{A}} Methodological Paradox},
  shorttitle = {Theory-Testing in Psychology and Physics},
  author = {Meehl, Paul E.},
  year = {1967},
  journal = {Philosophy of science},
  pages = {103--115}
}

@article{meyners_equivalence_2012,
  title = {Equivalence Tests \textendash{} {{A}} Review},
  author = {Meyners, Michael},
  year = {2012},
  month = dec,
  journal = {Food Quality and Preference},
  volume = {26},
  number = {2},
  pages = {231--245},
  issn = {09503293},
  doi = {10.1016/j.foodqual.2012.05.003},
  langid = {english}
}

@article{meyvis_increasing_2018,
  title = {Increasing the {{Power}} of {{Your Study}} by {{Increasing}} the {{Effect Size}}},
  author = {Meyvis, Tom and Van Osselaer, Stijn M J},
  year = {2018},
  month = feb,
  journal = {Journal of Consumer Research},
  volume = {44},
  number = {5},
  pages = {1157--1173},
  issn = {0093-5301},
  doi = {10.1093/jcr/ucx110},
  abstract = {As in other social sciences, published findings in consumer research tend to overestimate the size of the effect being investigated, due to both file drawer effects and abuse of researcher degrees of freedom, including opportunistic analysis decisions. Given that most effect sizes are substantially smaller than would be apparent from published research, there has been a widespread call to increase power by increasing sample size. We propose that, aside from increasing sample size, researchers can also increase power by boosting the effect size. If done correctly, removing participants, using covariates, and optimizing experimental designs, stimuli, and measures can boost effect size without inflating researcher degrees of freedom. In fact, careful planning of studies and analyses to maximize effect size is essential to be able to study many psychologically interesting phenomena when massive sample sizes are not feasible.}
}

@article{milgram_maintaining_1978,
  title = {On Maintaining Urban Norms: {{A}} Field Experiment in the Subway},
  shorttitle = {On Maintaining Urban Norms},
  author = {Milgram, Stanley and Sabini, John},
  year = {1978},
  journal = {Advances in environmental psychology},
  volume = {1},
  pages = {31--40}
}

@book{millar_maximum_2011,
  title = {Maximum Likelihood Estimation and Inference: With Examples in {{R}}, {{SAS}}, and {{ADMB}}},
  shorttitle = {Maximum Likelihood Estimation and Inference},
  author = {Millar, R. B.},
  year = {2011},
  series = {Statistics in Practice},
  publisher = {{Wiley}},
  address = {{Chichester, West Sussex}},
  isbn = {978-0-470-09482-2},
  lccn = {QA276.8 .M55 2011},
  keywords = {Chance,Estimation theory,Mathematical models}
}

@article{miller_quest_2019,
  title = {The Quest for an Optimal Alpha},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2019},
  month = jan,
  journal = {PLOS ONE},
  volume = {14},
  number = {1},
  pages = {e0208631},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0208631},
  abstract = {Researchers who analyze data within the framework of null hypothesis significance testing must choose a critical ``alpha'' level, {$\alpha$}, to use as a cutoff for deciding whether a given set of data demonstrates the presence of a particular effect. In most fields, {$\alpha$} = 0.05 has traditionally been used as the standard cutoff. Many researchers have recently argued for a change to a more stringent evidence cutoff such as {$\alpha$} = 0.01, 0.005, or 0.001, noting that this change would tend to reduce the rate of false positives, which are of growing concern in many research areas. Other researchers oppose this proposed change, however, because it would correspondingly tend to increase the rate of false negatives. We show how a simple statistical model can be used to explore the quantitative tradeoff between reducing false positives and increasing false negatives. In particular, the model shows how the optimal {$\alpha$} level depends on numerous characteristics of the research area, and it reveals that although {$\alpha$} = 0.05 would indeed be approximately the optimal value in some realistic situations, the optimal {$\alpha$} could actually be substantially larger or smaller in other situations. The importance of the model lies in making it clear what characteristics of the research area have to be specified to make a principled argument for using one {$\alpha$} level rather than another, and the model thereby provides a blueprint for researchers seeking to justify a particular {$\alpha$} level.},
  langid = {english},
  keywords = {Decision theory,Economic growth,Health economics,Medicine and health sciences,Psychology,Publication ethics,Statistical methods,Statistical models},
  annotation = {00000}
}

@article{miller_what_2009,
  title = {What Is the Probability of Replicating a Statistically Significant Effect?},
  author = {Miller, Jeff},
  year = {2009},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {4},
  pages = {617--640},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.4.617},
  langid = {english}
}

@article{mitroff_systemic_1974,
  title = {On Systemic Problem Solving and the Error of the Third Kind},
  author = {Mitroff, Ian I. and Featheringham, Tom R.},
  year = {1974},
  month = nov,
  journal = {Behavioral Science},
  volume = {19},
  number = {6},
  pages = {383--393},
  issn = {00057940, 10991743},
  doi = {10/cjqj8h},
  langid = {english},
  annotation = {00292}
}

@misc{morey_power_2020,
  type = {Blog},
  title = {Power and Precision},
  author = {Morey, Richard D.},
  year = {2020},
  month = jun,
  abstract = {Why the push for replacing ``power'' with ``precision'' is misguided},
  howpublished = {https://medium.com/@richarddmorey/power-and-precision-47f644ddea5e}
}

@article{morey_pre-registered_2021,
  title = {A Pre-Registered, Multi-Lab Non-Replication of the Action-Sentence Compatibility Effect ({{ACE}})},
  author = {Morey, Richard D. and Kaschak, Michael P. and {D{\'i}ez-{\'A}lamo}, Antonio M. and Glenberg, Arthur M. and Zwaan, Rolf A. and Lakens, Dani{\"e}l and Ib{\'a}{\~n}ez, Agust{\'i}n and Garc{\'i}a, Adolfo and Gianelli, Claudia and Jones, John L. and Madden, Julie and Alifano, Florencia and Bergen, Benjamin and Bloxsom, Nicholas G. and Bub, Daniel N. and Cai, Zhenguang G. and Chartier, Christopher R. and Chatterjee, Anjan and Conwell, Erin and Cook, Susan Wagner and Davis, Joshua D. and Evers, Ellen R. K. and Girard, Sandrine and Harter, Derek and Hartung, Franziska and Herrera, Eduar and Huettig, Falk and Humphries, Stacey and Juanchich, Marie and K{\"u}hne, Katharina and Lu, Shulan and Lynes, Tom and Masson, Michael E. J. and Ostarek, Markus and Pessers, Sebastiaan and Reglin, Rebecca and Steegen, Sara and Thiessen, Erik D. and Thomas, Laura E. and Trott, Sean and Vandekerckhove, Joachim and Vanpaemel, Wolf and Vlachou, Maria and Williams, Kristina and {Ziv-Crispel}, Noam},
  year = {2021},
  month = nov,
  journal = {Psychonomic Bulletin \& Review},
  issn = {1531-5320},
  doi = {10.3758/s13423-021-01927-8},
  abstract = {The Action-sentence Compatibility Effect (ACE) is a well-known demonstration of the role of motor activity in the comprehension of language. Participants are asked to make sensibility judgments on sentences by producing movements toward the body or away from the body. The ACE is the finding that movements are faster when the direction of the movement (e.g., toward) matches the direction of the action in the to-be-judged sentence (e.g., Art gave you the pen describes action toward you). We report on a pre-registered, multi-lab replication of one version of the ACE. The results show that none of the 18 labs involved in the study observed a reliable ACE, and that the meta-analytic estimate of the size of the ACE was essentially zero.},
  langid = {english}
}

@article{morris_using_2019,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = {2019},
  month = may,
  journal = {Statistics in Medicine},
  volume = {38},
  number = {11},
  eprint = {1712.03198},
  eprinttype = {arxiv},
  pages = {2074--2102},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.8086},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudorandom sampling. The key strength of simulation studies is the ability to understand the behaviour of statistical methods because some 'truth' (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analysed and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting and presentation. In particular, this tutorial provides: a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods and performance measures ('ADEMP'); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine that included at least one simulation study and identify areas for improvement.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology}
}

@article{morse_significance_1995,
  title = {The {{Significance}} of {{Saturation}}},
  author = {Morse, Janice M.},
  year = {1995},
  month = may,
  journal = {Qualitative Health Research},
  volume = {5},
  number = {2},
  pages = {147--149},
  publisher = {{SAGE Publications Inc}},
  issn = {1049-7323},
  doi = {10.1177/104973239500500201},
  langid = {english}
}

@article{moshontz_psychological_2018,
  title = {The {{Psychological Science Accelerator}}: {{Advancing}} Psychology through a Distributed Collaborative Network},
  shorttitle = {The {{Psychological Science Accelerator}}},
  author = {Moshontz, Hannah and Campbell, Lorne and Ebersole, Charles R. and IJzerman, Hans and Urry, Heather L. and Forscher, Patrick S. and Grahe, Jon E. and McCarthy, Randy J. and Musser, Erica D. and Antfolk, Jan},
  year = {2018},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {501--515},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}},
  doi = {10.1177/2515245918797607}
}

@article{mrozek_what_2002,
  title = {What Determines the Value of Life? A Meta-Analysis},
  shorttitle = {What Determines the Value of Life?},
  author = {Mrozek, Janusz R. and Taylor, Laura O.},
  year = {2002},
  journal = {Journal of Policy Analysis and Management},
  volume = {21},
  number = {2},
  pages = {253--270},
  issn = {1520-6688},
  doi = {10.1002/pam.10026},
  abstract = {A large literature has developed in which labor market contracts are used to estimate the value of a statistical life (VSL). Reported estimates of the VSL vary substantially, from less than \$100,000 to more than \$25 million. This research uses meta-analysis to quantitatively assess the VSL literature. Results from existing studies are pooled to identify the systematic relationships between VSL estimates and each study's particular features, such as the sample composition and research methods. This meta-analysis suggests that a VSL range of approximately \$1.5 million to \$2.5 million (in 1998 dollars) is what can be reasonably inferred from past labor-market studies when ``best practice'' assumptions are invoked. This range is considerably below many previous qualitative reviews of this literature. \textcopyright{} 2002 by the Association for Public Policy Analysis and Management.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pam.10026}
}

@article{mudge_setting_2012,
  title = {Setting an {{Optimal}} {$\alpha$} {{That Minimizes Errors}} in {{Null Hypothesis Significance Tests}}},
  author = {Mudge, Joseph F. and Baker, Leanne F. and Edge, Christopher B. and Houlahan, Jeff E.},
  year = {2012},
  month = feb,
  journal = {PLOS ONE},
  volume = {7},
  number = {2},
  pages = {e32734},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0032734},
  abstract = {Null hypothesis significance testing has been under attack in recent years, partly owing to the arbitrary nature of setting {$\alpha$} (the decision-making threshold and probability of Type I error) at a constant value, usually 0.05. If the goal of null hypothesis testing is to present conclusions in which we have the highest possible confidence, then the only logical decision-making threshold is the value that minimizes the probability (or occasionally, cost) of making errors. Setting {$\alpha$} to minimize the combination of Type I and Type II error at a critical effect size can easily be accomplished for traditional statistical tests by calculating the {$\alpha$} associated with the minimum average of {$\alpha$} and {$\beta$} at the critical effect size. This technique also has the flexibility to incorporate prior probabilities of null and alternate hypotheses and/or relative costs of Type I and Type II errors, if known. Using an optimal {$\alpha$} results in stronger scientific inferences because it estimates and minimizes both Type I errors and relevant Type II errors for a test. It also results in greater transparency concerning assumptions about relevant effect size(s) and the relative costs of Type I and II errors. By contrast, the use of {$\alpha$} = 0.05 results in arbitrary decisions about what effect sizes will likely be considered significant, if real, and results in arbitrary amounts of Type II error for meaningful potential effect sizes. We cannot identify a rationale for continuing to arbitrarily use {$\alpha$} = 0.05 for null hypothesis significance tests in any field, when it is possible to determine an optimal {$\alpha$}.},
  keywords = {Agricultural soil science,Decision making,Experimental design,Freshwater fish,Gene expression,Lakes,Research errors,Shores}
}

@article{mullan_town_1985,
  title = {The Town Meeting for Technology: {{The}} Maturation of Consensus Conferences},
  shorttitle = {The {{Town Meeting}} for {{Technology}}},
  author = {Mullan, Fitzhugh and Jacoby, Itzhak},
  year = {1985},
  month = aug,
  journal = {JAMA},
  volume = {254},
  number = {8},
  pages = {1068--1072},
  issn = {0098-7484},
  doi = {10.1001/jama.1985.03360080080035},
  abstract = {DURING the past 7 1/2 years, the National Institutes of Health (NIH) has sponsored 50 consensus development conferences assessing a wide diversity of important biomedical topics.The first five years of this new effort were a time of experimentation. Formats and approaches of these first-generation conferences changed to some extent with new topics. Gradually, however, a set of common principles emerged for conducting a consensus conference effectively. Starting in 1982, the second generation of consensus conferences were held in conformance with these principles. Careful assessment of these more recent experiences stimulated plans for a new approach to this effort, involving formal methods for data synthesis. At the onset of a new generation of consensus conferences, it is worthwhile to examine the evolution of the consensus development process to its current status and its potential for further growth as a part of the complex decision-making apparatus of our health care system.}
}

@book{murphy_statistical_2014,
  title = {Statistical Power Analysis: A Simple and General Model for Traditional and Modern Hypothesis Tests},
  shorttitle = {Statistical Power Analysis},
  author = {Murphy, Kevin R. and Myors, Brett and Wolach, Allen H.},
  year = {2014},
  edition = {Fourth edition},
  publisher = {{Routledge, Taylor \& Francis Group}},
  address = {{New York}},
  isbn = {978-1-84872-587-4 978-1-84872-588-1},
  lccn = {QA277 .M87 2014},
  keywords = {Statistical hypothesis testing,Statistical power analysis}
}

@article{murphy_testing_1999,
  title = {Testing the Hypothesis That Treatments Have Negligible Effects: {{Minimum-effect}} Tests in the General Linear Model.},
  shorttitle = {Testing the Hypothesis That Treatments Have Negligible Effects},
  author = {Murphy, Kevin R. and Myors, Brett},
  year = {1999},
  journal = {Journal of Applied Psychology},
  volume = {84},
  number = {2},
  pages = {234--248},
  doi = {10.1037/0021-9010.84.2.234}
}

@article{neyman_inductive_1957,
  title = {"{{Inductive Behavior}}" as a {{Basic Concept}} of {{Philosophy}} of {{Science}}},
  author = {Neyman, Jerzy},
  year = {1957},
  journal = {Revue de l'Institut International de Statistique / Review of the International Statistical Institute},
  volume = {25},
  number = {1/3},
  pages = {7},
  issn = {03731138},
  doi = {10.2307/1401671}
}

@article{neyman_problem_1933,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses.},
  author = {Neyman, Jerzy and Pearson, E. S.},
  year = {1933},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
  volume = {231},
  number = {694-706},
  pages = {289--337},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.1933.0009},
  langid = {english}
}

@article{nickerson_null_2000,
  title = {Null Hypothesis Significance Testing: {{A}} Review of an Old and Continuing Controversy.},
  shorttitle = {Null Hypothesis Significance Testing},
  author = {Nickerson, Raymond S.},
  year = {2000},
  journal = {Psychological Methods},
  volume = {5},
  number = {2},
  pages = {241--301},
  issn = {1082-989X},
  doi = {10.1037//1082-989X.5.2.241},
  langid = {english}
}

@book{niiniluoto_critical_1999,
  title = {Critical {{Scientific Realism}}},
  author = {Niiniluoto, Ilkka},
  year = {1999},
  publisher = {{Oxford University Press}},
  abstract = {Ilkka Niiniluoto comes to the rescue of scientific realism, showing that reports of its death have been greatly exaggerated. Philosophical realism holds that the aim of a particular discourse is to make true statements about its subject-matter. Niiniluoto surveys the different varieties ofrealism in ontology, semantics, epistemology, theory construction, and methodology. He then sets out his own original version, and defends it against competing theories in the philosophy of science. Niiniluoto's critical scientific realism is founded upon the notion of truth as correspondencebetween language and reality, and characterizes scientific progress in terms of increasing truthlikeness. This makes it possible not only to take seriously, but also to make precise, the troublesome idea that scientific theories typically are false but nevertheless close to the truth.},
  googlebooks = {Ng\_p\_3XCHxAC},
  isbn = {978-0-19-823833-1},
  langid = {english},
  keywords = {Philosophy / Metaphysics,Science / Philosophy \& Social Aspects}
}

@article{niiniluoto_verisimilitude_1998,
  title = {Verisimilitude: {{The Third Period}}},
  author = {Niiniluoto, Ilkka},
  year = {1998},
  journal = {The British Journal for the Philosophy of Science},
  volume = {49},
  pages = {1--29}
}

@article{norman_interpretation_2003,
  title = {Interpretation of Changes in Health-Related Quality of Life: The Remarkable Universality of Half a Standard Deviation},
  shorttitle = {Interpretation of Changes in Health-Related Quality of Life},
  author = {Norman, Geoffrey R. and Sloan, Jeff A. and Wyrwich, Kathleen W.},
  year = {2003},
  month = may,
  journal = {Medical Care},
  volume = {41},
  number = {5},
  pages = {582--592},
  issn = {0025-7079},
  doi = {10.1097/01.MLR.0000062554.74615.4C},
  abstract = {BACKGROUND: A number of studies have computed the minimally important difference (MID) for health-related quality of life instruments. OBJECTIVE: To determine whether there is consistency in the magnitude of MID estimates from different instruments. METHODS: We conducted a systematic review of the literature to identify studies that computed an MID and contained sufficient information to compute an effect size (ES). Thirty-eight studies fulfilled the criteria, resulting in 62 ESs. RESULTS: For all but 6 studies, the MID estimates were close to one half a SD (mean = 0.495, SD = 0.155). There was no consistent relationship with factors such as disease-specific or generic instrument or the number of response options. Negative changes were not associated with larger ESs. Population-based estimation procedures and brief follow-up were associated with smaller ESs, and acute conditions with larger ESs. An explanation for this consistency is that research in psychology has shown that the limit of people's ability to discriminate over a wide range of tasks is approximately 1 part in 7, which is very close to half a SD. CONCLUSION: In most circumstances, the threshold of discrimination for changes in health-related quality of life for chronic diseases appears to be approximately half a SD.},
  langid = {english},
  pmid = {12719681},
  keywords = {Chronic Disease,Data Interpretation; Statistical,Humans,Life Change Events,Psychometrics,Quality of Life,Self Efficacy,Sickness Impact Profile,Surveys and Questionnaires,Treatment Outcome,United States}
}

@article{norman_truly_2004,
  title = {The Truly Remarkable Universality of Half a Standard Deviation: Confirmation through Another Look},
  shorttitle = {The Truly Remarkable Universality of Half a Standard Deviation},
  author = {Norman, Geoffrey R. and Sloan, Jeff A. and Wyrwich, Kathleen W.},
  year = {2004},
  journal = {Expert review of pharmacoeconomics \& outcomes research},
  volume = {4},
  number = {5},
  pages = {581--585}
}

@article{nowok_synthpop_2016,
  title = {Synthpop: {{Bespoke Creation}} of {{Synthetic Data}} in {{R}}},
  shorttitle = {Synthpop},
  author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
  year = {2016},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {74},
  number = {1},
  pages = {1--26},
  issn = {1548-7660},
  doi = {10.18637/jss.v074.i11},
  copyright = {Copyright (c) 2016 Beata Nowok, Gillian M. Raab, Chris Dibben},
  langid = {english},
  keywords = {CART,disclosure control,R,synthetic data,UK longitudinal studies}
}

@article{nuijten_prevalence_2015,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985\textendash 2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and {van Assen}, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2015},
  month = oct,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2},
  langid = {english}
}

@article{nunnally_place_1960,
  title = {The Place of Statistics in Psychology},
  author = {Nunnally, Jum},
  year = {1960},
  journal = {Educational and Psychological Measurement},
  volume = {20},
  number = {4},
  pages = {641--650},
  doi = {10.1177/001316446002000401},
  annotation = {00190}
}

@article{obels_analysis_2020,
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  author = {Obels, Pepijn and Lakens, Dani{\"e}l and Coles, Nicholas A. and Gottfried, Jaroslav and Green, Seth A.},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {229--237},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920918872},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to reuse or check published research. However, these benefits will emerge only if researchers can reproduce the analyses reported in published articles and if data are annotated well enough so that it is clear what all variable and value labels mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify those that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature from 2014 to 2018 and attempted to independently computationally reproduce the main results in each article. Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available. Both data and code for 36 of the articles were shared. We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles. Although the percentage of articles for which both data and code were shared (36 out of 62, or 58\%) and the percentage of articles for which main results could be computationally reproduced (21 out of 36, or 58\%) were relatively high compared with the percentages found in other studies, there is clear room for improvement. We provide practical recommendations based on our observations and cite examples of good research practices in the studies whose main results we reproduced.},
  langid = {english}
}

@article{odonnell_registered_2018,
  title = {Registered {{Replication Report}}: {{Dijksterhuis}} and van {{Knippenberg}} (1998)},
  shorttitle = {Registered {{Replication Report}}},
  author = {O'Donnell, Michael and Nelson, Leif D. and Ackermann, Evi and Aczel, Balazs and Akhtar, Athfah and Aldrovandi, Silvio and Alshaif, Nasseem and Andringa, Ronald and Aveyard, Mark and Babincak, Peter and Balatekin, Nursena and Baldwin, Scott A. and Banik, Gabriel and Baskin, Ernest and Bell, Raoul and Bia{\l}obrzeska, Olga and Birt, Angie R. and Boot, Walter R. and Braithwaite, Scott R. and Briggs, Jessie C. and Buchner, Axel and Budd, Desiree and Budzik, Kathryn and Bullens, Lottie and Bulley, Richard L. and Cannon, Peter R. and Cantarero, Katarzyna and Cesario, Joseph and Chambers, Stephanie and Chartier, Christopher R. and Chekroun, Peggy and Chong, Clara and Cleeremans, Axel and Coary, Sean P. and Coulthard, Jacob and Cramwinckel, Florien M. and Denson, Thomas F. and {D{\'i}az-Lago}, Marcos and DiDonato, Theresa E. and Drummond, Aaron and Eberlen, Julia and Ebersbach, Titus and Edlund, John E. and Finnigan, Katherine M. and Fisher, Justin and Frankowska, Natalia and {Garc{\'i}a-S{\'a}nchez}, Efra{\'i}n and Golom, Frank D. and Graves, Andrew J. and Greenberg, Kevin and Hanioti, Mando and Hansen, Heather A. and Harder, Jenna A. and Harrell, Erin R. and Hartanto, Andree and Inzlicht, Michael and Johnson, David J. and Karpinski, Andrew and Keller, Victor N. and Klein, Olivier and Koppel, Lina and Krahmer, Emiel and Lantian, Anthony and Larson, Michael J. and L{\'e}gal, Jean-Baptiste and Lucas, Richard E. and Lynott, Dermot and Magaldino, Corey M. and Massar, Karlijn and McBee, Matthew T. and McLatchie, Neil and Melia, Nadhilla and Mensink, Michael C. and Mieth, Laura and {Moore-Berg}, Samantha and Neeser, Geraldine and Newell, Ben R. and Noordewier, Marret K. and Ali {\"O}zdo{\u g}ru, Asil and Pantazi, Myrto and Parzuchowski, Micha{\l} and Peters, Kim and Philipp, Michael C. and Pollmann, Monique M. H. and Rentzelas, Panagiotis and {Rodr{\'i}guez-Bail{\'o}n}, Rosa and Philipp R{\"o}er, Jan and Ropovik, Ivan and Roque, Nelson A. and Rueda, Carolina and Rutjens, Bastiaan T. and Sackett, Katey and Salamon, Janos and {S{\'a}nchez-Rodr{\'i}guez}, {\'A}ngel and Saunders, Blair and Schaafsma, Juliette and {Schulte-Mecklenbeck}, Michael and Shanks, David R. and Sherman, Martin F. and Steele, Kenneth M. and Steffens, Niklas K. and Sun, Jessie and Susa, Kyle J. and Szaszi, Barnabas and Szollosi, Aba and Tamayo, Ricardo M. and Tingh{\"o}g, Gustav and Tong, Yuk-yue and Tweten, Carol and Vadillo, Miguel A. and Valcarcel, Deisy and {Van der Linden}, Nicolas and {van Elk}, Michiel and {van Harreveld}, Frenk and V{\"a}stfj{\"a}ll, Daniel and Vazire, Simine and Verduyn, Philippe and Williams, Matt N. and Willis, Guillermo B. and Wood, Sarah E. and Yang, Chunliang and Zerhouni, Oulmann and Zheng, Robert and Zrubka, Mark},
  year = {2018},
  month = mar,
  journal = {Perspectives on Psychological Science},
  volume = {13},
  number = {2},
  pages = {268--294},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691618755704},
  abstract = {Dijksterhuis and van Knippenberg (1998) reported that participants primed with a category associated with intelligence (``professor'') subsequently performed 13\% better on a trivia test than participants primed with a category associated with a lack of intelligence (``soccer hooligans''). In two unpublished replications of this study designed to verify the appropriate testing procedures, Dijksterhuis, van Knippenberg, and Holland observed a smaller difference between conditions (2\%\textendash 3\%) as well as a gender difference: Men showed the effect (9.3\% and 7.6\%), but women did not (0.3\% and -0.3\%). The procedure used in those replications served as the basis for this multilab Registered Replication Report. A total of 40 laboratories collected data for this project, and 23 of these laboratories met all inclusion criteria. Here we report the meta-analytic results for those 23 direct replications (total N = 4,493), which tested whether performance on a 30-item general-knowledge trivia task differed between these two priming conditions (results of supplementary analyses of the data from all 40 labs, N = 6,454, are also reported). We observed no overall difference in trivia performance between participants primed with the ``professor'' category and those primed with the ``hooligan'' category (0.14\%) and no moderation by gender.},
  langid = {english},
  keywords = {intelligence,priming,replication}
}

@article{okada_is_2013,
  title = {Is {{Omega Squared Less Biased}}? A {{Comparison}} of {{Three Major Effect Size Indices}} in {{One-Way Anova}}},
  shorttitle = {Is {{Omega Squared Less Biased}}?},
  author = {Okada, Kensuke},
  year = {2013},
  month = jul,
  journal = {Behaviormetrika},
  volume = {40},
  number = {2},
  pages = {129--147},
  issn = {0385-7417, 1349-6964},
  doi = {10.2333/bhmk.40.129},
  langid = {english}
}

@article{olejnik_generalized_2003,
  title = {Generalized {{Eta}} and {{Omega Squared Statistics}}: {{Measures}} of {{Effect Size}} for {{Some Common Research Designs}}.},
  shorttitle = {Generalized {{Eta}} and {{Omega Squared Statistics}}},
  author = {Olejnik, Stephen and Algina, James},
  year = {2003},
  journal = {Psychological Methods},
  volume = {8},
  number = {4},
  pages = {434--447},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.8.4.434},
  langid = {english}
}

@article{olsson-collentine_heterogeneity_2020,
  title = {Heterogeneity in Direct Replications in Psychology and Its Association with Effect Size},
  author = {{Olsson-Collentine}, Anton and Wicherts, Jelte M. and {van Assen}, Marcel A. L. M.},
  year = {2020},
  journal = {Psychological Bulletin},
  volume = {146},
  number = {10},
  pages = {922--940},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/bul0000294},
  abstract = {We examined the evidence for heterogeneity (of effect sizes) when only minor changes to sample population and settings were made between studies and explored the association between heterogeneity and average effect size in a sample of 68 meta-analyses from 13 preregistered multilab direct replication projects in social and cognitive psychology. Among the many examined effects, examples include the Stroop effect, the ``verbal overshadowing'' effect, and various priming effects such as ``anchoring'' effects. We found limited heterogeneity; 48/68 (71\%) meta-analyses had nonsignificant heterogeneity, and most (49/68; 72\%) were most likely to have zero to small heterogeneity. Power to detect small heterogeneity (as defined by Higgins, Thompson, Deeks, \& Altman, 2003) was low for all projects (mean 43\%), but good to excellent for medium and large heterogeneity. Our findings thus show little evidence of widespread heterogeneity in direct replication studies in social and cognitive psychology, suggesting that minor changes in sample population and settings are unlikely to affect research outcomes in these fields of psychology. We also found strong correlations between observed average effect sizes (standardized mean differences and log odds ratios) and heterogeneity in our sample. Our results suggest that heterogeneity and moderation of effects is unlikely for a 0 average true effect size, but increasingly likely for larger average true effect size. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Cognitive Psychology,Experimental Laboratories,Experimental Replication,Homogeneity of Variance,Meta Analysis,Priming,Psychology,Social Psychology,Stroop Effect}
}

@article{orben_crud_2020,
  title = {Crud ({{Re}}){{Defined}}},
  shorttitle = {Crud ({{Re}}){{Defined}}},
  author = {Orben, Amy and Lakens, Dani{\"e}l},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  doi = {10.1177/2515245920917961},
  abstract = {The idea that in behavioral research everything correlates with everything else was a niche area of the scientific literature for more than half a century. With...},
  langid = {english}
}

@misc{panzarella_denouncing_2020,
  title = {Denouncing the {{Use}} of {{Field-Specific Effect Size Distributions}} to {{Inform Magnitude}}},
  author = {Panzarella, Emily and Beribisky, Nataly and Cribbie, Rob},
  year = {2020},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/5k64e},
  abstract = {An effect size (ES) provides valuable information regarding the magnitude of effects, with the interpretation of magnitude being the most important. Interpreting ES magnitude requires combining information from the numerical ES value and the context of the research. However, many researchers adopt popular benchmarks such as those proposed by Cohen. More recently, researchers have proposed interpreting ES magnitude relative to the distribution of observed ESs in a specific field, creating unique benchmarks for declaring effects small, medium or large. However, there is no valid rationale whatsoever for this approach. This study was carried out in two parts: 1) We identified articles that proposed the use of field-specific ES distributions to interpret magnitude (primary articles); and 2) We identified articles that cited the primary articles and classified them by year and publication type. The first type consisted of methodological papers. The second type included articles that interpreted ES magnitude using the approach proposed in the primary articles. There has been a steady increase in the number of methodological and substantial articles discussing or adopting the approach of interpreting ES magnitude by considering the distribution of observed ES in that field, even though the approach is devoid of a theoretical framework. It is hoped that this research will restrict the practice of interpreting ES magnitude relative to the distribution of ES values in a field and instead encourage researchers to interpret such by considering the specific context of the study.},
  keywords = {Cohen's d,effect size,effect size distribution,effect size magnitude,Pearson's r,psychology,quantitative methods,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods}
}

@article{parker_sample_2003,
  title = {Sample {{Size}}},
  author = {Parker, Robert A and Berman, Nancy G},
  year = {2003},
  month = aug,
  journal = {The American Statistician},
  volume = {57},
  number = {3},
  pages = {166--170},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1198/0003130031919},
  abstract = {Conventionally, sample size calculations are viewed as calculations determining the right number of subjects needed for a study. Such calculations follow the classical paradigm: ``for a difference X, I need sample size Y.'' We argue that the paradigm ``for a sample size Y, I get information Z'' is more appropriate for many studies and reflects the information needed by scientists when planning a study. This approach applies to both physiological studies and Phase I and II interventional studies. We provide actual examples from our own consulting work to demonstrate this. We conclude that sample size should be viewed not as a unique right number, but rather as a factor needed to assess the utility of a study.}
}

@article{parkhurst_statistical_2001,
  title = {Statistical Significance Tests: {{Equivalence}} and Reverse Tests Should Reduce Misinterpretation},
  shorttitle = {Statistical {{Significance Tests}}},
  author = {Parkhurst, David F.},
  year = {2001},
  journal = {Bioscience},
  volume = {51},
  number = {12},
  pages = {1051--1057},
  doi = {10.1641/0006-3568(2001)051[1051:SSTEAR]2.0.CO;2}
}

@article{parsons_psychological_2019,
  title = {Psychological {{Science Needs}} a {{Standard Practice}} of {{Reporting}} the {{Reliability}} of {{Cognitive-Behavioral Measurements}}},
  author = {Parsons, Sam and Kruijt, Anne-Wil and Fox, Elaine},
  year = {2019},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {4},
  pages = {378--395},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919879695},
  abstract = {Psychological science relies on behavioral measures to assess cognitive processing; however, the field has not yet developed a tradition of routinely examining the reliability of these behavioral measures. Reliable measures are essential to draw robust inferences from statistical analyses, and subpar reliability has severe implications for measures' validity and interpretation. Without examining and reporting the reliability of measurements used in an analysis, it is nearly impossible to ascertain whether results are robust or have arisen largely from measurement error. In this article, we propose that researchers adopt a standard practice of estimating and reporting the reliability of behavioral assessments of cognitive processing. We illustrate the need for this practice using an example from experimental psychopathology, the dot-probe task, although we argue that reporting reliability is relevant across fields (e.g., social cognition and cognitive psychology). We explore several implications of low measurement reliability and the detrimental impact that failure to assess measurement reliability has on interpretability and comparison of results and therefore research quality. We argue that researchers in the field of cognition need to report measurement reliability as routine practice so that more reliable assessment tools can be developed. To provide some guidance on estimating and reporting reliability, we describe the use of bootstrapped split-half estimation and intraclass correlation coefficients to estimate internal consistency and test-retest reliability, respectively. For future researchers to build upon current results, it is imperative that all researchers provide psychometric information sufficient for estimating the accuracy of inferences and informing further development of cognitive-behavioral assessments.},
  langid = {english},
  keywords = {cognitive-behavioral tasks,estimating and reporting,open materials,psychometrics,reliability}
}

@book{pawitan_all_2001,
  title = {In All Likelihood: Statistical Modelling and Inference Using Likelihood},
  shorttitle = {In All Likelihood},
  author = {Pawitan, Yudi},
  year = {2001},
  publisher = {{Clarendon Press ; Oxford University Press}},
  address = {{Oxford : New York}},
  isbn = {978-0-19-850765-9},
  lccn = {QA276 .P286 2001},
  keywords = {Mathematical statistics}
}

@article{perneger_whats_1998,
  title = {What's Wrong with {{Bonferroni}} Adjustments},
  author = {Perneger, Thomas V.},
  year = {1998},
  journal = {Bmj},
  volume = {316},
  number = {7139},
  pages = {1236--1238}
}

@article{perugini_practical_2018,
  title = {A {{Practical Primer To Power Analysis}} for {{Simple Experimental Designs}}},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  year = {2018},
  month = jul,
  journal = {International Review of Social Psychology},
  volume = {31},
  number = {1},
  pages = {20},
  issn = {2397-8570},
  doi = {10.5334/irsp.181},
  abstract = {Power analysis is an important tool to use when planning studies. This contribution aims to remind readers what power analysis is, emphasize why it matters, and articulate when and how it should be used. The focus is on applications of power analysis for experimental designs often encountered in psychology, starting from simple two-group independent and paired groups and moving to one-way analysis of variance, factorial designs, contrast analysis, trend analysis, regression analysis, analysis of covariance, and mediation analysis. Special attention is given to the application of power analysis to moderation designs, considering both dichotomous and continuous predictors and moderators. Illustrative practical examples based on G*Power and R packages are provided throughout the article. Annotated code for the examples with R and dedicated computational tools are made freely available at a dedicated web page (https://github.com/mcfanda/primerPowerIRSP). Applications of power analysis for more complex designs are briefly mentioned, and some important general issues related to power analysis are discussed.},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  keywords = {effect size,moderation,power analysis,sensitivity analysis,uncertainty}
}

@article{perugini_safeguard_2014,
  title = {Safeguard Power as a Protection against Imprecise Power Estimates},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  year = {2014},
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {3},
  pages = {319--332},
  doi = {10.1177/1745691614528519}
}

@article{peters_performance_2007,
  title = {Performance of the Trim and Fill Method in the Presence of Publication Bias and Between-Study Heterogeneity},
  author = {Peters, Jaime L. and Sutton, Alex J. and Jones, David R. and Abrams, Keith R. and Rushton, Lesley},
  year = {2007},
  month = nov,
  journal = {Statistics in Medicine},
  volume = {26},
  number = {25},
  pages = {4544--4562},
  issn = {0277-6715},
  doi = {10.1002/sim.2889},
  abstract = {The trim and fill method allows estimation of an adjusted meta-analysis estimate in the presence of publication bias. To date, the performance of the trim and fill method has had little assessment. In this paper, we provide a more comprehensive examination of different versions of the trim and fill method in a number of simulated meta-analysis scenarios, comparing results with those from usual unadjusted meta-analysis models and two simple alternatives, namely use of the estimate from: (i) the largest; or (ii) the most precise study in the meta-analysis. Findings suggest a great deal of variability in the performance of the different approaches. When there is large between-study heterogeneity the trim and fill method can underestimate the true positive effect when there is no publication bias. However, when publication bias is present the trim and fill method can give estimates that are less biased than the usual meta-analysis models. Although results suggest that the use of the estimate from the largest or most precise study seems a reasonable approach in the presence of publication bias, when between-study heterogeneity exists our simulations show that these estimates are quite biased. We conclude that in the presence of publication bias use of the trim and fill method can help to reduce the bias in pooled estimates, even though the performance of this method is not ideal. However, because we do not know whether funnel plot asymmetry is truly caused by publication bias, and because there is great variability in the performance of different trim and fill estimators and models in various meta-analysis scenarios, we recommend use of the trim and fill method as a form of sensitivity analysis as intended by the authors of the method.},
  langid = {english},
  pmid = {17476644},
  keywords = {Computer Simulation,Coronary Restenosis,Data Interpretation; Statistical,Genotype,Humans,Meta-Analysis as Topic,Peptidyl-Dipeptidase A,Publication Bias}
}

@article{phillips_statistical_2001,
  title = {Statistical Significance of Sediment Toxicity Test Results: {{Threshold}} Values Derived by the Detectable Significance Approach},
  shorttitle = {Statistical Significance of Sediment Toxicity Test Results},
  author = {Phillips, Bryn M. and Hunt, John W. and Anderson, Brian S. and Puckett, H. Max and Fairey, Russell and Wilson, Craig J. and Tjeerdema, Ron},
  year = {2001},
  journal = {Environmental Toxicology and Chemistry},
  volume = {20},
  number = {2},
  pages = {371--373},
  issn = {1552-8618},
  doi = {10.1002/etc.5620200218},
  abstract = {A number of methods have been employed to determine the statistical significance of sediment toxicity test results. To allow consistency among comparisons, regardless of among-replicate variability, a protocol-specific approach has been used that considers protocol performance over a large number of comparisons. Ninetieth-percentile minimum significant difference (MSD) values were calculated to determine a critical threshold for statistically significant sample toxicity. Significant toxicity threshold values (as a percentage of laboratory control values) are presented for six species and nine endpoints based on data from as many as 720 stations. These threshold values are useful for interpreting sediment toxicity data from large studies and in eliminating cases where statistical significance is assigned in individual cases because among-replicate variability is small.},
  langid = {english},
  keywords = {Minimum significant difference,Sediment toxicity,Statistics},
  annotation = {\_eprint: https://setac.onlinelibrary.wiley.com/doi/pdf/10.1002/etc.5620200218}
}

@article{pickett_questionable_2017,
  title = {Questionable, {{Objectionable}} or {{Criminal}}? {{Public Opinion}} on {{Data Fraud}} and {{Selective Reporting}} in {{Science}}},
  shorttitle = {Questionable, {{Objectionable}} or {{Criminal}}?},
  author = {Pickett, Justin T. and Roche, Sean Patrick},
  year = {2017},
  month = mar,
  journal = {Science and Engineering Ethics},
  pages = {1--21},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-017-9886-2},
  abstract = {Data fraud and selective reporting both present serious threats to the credibility of science. However, there remains considerable disagreement among scientists about how best to sanction data fraud, and about the ethicality of selective reporting. The public is arguably the largest stakeholder in the reproducibility of science; research is primarily paid for with public funds, and flawed science threatens the public's welfare. Members of the public are able to make meaningful judgments about the morality of different behaviors using moral intuitions. Legal scholars emphasize that to maintain legitimacy, social control policies must be developed with some consideration given to the public's moral intuitions. Although there is a large literature on popular attitudes toward science, there is no existing evidence about public opinion on data fraud or selective reporting. We conducted two studies\textemdash a survey experiment with a nationwide convenience sample (N = 821), and a follow-up survey with a representative sample of US adults (N = 964)\textemdash to explore community members' judgments about the morality of data fraud and selective reporting in science. The findings show that community members make a moral distinction between data fraud and selective reporting, but overwhelmingly judge both behaviors to be immoral and deserving of punishment. Community members believe that scientists who commit data fraud or selective reporting should be fired and banned from receiving funding. For data fraud, most Americans support criminal penalties. Results from an ordered logistic regression analysis reveal few demographic and no significant partisan differences in punitiveness toward data fraud.},
  langid = {english}
}

@article{pocock_group_1977,
  title = {Group Sequential Methods in the Design and Analysis of Clinical Trials},
  author = {Pocock, Stuart J.},
  year = {1977},
  month = aug,
  journal = {Biometrika},
  volume = {64},
  number = {2},
  pages = {191--199},
  issn = {0006-3444},
  doi = {10.1093/biomet/64.2.191},
  abstract = {In clinical trials with sequential patient entry, fixed sample size designs are unjustified on ethical grounds and sequential designs are often impracticable. One solution is a group sequential design dividing patient entry into a number of equal-sized groups so that the decision to stop the trial or continue is based on repeated significance tests of the accumulated data after each group is evaluated. Exact results are obtained for a trial with two treatments and a normal response with known variance. The design problem of determining the required size and number of groups is also considered. Simulation shows that these normal results may be adapted to other types of response data. An example shows that group sequential designs can sometimes be statistically superior to standard sequential designs.}
}

@article{polanin_transparency_2020,
  title = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}: {{A Meta-Review}}},
  shorttitle = {Transparency and {{Reproducibility}} of {{Meta-Analyses}} in {{Psychology}}},
  author = {Polanin, Joshua R. and Hennessy, Emily A. and Tsuji, Sho},
  year = {2020},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {15},
  number = {4},
  pages = {1026--1041},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620906416},
  abstract = {Systematic review and meta-analysis are possible as viable research techniques only through transparent reporting of primary research; thus, one might expect meta-analysts to demonstrate best practice in their reporting of results and have a high degree of transparency leading to reproducibility of their work. This assumption has yet to be fully tested in the psychological sciences. We therefore aimed to assess the transparency and reproducibility of psychological meta-analyses. We conducted a meta-review by sampling 150 studies from Psychological Bulletin to extract information about each review's transparent and reproducible reporting practices. The results revealed that authors reported on average 55\% of criteria and that transparent reporting practices increased over the three decades studied (b = 1.09, SE = 0.24, t = 4.519, p {$<$} .001). Review authors consistently reported eligibility criteria, effect-size information, and synthesis techniques. Review authors, however, on average, did not report specific search results, screening and extraction procedures, and most importantly, effect-size and moderator information from each individual study. Far fewer studies provided statistical code required for complete analytical replication. We argue that the field of psychology and research synthesis in general should require review authors to report these elements in a transparent and reproducible manner.},
  langid = {english}
}

@book{popper_logic_2002,
  title = {{The logic of scientific discovery}},
  author = {Popper, Karl R},
  year = {2002},
  publisher = {{Routledge}},
  address = {{London; New York}},
  abstract = {When first published in 1959, this book revolutionized contemporary thinking about science and knowledge. It remains the one of the most widely read books about science to come out of the twentieth century.},
  isbn = {978-0-203-99462-7 978-0-415-27843-0 978-0-415-27844-7},
  langid = {Translated from the German.}
}

@book{proschan_statistical_2006,
  title = {Statistical Monitoring of Clinical Trials: A Unified Approach},
  shorttitle = {Statistical Monitoring of Clinical Trials},
  author = {Proschan, Michael A. and Lan, K. K. Gordan and Wittes, Janet Turk},
  year = {2006},
  series = {Statistics for Biology and Health},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-0-387-30059-7},
  lccn = {R853.C55 .P76 2006},
  keywords = {Bayes Theorem,Clinical Trials,Data Interpretation; Statistical,Drugs,Statistical methods,Statistics,statistics \& numerical data,Testing}
}

@article{proschan_two-stage_2005,
  title = {Two-{{Stage Sample Size Re-Estimation Based}} on a {{Nuisance Parameter}}: {{A Review}}},
  shorttitle = {Two-{{Stage Sample Size Re-Estimation Based}} on a {{Nuisance Parameter}}},
  author = {Proschan, Michael A.},
  year = {2005},
  month = jul,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {15},
  number = {4},
  pages = {559--574},
  publisher = {{Taylor \& Francis}},
  issn = {1054-3406},
  doi = {10.1081/BIP-200062852},
  abstract = {Sample size calculations are important and difficult in clinical trails because they depend on the nuisance parameter and treatment effect. Recently, much attention has been focused on two-stage methods whereby the first stage constitutes an internal pilot study used to estimate parameters and revise the final sample size. This paper reviews two-stage methods based on estimation of nuisance parameters in either a continuous or dichotomous outcome setting.},
  pmid = {16022163},
  keywords = {Adaptive methods,Ancillary statistic,Blinding,Clinical trials,Conditioning,Continuous outcome,Correlation,Dichotomous outcome,Independence,Internal pilot study,Lumping,Pooling,Power,Restricted design,Unrestricted design},
  annotation = {\_eprint: https://doi.org/10.1081/BIP-200062852}
}

@article{quertemont_how_2011,
  title = {How to {{Statistically Show}} the {{Absence}} of an {{Effect}}},
  author = {Quertemont, Etienne},
  year = {2011},
  month = aug,
  journal = {Psychologica Belgica},
  volume = {51},
  number = {2},
  pages = {109--127},
  issn = {2054-670X, 0033-2879},
  doi = {10.5334/pb-51-2-109},
  annotation = {00015}
}

@book{ravetz_scientific_1995,
  title = {Scientific {{Knowledge}} and {{Its Social Problems}}},
  author = {Ravetz, Jerome},
  year = {1995},
  month = jan,
  edition = {Reprint edition},
  publisher = {{Transaction Publishers}},
  address = {{New Brunswick, N.J}},
  abstract = {Science is continually confronted by new and difficult social and ethical problems. Some of these problems have arisen from the transformation of the academic science of the prewar period into the industrialized science of the present. Traditional theories of science are now widely recognized as obsolete. In Scientific Knowledge and Its Social Problems (originally published in 1971), Jerome R. Ravetz analyzes the work of science as the creation and investigation of problems. He demonstrates the role of choice and value judgment, and the inevitability of error, in scientific research. Ravetz's new introductory essay is a masterful statement of how our understanding of science has evolved over the last two decades.},
  isbn = {978-1-56000-851-4},
  langid = {english}
}

@article{rice_heads_1994,
  title = {'{{Heads I}} Win, Tails You Lose': Testing Directional Alternative Hypotheses in Ecological and Evolutionary Research},
  shorttitle = {'{{Heads I}} Win, Tails You Lose'},
  author = {Rice, W. R. and Gaines, S. D.},
  year = {1994},
  month = jun,
  journal = {Trends in Ecology \& Evolution},
  volume = {9},
  number = {6},
  pages = {235--237},
  issn = {0169-5347},
  doi = {10.1016/0169-5347(94)90258-5},
  abstract = {Whenever experiments make a priori predictions about the direction of change in some parameter, one-tailed test statistics offer a potentially large gain in power over the corresponding two-tailed test. This gain is rarely used in ecology and evolution because of (1) the belief that one-tailed procedures are unavailable for most statistical tests and (2) an inherent dilemma in one-tailed tests: how do we handle large parameter changes in the unanticipated direction? The first problem is a misconception, whereas the second is easily resolved by recognizing that one- and two-tailed tests are simply extremes in a continuum of testing options.},
  langid = {english},
  pmid = {21236837},
  annotation = {00231}
}

@article{richard_one_2003,
  title = {One {{Hundred Years}} of {{Social Psychology Quantitatively Described}}.},
  author = {Richard, F. D. and Bond, Charles F. and {Stokes-Zoota}, Juli J.},
  year = {2003},
  journal = {Review of General Psychology},
  volume = {7},
  number = {4},
  pages = {331--363},
  issn = {1939-1552, 1089-2680},
  doi = {10.1037/1089-2680.7.4.331},
  langid = {english},
  annotation = {00683}
}

@article{richardson_eta_2011,
  title = {Eta Squared and Partial Eta Squared as Measures of Effect Size in Educational Research},
  author = {Richardson, John T.E.},
  year = {2011},
  month = jan,
  journal = {Educational Research Review},
  volume = {6},
  number = {2},
  pages = {135--147},
  issn = {1747938X},
  doi = {10.1016/j.edurev.2010.12.001},
  langid = {english},
  annotation = {00564}
}

@article{rijnsoever_i_2017,
  title = {({{I Can}}'t {{Get No}}) {{Saturation}}: {{A}} Simulation and Guidelines for Sample Sizes in Qualitative Research},
  shorttitle = {({{I Can}}'t {{Get No}}) {{Saturation}}},
  author = {van Rijnsoever, Frank J.},
  year = {2017},
  month = jul,
  journal = {PLOS ONE},
  volume = {12},
  number = {7},
  pages = {e0181689},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0181689},
  abstract = {I explore the sample size in qualitative research that is required to reach theoretical saturation. I conceptualize a population as consisting of sub-populations that contain different types of information sources that hold a number of codes. Theoretical saturation is reached after all the codes in the population have been observed once in the sample. I delineate three different scenarios to sample information sources: ``random chance,'' which is based on probability sampling, ``minimal information,'' which yields at least one new code per sampling step, and ``maximum information,'' which yields the largest number of new codes per sampling step. Next, I use simulations to assess the minimum sample size for each scenario for systematically varying hypothetical populations. I show that theoretical saturation is more dependent on the mean probability of observing codes than on the number of codes in a population. Moreover, the minimal and maximal information scenarios are significantly more efficient than random chance, but yield fewer repetitions per code to validate the findings. I formulate guidelines for purposive sampling and recommend that researchers follow a minimum information scenario.},
  langid = {english},
  keywords = {Computer and information sciences,Data management,Medicine and health sciences,Number theory,Probability theory,Qualitative studies,Simulation and modeling,Social sciences}
}

@article{rogers_using_1993,
  title = {Using Significance Tests to Evaluate Equivalence between Two Experimental Groups.},
  author = {Rogers, James L. and Howard, Kenneth I. and Vessey, John T.},
  year = {1993},
  journal = {Psychological bulletin},
  volume = {113},
  number = {3},
  pages = {553--565},
  doi = {http://dx.doi.org/10.1037/0033-2909.113.3.553},
  annotation = {00477}
}

@book{rosenthal_contrasts_2000,
  title = {Contrasts and Effect Sizes in Behavioral Research: A Correlational Approach},
  shorttitle = {Contrasts and Effect Sizes in Behavioral Research},
  author = {Rosenthal, Robert and Rosnow, Ralph L. and Rubin, Donald B.},
  year = {2000},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, U.K. ; New York}},
  isbn = {978-0-521-65258-2 978-0-521-65980-2},
  langid = {english},
  lccn = {BF39.2.A52 R67 2000},
  keywords = {Analysis of variance,Psychology,Psychometrics,Social sciences,Statistical methods}
}

@article{rosnow_statistical_1989,
  title = {Statistical Procedures and the Justification of Knowledge in Psychological Science.},
  author = {Rosnow, Ralph L. and Rosenthal, Robert},
  year = {1989},
  journal = {American psychologist},
  volume = {44},
  number = {10},
  pages = {1276},
  doi = {10.1037/0003-066X.44.10.1276},
  annotation = {01007}
}

@article{rouder_bayesian_2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  langid = {english},
  annotation = {01475}
}

@article{rouder_bayesian_2009-1,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  year = {2009},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/PBR.16.2.225},
  langid = {english},
  annotation = {01475}
}

@book{royall_statistical_1997,
  title = {Statistical {{Evidence}}: {{A Likelihood Paradigm}}},
  shorttitle = {Statistical {{Evidence}}},
  author = {Royall, Richard},
  year = {1997},
  month = jun,
  publisher = {{Chapman and Hall/CRC}},
  address = {{London ; New York}},
  abstract = {Interpreting statistical data as evidence, Statistical Evidence: A Likelihood Paradigm focuses on the law of likelihood, fundamental to solving many of the problems associated with interpreting data in this way. Statistics has long neglected this principle, resulting in a seriously defective methodology. This book redresses the balance, explaining why science has clung to a defective methodology despite its well-known defects. After examining the strengths and weaknesses of the work of Neyman and Pearson and the Fisher paradigm, the author proposes an alternative paradigm which provides, in the law of likelihood, the explicit concept of evidence missing from the other paradigms. At the same time, this new paradigm retains the elements of objective measurement and control of the frequency of misleading results, features which made the old paradigms so important to science. The likelihood paradigm leads to statistical methods that have a compelling rationale and an elegant simplicity,  no longer forcing the reader to choose between frequentist and Bayesian statistics.},
  isbn = {978-0-412-04411-3},
  langid = {english}
}

@article{rozeboom_fallacy_1960,
  title = {The Fallacy of the Null-Hypothesis Significance Test.},
  author = {Rozeboom, William W.},
  year = {1960},
  journal = {Psychological bulletin},
  volume = {57},
  number = {5},
  pages = {416--428},
  doi = {10.1037/h0042040},
  annotation = {00807}
}

@article{rucker_undue_2008,
  title = {Undue Reliance on {{I}}(2) in Assessing Heterogeneity May Mislead},
  author = {R{\"u}cker, Gerta and Schwarzer, Guido and Carpenter, James R. and Schumacher, Martin},
  year = {2008},
  month = nov,
  journal = {BMC medical research methodology},
  volume = {8},
  pages = {79},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-8-79},
  abstract = {BACKGROUND: The heterogeneity statistic I(2), interpreted as the percentage of variability due to heterogeneity between studies rather than sampling error, depends on precision, that is, the size of the studies included. METHODS: Based on a real meta-analysis, we simulate artificially 'inflating' the sample size under the random effects model. For a given inflation factor M = 1, 2, 3,... and for each trial i, we create a M-inflated trial by drawing a treatment effect estimate from the random effects model, using s(i)(2)/M as within-trial sampling variance. RESULTS: As precision increases, while estimates of the heterogeneity variance tau(2) remain unchanged on average, estimates of I(2) increase rapidly to nearly 100\%. A similar phenomenon is apparent in a sample of 157 meta-analyses. CONCLUSION: When deciding whether or not to pool treatment estimates in a meta-analysis, the yard-stick should be the clinical relevance of any heterogeneity present. tau(2), rather than I(2), is the appropriate measure for this purpose.},
  langid = {english},
  pmcid = {PMC2648991},
  pmid = {19036172},
  keywords = {Data Interpretation; Statistical,Humans,Models; Statistical,Randomized Controlled Trials as Topic,Reproducibility of Results,Research Design,Sample Size,Sensitivity and Specificity}
}

@article{scheel_excess_2021,
  title = {An Excess of Positive Results:  {{Comparing}} the Standard Psychology Literature with Registered Reports},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Dani{\"e}l},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {25152459211007467},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/25152459211007467},
  abstract = {Selectively publishing results that support the tested hypotheses (``positive'' results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  langid = {english},
  keywords = {hypothesis testing,open data,preregistered,publication bias,Registered Reports}
}

@article{scheel_why_2020,
  title = {Why {{Hypothesis Testers Should Spend Less Time Testing Hypotheses}}},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Dani{\"e}l},
  year = {2020},
  month = dec,
  journal = {Perspectives on Psychological Science},
  pages = {1745691620966795},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620966795},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound ``derivation chain'' between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology's reform movement and help us to develop strong, testable theories, as Paul Meehl urged.},
  langid = {english},
  keywords = {exploratory research,hypothesis testing,replication crisis}
}

@article{scheel_why_2021,
  title = {Why {{Hypothesis Testers Should Spend Less Time Testing Hypotheses}}},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Dani{\"e}l},
  year = {2021},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {4},
  pages = {744--755},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620966795},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound ``derivation chain'' between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology's reform movement and help us to develop strong, testable theories, as Paul Meehl urged.},
  langid = {english},
  keywords = {exploratory research,hypothesis testing,replication crisis}
}

@article{schimmack_ironic_2012,
  title = {The Ironic Effect of Significant Results on the Credibility of Multiple-Study Articles.},
  author = {Schimmack, Ulrich},
  year = {2012},
  journal = {Psychological Methods},
  volume = {17},
  number = {4},
  pages = {551--566},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0029487},
  abstract = {Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power has not improved. At the same time, the number of studies has increased from a single study to multiple studies within a single article. It has been overlooked that multiple study articles are severely underpowered because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple study articles undermine the credibility of the reported results and it is likely that questionable research practices contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple study articles is illustrated using Bem's (2011) article on extrasensory perception and Gailliot et al.'s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with non-significant results, if these studies had high power to replicate a published finding.},
  langid = {english},
  annotation = {00000}
}

@article{schnuerch_controlling_2020,
  title = {Controlling Decision Errors with Minimal Costs: {{The}} Sequential Probability Ratio t Test.},
  shorttitle = {Controlling Decision Errors with Minimal Costs},
  author = {Schnuerch, Martin and Erdfelder, Edgar},
  year = {2020},
  journal = {Psychological methods},
  volume = {25},
  number = {2},
  pages = {206--226},
  publisher = {{American Psychological Association}},
  doi = {10.1037/met0000234},
  abstract = {For several years, the public debate in psychological science has been dominated by what is referred to as the reproducibility crisis. This crisis has, inter alia, drawn attention to the need for proper control of statistical decision errors in testing psychological hypotheses. However, conventional methods of error probability control often require fairly large samples. Sequential statistical tests provide an attractive alternative: They can be applied repeatedly during the sampling process and terminate whenever there is sufficient evidence in the data for one of the hypotheses of interest. Thus, sequential tests may substantially reduce the required sample size without compromising predefined error probabilities. Herein, we discuss the most efficient sequential design, the sequential probability ratio test (SPRT), and show how it is easily implemented for a 2-sample t test using standard statistical software. We demonstrate, by means of simulations, that the SPRT not only reliably controls error probabilities but also typically requires substantially smaller samples than standard t tests and other common sequential designs. Moreover, we investigate the robustness of the SPRT against violations of its assumptions. Finally, we illustrate the sequential t test by applying it to an empirical example and provide recommendations on how psychologists can employ it in their own research to benefit from its desirable properties.}
}

@article{schoemann_determining_2017,
  title = {Determining {{Power}} and {{Sample Size}} for {{Simple}} and {{Complex Mediation Models}}},
  author = {Schoemann, Alexander M. and Boulton, Aaron J. and Short, Stephen D.},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {379--386},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550617715068},
  abstract = {Mediation analyses abound in social and personality psychology. Current recommendations for assessing power and sample size in mediation models include using a Monte Carlo power analysis simulation and testing the indirect effect with a bootstrapped confidence interval. Unfortunately, these methods have rarely been adopted by researchers due to limited software options and the computational time needed. We propose a new method and convenient tools for determining sample size and power in mediation models. We demonstrate our new method through an easy-to-use application that implements the method. These developments will allow researchers to quickly and easily determine power and sample size for simple and complex mediation models.},
  langid = {english},
  keywords = {mediation,power,R,sample size}
}

@article{schoenfeld_procon_2005,
  title = {Pro/Con Clinical Debate: {{It}} Is Acceptable to Stop Large Multicentre Randomized Controlled Trials at Interim Analysis for Futility},
  shorttitle = {Pro/Con Clinical Debate},
  author = {Schoenfeld, David A and Meade, Maureen O},
  year = {2005},
  journal = {Critical Care},
  volume = {9},
  number = {1},
  pages = {34--36},
  issn = {1364-8535},
  doi = {10.1186/cc3013},
  abstract = {A few recent, large, well-publicized trials in critical care medicine have been stopped for futility. In the critical care setting, stopping for futility means that independent review committees have elected to stop the trial early \textendash{} based on predetermined rules \textendash{} since the likelihood of finding a treatment effect is low. For bedside clinicians the idea of futility in a clinical trial can be confusing. In the present article, two experts in the conduct of clinical trials debate the role of futility-stopping rules.},
  pmcid = {PMC1065108},
  pmid = {15693981}
}

@article{schonbrodt_at_2013,
  title = {At What Sample Size Do Correlations Stabilize?},
  author = {Sch{\"o}nbrodt, Felix D. and Perugini, Marco},
  year = {2013},
  month = oct,
  journal = {Journal of Research in Personality},
  volume = {47},
  number = {5},
  pages = {609--612},
  issn = {00926566},
  doi = {10.1016/j.jrp.2013.05.009},
  langid = {english},
  annotation = {00000}
}

@article{schonbrodt_sequential_2017,
  title = {Sequential Hypothesis Testing with {{Bayes}} Factors: {{Efficiently}} Testing Mean Differences},
  shorttitle = {Sequential Hypothesis Testing with {{Bayes}} Factors},
  author = {Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
  year = {2017},
  month = jun,
  journal = {Psychological Methods},
  volume = {22},
  number = {2},
  pages = {322--339},
  issn = {1939-1463},
  doi = {10.1037/MET0000061},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference. (PsycINFO Database Record},
  langid = {english},
  pmid = {26651986},
  keywords = {Bayes Theorem,Data Interpretation; Statistical,Humans,Probability,Research Design,Sample Size},
  annotation = {00070}
}

@article{schuirmann_comparison_1987,
  title = {A Comparison of the Two One-Sided Tests Procedure and the Power Approach for Assessing the Equivalence of Average Bioavailability},
  author = {Schuirmann, Donald J.},
  year = {1987},
  journal = {Journal of pharmacokinetics and biopharmaceutics},
  volume = {15},
  number = {6},
  pages = {657--680}
}

@article{schulz_sample_2005,
  title = {Sample Size Calculations in Randomised Trials: Mandatory and Mystical},
  shorttitle = {Sample Size Calculations in Randomised Trials},
  author = {Schulz, Kenneth F. and Grimes, David A.},
  year = {2005},
  journal = {The Lancet},
  volume = {365},
  number = {9467},
  pages = {1348--1353},
  doi = {10.1016/S0140-6736(05)61034-3}
}

@article{schumi_through_2011,
  title = {Through the Looking Glass: Understanding Non-Inferiority},
  shorttitle = {Through the Looking Glass},
  author = {Schumi, Jennifer and Wittes, Janet T.},
  year = {2011},
  month = may,
  journal = {Trials},
  volume = {12},
  number = {1},
  pages = {106},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-12-106},
  abstract = {Non-inferiority trials test whether a new product is not unacceptably worse than a product already in use. This paper introduces concepts related to non-inferiority, and discusses the regulatory views of both the European Medicines Agency and the United States Food and Drug Administration.},
  langid = {english}
}

@book{schweder_confidence_2016,
  title = {Confidence, {{Likelihood}}, {{Probability}}: {{Statistical Inference}} with {{Confidence Distributions}}},
  shorttitle = {Confidence, {{Likelihood}}, {{Probability}}},
  author = {Schweder, Tore and Hjort, Nils Lid},
  year = {2016},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139046671},
  abstract = {This lively book lays out a methodology of confidence distributions and puts them through their paces. Among other merits, they lead to optimal combinations of confidence from different sources of information, and they can make complex models amenable to objective and indeed prior-free analysis for less subjectively inclined statisticians. The generous mixture of theory, illustrations, applications and exercises is suitable for statisticians at all levels of experience, as well as for data-oriented scientists. Some confidence distributions are less dispersed than their competitors. This concept leads to a theory of risk functions and comparisons for distributions of confidence. Neyman\textendash Pearson type theorems leading to optimal confidence are developed and richly illustrated. Exact and optimal confidence distribution is the gold standard for inferred epistemic distributions. Confidence distributions and likelihood functions are intertwined, allowing prior distributions to be made part of the likelihood. Meta-analysis in likelihood terms is developed and taken beyond traditional methods, suiting it in particular to combining information across diverse data sources.},
  isbn = {978-0-521-86160-1}
}

@article{seaman_equivalence_1998,
  title = {Equivalence Confidence Intervals for Two-Group Comparisons of Means.},
  author = {Seaman, Michael A. and Serlin, Ronald C.},
  year = {1998},
  month = dec,
  journal = {Psychological Methods},
  volume = {3},
  number = {4},
  pages = {403--411},
  issn = {1082-989X},
  doi = {http://dx.doi.org.dianus.libr.tue.nl/10.1037/1082-989X.3.4.403},
  abstract = {Methods for determining whether 2 means are practically equivalent are discussed. Existing equivalency-testing procedures are reviewed and compared. An equivalence confidence interval is proposed and compared with the traditional confidence interval for a mean difference. The use of this new interval is described for both confirmatory and exploratory research and is shown to fit within the context of good-enough methods. Adaptations and extensions of the interval are proposed. (PsycINFO Database Record (c) 2013 APA, all rights reserved)(journal abstract)},
  copyright = {\textcopyright{} American Psychological Association 1998},
  langid = {english},
  keywords = {Confidence Limits (Statistics) (major),Mean (major),Statistical Analysis (major)}
}

@article{sedlmeier_studies_1989,
  title = {Do Studies of Statistical Power Have an Effect on the Power of Studies?},
  author = {Sedlmeier, Peter and Gigerenzer, Gerd},
  year = {1989},
  journal = {Psychological Bulletin},
  volume = {105},
  number = {2},
  pages = {309--316},
  doi = {10.1037/0033-2909.105.2.309}
}

@article{shmueli_explain_2010,
  ids = {shmueli_explain_2010-1},
  title = {To Explain or to Predict?},
  author = {Shmueli, Galit},
  year = {2010},
  journal = {Statistical science},
  volume = {25},
  number = {3},
  pages = {289--310}
}

@article{simmons_false-positive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  langid = {english}
}

@misc{simmons_life_2013,
  title = {Life after {{P-Hacking}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2013},
  month = jan,
  address = {{New Orleans, LA}}
}

@article{simonsohn_p-curve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer.},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {534}
}

@article{simonsohn_small_2015,
  title = {Small Telescopes: {{Detectability}} and the Evaluation of Replication Results},
  author = {Simonsohn, Uri},
  year = {2015},
  month = may,
  journal = {Psychological Science},
  volume = {26},
  number = {5},
  pages = {559--569},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797614567341},
  abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ``unsuccessful'' replication attempts (i.e., studies yielding p {$>$} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ``protecting'' true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
  langid = {english},
  pmid = {25800521},
  keywords = {hypothesis testing,open materials,replication,statistical power}
}

@article{skipper_sacredness_1967,
  title = {The {{Sacredness}} of .05: {{A Note}} Concerning the {{Uses}} of {{Statistical Levels}} of {{Significance}} in {{Social Science}}},
  shorttitle = {The {{Sacredness}} of .05},
  author = {Skipper, James K. and Guenther, Anthony L. and Nass, Gilbert},
  year = {1967},
  journal = {The American Sociologist},
  volume = {2},
  number = {1},
  pages = {16--18},
  issn = {0003-1232},
  doi = {https://www.jstor.org/stable/27701229}
}

@article{smaldino_natural_2016,
  title = {The Natural Selection of Bad Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  year = {2016},
  month = sep,
  journal = {Royal Society Open Science},
  volume = {3},
  number = {9},
  pages = {160384},
  issn = {2054-5703},
  doi = {10.1098/rsos.160384},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing\textemdash no deliberate cheating nor loafing\textemdash by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  copyright = {\textcopyright{} 2016 The Authors.. http://creativecommons.org/licenses/by/4.0/Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
  langid = {english}
}

@book{smithson_confidence_2003,
  title = {Confidence Intervals},
  author = {Smithson, Michael},
  year = {2003},
  series = {Sage University Papers. {{Quantitative}} Applications in the Social Sciences},
  number = {no. 07/140},
  publisher = {{Sage Publications}},
  address = {{Thousand Oaks, Calif}},
  isbn = {978-0-7619-2499-9},
  lccn = {HA31.2 .S59 2003},
  keywords = {Confidence intervals,Mathematics,Social sciences,Statistical methods}
}

@article{sotola_garbage_2022,
  title = {Garbage {{In}}, {{Garbage Out}}? {{Evaluating}} the {{Evidentiary Value}} of {{Published Meta-analyses Using Z-Curve Analysis}}},
  shorttitle = {Garbage {{In}}, {{Garbage Out}}?},
  author = {Sotola, Lukas K.},
  year = {2022},
  month = feb,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {32571},
  issn = {2474-7394},
  doi = {10.1525/collabra.32571},
  abstract = {The purpose of the current work was to examine the evidentiary value of the studies that have been included in published meta-analyses as a way of investigating the evidentiary value of the meta-analyses themselves. The studies included in 25 meta-analyses published in the last 10 years in Psychological Bulletin that investigated experimental mean differences were z-curved. Z-curve is a meta-analytic technique that allows one to estimate the predicted replicability, average power, publication bias, and false discovery rate of a population of studies. The results of the z-curves estimated a substantial file drawer in three-quarters of the meta-analyses; and in one-third of the meta-analyses, up to half of the studies are not expected to replicate and up to one-fifth of the studies included could be false positives. Possible reasons for these findings are discussed, and caution in interpreting published meta-analyses is recommended.}
}

@article{spanos_who_2013,
  title = {Who Should Be Afraid of the {{Jeffreys-Lindley}} Paradox?},
  author = {Spanos, Aris},
  year = {2013},
  journal = {Philosophy of Science},
  volume = {80},
  number = {1},
  pages = {73--93},
  publisher = {{University of Chicago Press Chicago, IL}},
  doi = {10.1086/668875}
}

@article{spellman_short_2015,
  title = {A {{Short}} ({{Personal}}) {{Future History}} of {{Revolution}} 2.0},
  author = {Spellman, Barbara A.},
  year = {2015},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {10},
  number = {6},
  pages = {886--899},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691615609918},
  abstract = {Crisis of replicability is one term that psychological scientists use for the current introspective phase we are in\textemdash I argue instead that we are going through a revolution analogous to a political revolution. Revolution 2.0 is an uprising focused on how we should be doing science now (i.e., in a 2.0 world). The precipitating events of the revolution have already been well-documented: failures to replicate, questionable research practices, fraud, etc. And the fact that none of these events is new to our field has also been well-documented. I suggest four interconnected reasons as to why this time is different: changing technology, changing demographics of researchers, limited resources, and misaligned incentives. I then describe two reasons why the revolution is more likely to catch on this time: technology (as part of the solution) and the fact that these concerns cut across social and life sciences\textemdash that is, we are not alone. Neither side in the revolution has behaved well, and each has characterized the other in extreme terms (although, of course, each has had a few extreme actors). Some suggested reforms are already taking hold (e.g., journals asking for more transparency in methods and analysis decisions; journals publishing replications) but the feared tyrannical requirements have, of course, not taken root (e.g., few journals require open data; there is no ban on exploratory analyses). Still, we have not yet made needed advances in the ways in which we accumulate, connect, and extract conclusions from our aggregated research. However, we are now ready to move forward by adopting incremental changes and by acknowledging the multiplicity of goals within psychological science.},
  langid = {english},
  pmid = {26581743},
  keywords = {journal practices,Methodology,replication,scientific practices}
}

@book{spiegelhalter_art_2019,
  title = {The {{Art}} of {{Statistics}}: {{How}} to {{Learn}} from {{Data}}},
  shorttitle = {The {{Art}} of {{Statistics}}},
  author = {Spiegelhalter, David},
  year = {2019},
  month = sep,
  edition = {Illustrated edition},
  publisher = {{Basic Books}},
  address = {{New York}},
  abstract = {The definitive guide to statistical thinkingStatistics are everywhere, as integral to science as they are to business, and in the popular media hundreds of times a day. In this age of big data, a basic grasp of statistical literacy is more important than ever if we want to separate the fact from the fiction, the ostentatious embellishments from the raw evidence -- and even more so if we hope to participate in the future, rather than being simple bystanders.In The Art of Statistics, world-renowned statistician David Spiegelhalter shows readers how to derive knowledge from raw data by focusing on the concepts and connections behind the math. Drawing on real world examples to introduce complex issues, he shows us how statistics can help us determine the luckiest passenger on the Titanic, whether a notorious serial killer could have been caught earlier, and if screening for ovarian cancer is beneficial. The Art of Statistics not only shows us how mathematicians have used statistical science to solve these problems -- it teaches us how we too can think like statisticians. We learn how to clarify our questions, assumptions, and expectations when approaching a problem, and -- perhaps even more importantly -- we learn how to responsibly interpret the answers we receive.Combining the incomparable insight of an expert with the playful enthusiasm of an aficionado, The Art of Statistics is the definitive guide to stats that every modern person needs.},
  isbn = {978-1-5416-1851-0},
  langid = {english}
}

@article{spiegelhalter_monitoring_1986,
  title = {Monitoring Clinical Trials: Conditional or Predictive Power?},
  shorttitle = {Monitoring Clinical Trials},
  author = {Spiegelhalter, David J. and Freedman, Laurence S. and Blackburn, Patrick R.},
  year = {1986},
  journal = {Controlled clinical trials},
  volume = {7},
  number = {1},
  pages = {8--17},
  publisher = {{Elsevier}},
  doi = {10.1016/0197-2456(86)90003-6}
}

@article{stanley_finding_2017,
  title = {Finding the Power to Reduce Publication Bias: {{Finding}} the Power to Reduce Publication Bias},
  shorttitle = {Finding the Power to Reduce Publication Bias},
  author = {Stanley, T. D. and Doucouliagos, Hristos and Ioannidis, John P. A.},
  year = {2017},
  journal = {Statistics in Medicine},
  issn = {02776715},
  doi = {10.1002/sim.7228},
  langid = {english}
}

@article{stanley_meta-regression_2014,
  title = {Meta-Regression Approximations to Reduce Publication Selection Bias.},
  shorttitle = {Meta-Regression Approximations to Reduce Publication Selection Bias},
  author = {Stanley, T. D. and Doucouliagos, Hristos},
  year = {2014},
  month = mar,
  journal = {Research Synthesis Methods},
  volume = {5},
  number = {1},
  pages = {60--78},
  issn = {17592879},
  doi = {10.1002/jrsm.1095},
  abstract = {Publication selection bias represents a serious challenge to the integrity of all empirical sciences. We develop meta-regression approximations that are shown to reduce this bias and outperform conventional meta-analytic methods. Our approach is derived from Taylor polynomial approximations to the conditional mean of a truncated distribution. Monte Carlo simulations demonstrate how a new hybrid estimator provides a practical solution. These meta-regression methods are applied to several policy-relevant areas of research including: antidepressant effectiveness, the value of a statistical life and the employment effect of minimum wages and alter what we think we know.},
  langid = {english}
}

@article{stefan_tutorial_2019,
  title = {A Tutorial on {{Bayes Factor Design Analysis}} Using an Informed Prior},
  author = {Stefan, Angelika M. and Gronau, Quentin F. and Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan},
  year = {2019},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {3},
  pages = {1042--1058},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-01189-8},
  abstract = {Well-designed experiments are likely to yield compelling evidence with efficient sample sizes. Bayes Factor Design Analysis (BFDA) is a recently developed methodology that allows researchers to balance the informativeness and efficiency of their experiment (Sch\"onbrodt \& Wagenmakers, Psychonomic Bulletin \& Review, 25(1), 128\textendash 142 2018). With BFDA, researchers can control the rate of misleading evidence but, in addition, they can plan for a target strength of evidence. BFDA can be applied to fixed-N and sequential designs. In this tutorial paper, we provide an introduction to BFDA and analyze how the use of informed prior distributions affects the results of the BFDA. We also present a user-friendly web-based BFDA application that allows researchers to conduct BFDAs with ease. Two practical examples highlight how researchers can use a BFDA to plan for informative and efficient research designs.},
  langid = {english}
}

@article{steiger_beyond_2004,
  title = {Beyond the {{F Test}}: {{Effect Size Confidence Intervals}} and {{Tests}} of {{Close Fit}} in the {{Analysis}} of {{Variance}} and {{Contrast Analysis}}.},
  shorttitle = {Beyond the {{F Test}}},
  author = {Steiger, James H.},
  year = {2004},
  journal = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {164--182},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.164},
  langid = {english}
}

@article{stewart_ipd_2002,
  title = {To {{IPD}} or Not to {{IPD}}?: {{Advantages}} and {{Disadvantages}} of {{Systematic Reviews Using Individual Patient Data}}},
  shorttitle = {To {{IPD}} or Not to {{IPD}}?},
  author = {Stewart, Lesley A. and Tierney, Jayne F.},
  year = {2002},
  month = mar,
  journal = {Evaluation \& the Health Professions},
  volume = {25},
  number = {1},
  pages = {76--97},
  issn = {0163-2787, 1552-3918},
  doi = {10.1177/0163278702025001006},
  abstract = {Systematic reviews and meta-analyses that obtain original research data on individual participants enrolled in trials have been described as the gold standard of review. However, they may take longer and be more resource intensive than other types of review. The authors describe potential advantages and disadvantages of the individual patient data (IPD) approach, including benefits from improved data quality, benefits afforded by the type of analyses that can be done, and advantages in achieving consensus around results and interpretation by an international multi disciplinary team. Disadvantages and barriers relating to resource and expertise, negotiating collaboration, and software requirements are also discussed. At the outset, reviewers should consider the methodological factors likely to influence results in their particular review setting, together with time and resource constraints, so that an active decision can be made about whether to extract data frompublished reports, collect addi tional or replacement summary data from trialists, or collect IPD.},
  langid = {english}
}

@incollection{taper_philosophy_2011,
  title = {Philosophy of {{Statistics}}},
  booktitle = {Evidence, Evidence Functions, and Error Probabilities},
  author = {Taper, Mark L. and Lele, Subhash R.},
  editor = {Bandyophadhyay, P. S. and Forster, M. R.},
  year = {2011},
  pages = {513--531},
  publisher = {{Elsevier, USA}}
}

@article{taylor_bias_1996,
  title = {Bias in Linear Model Power and Sample Size Calculation Due to Estimating Noncentrality},
  author = {Taylor, Douglas J. and Muller, Keith E.},
  year = {1996},
  journal = {Communications in Statistics-Theory and Methods},
  volume = {25},
  number = {7},
  pages = {1595--1610},
  doi = {10.1080/03610929608831787}
}

@article{teare_sample_2014,
  title = {Sample Size Requirements to Estimate Key Design Parameters from External Pilot Randomised Controlled Trials: A Simulation Study},
  shorttitle = {Sample Size Requirements to Estimate Key Design Parameters from External Pilot Randomised Controlled Trials},
  author = {Teare, M. Dawn and Dimairo, Munyaradzi and Shephard, Neil and Hayman, Alex and Whitehead, Amy and Walters, Stephen J.},
  year = {2014},
  month = jul,
  journal = {Trials},
  volume = {15},
  number = {1},
  pages = {264},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-15-264},
  abstract = {External pilot or feasibility studies can be used to estimate key unknown parameters to inform the design of the definitive randomised controlled trial (RCT). However, there is little consensus on how large pilot studies need to be, and some suggest inflating estimates to adjust for the lack of precision when planning the definitive RCT.},
  langid = {english}
}

@article{tendeiro_review_2019,
  title = {A Review of Issues about Null Hypothesis {{Bayesian}} Testing},
  author = {Tendeiro, Jorge N. and Kiers, Henk A. L.},
  year = {2019},
  month = may,
  journal = {Psychological Methods},
  issn = {1939-1463},
  doi = {10.1037/met0000221},
  abstract = {Null hypothesis significance testing (NHST) has been under scrutiny for decades. The literature shows overwhelming evidence of a large range of problems affecting NHST. One of the proposed alternatives to NHST is using Bayes factors instead of p values. Here we denote the method of using Bayes factors to test point null models as "null hypothesis Bayesian testing" (NHBT). In this article we offer a wide overview of potential issues (limitations or sources of misinterpretation) with NHBT which is currently missing in the literature. We illustrate many of the shortcomings of NHBT by means of reproducible examples. The article concludes with a discussion of NHBT in particular and testing in general. In particular, we argue that posterior model probabilities should be given more emphasis than Bayes factors, because only the former provide direct answers to the most common research questions under consideration. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {31094544}
}

@article{ter_schure_accumulation_2019,
  title = {Accumulation {{Bias}} in {{Meta-Analysis}}: {{The Need}} to {{Consider Time}} in {{Error Control}}},
  shorttitle = {Accumulation {{Bias}} in {{Meta-Analysis}}},
  author = {{ter Schure}, Judith and Gr{\"u}nwald, Peter D.},
  year = {2019},
  month = may,
  journal = {arXiv:1905.13494 [math, stat]},
  eprint = {1905.13494},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Studies accumulate over time and meta-analyses are mainly retrospective. These two characteristics introduce dependencies between the analysis time, at which a series of studies is up for meta-analysis, and results within the series. Dependencies introduce bias --- Accumulation Bias --- and invalidate the sampling distribution assumed for p-value tests, thus inflating type-I errors. But dependencies are also inevitable, since for science to accumulate efficiently, new research needs to be informed by past results. Here, we investigate various ways in which time influences error control in meta-analysis testing. We introduce an Accumulation Bias Framework that allows us to model a wide variety of practically occurring dependencies, including study series accumulation, meta-analysis timing, and approaches to multiple testing in living systematic reviews. The strength of this framework is that it shows how all dependencies affect p-value-based tests in a similar manner. This leads to two main conclusions. First, Accumulation Bias is inevitable, and even if it can be approximated and accounted for, no valid p-value tests can be constructed. Second, tests based on likelihood ratios withstand Accumulation Bias: they provide bounds on error probabilities that remain valid despite the bias. We leave the reader with a choice between two proposals to consider time in error control: either treat individual (primary) studies and meta-analyses as two separate worlds --- each with their own timing --- or integrate individual studies in the meta-analysis world. Taking up likelihood ratios in either approach allows for valid tests that relate well to the accumulating nature of scientific knowledge. Likelihood ratios can be interpreted as betting profits, earned in previous studies and invested in new ones, while the meta-analyst is allowed to cash out at any time and advise against future studies.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{terrin_adjusting_2003,
  title = {Adjusting for Publication Bias in the Presence of Heterogeneity},
  author = {Terrin, Norma and Schmid, Christopher H. and Lau, Joseph and Olkin, Ingram},
  year = {2003},
  month = jul,
  journal = {Statistics in Medicine},
  volume = {22},
  number = {13},
  pages = {2113--2126},
  issn = {0277-6715},
  doi = {10.1002/sim.1461},
  abstract = {It is known that the existence of publication bias can influence the conclusions of a meta-analysis. Some methods have been developed to deal with publication bias, but issues remain. One particular method called 'trim and fill' is designed to adjust for publication bias. The method, which is intuitively appealing and comprehensible by non-statisticians, is based on a simple and popular graphical tool called the funnel plot. We present a simulation study designed to evaluate the behaviour of this method. Our results indicate that when the studies are heterogeneous (that is, when they estimate different effects), trim and fill may inappropriately adjust for publication bias where none exists. We found that trim and fill may spuriously adjust for non-existent bias if (i) the variability among studies causes some precisely estimated studies to have effects far from the global mean or (ii) an inverse relationship between treatment efficacy and sample size is introduced by the studies' a priori power calculations. The results suggest that the funnel plot itself is inappropriate for heterogeneous meta-analyses. Selection modelling is an alternative method warranting further study. It performed better than trim and fill in our simulations, although its frequency of convergence varied, depending on the simulation parameters.},
  langid = {english},
  pmid = {12820277},
  keywords = {Computer Simulation,Data Interpretation; Statistical,Meta-Analysis as Topic,Models; Statistical,Odds Ratio,Publication Bias,Treatment Outcome}
}

@article{thompson_effect_2007,
  title = {Effect Sizes, Confidence Intervals, and Confidence Intervals for Effect Sizes},
  author = {Thompson, Bruce},
  year = {2007},
  month = may,
  journal = {Psychology in the Schools},
  volume = {44},
  number = {5},
  pages = {423--432},
  issn = {00333085, 15206807},
  doi = {10.1002/pits.20234},
  langid = {english}
}

@book{thompson_sampling_2012,
  title = {Sampling},
  author = {Thompson, Steven K.},
  year = {2012},
  series = {Wiley Series in Probability and Statistics},
  edition = {3rd ed},
  publisher = {{Wiley}},
  address = {{Hoboken, N.J}},
  abstract = {"The Third Edition retains the general organization of the prior two editions, but it incorporates new material throughout the text. The book is organized into six parts: Part I covers basic sampling from simple random sampling to unequal probability sampling; Part II treats the use of auxiliary data with ratio and regression estimation and looks at the ideas of sufficient data, model, and design in practical sampling; Part III covers major useful designs such as stratified, cluster and systematic, multistage, and double and network sampling; Part IV examines detectability methods for elusive populations, and basic problems in detectability, visibility, and catchability are discussed; Part V concerns spatial sampling with the prediction methods of geostatistics, considerations of efficient spatial designs, and comparisons of different observational methods including plot shapes and detection aspects; and Part VI introduces adaptive sampling designs in which the sampling procedure depends on what is observed during the survey. For this new edition, the author has focused on thoroughly updating the book with a special emphasis on the first 14 chapters since these topics are invariably covered in basic sampling courses. The author has also implemented new approaches to explain the various techniques in the book, and as a result, new examples and explanations have been added throughout. In an effort to improve the presentation and visualization of the book, new figures as well as replacement figures for previously existing figures have been added. This book has continuously stood out from other sampling texts since the figures evoke the idea of each sampling design. The new figures will help readers to better visualize and understand the underlying concepts such as the different sampling strategies"--},
  isbn = {978-0-470-40231-3},
  langid = {english},
  lccn = {QA276.6 .T58 2012},
  keywords = {Sampling (Statistics)},
  annotation = {OCLC: ocn746489136}
}

@article{tran_predicting_2017,
  title = {Predicting Data Saturation in Qualitative Surveys with Mathematical Models from Ecological Research},
  author = {Tran, Viet-Thi and Porcher, Raphael and Tran, Viet-Chi and Ravaud, Philippe},
  year = {2017},
  month = feb,
  journal = {Journal of Clinical Epidemiology},
  volume = {82},
  pages = {71-78.e2},
  publisher = {{Elsevier}},
  issn = {0895-4356, 1878-5921},
  doi = {10.1016/j.jclinepi.2016.10.001},
  abstract = {{$<$}h2{$>$}Abstract{$<$}/h2{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}Sample size in surveys with open-ended questions relies on the principle of data saturation. Determining the point of data saturation is complex because researchers have information on only what they have found. The decision to stop data collection is solely dictated by the judgment and experience of researchers. In this article, we present how mathematical modeling may be used to describe and extrapolate the accumulation of themes during a study to help researchers determine the point of data saturation.{$<$}/p{$><$}h3{$>$}Study Design and Setting{$<$}/h3{$><$}p{$>$}The model considers a latent distribution of the probability of elicitation of all themes and infers the accumulation of themes as arising from a mixture of zero-truncated binomial distributions. We illustrate how the model could be used with data from a survey with open-ended questions on the burden of treatment involving 1,053 participants from 34 different countries and with various conditions. The performance of the model in predicting the number of themes to be found with the inclusion of new participants was investigated by Monte Carlo simulations. Then, we tested how the slope of the expected theme accumulation curve could be used as a stopping criterion for data collection in surveys with open-ended questions.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}By doubling the sample size after the inclusion of initial samples of 25 to 200 participants, the model reliably predicted the number of themes to be found. Mean estimation error ranged from 3\% to 1\% with simulated data and was {$<$}2\% with data from the study of the burden of treatment. Sequentially calculating the slope of the expected theme accumulation curve for every five new participants included was a feasible approach to balance the benefits of including these new participants in the study. In our simulations, a stopping criterion based on a value of 0.05 for this slope allowed for identifying 97.5\% of the themes while limiting the inclusion of participants eliciting nothing new in the study.{$<$}/p{$><$}h3{$>$}Conclusion{$<$}/h3{$><$}p{$>$}Mathematical models adapted from ecological research can accurately predict the point of data saturation in surveys with open-ended questions.{$<$}/p{$>$}},
  langid = {english},
  pmid = {27789316}
}

@book{tukey_exploratory_1977,
  title = {Exploratory {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1977},
  edition = {1 edition},
  publisher = {{Pearson}},
  address = {{Reading, Mass}},
  abstract = {The approach in this introductory book is that of informal study of the data. Methods range from plotting picture-drawing techniques to rather elaborate numerical summaries. Several of the methods are the original creations of the author, and all can be carried out either with pencil or aided by hand-held calculator.},
  isbn = {978-0-201-07616-5},
  langid = {english}
}

@article{tukey_future_1962,
  ids = {tukey_future_1962-1},
  title = {The {{Future}} of {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1962},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {33},
  number = {1},
  pages = {1--67},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177704711},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  mrnumber = {MR133937},
  zmnumber = {0107.36401}
}

@misc{tunc_falsificationist_2020,
  title = {A {{Falsificationist Treatment}} of {{Auxiliary Hypotheses}} in {{Social}} and {{Behavioral Sciences}}: {{Systematic Replications Framework}}},
  shorttitle = {A {{Falsificationist Treatment}} of {{Auxiliary Hypotheses}} in {{Social}} and {{Behavioral Sciences}}},
  author = {Tun{\c c}, Duygu Uygun and Tun{\c c}, Mehmet Necip},
  year = {2020},
  month = may,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/pdm7y},
  abstract = {Auxiliary hypotheses (AHs) are indispensable in hypothesis-testing, because without them specification of testable predictions and consequently falsification is impossible. However, as AHs enter the test along with the main hypothesis, non-corroborative findings are ambiguous. Due to this ambiguity, AHs may also be employed to deflect falsification by providing ``alternative explanations'' of findings. This is not fatal to the extent that AHs are independently validated and safely relegated to background knowledge. But this is not always possible, especially in the so-called ``softer'' sciences where often theories are loosely organized, measurements are noisy, and constructs are vague. The Systematic Replications Framework (SRF) provides a methodological solution by disentangling the implications of the findings for the main hypothesis and the AHs through pre-planned series of systematically interlinked close and conceptual replications. SRF facilitates testing alternative explanations associated with different AHs and thereby increases test severity across a battery of tests. In this way, SRF assesses whether the corroboration of a hypothesis is conditional on particular AHs, and thus allows for a more objective evaluation of its empirical support and whether post hoc modifications to the theory are progressive or degenerative in the Lakatosian sense. Finally, SRF has several advantages over randomization-based systematic replication proposals, which generally assume a problematic neo-operationalist approach that prescribes exploration-oriented strategies in confirmatory contexts.},
  langid = {american},
  keywords = {adversarial collaboration,auxiliary hypotheses,Duhem-Quine Thesis,empirical underdetermination,falsificationism,Meta-Psychology,Quantitative Methods,replication,Social and Behavioral Sciences,Theory and Philosophy of Science}
}

@article{tversky_belief_1971,
  title = {Belief in the Law of Small Numbers},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1971},
  journal = {Psychological Bulletin},
  volume = {76},
  number = {2},
  pages = {105--110},
  issn = {1939-1455(Electronic);0033-2909(Print)},
  doi = {10.1037/h0031322},
  abstract = {Reports that people have erroneous intuitions about the laws of chance. In particular, they regard a sample randomly drawn from a population as highly representative, I.e., similar to the population in all essential characteristics. The prevalence of the belief and its unfortunate consequences for psychological research are illustrated by the responses of 84 professional psychologists to a questionnaire concerning research decisions.},
  copyright = {(c) 2015 APA, all rights reserved},
  keywords = {*Methodology,*Statistical Analysis,Consequence}
}

@article{tversky_features_1977,
  title = {Features of Similarity},
  author = {Tversky, Amos},
  year = {1977},
  journal = {Psychological review},
  volume = {84},
  number = {4},
  pages = {327--352},
  doi = {10.1037/0033-295X.84.4.327}
}

@article{ulrich_properties_2018,
  title = {Some Properties of P-Curves, with an Application to Gradual Publication Bias},
  author = {Ulrich, Rolf and Miller, Jeff},
  year = {2018},
  journal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {546--560},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/met0000125},
  abstract = {p-curves provide a useful window for peeking into the file drawer in a way that might reveal p-hacking (Simonsohn, Nelson, \& Simmons, 2014a). The properties of p-curves are commonly investigated by computer simulations. On the basis of these simulations, it has been proposed that the skewness of this curve can be used as a diagnostic tool to decide whether the significant p values within a certain domain of research suggest the presence of p-hacking or actually demonstrate that there is a true effect. Here we introduce a rigorous mathematical approach that allows the properties of p-curves to be examined without simulations. This approach allows the computation of a p-curve for any statistic whose sampling distribution is known and thereby allows a thorough evaluation of its properties. For example, it shows under which conditions p-curves would exhibit the shape of a monotone decreasing function. In addition, we used weighted distribution functions to analyze how 2 different types of publication bias (i.e., cliff effects and gradual publication bias) influence the shapes of p-curves. The results of 2 survey experiments with more than 1,000 participants support the existence of a cliff effect at p = .05 and also suggest that researchers tend to be more likely to recommend submission of an article as the level of statistical significance increases beyond this p level. This gradual bias produces right-skewed p-curves mimicking the existence of real effects even when no such effects are actually present. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  keywords = {Computer Simulation,Effect Size (Statistical),Hypothesis Testing,Meta Analysis,Psychology,Scientific Communication,Statistical Significance,Statistical Tests}
}

@misc{uygun_tunc_epistemic_2021,
  title = {The {{Epistemic}} and {{Pragmatic Function}} of {{Dichotomous Claims Based}} on {{Statistical Hypothesis Tests}}},
  author = {Uygun Tun{\c c}, Duygu and Tun{\c c}, Mehmet Necip and Lakens, Dani{\"e}l},
  year = {2021},
  month = feb,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/af9by},
  abstract = {Researchers commonly make dichotomous claims based on continuous test statistics. Many have branded the practice as misuse of statistics, and criticize scientists for suffering from ``dichotomania''. However, the role dichotomous claims play in science is not primarily a statistical one, but an epistemological and pragmatic one. The epistemological function of dichotomous claims consists in transforming data into factual statements that can falsify a universal statement. This transformation requires pre-specified methodological decision procedures such as statistical hypothesis testing (e.g., Neyman-Pearson tests). From the perspective of methodological falsificationism these decision procedures are necessary, as probabilistic statements (e.g. continuous test statistics) cannot function as falsifiers of substantive hypotheses. However, they are not sufficient since for dichotomous claims to have any implication regarding theoretical claims about phenomena, there should be a valid derivation chain linking theoretical, experimental and data models. The pragmatic function of dichotomous claims is facilitating scrutiny and criticism among peers by generating contestable statements, a process referred to by Popper as 'conjectures and refutations', through which we can determine which theories withstand scrutiny the best. Abandoning dichotomous claims to combat the misuse of statistics would not improve scientific inferences but will sacrifice these crucial epistemic and pragmatic functions.},
  keywords = {basic statements,dichotomous claims,methodological falsificationism,Quantitative Methods,Social and Behavioral Sciences,statisitical hypothesis testing,Theory and Philosophy of Science,theory testing}
}

@article{valentine_how_2010,
  title = {How {{Many Studies Do You Need}}?: {{A Primer}} on {{Statistical Power}} for {{Meta-Analysis}}},
  shorttitle = {How {{Many Studies Do You Need}}?},
  author = {Valentine, Jeffrey C. and Pigott, Therese D. and Rothstein, Hannah R.},
  year = {2010},
  month = apr,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {35},
  number = {2},
  pages = {215--247},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/1076998609346961},
  abstract = {In this article, the authors outline methods for using fixed and random effects power analysis in the context of meta-analysis. Like statistical power analysis for primary studies, power analysis for meta-analysis can be done either prospectively or retrospectively and requires assumptions about parameters that are unknown. The authors provide some suggestions for thinking about these parameters, in particular for the random effects variance component. The authors also show how the typically uninformative retrospective power analysis can be made more informative. The authors then discuss the value of confidence intervals, show how they could be used in addition to or instead of retrospective power analysis, and also demonstrate that confidence intervals can convey information more effectively in some situations than power analyses alone. Finally, the authors take up the question ``How many studies do you need to do a meta-analysis?'' and show that, given the need for a conclusion, the answer is ``two studies,'' because all other synthesis techniques are less transparent and/or are less likely to be valid. For systematic reviewers who choose not to conduct a quantitative synthesis, the authors provide suggestions for both highlighting the current limitations in the research base and for displaying the characteristics and results of studies that were found to meet inclusion criteria.},
  langid = {english},
  keywords = {meta-analysis,research methodology,statistics}
}

@article{van_de_schoot_analyzing_2015,
  title = {Analyzing Small Data Sets Using {{Bayesian}} Estimation: The Case of Posttraumatic Stress Symptoms Following Mechanical Ventilation in Burn Survivors},
  shorttitle = {Analyzing Small Data Sets Using {{Bayesian}} Estimation},
  author = {{van de Schoot}, Rens and Broere, Joris J. and Perryck, Koen H. and {Zondervan-Zwijnenburg}, Mari{\"e}lle and {van Loey}, Nancy E.},
  year = {2015},
  month = mar,
  journal = {European Journal of Psychotraumatology},
  volume = {6},
  issn = {2000-8198},
  doi = {10.3402/ejpt.v6.25216},
  abstract = {Background The analysis of small data sets in longitudinal studies can lead to power issues and often suffers from biased parameter values. These issues can be solved by using Bayesian estimation in conjunction with informative prior distributions. By means of a simulation study and an empirical example concerning posttraumatic stress symptoms (PTSS) following mechanical ventilation in burn survivors, we demonstrate the advantages and potential pitfalls of using Bayesian estimation. Methods First, we show how to specify prior distributions and by means of a sensitivity analysis we demonstrate how to check the exact influence of the prior (mis-) specification. Thereafter, we show by means of a simulation the situations in which the Bayesian approach outperforms the default, maximum likelihood and approach. Finally, we re-analyze empirical data on burn survivors which provided preliminary evidence of an aversive influence of a period of mechanical ventilation on the course of PTSS following burns. Results Not suprisingly, maximum likelihood estimation showed insufficient coverage as well as power with very small samples. Only when Bayesian analysis, in conjunction with informative priors, was used power increased to acceptable levels. As expected, we showed that the smaller the sample size the more the results rely on the prior specification. Conclusion We show that two issues often encountered during analysis of small samples, power and biased parameters, can be solved by including prior information into Bayesian analysis. We argue that the use of informative priors should always be reported together with a sensitivity analysis.},
  pmcid = {PMC4357639},
  pmid = {25765534}
}

@article{van_de_schoot_use_2021,
  title = {The {{Use}} of {{Questionable Research Practices}} to {{Survive}} in {{Academia Examined With Expert Elicitation}}, {{Prior-Data Conflicts}}, {{Bayes Factors}} for {{Replication Effects}}, and the {{Bayes Truth Serum}}},
  author = {{van de Schoot}, Rens and Winter, Sonja D. and Griffioen, Elian and Grimmelikhuijsen, Stephan and Arts, Ingrid and Veen, Duco and Grandfield, Elizabeth M. and Tummers, Lars G.},
  year = {2021},
  journal = {Frontiers in Psychology},
  volume = {12},
  issn = {1664-1078},
  abstract = {The popularity and use of Bayesian methods have increased across many research domains. The current article demonstrates how some less familiar Bayesian methods can be used. Specifically, we applied expert elicitation, testing for prior-data conflicts, the Bayesian Truth Serum, and testing for replication effects via Bayes Factors in a series of four studies investigating the use of questionable research practices (QRPs). Scientifically fraudulent or unethical research practices have caused quite a stir in academia and beyond. Improving science starts with educating Ph.D. candidates: the scholars of tomorrow. In four studies concerning 765 Ph.D. candidates, we investigate whether Ph.D. candidates can differentiate between ethical and unethical or even fraudulent research practices. We probed the Ph.D.s' willingness to publish research from such practices and tested whether this is influenced by (un)ethical behavior pressure from supervisors or peers. Furthermore, 36 academic leaders (deans, vice-deans, and heads of research) were interviewed and asked to predict what Ph.D.s would answer for different vignettes. Our study shows, and replicates, that some Ph.D. candidates are willing to publish results deriving from even blatant fraudulent behavior\textendash data fabrication. Additionally, some academic leaders underestimated this behavior, which is alarming. Academic leaders have to keep in mind that Ph.D. candidates can be under more pressure than they realize and might be susceptible to using QRPs. As an inspiring example and to encourage others to make their Bayesian work reproducible, we published data, annotated scripts, and detailed output on the Open Science Framework (OSF).}
}

@book{van_fraassen_scientific_1980,
  title = {The Scientific Image},
  author = {Van Fraassen, Bas C.},
  year = {1980},
  series = {Clarendon Library of Logic and Philosophy},
  publisher = {{Clarendon Press ; Oxford University Press}},
  address = {{Oxford : New York}},
  isbn = {978-0-19-824424-0 978-0-19-824427-1},
  lccn = {Q175 .V335 1980},
  keywords = {Philosophy,Science}
}

@article{van_t_veer_pre-registration_2016,
  title = {Pre-Registration in Social Psychology\textemdash{{A}} Discussion and Suggested Template},
  author = {{van 't Veer}, Anna Elisabeth and {Giner-Sorolla}, Roger},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {2--12},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2016.03.004},
  abstract = {Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied\textemdash reviewed and unreviewed pre-registration\textemdash and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.},
  keywords = {Pre-registration,Research methods,Reviewed pre-registration (RPR),Solid science,Unreviewed pre-registration (UPR)},
  annotation = {00000}
}

@article{vazire_quality_2017,
  title = {Quality {{Uncertainty Erodes Trust}} in {{Science}}},
  author = {Vazire, Simine},
  year = {2017},
  month = feb,
  journal = {Collabra: Psychology},
  volume = {3},
  number = {1},
  pages = {1},
  issn = {2474-7394},
  doi = {10.1525/collabra.74},
  abstract = {Article: Quality Uncertainty Erodes Trust in Science},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english}
}

@article{verschuere_registered_2018,
  title = {Registered {{Replication Report}} on {{Mazar}}, {{Amir}}, and {{Ariely}} (2008)},
  author = {Verschuere, Bruno and Meijer, Ewout H. and Jim, Ariane and Hoogesteyn, Katherine and Orthey, Robin and McCarthy, Randy J. and Skowronski, John J. and Acar, Oguz A. and Aczel, Balazs and Bakos, Bence E. and Barbosa, Fernando and Baskin, Ernest and B{\`e}gue, Laurent and {Ben-Shakhar}, Gershon and Birt, Angie R. and Blatz, Lisa and Charman, Steve D. and Claesen, Aline and Clay, Samuel L. and Coary, Sean P. and Crusius, Jan and Evans, Jacqueline R. and Feldman, Noa and {Ferreira-Santos}, Fernando and Gamer, Matthias and Gomes, Sara and {Gonz{\'a}lez-Iraizoz}, Marta and Holzmeister, Felix and Huber, Juergen and Isoni, Andrea and Jessup, Ryan K. and Kirchler, Michael and {klein Selle}, Nathalie and Koppel, Lina and Kovacs, Marton and Laine, Tei and Lentz, Frank and Loschelder, David D. and Ludvig, Elliot A. and Lynn, Monty L. and Martin, Scott D. and McLatchie, Neil M. and Mechtel, Mario and Nahari, Galit and {\"O}zdo{\u g}ru, Asil Ali and Pasion, Rita and Pennington, Charlotte R. and Roets, Arne and Rozmann, Nir and Scopelliti, Irene and Spiegelman, Eli and Suchotzki, Kristina and Sutan, Angela and Szecsi, Peter and Tingh{\"o}g, Gustav and Tisserand, Jean-Christian and Tran, Ulrich S. and Van Hiel, Alain and Vanpaemel, Wolf and V{\"a}stfj{\"a}ll, Daniel and Verliefde, Thomas and Vezirian, K{\'e}vin and Voracek, Martin and Warmelink, Lara and Wick, Katherine and Wiggins, Bradford J. and Wylie, Keith and Y{\i}ld{\i}z, Ezgi},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {299--317},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918781032},
  abstract = {The self-concept maintenance theory holds that many people will cheat in order to maximize self-profit, but only to the extent that they can do so while maintaining a positive self-concept. Mazar, Amir, and Ariely (2008, Experiment 1) gave participants an opportunity and incentive to cheat on a problem-solving task. Prior to that task, participants either recalled the Ten Commandments (a moral reminder) or recalled 10 books they had read in high school (a neutral task). Results were consistent with the self-concept maintenance theory. When given the opportunity to cheat, participants given the moral-reminder priming task reported solving 1.45 fewer matrices than did those given a neutral prime (Cohen's d = 0.48); moral reminders reduced cheating. Mazar et al.'s article is among the most cited in deception research, but their Experiment 1 has not been replicated directly. This Registered Replication Report describes the aggregated result of 25 direct replications (total N = 5,786), all of which followed the same preregistered protocol. In the primary meta-analysis (19 replications, total n = 4,674), participants who were given an opportunity to cheat reported solving 0.11 more matrices if they were given a moral reminder than if they were given a neutral reminder (95\% confidence interval = [-0.09, 0.31]). This small effect was numerically in the opposite direction of the effect observed in the original study (Cohen's d = -0.04).},
  langid = {english},
  keywords = {cheating,honesty,Many Labs,morality,open data,open materials,preregistered,replication}
}

@article{viamonte_cost-benefit_2006,
  title = {A {{Cost-Benefit Analysis}} of {{Risk-Reduction Strategies Targeted}} at {{Older Drivers}}},
  author = {Viamonte, Sarah M. and Ball, Karlene K. and Kilgore, Meredith},
  year = {2006},
  month = dec,
  journal = {Traffic Injury Prevention},
  volume = {7},
  number = {4},
  pages = {352--359},
  publisher = {{Taylor \& Francis}},
  issn = {1538-9588},
  doi = {10.1080/15389580600791362},
  abstract = {Objective. The risk of motor-vehicle collisions increases as driving-related functional abilities decline. These declines can accompany normal or pathological aging and can be identified through driving-related functional screening exams upon license renewal. The objective of this cost-benefit analysis was to determine the utility of four functional screening procedures used to identify drivers at risk for motor-vehicle collisions, as well as an intervention designed to maintain or improve functional abilities. Additionally, this study sought to determine the expected cost per driver if an intervention was designed to target only those drivers who failed the functional ability-based driving screen, versus the expected cost per driver if the intervention was distributed en masse to all drivers 75 years and older. Improving functional abilities in older adults has potential far-reaching health and financial impacts which are broader than their impact of maintaining mobility. Methods. A decision tree was constructed to evaluate the expected costs and benefits of (a) screening all drivers and intervening when indicated (several screening batteries of varying length were considered), (b) no screening, but intervening with all drivers of older age, or (c) neither screening nor intervening (i.e., re-licensing per usual). Test characteristics and risk probabilities were based on a cohort of drivers aged 75 and older from a previous study (Ball et al., 2006). Relevant sensitivity analyses were conducted. Results. Providing all drivers with the speed-of-processing intervention is the most cost-beneficial option (expected cost per driver = \$493.30), even if the cost of the intervention doubles. Sensitivity analysis indicated the effectiveness of the intervention could drop from 86\% to 25\% and the preventative approach of intervening with all drivers remains the most cost-beneficial strategy. The least cost-beneficial option is almost always re-licensing per usual (expected cost per driver = \$1,562.84). Conclusion. Screening drivers upon license renewal is not currently beneficial because the available technology cannot consistently identify drivers at risk for a collision. However, the speed-of-processing intervention has demonstrated efficacy in improving driving competence (Roenker et al., 2003) and is a non-invasive, moderate-cost intervention that has the potential to protect the safety and mobility, as well as the financial interests, of older drivers and the community at large.},
  pmid = {17114092},
  keywords = {Cost-Benefit Analysis,Driver Screen,Intervention,Senior Driver},
  annotation = {\_eprint: https://doi.org/10.1080/15389580600791362}
}

@article{viechtbauer_conducting_2010,
  title = {Conducting Meta-Analyses in {{R}} with the Metafor Package},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  journal = {J Stat Softw},
  volume = {36},
  number = {3},
  pages = {1--48},
  doi = {http://dx.doi.org/10.18637/jss.v036.i03}
}

@article{viechtbauer_simple_2015,
  title = {A Simple Formula for the Calculation of Sample Size in Pilot Studies},
  author = {Viechtbauer, Wolfgang and Smits, Luc and Kotz, Daniel and Bud{\'e}, Luc and Spigt, Mark and Serroyen, Jan and Crutzen, Rik},
  year = {2015},
  month = nov,
  journal = {Journal of Clinical Epidemiology},
  volume = {68},
  number = {11},
  pages = {1375--1379},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2015.04.014},
  abstract = {One of the goals of a pilot study is to identify unforeseen problems, such as ambiguous inclusion or exclusion criteria or misinterpretations of questionnaire items. Although sample size calculation methods for pilot studies have been proposed, none of them are directed at the goal of problem detection. In this article, we present a simple formula to calculate the sample size needed to be able to identify, with a chosen level of confidence, problems that may arise with a given probability. If a problem exists with 5\% probability in a potential study participant, the problem will almost certainly be identified (with 95\% confidence) in a pilot study including 59 participants.},
  langid = {english},
  pmid = {26146089},
  keywords = {Humans,Mathematical Concepts,Pilot Projects,Pilot study,Problem detection,Rule of three,Sample size,Sample Size,Unforeseen problems}
}

@article{vohs_multisite_2021,
  title = {A {{Multisite Preregistered Paradigmatic Test}} of the {{Ego-Depletion Effect}}},
  author = {Vohs, Kathleen D. and Schmeichel, Brandon J. and Lohmann, Sophie and Gronau, Quentin F. and Finley, Anna J. and Ainsworth, Sarah E. and Alquist, Jessica L. and Baker, Michael D. and Brizi, Ambra and Bunyi, Angelica and Butschek, Grant J. and Campbell, Collier and Capaldi, Jonathan and Cau, Chuting and Chambers, Heather and Chatzisarantis, Nikos L. D. and Christensen, Weston J. and Clay, Samuel L. and Curtis, Jessica and De Cristofaro, Valeria and {del Rosario}, Kareena and Diel, Katharina and Do{\u g}ruol, Yasemin and Doi, Megan and Donaldson, Tina L. and Eder, Andreas B. and Ersoff, Mia and Eyink, Julie R. and Falkenstein, Angelica and Fennis, Bob M. and Findley, Matthew B. and Finkel, Eli J. and Forgea, Victoria and Friese, Malte and Fuglestad, Paul and {Garcia-Willingham}, Natasha E. and Geraedts, Lea F. and Gervais, Will M. and Giacomantonio, Mauro and Gibson, Bryan and Gieseler, Karolin and Gineikiene, Justina and Gloger, Elana M. and Gobes, Carina M. and Grande, Maria and Hagger, Martin S. and Hartsell, Bethany and Hermann, Anthony D. and Hidding, Jasper J. and Hirt, Edward R. and Hodge, Josh and Hofmann, Wilhelm and Howell, Jennifer L. and Hutton, Robert D. and Inzlicht, Michael and James, Lily and Johnson, Emily and Johnson, Hannah L. and Joyce, Sarah M. and Joye, Yannick and Kaben, Jan Helge and Kammrath, Lara K. and Kelly, Caitlin N. and Kissell, Brian L. and Koole, Sander L. and Krishna, Anand and Lam, Christine and Lee, Kelemen T. and Lee, Nick and Leighton, Dana C. and Loschelder, David D. and Maranges, Heather M. and Masicampo, E. J. and Mazara, Kennedy and McCarthy, Samantha and McGregor, Ian and Mead, Nicole L. and Mendes, Wendy B. and Meslot, Carine and Michalak, Nicholas M. and Milyavskaya, Marina and Miyake, Akira and {Moeini-Jazani}, Mehrad and Muraven, Mark and Nakahara, Erin and Patel, Krishna and Petrocelli, John V. and Pollak, Katja M. and Price, Mindi M. and Ramsey, Haley J. and Rath, Maximilian and Robertson, Jacob A. and Rockwell, Rachael and Russ, Isabella F. and Salvati, Marco and Saunders, Blair and Scherer, Anne and Sch{\"u}tz, Astrid and Schmitt, Kristin N. and Segerstrom, Suzanne C. and Serenka, Benjamin and Sharpinskyi, Konstantyn and Shaw, Meaghan and Sherman, Janelle and Song, Yu and Sosa, Nicholas and Spillane, Kaitlyn and Stapels, Julia and Stinnett, Alec J. and Strawser, Hannah R. and Sweeny, Kate and Theodore, Dominic and Tonnu, Karine and {van Oldenbeuving}, Yasmijn and {vanDellen}, Michelle R. and Vergara, Raiza C. and Walker, Jasmine S. and Waugh, Christian E. and Weise, Feline and Werner, Kaitlyn M. and Wheeler, Craig and White, Rachel A. and Wichman, Aaron L. and Wiggins, Bradford J. and Wills, Julian A. and Wilson, Janie H. and Wagenmakers, Eric-Jan and Albarrac{\'i}n, Dolores},
  year = {2021},
  month = oct,
  journal = {Psychological Science},
  volume = {32},
  number = {10},
  pages = {1566--1581},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797621989733},
  abstract = {We conducted a preregistered multilaboratory project (k = 36; N = 3,531) to assess the size and robustness of ego-depletion effects using a novel replication method, termed the paradigmatic replication approach. Each laboratory implemented one of two procedures that was intended to manipulate self-control and tested performance on a subsequent measure of self-control. Confirmatory tests found a nonsignificant result (d = 0.06). Confirmatory Bayesian meta-analyses using an informed-prior hypothesis ({$\delta$} = 0.30, SD = 0.15) found that the data were 4 times more likely under the null than the alternative hypothesis. Hence, preregistered analyses did not find evidence for a depletion effect. Exploratory analyses on the full sample (i.e., ignoring exclusion criteria) found a statistically significant effect (d = 0.08); Bayesian analyses showed that the data were about equally likely under the null and informed-prior hypotheses. Exploratory moderator tests suggested that the depletion effect was larger for participants who reported more fatigue but was not moderated by trait self-control, willpower beliefs, or action orientation.},
  langid = {english},
  keywords = {ego depletion,open data,open materials,preregistered,registered replication,self-control}
}

@article{vosgerau_99_2019,
  title = {99\% Impossible: {{A}} Valid, or Falsifiable, Internal Meta-Analysis},
  shorttitle = {99\% Impossible},
  author = {Vosgerau, Joachim and Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2019},
  month = sep,
  journal = {Journal of Experimental Psychology. General},
  volume = {148},
  number = {9},
  pages = {1628--1639},
  issn = {1939-2222},
  doi = {10.1037/xge0000663},
  abstract = {Several researchers have relied on, or advocated for, internal meta-analysis, which involves statistically aggregating multiple studies in a paper to assess their overall evidential value. Advocates of internal meta-analysis argue that it provides an efficient approach to increasing statistical power and solving the file-drawer problem. Here we show that the validity of internal meta-analysis rests on the assumption that no studies or analyses were selectively reported. That is, the technique is only valid if (a) all conducted studies were included (i.e., an empty file drawer), and (b) for each included study, exactly one analysis was attempted (i.e., there was no p-hacking). We show that even very small doses of selective reporting invalidate internal meta-analysis. For example, the kind of minimal p-hacking that increases the false-positive rate of 1 study to just 8\% increases the false-positive rate of a 10-study internal meta-analysis to 83\%. If selective reporting is approximately zero, but not exactly zero, then internal meta-analysis is invalid. To be valid, (a) an internal meta-analysis would need to contain exclusively studies that were properly preregistered, (b) those preregistrations would have to be followed in all essential aspects, and (c) the decision of whether to include a given study in an internal meta-analysis would have to be made before any of those studies are run. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  langid = {english},
  pmid = {31464485},
  keywords = {Humans,Meta-Analysis as Topic,Publication Bias,Reproducibility of Results}
}

@article{vuorre_curating_2018,
  title = {Curating {{Research Assets}}: {{A Tutorial}} on the {{Git Version Control System}}},
  shorttitle = {Curating {{Research Assets}}},
  author = {Vuorre, Matti and Curley, James P.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {219--236},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918754826},
  abstract = {Recent calls for improving reproducibility have increased attention to the ways in which researchers curate, share, and collaborate on their research assets. In this Tutorial, we explain how version control systems, such as the popular Git program, support these functions and then show how to use Git with a graphical interface in the RStudio program. This Tutorial is written for researchers with no previous experience using version control systems and covers both single-user and collaborative workflows. The online Supplemental Material provides information on advanced Git command-line functions. Git presents an elegant solution to specific challenges to curating, sharing, and collaborating on research assets and can be implemented in common workflows with little extra effort.},
  langid = {english},
  keywords = {Git,open materials,open science,reproducibility,research methods,version control}
}

@article{wacholder_assessing_2004,
  title = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}: {{An Approach}} for {{Molecular Epidemiology Studies}}},
  shorttitle = {Assessing the {{Probability That}} a {{Positive Report}} Is {{False}}},
  author = {Wacholder, S. and Chanock, S. and {Garcia-Closas}, M. and {El ghormli}, L. and Rothman, N.},
  year = {2004},
  month = mar,
  journal = {JNCI Journal of the National Cancer Institute},
  volume = {96},
  number = {6},
  pages = {434--442},
  issn = {0027-8874, 1460-2105},
  doi = {10.1093/jnci/djh075},
  langid = {english}
}

@article{wagenmakers_practical_2007,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Wagenmakers, Eric-Jan},
  year = {2007},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1069-9384},
  abstract = {In the field of psychology, the practice of p value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of the p value procedure. In particular, p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover, p values do not quantify statistical evidence. This article reviews these p value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to these p value problems is to adopt a model selection perspective and use the Bayesian information criterion (BIC) for statistical inference (Raftery, 1995). The BIC provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from SPSS output.},
  langid = {english},
  pmid = {18087943},
  keywords = {Bayes Theorem,Humans,Models; Psychological,Psychology}
}

@article{wagenmakers_registered_2016,
  title = {Registered {{Replication Report}}: {{Strack}}, {{Martin}}, \& {{Stepper}} (1988)},
  shorttitle = {Registered {{Replication Report}}},
  author = {Wagenmakers, E.-J. and Beek, T. and Dijkhoff, L. and Gronau, Q. F. and Acosta, A. and Adams, R. B. and Albohn, D. N. and Allard, E. S. and Benning, S. D. and {Blouin-Hudon}, E.-M. and Bulnes, L. C. and Caldwell, T. L. and {Calin-Jageman}, R. J. and Capaldi, C. A. and Carfagno, N. S. and Chasten, K. T. and Cleeremans, A. and Connell, L. and DeCicco, J. M. and Dijkstra, K. and Fischer, A. H. and Foroni, F. and Hess, U. and Holmes, K. J. and Jones, J. L. H. and Klein, O. and Koch, C. and Korb, S. and Lewinski, P. and Liao, J. D. and Lund, S. and Lupianez, J. and Lynott, D. and Nance, C. N. and Oosterwijk, S. and Ozdo{\u g}ru, A. A. and {Pacheco-Unguetti}, A. P. and Pearson, B. and Powis, C. and Riding, S. and Roberts, T.-A. and Rumiati, R. I. and Senden, M. and {Shea-Shumsky}, N. B. and Sobocko, K. and Soto, J. A. and Steiner, T. G. and Talarico, J. M. and {van Allen}, Z. M. and Vandekerckhove, M. and Wainwright, B. and Wayand, J. F. and Zeelenberg, R. and Zetzer, E. E. and Zwaan, R. A.},
  year = {2016},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {6},
  pages = {917--928},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691616674458},
  langid = {english},
  pmid = {27784749},
  keywords = {facial feedback hypothesis,many-labs,preregistration,replication}
}

@article{wald_sequential_1945,
  title = {Sequential Tests of Statistical Hypotheses},
  author = {Wald, Abraham},
  year = {1945},
  journal = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  pages = {117--186},
  doi = {https://www.jstor.org/stable/2240273}
}

@article{wasserstein_moving_2019,
  title = {Moving to a {{World Beyond}} ``p {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1583913}
}

@book{wassmer_group_2016,
  title = {Group {{Sequential}} and {{Confirmatory Adaptive Designs}} in {{Clinical Trials}}},
  author = {Wassmer, Gernot and Brannath, Werner},
  year = {2016},
  series = {Springer {{Series}} in {{Pharmaceutical Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-32562-0},
  isbn = {978-3-319-32560-6 978-3-319-32562-0},
  langid = {english}
}

@misc{wassmer_rpact_2019,
  title = {Rpact: {{Confirmatory}} Adaptive Clinical Trial Design and Analysis},
  author = {Wassmer, Gernot and Pahlke, Friedrich},
  year = {2019}
}

@article{weinshall-margel_overlooked_2011,
  title = {Overlooked Factors in the Analysis of Parole Decisions},
  author = {{Weinshall-Margel}, Keren and Shapard, John},
  year = {2011},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {42},
  pages = {E833-E833},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1110910108},
  abstract = {Danziger et al. (1) concluded that meal breaks taken by Israeli parole boards influence the boards' decisions. This conclusion depends on the order of cases being random or at least exogenous to the timing of meal breaks. We examined data provided by the authors and obtained additional data from 12 hearing days ( n = 227 decisions).* We also interviewed three attorneys, a parole panel judge, and five personnel at Israeli Prison Services and Court Management, learning that case ordering is not random and that several factors contribute to the downward trend in prisoner success between meal breaks. The most important is that the board tries to complete all cases from one prison before it takes a break and to start with another \ldots{}  [{$\carriagereturn$}][1]1To whom correspondence should be addressed. E-mail: johnshapard\{at\}gmail.com.  [1]: \#xref-corresp-1-1},
  chapter = {Letter},
  langid = {english},
  pmid = {21987788}
}

@book{wellek_testing_2010,
  title = {Testing Statistical Hypotheses of Equivalence and Noninferiority},
  author = {Wellek, Stefan},
  year = {2010},
  edition = {2nd ed},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  isbn = {978-1-4398-0818-4},
  lccn = {QA277 .W46 2010},
  keywords = {Statistical hypothesis testing}
}

@article{westberg_combining_1985,
  title = {Combining {{Independent Statistical Tests}}},
  author = {Westberg, Margareta},
  year = {1985},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {34},
  number = {3},
  pages = {287--296},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0039-0526},
  doi = {10.2307/2987655},
  abstract = {In the present study two well-known combination methods, Fisher's and Tippett's, are compared according to their power. The calculations are made for normally and chi-square distributed test statistics. None of the two procedures is uniformly better than the other according to the power but sometimes the power curves cross each other. The calculated power-graphs give guidelines for when to use Fisher's method and when to use Tippett's.}
}

@article{westfall_statistical_2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {5},
  pages = {2020--2045},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Sampling (Experimental),Statistical Power,Stimulus Parameters}
}

@article{westfall_statistical_2014-1,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli.},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {5},
  pages = {2020}
}

@article{westlake_use_1972,
  title = {Use of {{Confidence Intervals}} in {{Analysis}} of {{Comparative Bioavailability Trials}}},
  author = {Westlake, Wilfred J.},
  year = {1972},
  month = aug,
  journal = {Journal of Pharmaceutical Sciences},
  volume = {61},
  number = {8},
  pages = {1340--1341},
  issn = {0022-3549},
  doi = {10.1002/JPS.2600610845},
  abstract = {Journal of Pharmaceutical Sciences, pharmacokinetics, biopharmaceutics, pharmacodynamics, drug development, protein-peptide chemistry, drug delivery},
  langid = {english},
  pmid = {5050398},
  keywords = {Biopharmaceutics,Drug Compounding,Humans,Pharmaceutical Preparations,Statistics as Topic},
  annotation = {00297}
}

@article{wicherts_degrees_2016,
  title = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}: {{A Checklist}} to {{Avoid}} p-{{Hacking}}},
  shorttitle = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}},
  author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and Aert, Van and M, Robbie C. and Assen, Van and M, Marcel A. L.},
  year = {2016},
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01832},
  abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
  langid = {english},
  keywords = {BIAS,Experimental design (study designs),p-hacking,questionable research practices,Research methods education,significance chasing,significance testing}
}

@article{wiebels_leveraging_2021,
  title = {Leveraging {{Containers}} for {{Reproducible Psychological Research}}},
  author = {Wiebels, Kristina and Moreau, David},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {25152459211017853},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/25152459211017853},
  abstract = {Containers have become increasingly popular in computing and software engineering and are gaining traction in scientific research. They allow packaging up all code and dependencies to ensure that analyses run reliably across a range of operating systems and software versions. Despite being a crucial component for reproducible science, containerization has yet to become mainstream in psychology. In this tutorial, we describe the logic behind containers, what they are, and the practical problems they can solve. We walk the reader through the implementation of containerization within a research workflow with examples using Docker and R. Specifically, we describe how to use existing containers, build personalized containers, and share containers alongside publications. We provide a worked example that includes all steps required to set up a container for a research project and can easily be adapted and extended. We conclude with a discussion of the possibilities afforded by the large-scale adoption of containerization, especially in the context of cumulative, open science, toward a more efficient and inclusive research ecosystem.},
  langid = {english},
  keywords = {containerization,open materials,replication,reproducibility,research workflow,software}
}

@article{wigboldus_encourage_2016,
  title = {Encourage {{Playing}} with {{Data}} and {{Discourage Questionable Reporting Practices}}},
  author = {Wigboldus, Daniel H. J. and Dotsch, Ron},
  year = {2016},
  month = mar,
  journal = {Psychometrika},
  volume = {81},
  number = {1},
  pages = {27--32},
  issn = {1860-0980},
  doi = {10.1007/s11336-015-9445-1},
  langid = {english},
  keywords = {Confirmatory Analysis,Data Analysis Phase,Data Analysis Plan,Reporting Practice,Research Practice}
}

@article{williams_impact_1995,
  title = {Impact of {{Measurement Error}} on {{Statistical Power}}: {{Review}} of an {{Old Paradox}}},
  shorttitle = {Impact of {{Measurement Error}} on {{Statistical Power}}},
  author = {Williams, Richard H. and Zimmerman, Donald W. and Zumbo, Bruno D.},
  year = {1995},
  month = jul,
  journal = {The Journal of Experimental Education},
  volume = {63},
  number = {4},
  pages = {363--370},
  publisher = {{Routledge}},
  issn = {0022-0973},
  doi = {10.1080/00220973.1995.9943470},
  abstract = {The relation between test reliability and statistical power has been a controversial issue, perhaps due in part to a 1975 publication in the Psychological Bulletin by Overall and Woodward, ``Unreliability of Difference Scores: A Paradox for the Measurement of Change'', in which they demonstrated that a Student t test based on pretest-posttest differences can attain its greatest power when the difference score reliability is zero. In the present article, the authors attempt to explain this paradox by demonstrating in several ways that power is not a mathematical function of reliability unless either true score variance or error score variance is constant.},
  annotation = {\_eprint: https://doi.org/10.1080/00220973.1995.9943470}
}

@article{wilson_practical_2015,
  title = {A {{Practical Guide}} to {{Value}} of {{Information Analysis}}},
  author = {Wilson, Edward C. F.},
  year = {2015},
  month = feb,
  journal = {PharmacoEconomics},
  volume = {33},
  number = {2},
  pages = {105--121},
  issn = {1179-2027},
  doi = {10.1007/s40273-014-0219-x},
  abstract = {Value of information analysis is a quantitative method to estimate the return on investment in proposed research projects. It can be used in a number of ways. Funders of research may find it useful to rank projects in terms of the expected return on investment from a variety of competing projects. Alternatively, trialists can use the principles to identify the efficient sample size of a proposed study as an alternative to traditional power calculations, and finally, a value of information analysis can be conducted alongside an economic evaluation as a quantitative adjunct to the `future research' or `next steps' section of a study write up. The purpose of this paper is to present a brief introduction to the methods, a step-by-step guide to calculation and a discussion of issues that arise in their application to healthcare decision making. Worked examples are provided in the accompanying online appendices as Microsoft Excel spreadsheets.},
  langid = {english}
}

@article{wilson_vanvoorhis_understanding_2007,
  title = {Understanding Power and Rules of Thumb for Determining Sample Sizes},
  author = {Wilson VanVoorhis, Carmen R. and Morgan, Betsy L.},
  year = {2007},
  journal = {Tutorials in quantitative methods for psychology},
  volume = {3},
  number = {2},
  pages = {43--50},
  doi = {10.20982/tqmp.03.2.p043}
}

@book{winer_statistical_1962,
  title = {Statistical Principles in Experimental Design},
  author = {Winer, B. J},
  year = {1962},
  publisher = {{New York : McGraw-Hill}},
  abstract = {"Written primarily for students and research workers in the area of the behavioral sciences, this book is meant to provide a text and comprehensive reference source on statistical principles underlying experimental design. Particular emphasis is given to those designs that are likely to prove useful in research in the behavioral sciences. The book primarily emphasizes the logical basis of principles underlying designs for experiments rather than mathematical derivations associated with relevant sampling distributions. The topics selected for inclusion are those covered in courses taught by the author during the past several years. Students in these courses have widely varying backgrounds in mathematics and come primarily from the fields of psychology, education, economics, sociology, and industrial engineering. It has been the intention of the author to keep the book at a readability level appropriate for students having a mathematical background equivalent to freshman college algebra. From experience with those sections of the book which have been used as text material in dittoed form, there is evidence to indicate that, in large measure, the desired readability level has been attained. Admittedly, however, there are some sections in the book where this readability goal has not been achieved. The first course in design, as taught by the author, has as a prerequisite a basic course in statistical inference. The contents of Chaps. 1 and 2 review the highlights of what is included in the prerequisite material. These chapters are not meant to provide the reader with a first exposure to these topics. They are intended to provide a review of terminology and notation for the concepts which are more fully developed in later chapters. By no means is all the material included in the book covered in a one semester course. In a course of this length, the author has included Chaps. 3, 4, parts of 5, 6, parts of 7, parts of 10, and parts of 11. Chapters 8 through 11 were written to be somewhat independent of each other. Hence one may read, with understanding, in these chapters without undue reference to material in the others. In general, the discussion of principles, interpretations of illustrative examples, and computational procedures are included in successive sections within the same chapter. However, to facilitate the use of the book as a reference source, this procedure is not followed in Chaps. 5 and 6. Basic principles associated with a large class of designs for factorial experiments are discussed in Chap. 5. Detailed illustrative examples of these designs are presented in Chap. 6. For teaching purposes, the author includes relevant material from Chap. 6 with the corresponding material in Chap. 5. Selected topics from Chaps. 7 through 11 have formed the basis for a second course in experimental design. Relatively complete tables for sampling distributions of statistics used in the analysis of experimental designs are included in the Appendix. Ample references to source materials having mathematical proofs for the principles stated in the text are provided"--Preface. (PsycINFO Database Record (c) 2008 APA, all rights reserved).  x, 672 p. : ill. ; 24 cm. Social sciences -- Statistical methods. Psychology -- Statistical methods. Research. Plan d'exp\'erience. Experimentelle Psychologie. Statistik. Versuchsplanung. Psychology -- Research. Social sciences -- Research. Experimenteel ontwerp. Variantieanalyse. Experimental design. Statistics as Topic.},
  langid = {english}
}

@article{wittes_role_1990,
  title = {The Role of Internal Pilot Studies in Increasing the Efficiency of Clinical Trials},
  author = {Wittes, Janet and Brittain, Erica},
  year = {1990},
  journal = {Statistics in Medicine},
  volume = {9},
  number = {1-2},
  pages = {65--72},
  issn = {1097-0258},
  doi = {10.1002/sim.4780090113},
  abstract = {Investigators often design clinical trials without knowing precisely the values of such necessary parameters as the variances or the event rates in the control group. In order to determine reasonable values for such parameters, they may design a small pilot study external to the main trial. In this paper we propose designs, which we term internal pilot studies, that designate a portion of the main trial as a pilot phase. At the end of the internal pilot study, the investigators recompute preselected parameters and recalculate required sample size. The study then proceeds with the modifications dictated by the internal pilot. Final analyses of the results incorporate all data, disregarding the fact that part of the data came from a pilot phase. As one example of this type of design, we consider a study to compare two normally distributed means. By simulation, we show a numerical example for which the effect of the procedure on the {$\alpha$}-level is negligible, but the potential gain in power considerable. We urge considering a similar approach for a number of types of endpoints.},
  copyright = {Copyright \textcopyright{} 1990 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780090113}
}

@misc{wong_potential_2021,
  title = {On the {{Potential Mismatch}} between the {{Function}} of the {{Bayes Factor}} and {{Researchers}}' {{Expectations}}},
  author = {Wong, Tsz Keung and Kiers, Henk and Tendeiro, Jorge},
  year = {2021},
  month = aug,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/86p4k},
  abstract = {The aim of this study is to investigate whether there is a potential mismatch between the usability of a statistical tool and psychology researchers' expectation of it. Bayesian statistics is often promoted as an ideal substitute for frequentists statistics since it coincides better with researchers' expectations and needs. A particular incidence of this is the proposal of replacing Null Hypothesis Significance Testing (NHST) by Null Hypothesis Bayesian Testing (NHBT) using the Bayes factor. In this paper, it is studied to what extent the usability and expectations of NHBT match well. First, a study of the reporting practices in 73 psychological publications was carried out. It was found that eight Questionable Reporting and Interpreting Practices (QRIPs) occur more than once among the practitioners when doing NHBT. Specifically, our analysis provides insight into possible mismatches and their occurrence frequencies. A follow-up survey study has been conducted to assess such mismatches. The sample (N = 108) consisted of psychology researchers, experts in methodology (and/or statistics), and applied researchers in fields other than psychology. The data show that discrepancies exist among the participants. Interpreting the Bayes Factor as posterior odds and not acknowledging the notion of relative evidence in the Bayes Factor are arguably the most concerning ones. The results of the paper suggest that a shift of statistical paradigm cannot solve the problem of misinterpretation altogether if the users are not well acquainted with the tools.},
  langid = {american},
  keywords = {Bayes Factor,Null Hypothesis Bayesian Testing,Quantitative Methods,Questionable Reporting and Interpreting Practice,Social and Behavioral Sciences,Statistical Methods,Statistical test}
}

@article{wynants_prediction_2020,
  title = {Prediction Models for Diagnosis and Prognosis of Covid-19: Systematic Review and Critical Appraisal},
  shorttitle = {Prediction Models for Diagnosis and Prognosis of Covid-19},
  author = {Wynants, Laure and Calster, Ben Van and Collins, Gary S. and Riley, Richard D. and Heinze, Georg and Schuit, Ewoud and Bonten, Marc M. J. and Dahly, Darren L. and Damen, Johanna A. and Debray, Thomas P. A. and de Jong, Valentijn M. T. and Vos, Maarten De and Dhiman, Paula and Haller, Maria C. and Harhay, Michael O. and Henckaerts, Liesbet and Heus, Pauline and Kammer, Michael and Kreuzberger, Nina and Lohmann, Anna and Luijken, Kim and Ma, Jie and Martin, Glen P. and McLernon, David J. and Navarro, Constanza L. Andaur and Reitsma, Johannes B. and Sergeant, Jamie C. and Shi, Chunhu and Skoetz, Nicole and Smits, Luc J. M. and Snell, Kym I. E. and Sperrin, Matthew and Spijker, Ren{\'e} and Steyerberg, Ewout W. and Takada, Toshihiko and Tzoulaki, Ioanna and van Kuijk, Sander M. J. and van Bussel, Bas C. T. and van der Horst, Iwan C. C. and van Royen, Florien S. and Verbakel, Jan Y. and Wallisch, Christine and Wilkinson, Jack and Wolff, Robert and Hooft, Lotty and Moons, Karel G. M. and van Smeden, Maarten},
  year = {2020},
  month = apr,
  journal = {BMJ},
  volume = {369},
  pages = {m1328},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {1756-1833},
  doi = {10.1136/bmj.m1328},
  abstract = {Objective To review and appraise the validity and usefulness of published and preprint reports of prediction models for diagnosing coronavirus disease 2019 (covid-19) in patients with suspected infection, for prognosis of patients with covid-19, and for detecting people in the general population at increased risk of covid-19 infection or being admitted to hospital with the disease. Design Living systematic review and critical appraisal by the COVID-PRECISE (Precise Risk Estimation to optimise covid-19 Care for Infected or Suspected patients in diverse sEttings) group. Data sources PubMed and Embase through Ovid, up to 1 July 2020, supplemented with arXiv, medRxiv, and bioRxiv up to 5 May 2020. Study selection Studies that developed or validated a multivariable covid-19 related prediction model. Data extraction At least two authors independently extracted data using the CHARMS (critical appraisal and data extraction for systematic reviews of prediction modelling studies) checklist; risk of bias was assessed using PROBAST (prediction model risk of bias assessment tool). Results 37 421 titles were screened, and 169 studies describing 232 prediction models were included. The review identified seven models for identifying people at risk in the general population; 118 diagnostic models for detecting covid-19 (75 were based on medical imaging, 10 to diagnose disease severity); and 107 prognostic models for predicting mortality risk, progression to severe disease, intensive care unit admission, ventilation, intubation, or length of hospital stay. The most frequent types of predictors included in the covid-19 prediction models are vital signs, age, comorbidities, and image features. Flu-like symptoms are frequently predictive in diagnostic models, while sex, C reactive protein, and lymphocyte counts are frequent prognostic factors. Reported C index estimates from the strongest form of validation available per model ranged from 0.71 to 0.99 in prediction models for the general population, from 0.65 to more than 0.99 in diagnostic models, and from 0.54 to 0.99 in prognostic models. All models were rated at high or unclear risk of bias, mostly because of non-representative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, high risk of model overfitting, and unclear reporting. Many models did not include a description of the target population (n=27, 12\%) or care setting (n=75, 32\%), and only 11 (5\%) were externally validated by a calibration plot. The Jehi diagnostic model and the 4C mortality score were identified as promising models. Conclusion Prediction models for covid-19 are quickly entering the academic literature to support medical decision making at a time when they are urgently needed. This review indicates that almost all pubished prediction models are poorly reported, and at high risk of bias such that their reported predictive performance is probably optimistic. However, we have identified two (one diagnostic and one prognostic) promising models that should soon be validated in multiple cohorts, preferably through collaborative efforts and data sharing to also allow an investigation of the stability and heterogeneity in their performance across populations and settings. Details on all reviewed models are publicly available at https://www.covprecise.org/. Methodological guidance as provided in this paper should be followed because unreliable predictions could cause more harm than benefit in guiding clinical decisions. Finally, prediction model authors should adhere to the TRIPOD (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) reporting guideline. Systematic review registration Protocol https://osf.io/ehc47/, registration https://osf.io/wy245. Readers' note This article is a living systematic review that will be updated to reflect emerging evidence. Updates may occur for up to two years from the date of original publication. This version is update 3 of the original article published on 7 April 2020 (BMJ 2020;369:m1328). Previous updates can be found as data supplements (https://www.bmj.com/content/369/bmj.m1328/related\#datasupp). When citing this paper please consider adding the update number and date of access for clarity.},
  chapter = {Research},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2019. Re-use permitted under CC                 BY. No commercial re-use. See rights and permissions. Published by                 BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {32265220}
}

@article{yarkoni_choosing_2017,
  title = {Choosing {{Prediction Over Explanation}} in {{Psychology}}: {{Lessons From Machine Learning}}},
  shorttitle = {Choosing {{Prediction Over Explanation}} in {{Psychology}}},
  author = {Yarkoni, Tal and Westfall, Jacob},
  year = {2017},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {12},
  number = {6},
  pages = {1100--1122},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691617693393},
  abstract = {Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology's near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.},
  langid = {english},
  annotation = {00102}
}

@article{yuan_post_2005,
  title = {On the {{Post Hoc Power}} in {{Testing Mean Differences}}},
  author = {Yuan, Ke-Hai and Maxwell, Scott},
  year = {2005},
  month = jun,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {30},
  number = {2},
  pages = {141--167},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/10769986030002141},
  abstract = {Retrospective or post hoc power analysis is recommended by reviewers and editors of many journals. Little literature has been found that gave a serious study of the post hoc power. When the sample size is large, the observed effect size is a good estimator of the true effect size. One would hope that the post hoc power is also a good estimator of the true power. This article studies whether such a power estimator provides valuable information about the true power., Using analytical, numerical, and Monte Carlo approaches, our results show that the estimated power does not provide useful information when the true power is small. It is almost always a biased estimator of the true power. The bias can be negative or positive. Large sample size alone does not guarantee the post hoc power to be a good estimator of the true power. Actually, when the population variance is known, the cumulative distribution function of the post hoc power is solely a function of the population power. This distribution is uniform when the true power equals 0.5 and highly skewed when the true power is near 0 or 1. When the population variance is unknown, the post hoc power behaves essentially the same as when the variance is known.},
  langid = {english}
}

@article{zabell_r_1992,
  title = {R. {{A}}. {{Fisher}} and {{Fiducial Argument}}},
  author = {Zabell, S. L.},
  year = {1992},
  month = aug,
  journal = {Statistical Science},
  volume = {7},
  number = {3},
  pages = {369--387},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177011233},
  abstract = {The fiducial argument arose from Fisher's desire to create an inferential alternative to inverse methods. Fisher discovered such an alternative in 1930, when he realized that pivotal quantities permit the derivation of probability statements concerning an unknown parameter independent of any assumption concerning its a priori distribution. The original fiducial argument was virtually indistinguishable from the confidence approach of Neyman, although Fisher thought its application should be restricted in ways reflecting his view of inductive reasoning, thereby blending an inferential and a behaviorist viewpoint. After Fisher attempted to extend the fiducial argument to the multiparameter setting, this conflict surfaced, and he then abandoned the unconditional sampling approach of his earlier papers for the conditional approach of his later work. Initially unable to justify his intuition about the passage from a probability assertion about a statistic (conditional on a parameter) to a probability assertion about a parameter (conditional on a statistic), Fisher thought in 1956 that he had finally discovered the way out of this enigma with his concept of recognizable subset. But the crucial argument for the relevance of this concept was founded on yet another intuition--one which, now clearly stated, was later demonstrated to be false by Buehler and Feddersen in 1963.},
  keywords = {Behrens-Fisher problem,Fiducial inference,Jerzy Neyman,Maurice Bartlett,R. A. Fisher,recognize subsets}
}

@article{zumbo_note_1998,
  title = {A Note on Misconceptions Concerning Prospective and Retrospective Power},
  author = {Zumbo, Bruno D. and Hubley, Anita M.},
  year = {1998},
  journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  volume = {47},
  number = {2},
  pages = {385--388},
  issn = {1467-9884},
  doi = {10.1111/1467-9884.00139},
  abstract = {This paper addresses some misconceptions concerning prospective and retrospective power when journal editors, reviewers and readers want to know the observed power of statistical tests within a completed study. In addition, a practical solution to both the problem of computing the power after a study has been conducted and the critical evaluation of null findings is suggested},
  langid = {english},
  keywords = {Effect size,Research design,Statistical power,Statistics},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9884.00139}
}


