<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.310">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="description" content="This open educational resource contains information to improve statistical inferences, design better experiments, and report scientific research more transparently.">
<title>Improving Your Statistical Inferences - 9&nbsp; Equivalence Testing and Interval Hypotheses</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./10-sequential.html" rel="next">
<link href="./08-samplesizejustification.html" rel="prev">
<link href="./images/logos/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0MK2WTGRM3"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-0MK2WTGRM3', { 'anonymize_ip': true});
</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><link rel="stylesheet" href="include/booktem.css">
<link rel="stylesheet" href="include/style.css">
<link rel="stylesheet" href="include/webex.css">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./09-equivalencetest.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Equivalence Testing and Interval Hypotheses</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Improving Your Statistical Inferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/Lakens/statistical_inferences" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Improving-Your-Statistical-Inferences.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Improving-Your-Statistical-Inferences.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <a href="https://twitter.com/intent/tweet?url=%7Curl%7C" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-pvalue.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Using <em>p</em>-values to test a hypothesis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-errorcontrol.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Error control</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-likelihoods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Likelihoods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesian statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-questions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Asking Statistical Questions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-effectsize.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Effect Sizes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-CI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Confidence Intervals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-samplesizejustification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sample Size Justification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-equivalencetest.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Equivalence Testing and Interval Hypotheses</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-sequential.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequential Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-meta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Meta-analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bias detection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-prereg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Preregistration and Transparency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-computationalreproducibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Computational Reproducibility</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-researchintegrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Research Integrity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-confirmationbias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Confirmation Bias and Organized Skepticism</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Change Log</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#equivalence-tests" id="toc-equivalence-tests" class="nav-link active" data-scroll-target="#equivalence-tests"><span class="header-section-number">9.1</span> Equivalence tests</a></li>
  <li><a href="#reporting-equivalence-tests" id="toc-reporting-equivalence-tests" class="nav-link" data-scroll-target="#reporting-equivalence-tests"><span class="header-section-number">9.2</span> Reporting Equivalence Tests</a></li>
  <li><a href="#sec-MET" id="toc-sec-MET" class="nav-link" data-scroll-target="#sec-MET"><span class="header-section-number">9.3</span> Minimum Effect Tests</a></li>
  <li><a href="#power-analysis-for-interval-hypothesis-tests" id="toc-power-analysis-for-interval-hypothesis-tests" class="nav-link" data-scroll-target="#power-analysis-for-interval-hypothesis-tests"><span class="header-section-number">9.4</span> Power Analysis for Interval Hypothesis Tests</a></li>
  <li><a href="#sec-ROPE" id="toc-sec-ROPE" class="nav-link" data-scroll-target="#sec-ROPE"><span class="header-section-number">9.5</span> The Bayesian ROPE procedure</a></li>
  <li><a href="#sec-whichinterval" id="toc-sec-whichinterval" class="nav-link" data-scroll-target="#sec-whichinterval"><span class="header-section-number">9.6</span> Which interval width should be used?</a></li>
  <li><a href="#sec-sesoi" id="toc-sec-sesoi" class="nav-link" data-scroll-target="#sec-sesoi"><span class="header-section-number">9.7</span> Setting the Smallest Effect Size of Interest</a></li>
  <li><a href="#specifying-a-sesoi-based-on-theory" id="toc-specifying-a-sesoi-based-on-theory" class="nav-link" data-scroll-target="#specifying-a-sesoi-based-on-theory"><span class="header-section-number">9.8</span> Specifying a SESOI based on theory</a></li>
  <li><a href="#anchor-based-methods-to-set-a-sesoi" id="toc-anchor-based-methods-to-set-a-sesoi" class="nav-link" data-scroll-target="#anchor-based-methods-to-set-a-sesoi"><span class="header-section-number">9.9</span> Anchor based methods to set a SESOI</a></li>
  <li><a href="#specifying-a-sesoi-based-on-a-cost-benefit-analysis" id="toc-specifying-a-sesoi-based-on-a-cost-benefit-analysis" class="nav-link" data-scroll-target="#specifying-a-sesoi-based-on-a-cost-benefit-analysis"><span class="header-section-number">9.10</span> Specifying a SESOI based on a cost-benefit analysis</a></li>
  <li><a href="#specifying-the-sesoi-using-the-small-telescopes-approach" id="toc-specifying-the-sesoi-using-the-small-telescopes-approach" class="nav-link" data-scroll-target="#specifying-the-sesoi-using-the-small-telescopes-approach"><span class="header-section-number">9.11</span> Specifying the SESOI using the small telescopes approach</a></li>
  <li><a href="#setting-the-smallest-effect-size-of-interest-to-the-minimal-statistically-detectable-effect" id="toc-setting-the-smallest-effect-size-of-interest-to-the-minimal-statistically-detectable-effect" class="nav-link" data-scroll-target="#setting-the-smallest-effect-size-of-interest-to-the-minimal-statistically-detectable-effect"><span class="header-section-number">9.12</span> Setting the Smallest Effect Size of Interest to the Minimal Statistically Detectable Effect</a></li>
  <li>
<a href="#test-yourself" id="toc-test-yourself" class="nav-link" data-scroll-target="#test-yourself"><span class="header-section-number">9.13</span> Test Yourself</a>
  <ul class="collapse">
<li><a href="#questions-about-equivalence-tests" id="toc-questions-about-equivalence-tests" class="nav-link" data-scroll-target="#questions-about-equivalence-tests"><span class="header-section-number">9.13.1</span> Questions about equivalence tests</a></li>
  <li><a href="#questions-about-the-small-telescopes-approach" id="toc-questions-about-the-small-telescopes-approach" class="nav-link" data-scroll-target="#questions-about-the-small-telescopes-approach"><span class="header-section-number">9.13.2</span> Questions about the small telescopes approach</a></li>
  <li><a href="#questions-about-specifying-the-sesoi-as-the-minimal-statistically-detectable-effect" id="toc-questions-about-specifying-the-sesoi-as-the-minimal-statistically-detectable-effect" class="nav-link" data-scroll-target="#questions-about-specifying-the-sesoi-as-the-minimal-statistically-detectable-effect"><span class="header-section-number">9.13.3</span> Questions about specifying the SESOI as the Minimal Statistically Detectable Effect</a></li>
  <li><a href="#open-questions" id="toc-open-questions" class="nav-link" data-scroll-target="#open-questions"><span class="header-section-number">9.13.4</span> Open Questions</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/Lakens/statistical_inferences/edit/master/09-equivalencetest.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/Lakens/statistical_inferences/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/Lakens/statistical_inferences/blob/master/09-equivalencetest.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-equivalencetest" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Equivalence Testing and Interval Hypotheses</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p>Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are commonly analyzed with a null hypothesis significance test. When a statistically significant <em>p</em>-value is observed, the null hypothesis can be rejected, and researchers can claim that the intervention works, or that there is a relationship between two variables, with a maximum error rate. But if the <em>p</em>-value is not statistically significant, researchers very often draw a logically incorrect conclusion: They conclude there is no effect based on <em>p</em> &gt; 0.05.</p>
<p>Open a result section of an article you are writing, or the result section of an article you have recently read. Search for “<em>p</em> &gt; 0.05”, and look carefully at what you or the scientists concluded (in the results section, but also check which claim they make in the discussion section). If you see the conclusion that there was ‘no effect’ or there was ‘no association between variables’, you have found an example where researchers forgot that <em>absence of evidence is not evidence of absence</em> <span class="citation" data-cites="altman_statistics_1995">(<a href="references.html#ref-altman_statistics_1995" role="doc-biblioref">Altman &amp; Bland, 1995</a>)</span>. A non-significant result in itself only tells us that we cannot reject the null hypothesis. It is tempting to ask after <em>p</em> &gt; 0.05 ‘so, is the true effect zero’? But the <em>p</em>-value from a null hypothesis significance test cannot answer that question. (remember the concept of 無 (<a href="https://en.wikipedia.org/wiki/Mu_(negative)#Non-dualistic_meaning">mu</a>) discussed in the chapter on <a href="01-pvalue.html#sec-misconception1"><em>p</em> values</a>: the answer is neither yes nor no, but we should ‘unask’ the question).</p>
<p>There should be many situations where researchers are interested in examining whether a meaningful effect is absent. For example, it can be important to show two groups do not differ on factors that might be a confound in the experimental design (e.g., examining whether a manipulation intended to increase fatigue did not affect the mood of the participants, by showing that positive and negative affect did not differ between the groups). Researchers might want to know if two interventions work equally well, especially when the newer intervention costs less or requires less effort (e.g., is online therapy just as efficient as in person therapy?). And other times we might be interested to demonstrate the absence of an effect because a theoretical model predicts there is no effect, or because we believe a previously published study was a false positive, and we expect to show the absence of an effect in a replication study <span class="citation" data-cites="dienes_using_2014">(<a href="references.html#ref-dienes_using_2014" role="doc-biblioref">Dienes, 2014</a>)</span>. And yet, when you ask researchers if they have ever designed a study where the goal was to show that there was no effect, for example by predicting that there would be no difference between two conditions, many people say they have never designed a study where their main prediction was that the effect size was 0. Researchers almost always predict there is a difference. One reason might be that many researchers would not even know how to statistically support a prediction of an effect size of 0, because they were not trained in the use of equivalence testing.</p>
<p>It is never possible to show an effect is <em>exactly</em> 0. Even if you collected data from every person in the world, the effect in any single study will randomly vary around the true effect size of 0 - you might end up with a mean difference that is very close to, but not exactly, zero, in any finite sample. <span class="citation" data-cites="hodges_testing_1954">Hodges &amp; Lehmann (<a href="references.html#ref-hodges_testing_1954" role="doc-biblioref">1954</a>)</span> were the first to discuss the statistical problem of testing whether two populations have the same mean. They suggest (p.&nbsp;264) to: “test that their means do not differ by more than an amount specified to represent the smallest difference of practical interest”. <span class="citation" data-cites="nunnally_place_1960">Nunnally (<a href="references.html#ref-nunnally_place_1960" role="doc-biblioref">1960</a>)</span> similarly proposed a ‘fixed-increment’ hypothesis where researchers compare an observed effect against a range of values that is deemed too small to be meaningful. Defining a range of values considered practically equivalent to the absence of an effect is known as an <strong>equivalence range</strong> <span class="citation" data-cites="bauer_unifying_1996">(<a href="references.html#ref-bauer_unifying_1996" role="doc-biblioref">Bauer &amp; Kieser, 1996</a>)</span> or a <strong>region of practical equivalence</strong> <span class="citation" data-cites="kruschke_bayesian_2013">(<a href="references.html#ref-kruschke_bayesian_2013" role="doc-biblioref">Kruschke, 2013</a>)</span>. The equivalence range should be specified in advance, and requires careful consideration of the smallest effect size of interest.</p>
<p>Although researchers have repeatedly attempted to introduce tests against an equivalence range in the social sciences <span class="citation" data-cites="cribbie_recommendations_2004 levine_communication_2008 hoenig_abuse_2001 rogers_using_1993 quertemont_how_2011">(<a href="references.html#ref-cribbie_recommendations_2004" role="doc-biblioref">Cribbie et al., 2004</a>; <a href="references.html#ref-hoenig_abuse_2001" role="doc-biblioref">Hoenig &amp; Heisey, 2001</a>; <a href="references.html#ref-levine_communication_2008" role="doc-biblioref">Levine et al., 2008</a>; <a href="references.html#ref-quertemont_how_2011" role="doc-biblioref">Quertemont, 2011</a>; <a href="references.html#ref-rogers_using_1993" role="doc-biblioref">Rogers et al., 1993</a>)</span>, this statistical approach has only recently become popular. During the replication crisis, researchers searched for tools to interpret null results when performing replication studies. Researchers wanted to be able to publish informative null results when replicating findings in the literature that they suspected were false positives. One notable example were the studies on pre-cognition by Daryl Bem, which ostensibly showed that participants were able to predict the future <span class="citation" data-cites="bem_feeling_2011">(<a href="references.html#ref-bem_feeling_2011" role="doc-biblioref">Bem, 2011</a>)</span>. Equivalence tests were proposed as a statistical approach to answer the question whether an observed effect is small enough to conclude that a previous study could not be replicated <span class="citation" data-cites="anderson_theres_2016 lakens_equivalence_2017 simonsohn_small_2015">(<a href="references.html#ref-anderson_theres_2016" role="doc-biblioref">Anderson &amp; Maxwell, 2016</a>; <a href="references.html#ref-lakens_equivalence_2017" role="doc-biblioref">Lakens, 2017</a>; <a href="references.html#ref-simonsohn_small_2015" role="doc-biblioref">Simonsohn, 2015</a>)</span>. Researchers specify a smallest effect size of interest (for example an effect of 0.5, so for a two-sided test any value outside a range from -0.5 to 0.5) and test whether effects more extreme than this range can be rejected. If so, they can reject the presence of effects that are deemed large enough to be meaningful.</p>
<p>One can distinguish a <strong>nil null hypothesis</strong>, where the null hypothesis is an effect of 0, from a <strong>non-nil null hypothesis</strong>, where the null hypothesis is any other effect than 0, for example effects more extreme than the smallest effect size of interest <span class="citation" data-cites="nickerson_null_2000">(<a href="references.html#ref-nickerson_null_2000" role="doc-biblioref">Nickerson, 2000</a>)</span>. As Nickerson writes:</p>
<blockquote class="blockquote">
<p>The distinction is an important one, especially relative to the controversy regarding the merits or shortcomings of NHST inasmuch as criticisms that may be valid when applied to nil hypothesis testing are not necessarily valid when directed at null hypothesis testing in the more general sense.</p>
</blockquote>
<p>Equivalence tests are a specific implementation of <strong>interval hypothesis tests</strong>, where instead of testing against a null hypothesis of no effect (that is, an effect size of 0; <strong>nil null hypothesis</strong>), an effect is tested against a null hypothesis that represents a range of non-zero effect sizes (<strong>non-nil null hypothesis</strong>). Indeed, one of the most widely suggested improvements that mitigates the most important limitations of null hypothesis significance testing is to replace the nil null hypothesis with the test of a range prediction (by specifying a non-nil null hypothesis) in an interval hypothesis test <span class="citation" data-cites="lakens_practical_2021">(<a href="references.html#ref-lakens_practical_2021" role="doc-biblioref">Lakens, 2021</a>)</span>. To illustrate the difference, Panel A in <a href="#fig-intervaltest">Figure&nbsp;<span>9.1</span></a> visualizes the results that are predicted in a two-sided null hypothesis test with a nil hypothesis, where the test examines whether an effect of 0 can be rejected. Panel B shows an interval hypothesis where an effect between 0.5 and 2.5 is predicted, where the non-nill null hypothesis consists of values smaller than 0.5 or larger than 2.5, and the interval hypothesis test examines whether values in these ranges can be rejected. Panel C illustrates an equivalence test, which is basically identical to an interval hypothesis test, but the predicted effects are located in a range around 0, and contain effects that are deemed too small to be meaningful.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-intervaltest" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="09-equivalencetest_files/figure-html/fig-intervaltest-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.1: Two-sided null hypothesis test (A), interval hypothesis test (B), equivalence test (C) and minimum effect test (D).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>When an equivalence test is reversed, a researcher designs a study to reject effects less extreme than a smallest effect size of interest (see Panel D in <a href="#fig-intervaltest">Figure&nbsp;<span>9.1</span></a>), it is called a <strong>minimum effect test</strong> <span class="citation" data-cites="murphy_testing_1999">(<a href="references.html#ref-murphy_testing_1999" role="doc-biblioref">Murphy &amp; Myors, 1999</a>)</span>. A researcher might not just be interested in rejecting an effect of 0 (as in a null hypothesis significance test) but in rejecting a range of effects that are too small to be meaningful. All else equal, a study designed to have high power for a minimum effect requires more observations than if the goal had been to reject an effect of zero. As the confidence interval needs to reject a value that is closer to the observed effect size (e.g., 0.1 instead of 0) it needs to be more narrow, which requires more observations.</p>
<p>One benefit of a minimum effect test compared to a null hypothesis test is that there is no distinction between statistical significance and practical significance. As the test value is chosen to represent the minimum effect of interest, whenever it is rejected, the effect is both statistically and practically significant <span class="citation" data-cites="murphy_statistical_2014">(<a href="references.html#ref-murphy_statistical_2014" role="doc-biblioref">Murphy et al., 2014</a>)</span>. Another benefit of minimum effect tests is that, especially in correlational studies in the social sciences, variables are often connected through causal structures that result in real but theoretically uninteresting nonzero correlations between variables, which has been labeled the ‘crud factor’ <span class="citation" data-cites="meehl_appraising_1990 orben_crud_2020">(<a href="references.html#ref-meehl_appraising_1990" role="doc-biblioref">Meehl, 1990</a>; <a href="references.html#ref-orben_crud_2020" role="doc-biblioref">Orben &amp; Lakens, 2020</a>)</span>. Because an effect of zero is unlikely to be true in large correlational datasets, rejecting a nil null hypothesis is not a severe test. Even if the hypothesis is incorrect, it is likely that an effect of 0 will be rejected due to <a href="05-questions.html#sec-crud">‘crud’</a>. For this reason, some researchers have suggested to test against a minimum effect of <em>r</em> = 0.1, as correlations below this threshold are quite common due to theoretically irrelevant correlations between variables <span class="citation" data-cites="ferguson_providing_2021">(<a href="references.html#ref-ferguson_providing_2021" role="doc-biblioref">Ferguson &amp; Heene, 2021</a>)</span>.</p>
<p><a href="#fig-intervaltest">Figure&nbsp;<span>9.1</span></a> illustrates two-sided tests, but it is often more intuitive and logical to perform one-sided tests. In that case, a minimum effect test would, for example, aim to reject effects smaller than 0.1, and an equivalence test would aim to reject effects larger than for example 0.1. Instead of specifying an upper and lower bound of a range, it is sufficient to specify a single value for one-sided tests. A final variation of a one-sided non-nil null hypothesis test is known as a test for <strong>non-inferiority</strong>, which examines if an effect is larger than the lower bound of an equivalence range. Such a test is for example performed when a novel intervention should not be noticeably worse than an existing intervention, but it can be a tiny bit worse. For example, if a difference between a novel and existing intervention is not smaller than -0.1, and effects smaller than -0.1 can be rejected, one can conclude an effect is non-inferior <span class="citation" data-cites="schumi_through_2011 mazzolari_myths_2022">(<a href="references.html#ref-mazzolari_myths_2022" role="doc-biblioref">Mazzolari et al., 2022</a>; <a href="references.html#ref-schumi_through_2011" role="doc-biblioref">Schumi &amp; Wittes, 2011</a>)</span>. We see that extending nil null hypothesis tests to non-nil null hypotheses allow researchers to ask questions that might be more interesting.</p>
<section id="equivalence-tests" class="level2" data-number="9.1"><h2 data-number="9.1" class="anchored" data-anchor-id="equivalence-tests">
<span class="header-section-number">9.1</span> Equivalence tests</h2>
<p>Equivalence tests were first developed in pharmaceutical sciences <span class="citation" data-cites="hauck_new_1984 westlake_use_1972">(<a href="references.html#ref-hauck_new_1984" role="doc-biblioref">Hauck &amp; Anderson, 1984</a>; <a href="references.html#ref-westlake_use_1972" role="doc-biblioref">Westlake, 1972</a>)</span> and later formalized as the <strong>two one-sided tests (TOST)</strong> approach to equivalence testing <span class="citation" data-cites="schuirmann_comparison_1987 seaman_equivalence_1998 wellek_testing_2010">(<a href="references.html#ref-schuirmann_comparison_1987" role="doc-biblioref">Schuirmann, 1987</a>; <a href="references.html#ref-seaman_equivalence_1998" role="doc-biblioref">Seaman &amp; Serlin, 1998</a>; <a href="references.html#ref-wellek_testing_2010" role="doc-biblioref">Wellek, 2010</a>)</span>. The TOST procedure entails performing two one-sided tests to examine whether the observed data is surprisingly larger than a lower equivalence boundary (<span class="math inline">\(\Delta_{L}\)</span>), or surprisingly smaller than an upper equivalence boundary (<span class="math inline">\(\Delta_{U}\)</span>):</p>
<p><span class="math display">\[
t_{L} = \frac{{\overline{M}}_{1} - {\overline{M}}_{2} - \Delta_{L}}{\sigma\sqrt{\frac{1}{n_{1}} + \frac{1}{n_{2}}}}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
t_{U} = \frac{{\overline{M}}_{1} - {\overline{M}}_{2}{- \Delta}_{U}}{\sigma\sqrt{\frac{1}{n_{1}} + \frac{1}{n_{2}}}}
\]</span></p>
<p>where <em>M</em> indicates the means of each sample, <em>n</em> is the sample size, and σ is the pooled standard deviation:</p>
<p><span class="math display">\[
\sigma = \sqrt{\frac{\left( n_{1} - 1 \right)\text{sd}_{1}^{2} + \left( n_{2} - 1 \right)\text{sd}_{2}^{2}}{n_{1} + \ n_{2} - 2}}
\]</span></p>
<p>If both one-sided tests are significant, we can reject the presence of effects large enough to be meaningful. The formulas are highly similar to the normal formula for the <em>t</em>-statistic. The difference between a NHST <em>t</em>-test and the TOST procedure is that the lower equivalence boundary <span class="math inline">\(\Delta_{L}\)</span> and the upper equivalence boundary <span class="math inline">\(\Delta_{U}\)</span> are subtracted from the mean difference between groups (in a normal <em>t</em>-test, we compare the mean difference against 0, and thus the delta drops out of the formula because it is 0).</p>
<p>To perform an equivalence test, you don’t need to learn any new statistical tests, as it is just the well-known <em>t</em>-test against a different value than 0. It is somewhat surprising that the use of <em>t</em>tests to perform equivalence tests is not taught alongside their use in null hypothesis significance tests, as there is some indication that this could prevent common misunderstandings of <em>p</em>-values <span class="citation" data-cites="parkhurst_statistical_2001">(<a href="references.html#ref-parkhurst_statistical_2001" role="doc-biblioref">Parkhurst, 2001</a>)</span>. Let’s look at an example of an equivalence test using the TOST procedure.</p>
<p>In a study where researchers are manipulating fatigue by asking participants to carry heavy boxes around, the researchers want to ensure the manipulation does not inadvertently alter participants’ moods. The researchers assess positive and negative emotions in both conditions, and want to claim there are no differences in positive mood. Let’s assume that positive mood in the experimental fatigue condition (<span class="math inline">\(m_1\)</span> = 4.55, <span class="math inline">\(sd_1\)</span> = 1.05, <span class="math inline">\(n_1\)</span> = 15) did not differ from the mood in the the control condition (<span class="math inline">\(m_2\)</span> = 4.87, <span class="math inline">\(sd_2\)</span> = 1.11, <span class="math inline">\(n_2\)</span> = 15). The researchers conclude: “Mood did not differ between conditions, <em>t</em> = -0.81, <em>p</em> = .42”. Of course, mood did differ between conditions, as 4.55 - 4.87 = -0.32. The claim is that there was no <em>meaningful</em> difference in mood, but to make such a claim in a correct manner, we first need to specify which difference in mood is large enough to be meaningful. For now, let’s assume the researcher consider any effect less extreme half a scale point too small to be meaningful. We now test if the observed mean difference of -0.32 is small enough such that we can reject the presence of effects that are large enough to matter.</p>
<p>The TOSTER package (originally created by myself but recently redesigned by <a href="https://aaroncaldwell.us/">Aaron Caldwell</a>) can be used to plot two <em>t</em>-distributions and their critical regions indicating when we can reject the presence of effects smaller than -0.5 and larger than 0.5. It can take some time to get used to the idea that we are rejecting values more extreme than the equivalence bounds. Try to consistently ask in any hypothesis test: Which values can the test reject? In a nil null hypothesis test, we can reject an effect of 0, and in the equivalence test in the Figure below, we can reject values lower than -0.5 and higher than 0.5. In <a href="#fig-tdistequivalence">Figure&nbsp;<span>9.2</span></a> we see two <em>t</em>-distributions centered on the upper and lower bound of the specified equivalence range (-0.5 and 0.5).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-tdistequivalence" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="09-equivalencetest_files/figure-html/fig-tdistequivalence-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.2: The mean difference and its confidence interval plotted below the <em>t</em>-distributions used to perform the two-one-sided tests against -0.5 and 0.5.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Below the two curves we see a line that represents the confidence interval ranging from -0.99 to 0.35, and a dot on the line that indicates the observed mean difference of -0.32. Let’s first look at the left curve. We see the green highlighted area in the tails that highlights which observed mean differences would be extreme enough to statistically reject an effect of -0.5. Our observed mean difference of -0.32 lies very close to -0.5, and if we look at the left distribution, the mean is not far enough away from -0.5 to fall in the green area that indicates when observed differences would be statistically significant. We can also perform the equivalence test using the TOSTER package, and look at the results.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>TOSTER<span class="sc">::</span><span class="fu">tsum_TOST</span>(<span class="at">m1 =</span> <span class="fl">4.55</span>, </span>
<span id="cb1-2"><a href="#cb1-2"></a>                  <span class="at">m2 =</span> <span class="fl">4.87</span>, </span>
<span id="cb1-3"><a href="#cb1-3"></a>                  <span class="at">sd1 =</span> <span class="fl">1.05</span>, </span>
<span id="cb1-4"><a href="#cb1-4"></a>                  <span class="at">sd2 =</span> <span class="fl">1.11</span>,</span>
<span id="cb1-5"><a href="#cb1-5"></a>                  <span class="at">n1 =</span> <span class="dv">15</span>, </span>
<span id="cb1-6"><a href="#cb1-6"></a>                  <span class="at">n2 =</span> <span class="dv">15</span>, </span>
<span id="cb1-7"><a href="#cb1-7"></a>                  <span class="at">low_eqbound =</span> <span class="sc">-</span><span class="fl">0.5</span>, </span>
<span id="cb1-8"><a href="#cb1-8"></a>                  <span class="at">high_eqbound =</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Welch Modified Two-Sample t-Test

The equivalence test was non-significant, t(27.91) = 0.456, p = 3.26e-01
The null hypothesis test was non-significant, t(27.91) = -0.811, p = 4.24e-01
NHST: don't reject null significance hypothesis that the effect is equal to zero 
TOST: don't reject null equivalence hypothesis

TOST Results 
                 t    df p.value
t-test     -0.8111 27.91   0.424
TOST Lower  0.4563 27.91   0.326
TOST Upper -2.0785 27.91   0.023

Effect Sizes 
               Estimate     SE              C.I. Conf. Level
Raw             -0.3200 0.3945 [-0.9912, 0.3512]         0.9
Hedges's g(av)  -0.2881 0.3930 [-0.8733, 0.3021]         0.9
Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
</div>
</div>
<p>In the line ‘t-test’ the output shows the traditional nil null hypothesis significance test (which we already knew was not statistically significant: <em>t</em> = 0.46, <em>p</em> = 0.42. Just like the default <em>t</em>-test in R, the tsum_TOST function will by default calculate Welch’s <em>t</em>-test (instead of Student’s <em>t</em>-test), which is a better default <span class="citation" data-cites="delacre_why_2017">(<a href="references.html#ref-delacre_why_2017" role="doc-biblioref">Delacre et al., 2017</a>)</span>, but you can request Student’s <em>t</em>-test by adding <code>var.equal = TRUE</code> as an argument to the function.</p>
<p>We also see a test indicated by TOST Lower. This is the first one-sided test examining if we can reject effects lower than -0.5. From the test result, we see this is not the case: <em>t</em> = 0.46, <em>p</em> = 0.33. This is an ordinary <em>t</em>-test, just against an effect of -0.5. Because we cannot reject differences more extreme than -0.5, it is possible that a difference we consider meaningful (e.g., a difference of -0.60) is present. When we look at the one-sided test against the upper bound of the equivalence range (0.5) we see that we can statistically reject the presence of mood effects larger than 0.5, as in the line TOST Upper we see <em>t</em> = -2.08, <em>p</em> = 0.02. Our final conclusion is therefore that, even though we can reject effects more extreme than 0.5 based on the observed mean difference of -0.32, we cannot reject effects more extreme than -0.5. Therefore, we cannot completely reject the presence of meaningful mood effects. As the data does not allow us to claim the effect is different from 0, nor that the effect is, if anything, too small to matter (based on an equivalence range from -0.5 to 0.5), the data are <strong>inconclusive</strong>. We cannot distinguish between a Type 2 error (there is an effect, but in this study we just did not detect it) or a true negative (there really is no effect large enough to matter).</p>
<p>Note that because we fail to reject the one-sided test against the lower equivalence bound, the possibility remains that there is a true effect size that is large enough to be considered meaningful. This statement is true, even when the effect size we have observed (-0.32) is closer to zero than to the equivalence bound of -0.5. One might think the observed effect size needs to be more extreme (i.e., &lt; -0.5 or &gt; 0.5) than the equivalence bound to maintain the possibility that there is an effect that is large enough to be considered meaningful. But that is not required. The 90% CI indicates that some values below -0.5 cannot be rejected. As we can expect that 90% of confidence intervals in the long run capture the true population parameter, it is perfectly possible that the true effect size is more extreme than -0.5. And, the effect might even be more extreme than the values captured by this confidence interval, as 10% of the time, the computed confidence interval is expected to not contain the true effect size. Therefore, when we fail to reject the smallest effect size of interest, we retain the possibility that an effect of interest exists. If we can reject the nil null hypothesis, but fail to reject values more extreme than the equivalence bounds, then we can claim there is an effect, and it might be large enough to be meaningful.</p>
<p>One way to reduce the probability of an inconclusive effect is to collect sufficient data. Let’s imagine the researchers had not collected 15 participants in each condition, but 200 participants. They otherwise observe exactly the same data. As explained in the chapter on <a href="07-CI.html">confidence intervals</a>, as the sample size increases, the confidence interval becomes more narrow. For a TOST equivalence test to be able to reject both the upper and lower bound of the equivalence range, the confidence interval needs to fall completely within the equivalence range. In <a href="#fig-ciequivalence1">Figure&nbsp;<span>9.3</span></a> we see the same result as in <a href="#fig-tdistequivalence">Figure&nbsp;<span>9.2</span></a>, but now if we had collected 200 observations. Because of the larger sample size, the confidence is more narrow than when we collected 15 participants. We see that the 90% confidence interval around the observed mean difference now excludes both the upper and lower equivalence bound. This means that we can now reject effects outside of the equivalence range (even though barely, with a <em>p</em> = 0.048 as the one-sided test against the lower equivalence bound is only just statistically significant).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output cell-output-stdout">
<pre><code>
Welch Modified Two-Sample t-Test

The equivalence test was significant, t(396.78) = 1.666, p = 4.82e-02
The null hypothesis test was significant, t(396.78) = -2.962, p = 3.24e-03
NHST: reject null significance hypothesis that the effect is equal to zero 
TOST: reject null equivalence hypothesis

TOST Results 
                t    df p.value
t-test     -2.962 396.8   0.003
TOST Lower  1.666 396.8   0.048
TOST Upper -7.590 396.8 &lt; 0.001

Effect Sizes 
               Estimate    SE               C.I. Conf. Level
Raw             -0.3200 0.108 [-0.4981, -0.1419]         0.9
Hedges's g(av)  -0.2956 0.104 [-0.4605, -0.1304]         0.9
Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-ciequivalence1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="09-equivalencetest_files/figure-html/fig-ciequivalence1-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.3: The mean difference and its confidence interval for an equivalence test with an equivalence range of -0.5 and 0.5.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In <a href="#fig-ciequivalence2">Figure&nbsp;<span>9.4</span></a> we see the the same results, but now visualized as a confidence density plot <span class="citation" data-cites="schweder_confidence_2016">(<a href="references.html#ref-schweder_confidence_2016" role="doc-biblioref">Schweder &amp; Hjort, 2016</a>)</span>, which is a graphical summary of the distribution of confidence. A confidence density plot allows you to see which effects can be rejected with difference confidence interval widths. We see the bounds of the green area (corresponding to a 90% confidence interval) fall inside the equivalence bounds. Thus, the equivalence test is statistically significant, and we can statistically reject the presence of effects outside the equivalence range. We can also see that the 95% confidence interval excludes 0, and therefore, a traditional null hypothesis significance test is also statistically significant.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ciequivalence2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="09-equivalencetest_files/figure-html/fig-ciequivalence2-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.4: The mean difference and its confidence interval for an equivalence test with an equivalence range of -0.5 and 0.5.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In other words, both the null hypothesis test and the equivalence test have yielded significant results. This means we can claim that the observed effect is statistically different from zero, and that the effect is statistically smaller than effects we deemed large enough to matter when we specified the equivalence range from -0.5 to 0.5. This illustrates how combining equivalence tests and nil null hypothesis tests can prevent us from mistaking statistically significant effects for practically significant effects. In this case, with 200 participants, we can reject an effect of 0, but the effect, if any, is not large enough to be meaningful.</p>
</section><section id="reporting-equivalence-tests" class="level2" data-number="9.2"><h2 data-number="9.2" class="anchored" data-anchor-id="reporting-equivalence-tests">
<span class="header-section-number">9.2</span> Reporting Equivalence Tests</h2>
<p>It is common practice to only report the test yielding the higher <em>p</em>-value of the two one-sided tests when reporting an equivalence test. Because both one-sided tests need to be statistically significant to reject the null hypothesis in an equivalence test (i.e., the presence of effects large enough to matter), when the larger of the two hypothesis tests rejects the equivalence bound, so does the other test. Unlike in null hypothesis significance tests it is not common to report standardized effect sizes for equivalence tests, but there can be situations where researchers might want to discuss how far the effect is removed from the equivalence bounds on the raw scale. Prevent the erroneous interpretation to claim there is ‘no effect’, that an effect is ‘absent’, that the true effect size is ‘zero’, or vague verbal descriptions, such as that two groups yielded ‘similar’ or ‘comparable’ data. A significant equivalence test rejects effects more extreme than the equivalence bounds. Smaller true effects have not been rejected, and thus it remains possible that there is a true effect. Because a TOST procedure is a frequentist test based on a <em>p</em>-value, all other <a href="01-pvalue.html#sec-misconceptions">misconceptions of <em>p</em>-values</a> should be prevented as well.</p>
<p>When summarizing the main result of an equivalence test, for example in an abstract, always report the equivalence range that the data is tested against. Reading ‘based on an equivalence test we concluded the absence of a meaningful effect’ means something very different if the equivalence bounds were <em>d</em> =-0.9 to 0.9 than when the bounds were <em>d</em> =-0.2 to <em>d</em> =0.2. So instead, write ‘based on an equivalence test with an equivalence range of <em>d</em> =-0.2 to 0.2, we conclude the absence of an effect we deemed meaningful’. Of course, whether peers agree you have correctly concluded the absence of a meaningful effect depends on whether they agree with your justification for a smallest effect of interest! A more neutral conclusion would be a statement such as: ‘based on an equivalence test, we rejected the presence of effects more extreme than -0.2 to 0.2, so we can act (with an error rate of alpha) as if the effect, if any, is less extreme than our equivalence range’. Here, you do not use value-laden terms such as ‘meaningful’. If both a null hypothesis test and an equivalence test are non-significant, the finding is best described as ‘inconclusive’: There is not enough data to reject the null, or the smallest effect size of interest. If both the null hypothesis test and the equivalence test are statistically significant, you can claim there is an effect, but at the same time claim the effect is too small to be of interest (given your justification for the equivalence range).</p>
<p>Equivalence bounds can be specified in raw effect sizes, or in standardized mean differences. It is better to specify the equivalence bounds in terms of raw effect sizes. Setting them in terms of Cohen’s <em>d</em> leads to bias in the statistical test, as the observed standard deviation has to be used to translate the specified Cohen’s <em>d</em> into a raw effect size for the equivalence test (and when you set equivalence bounds in standardized mean differences, TOSTER will warn: “Warning: setting bound type to SMD produces biased results!”). The bias is in practice not too problematic in any single equivalence test, and being able to specify the equivalence bounds in standardized mean differences lowers the threshold to perform an equivalence test when they do not know the standard deviation of their measure. But as equivalence testing becomes more popular, and fields establish smallest effect sizes of interest, they should do so in raw effect size differences, not in standardized effect size differences.</p>
</section><section id="sec-MET" class="level2" data-number="9.3"><h2 data-number="9.3" class="anchored" data-anchor-id="sec-MET">
<span class="header-section-number">9.3</span> Minimum Effect Tests</h2>
<p>If a researcher has specified a smallest effect size of interest, and is interested in testing whether the effect in the population is larger than this smallest effect of interest, a minimum effect test can be performed. As with any hypothesis test, we can reject the smallest effect of interest whenever the confidence interval around the observed effect does not overlap with it. In the case of a minimum effect test, however, the confidence interval should be fall completely beyond the smallest effect size of interest. For example, let’s assume a researcher performs a minimum effect test with 200 observations per condition against a smallest effect size of interest of a mean difference of 0.5.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output cell-output-stdout">
<pre><code>
Welch Modified Two-Sample t-Test

The minimal effect test was significant, t(396.78) = 12.588, p = 4.71e-04
The null hypothesis test was significant, t(396.78) = 7.960, p = 1.83e-14
NHST: reject null significance hypothesis that the effect is equal to zero 
TOST: reject null MET hypothesis

TOST Results 
                t    df p.value
t-test      7.960 396.8 &lt; 0.001
TOST Lower 12.588 396.8       1
TOST Upper  3.332 396.8 &lt; 0.001

Effect Sizes 
               Estimate    SE             C.I. Conf. Level
Raw              0.8600 0.108 [0.6819, 1.0381]         0.9
Hedges's g(av)   0.7945 0.125 [0.6234, 0.9646]         0.9
Note: SMD confidence intervals are an approximation. See vignette("SMD_calcs").</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-tmet" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="09-equivalencetest_files/figure-html/fig-tmet-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.5: The mean difference and its confidence interval plotted below the <em>t</em>-distributions used to perform the two-one-sided tests against -0.5 and 0.5 when performing a minimum effect test.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Below the two curves we again see a line that represents the confidence interval ranging from 0.68 to 1.04, and a dot on the line that indicates the observed mean difference of 0.86. The entire confidence interval lies well above the minimum effect of 0.5, and we can therefore not just reject the nil null hypothesis, but also effects smaller than the minimum effect of interest. Therefore, we can claim that the effect is large enough to be not just statistically significant, but also practically significant (as long as we have justified our smallest effect size of interest well). Because we have performed a two-sided minimum effect test, the minimum effect test would also have been significant if the confidence interval had been completely on the opposite side of -0.5.</p>
<p>Earlier we discussed how combining traditional NHST and an equivalence test could lead to more informative results. It is also possible to combine a minimum effect test and an equivalence test. One might even say that such a combination is the most informative test of a prediction whenever a smallest effect size of interest can be specified. In principle, this is true. As long as we are able to collect enough data, we will always get an informative and straightforward answer when we combine a minimum effect test with an equivalence test: Either we can reject all effects that are too small to be of interest, or we can reject all effects that are large enough to be of interest. As we will see below in the section on power analysis for interval hypotheses, whenever the true effect size is close to the smallest effect size of interest, a large amount of observations will need to be collected. And if the true effect size happens to be identical to the smallest effect size of interest, neither the minimum effect test nor the equivalence test can be correctly rejected (and any significant test would be a Type 1 error). If a researcher can collect sufficient data (so that the test has high statistical power), and is relatively confident that the true effect size will be larger or smaller than the smallest effect of interest, then the combination of a minimum effect test and an equivalence test can be attractive as such a hypothesis test is likely to yield an informative answer to the research question.</p>
</section><section id="power-analysis-for-interval-hypothesis-tests" class="level2" data-number="9.4"><h2 data-number="9.4" class="anchored" data-anchor-id="power-analysis-for-interval-hypothesis-tests">
<span class="header-section-number">9.4</span> Power Analysis for Interval Hypothesis Tests</h2>
<p>When designing a study it is a sensible strategy to always plan for both the presence and the absence of an effect. Several scientific journals require a sample size justification for Registered Reports where the statistical power to reject the null hypothesis is high, but where the study is also capable of demonstrating the absence of an effect, for example by also performing a power analysis for an equivalence test. As we saw in the chapter on <a href="02-errorcontrol.html">error control</a> and <a href="03-likelihoods.html">likelihoods</a> null results are to be expected, and if you only think about the possibility of observing a null effect when the data has been collected, it is often too late.</p>
<p>The statistical power for interval hypotheses depend on the alpha level, the sample size, the smallest effect of interest you decide to test against, and the true effect size. For an equivalence test, it is common to perform a power analysis assuming the true effect size is 0, but this might not always be realistic. The closer the expected effect size is to the smallest effect size of interest, the larger the sample size needed to reach a desired power. Don’t be tempted to assume a true effect size of 0, if you have good reason to expect a small but non-zero true effect size. The sample size that the power analysis indicates you need to collect might be smaller, but in reality you also have a higher probability of an inconclusive result. Earlier versions of TOSTER only enabled researchers to perform power analyses for equivalence tests assuming a true effect size of 0, but a new power function by Aaron Caldwell allows users to specify <code>delta</code>, the expected effect size.</p>
<p>Assume a researchers desired to achieve 90% power for an equivalence test with an equivalence range from -0.5 to 0.5, with an alpha level of 0.05, and assuming a population effect size of 0. A power analysis for an equivalence test can be performed to examine the required sample size.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>TOSTER<span class="sc">::</span><span class="fu">power_t_TOST</span>(<span class="at">power =</span> <span class="fl">0.9</span>, <span class="at">delta =</span> <span class="dv">0</span>,</span>
<span id="cb5-2"><a href="#cb5-2"></a>                     <span class="at">alpha =</span> <span class="fl">0.05</span>, <span class="at">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb5-3"><a href="#cb5-3"></a>                     <span class="at">low_eqbound =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">high_eqbound =</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample TOST power calculation 

          power = 0.9
           beta = 0.1
          alpha = 0.05
              n = 87.26261
          delta = 0
             sd = 1
         bounds = -0.5, 0.5

NOTE: n is number in *each* group</code></pre>
</div>
</div>
<p>We see that the required sample size is 88 participants in each condition for the independent <em>t</em>-test. Let’s compare this power analysis to a situation where the researcher expects a true effect of <em>d</em> = 0.1, instead of a true effect of 0. To be able to reliably reject effects larger than 0.5, we will need a larger sample size, just as how we need a larger sample size for a null hypothesis test powered to detect <em>d</em> = 0.4 than a null hypothesis test powered to detect <em>d</em> = 0.5.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>TOSTER<span class="sc">::</span><span class="fu">power_t_TOST</span>(<span class="at">power =</span> <span class="fl">0.9</span>, <span class="at">delta =</span> <span class="fl">0.1</span>,</span>
<span id="cb7-2"><a href="#cb7-2"></a>                     <span class="at">alpha =</span> <span class="fl">0.05</span>, <span class="at">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb7-3"><a href="#cb7-3"></a>                     <span class="at">low_eqbound =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">high_eqbound =</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample TOST power calculation 

          power = 0.9
           beta = 0.1
          alpha = 0.05
              n = 108.9187
          delta = 0.1
             sd = 1
         bounds = -0.5, 0.5

NOTE: n is number in *each* group</code></pre>
</div>
</div>
<p>We see the sample size has now increased to 109 participants in each condition. As mentioned before, it is not necessary to perform a two-sided equivalence test. It is also possible to perform a one-sided equivalence test. An example of a situation where such a directional test is appropriate is a replication study. If a previous study observed an effect of <em>d</em> = 0.48, and you perform a replication study, you might decide to consider any effect smaller than <em>d</em> = 0.2 a failure to replicate - including any effect in the opposite direction, such as an effect of <em>d</em> = -0.3. Although most software for equivalence tests requires you to specify an upper and lower bound for an equivalence range, you can mimic a one-sided test by setting the equivalence bound in the direction you want to ignore to a low value so that the one-sided test against this value will always be statistically significant. This can also be used to perform a power analysis for a minimum effect test, where one bound is the minimum effect of interest, and the other bound is set to an extreme value on the other side of the expected effect size.</p>
<p>In the power analysis for an equivalence test example below, the lower bound is set to -5 (it should be set low enough such that lowering it even further has no noticeable effect). We see that the new power function in the TOSTER package takes the directional prediction into account, and just as with directional predictions in a nil null hypothesis test, a directional prediction in an equivalence test is more efficient, and only 70 observations are needed to achieve 90% power.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># New TOSTER power functions allows power for expected non-zero effect.</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>TOSTER<span class="sc">::</span><span class="fu">power_t_TOST</span>(<span class="at">power =</span> <span class="fl">0.9</span>, <span class="at">delta =</span> <span class="dv">0</span>,</span>
<span id="cb9-3"><a href="#cb9-3"></a>                     <span class="at">alpha =</span> <span class="fl">0.05</span>, <span class="at">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb9-4"><a href="#cb9-4"></a>                     <span class="at">low_eqbound =</span> <span class="sc">-</span><span class="dv">5</span>, <span class="at">high_eqbound =</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample TOST power calculation 

          power = 0.9
           beta = 0.1
          alpha = 0.05
              n = 69.19784
          delta = 0
             sd = 1
         bounds = -5.0, 0.5

NOTE: n is number in *each* group</code></pre>
</div>
</div>
<p>Statistical software offers options for power analyses for some statistical tests, but not for all tests. Just as with power analysis for a nil null hypothesis test, it can be necessary to use a simulation-based approach to power analysis.</p>
</section><section id="sec-ROPE" class="level2" data-number="9.5"><h2 data-number="9.5" class="anchored" data-anchor-id="sec-ROPE">
<span class="header-section-number">9.5</span> The Bayesian ROPE procedure</h2>
<p>In Bayesian estimation, one way to argue for the absence of a meaningful effect is the <strong>region of practical equivalence</strong> (ROPE) procedure <span class="citation" data-cites="kruschke_bayesian_2013">(<a href="references.html#ref-kruschke_bayesian_2013" role="doc-biblioref">Kruschke, 2013</a>)</span>, which is “somewhat analogous to frequentist equivalence testing” <span class="citation" data-cites="kruschke_bayesian_2017">(<a href="references.html#ref-kruschke_bayesian_2017" role="doc-biblioref">Kruschke &amp; Liddell, 2017</a>)</span>. In the ROPE procedure, an equivalence range is specified, just as in equivalence testing, but the Bayesian highest density interval based on a posterior distribution (as explained in the chapter on <a href="04-bayes.html">Bayesian statistics</a>) is used instead of the confidence interval.</p>
<p>If the prior used by Kruschke was perfectly uniform, and the ROPE procedure and an equivalence test used the same confidence interval (e.g., 90%), the two tests would yield identical results. There would only be philosophical differences in how the numbers are interpreted. The <code>BEST</code> package in R that can be used to perform the ROPE procedure by default uses a ‘broad’ prior, and therefore results of the ROPE procedure and an equivalence test are not exactly the same, but they are very close. One might even argue the two tests are ‘practically equivalent’. In the R code below, random normally distributed data for two conditions is generated (with means of 0 and a standard deviation of 1) and the ROPE procedure and a TOST equivalence test are performed.</p>
<div class="cell" data-layout-align="center" data-hash="09-equivalencetest_cache/html/unnamed-chunk-6_b292e780813c94ef3def5e0188d27ca4">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="09-equivalencetest_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="09-equivalencetest_files/figure-html/unnamed-chunk-6-2.png" class="img-fluid figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>The 90% HDI ranges from -0.06 to 0.39, with an estimated mean based on the prior and the data of 0.164. The HDI falls completely between the upper and the lower bound of the equivalence range, and therefore values more extreme than -0.5 or 0.5 are deemed implausible. The 95% CI ranges from -0.07 to 0.36 with an observed mean difference of 0.15. We see that the numbers are not identical, because in Bayesian estimation the observed values are combined with a prior, and the mean estimate is not purely based on the data. But the results are very similar, and will in most cases lead to similar inferences. The BEST R package also enables researchers to perform simulation based power analyses, which take a long time but, when using a broad prior, yield a result that is basically identical to the sample size from a power analysis for an equivalence test. The biggest benefit of ROPE over TOST is that it allows you to incorporate prior information. If you have reliable prior information, ROPE can use this information, which is especially useful if you don’t have a lot of data. If you use informed priors, check the robustness of the posterior against reasonable changes in the prior in sensitivity analyses.</p>
</section><section id="sec-whichinterval" class="level2" data-number="9.6"><h2 data-number="9.6" class="anchored" data-anchor-id="sec-whichinterval">
<span class="header-section-number">9.6</span> Which interval width should be used?</h2>
<p>Because the TOST procedure is based on two one-sided tests, a 90% confidence interval is used when the one-sided tests are performed at an alpha level of 5%. Because both the test against the upper bound and the test against the lower bound needs to be statistically significant to declare equivalence (which as explained in the chapter on <a href="#sec-multiplecomparisons">error control</a> is an intersection-union approach to multiple testing) it is not necessary to correct for the fact that two tests are performed. If the alpha level is adjusted for multiple comparisons, or if the alpha level is justified instead of relying on the default 5% level (or both), the corresponding confidence interval should be used, where CI = 100 - (2 * <span class="math inline">\(\alpha\)</span>). Thus, the width of the confidence interval is directly related to the choice for the alpha level, as we are making decisions to reject the smallest effect size of interest, or not, based on whether the confidence interval excluded the effect that is tested against.</p>
<p>When using a Highest Density Interval from a Bayesian perspective, such as the ROPE procedure, the choice for a width of a confidence interval does not follow logically from a desired error rate, or any other principle. Kruschke <span class="citation" data-cites="kruschke_doing_2014">(<a href="references.html#ref-kruschke_doing_2014" role="doc-biblioref">2014</a>)</span> writes: “How should we define ‘reasonably credible’? One way is by saying that any points within the 95% HDI are reasonably credible.” McElreath <span class="citation" data-cites="mcelreath_statistical_2016">(<a href="references.html#ref-mcelreath_statistical_2016" role="doc-biblioref">2016</a>)</span> has recommended the use of 67%, 89%, and 97%, because “No reason. They are prime numbers, which makes them easy to remember.”. Both these suggestions lack a solid justification. As Gosset (or Student), observed <span class="citation" data-cites="gosset_application_1904">(<a href="references.html#ref-gosset_application_1904" role="doc-biblioref">1904</a>)</span>:</p>
<blockquote class="blockquote">
<p>Results are only valuable when the amount by which they probably differ from the truth is so small as to be insignificant for the purposes of the experiment. What the odds selected should be depends-<br>
1. On the degree of accuracy which the nature of the experiment allows, and<br>
2. On the importance of the issues at stake.</p>
</blockquote>
<p>There are only two principled solutions. First, if a highest density interval width is used to make claims, these claims will be made with certain error rates, and researchers should quantify the risk of erroneous claims by computing frequentist error rates. This would make the ROPE procedure a Bayesian/Frequentist compromise procedure, where the computation of a posterior distribution allows for Bayesian interpretations of which parameters values are believed to be most probable, while decisions based on whether or not the HDI falls within an equivalence range have a formally controlled error rate. Note that when using an informative prior, an HDI does not match a CI, and the error rate when using an HDI can only be derived through simulations. The second solution is to not make any claims, present the full posterior distribution, and let readers draw their own conclusions.</p>
</section><section id="sec-sesoi" class="level2" data-number="9.7"><h2 data-number="9.7" class="anchored" data-anchor-id="sec-sesoi">
<span class="header-section-number">9.7</span> Setting the Smallest Effect Size of Interest</h2>
<p>To perform an equivalence test we need to specify which observed values are too small to be meaningful. We can never say that an effect is exactly zero, but we can examine whether observed effects are too small to be theoretically or practically interesting. This requires that we specify the <strong>smallest effect size of interest</strong> (SESOI). The same concept goes by many names, such as a minimal important difference, or clinically significant difference <span class="citation" data-cites="king_point_2011">(<a href="references.html#ref-king_point_2011" role="doc-biblioref">King, 2011</a>)</span>. Take a moment to think about what the smallest effect size is that you would still consider theoretically or practically meaningful for the next study you are designing. It might be difficult to determine what the smallest effect size is that you would consider interesting, and the question what the smallest effect size of interest is might be something you have never really thought about to begin with. However, determining your smallest effect size of interest has important practical benefits. First, if researchers in a field are able to specify which effects would be too small to matter, it becomes very straightforward to power a study for the effects that are meaningful. The second benefit of specifying the smallest effect size of interest is that it makes your study falsifiable. Having your predictions falsified by someone else might not feel that great for you personally, but it is quite useful for science as a whole <span class="citation" data-cites="popper_logic_2002">(<a href="references.html#ref-popper_logic_2002" role="doc-biblioref">Popper, 2002</a>)</span>. After all, if there is no way a prediction can be wrong, why would anyone be impressed if the prediction is right?</p>
<p>To start thinking about which effect sizes matter, ask yourself whether <em>any</em> effect in the predicted direction is actually support for the alternative hypothesis. For example, would an effect size of a Cohen’s <em>d</em> of 10 be support for your hypothesis? In psychology, it should be rare that a theory predicts such a huge effect, and if you observed a <em>d</em> = 10, you would probably check for either a computation error, or a confound in the study. On the other end of the scale, would an effect of <em>d</em> = 0.001 be in line with the theoretically proposed mechanism? Such an effect is incredibly small, and is well below what an individual would notice, as it would fall below the <strong>just noticeable difference</strong> given perceptual and cognitive limitations. Therefore, a <em>d</em> = 0.001 would in most cases lead researchers to conclude “Well, this is really too small to be something that my theory has predicted, and such a small effect is practically equivalent to the absence of an effect.” However, when we make a directional prediction, we say that these types of effects are all part of our alternative hypothesis. Even though many researchers would agree such tiny effects are too small to matter, they still officially support for our alternative hypothesis if we have a directional prediction with a nil null hypothesis. Furthermore, researchers rarely have the resources to statistically reject the presence of effects this small, so the claim that such effects would still support a theoretical prediction makes the theory <strong>practically unfalsifiable</strong>: A researcher could simply respond to any replication study showing a non-significant small effect (e.g., <em>d</em> = 0.05) by saying: “That does not falsify my prediction. I suppose the effect is just a bit smaller than <em>d</em> = 0.05”, without ever having to admit the prediction is falsified. This is problematic, because if we do not have a process of replication and falsification, a scientific discipline risks a slide towards the unfalsifiable <span class="citation" data-cites="ferguson_vast_2012">(<a href="references.html#ref-ferguson_vast_2012" role="doc-biblioref">Ferguson &amp; Heene, 2012</a>)</span>. So whenever possible, when you design an experiment or you have a theory and a theoretical prediction, carefully think about, and clearly state, what the smallest effect size of interest is.</p>
</section><section id="specifying-a-sesoi-based-on-theory" class="level2" data-number="9.8"><h2 data-number="9.8" class="anchored" data-anchor-id="specifying-a-sesoi-based-on-theory">
<span class="header-section-number">9.8</span> Specifying a SESOI based on theory</h2>
<p>One example of a theoretically predicted smallest effect size of interest can be found in the study by Burriss et al. <span class="citation" data-cites="burriss_changes_2015">(<a href="references.html#ref-burriss_changes_2015" role="doc-biblioref">2015</a>)</span>, who examined whether women displayed increased redness in the face during the fertile phase of their ovulatory cycle. The hypothesis was that a slightly redder skin signals greater attractiveness and physical health, and that sending this signal to men yields an evolutionary advantage. This hypothesis presupposes that men can detect the increase in redness with the naked eye. Burriss et al.&nbsp;collected data from 22 women and showed that the redness of their facial skin indeed increased during their fertile period. However, this increase was not large enough for men to detect with the naked eye, so the hypothesis was falsified. Because the just-noticeable difference in redness of the skin can be measured, it was possible to establish a theoretically motivated SESOI. A theoretically motivated smallest effect size of interest can be derived from just-noticeable differences, which provide a lower bound on effect sizes that can influence individuals, or based on computational models, which can provide a lower bound on parameters in the model that will still be able to explain observed findings in the empirical literature.</p>
</section><section id="anchor-based-methods-to-set-a-sesoi" class="level2" data-number="9.9"><h2 data-number="9.9" class="anchored" data-anchor-id="anchor-based-methods-to-set-a-sesoi">
<span class="header-section-number">9.9</span> Anchor based methods to set a SESOI</h2>
<p>Building on the idea of a just-noticeable difference, psychologists are often interested in effects that are large enough to be noticed by single individuals. One procedure to estimate what constitutes a meaningful change on an individual level is the anchor-based method <span class="citation" data-cites="jaeschke_measurement_1989 norman_truly_2004 king_point_2011">(<a href="references.html#ref-jaeschke_measurement_1989" role="doc-biblioref">Jaeschke et al., 1989</a>; <a href="references.html#ref-king_point_2011" role="doc-biblioref">King, 2011</a>; <a href="references.html#ref-norman_truly_2004" role="doc-biblioref">Norman et al., 2004</a>)</span>. Measurements are collected at two time points (e.g., a quality of life measure before and after treatment). At the second time point, an independent measure (the anchor) is used to determine if individuals show no change compared to time point 1, or if they have improved, or worsened. Often, the patient is directly asked to answer the anchor question, and indicate if they subjectively feel the same, better, or worse at time point 2 compared to time point 1. <span class="citation" data-cites="button_minimal_2015">Button et al. (<a href="references.html#ref-button_minimal_2015" role="doc-biblioref">2015</a>)</span> used an anchor-based method to estimate that a minimal clinically important difference on the Beck Depression Inventory corresponded to a 17.5% reduction in scores from baseline.</p>
<p>Anvari and Lakens <span class="citation" data-cites="anvari_using_2021">(<a href="references.html#ref-anvari_using_2021" role="doc-biblioref">2021</a>)</span> applied the anchor-based method to examine a smallest effect of interest as measured by the widely used Positive and Negative Affect Scale (PANAS). Participants completed the 20 item PANAS at two time points several days apart (using a Likert scale going from 1 = “very slightly or not at all”, to 5 = “extremely”). At the second time point they were also asked to indicate if their affect had changed a little, a lot, or not at all. When people indicated their affect had changed “a little”, the average change in Likert units was 0.26 scale points for positive affect and 0.28 scale points for negative affect. Thus, an intervention to improve people’s affective state that should lead to what individuals subjectively consider at least a little improvement might set the SESOI at 0.3 units on the PANAS.</p>
</section><section id="specifying-a-sesoi-based-on-a-cost-benefit-analysis" class="level2" data-number="9.10"><h2 data-number="9.10" class="anchored" data-anchor-id="specifying-a-sesoi-based-on-a-cost-benefit-analysis">
<span class="header-section-number">9.10</span> Specifying a SESOI based on a cost-benefit analysis</h2>
<p>Another principled approach to justify a smallest effect size of interest is to perform a cost-benefit analysis. Research shows that cognitive training may improve mental abilities in older adults which might benefit older drivers <span class="citation" data-cites="ball_effects_2002">(<a href="references.html#ref-ball_effects_2002" role="doc-biblioref">Ball et al., 2002</a>)</span>. Based on these findings, Viamonte, Ball, and Kilgore <span class="citation" data-cites="viamonte_cost-benefit_2006">(<a href="references.html#ref-viamonte_cost-benefit_2006" role="doc-biblioref">2006</a>)</span> performed a cost-benefit analysis and concluded that based on the cost of the intervention ($247.50), the probability of an accident for drivers older than 75 (<em>p</em> = 0.0710), and the cost of an accident ($22,000), performing the intervention on all drivers aged 75 or older was more efficient than not intervening or only intervening after a screening test. Furthermore, sensitivity analyses revealed that intervening for all drivers would remain beneficial as long as the reduction in collision risk is 25%. Therefore, a 25% reduction in the probability of elderly above 75 getting into a car accident could be set as the smallest effect size of interest.</p>
<p>For another example, economists have examined the value of a statistical life, based on willingness to pay to reduce the risk of death, at $1.5 - $2.5 million (in the year 2000, in western countries, see Mrozek &amp; Taylor <span class="citation" data-cites="mrozek_what_2002">(<a href="references.html#ref-mrozek_what_2002" role="doc-biblioref">2002</a>)</span>). Building on this work, Abelson <span class="citation" data-cites="abelson_value_2003">(<a href="references.html#ref-abelson_value_2003" role="doc-biblioref">2003</a>)</span> calculated the willingness to pay to prevent acute health issues such as eye irritation at about $40-$50 per day. A researcher may be examining a psychological intervention that reduces the amount of times people touch their face close to their eyes, thereby reducing eye irritations caused by bacteria. If the intervention costs $20 per year to administer, it therefore should reduce the average number of days with eye irritation in the population by at least 0.5 days for the intervention to be worth the cost. A cost-benefit analysis can also be based on the resources required to empirically study a very small effect when weighed against the value this knowledge would have for the scientific community.</p>
</section><section id="specifying-the-sesoi-using-the-small-telescopes-approach" class="level2" data-number="9.11"><h2 data-number="9.11" class="anchored" data-anchor-id="specifying-the-sesoi-using-the-small-telescopes-approach">
<span class="header-section-number">9.11</span> Specifying the SESOI using the small telescopes approach</h2>
<p>Ideally, researchers who publish empirical claims would always specify which observations would falsify their claim. Regrettably, this is not yet common practice. This is particularly problematic when a researcher performs a close replication of earlier work. Because it is never possible to prove an effect is exactly zero, and the original authors seldom specify which range of effect sizes would falsify their hypotheses, it has proven to be very difficult to interpret the outcome of a replication study <span class="citation" data-cites="anderson_theres_2016">(<a href="references.html#ref-anderson_theres_2016" role="doc-biblioref">Anderson &amp; Maxwell, 2016</a>)</span>. When does the new data contradict the original finding?</p>
<p>Consider a study in which you want to test the idea of the wisdom of crowds. You ask 20 people to estimate the number of coins in a jar, expecting the average to be very close to the true value. The research question is whether the people can on average correctly guess the number of coins, which is 500. The observed mean guess by 20 people is 550, with a standard deviation of 100. The observed difference from the true value is statistically significant, <em>t</em>(19)=2.37, <em>p</em> = 0.0375, with a Cohen’s <em>d</em> of 0.5. Can it really be that the group average is so far off? Is there no Wisdom of Crowds? Was there something special about the coins you used that make it especially difficult to guess their number? Or was it just a fluke? You set out to perform a close replication of this study.</p>
<p>You want your study to be informative, regardless of whether there is an effect or not. This means you need to design a replication study that will allow you to draw an informative conclusion, regardless of whether the alternative hypothesis is true (the crowd will not estimate the true number of coins accurately) or whether the null hypothesis is true (the crowd will guess 500 coins, and the original study was a fluke). But since the original researcher did not specify a smallest effect size of interest, when would a replication study allow you to conclude the original study is contradicted by the new data? Observing a mean of exactly 500 would perhaps be considered by some to be quite convincing, but due to random variation you will (almost) never find a mean score of exactly 500. A non-significant result can’t be interpreted as the absence of an effect, because your study might have too small a sample size to detect meaningful effects, and the result might be a Type 2 error. So how can we move forward and define an effect size that is meaningful? How can you design a study that has the ability to falsify a previous finding?</p>
<p>Uri Simonsohn <span class="citation" data-cites="simonsohn_small_2015">(<a href="references.html#ref-simonsohn_small_2015" role="doc-biblioref">2015</a>)</span> defines a small effect as “one that would give 33% power to the original study”. In other words, the effect size that would give the original study odds of 2:1 <em>against</em> observing a statistically significant result if there was an effect. The idea is that if the original study had 33% power, the probability of observing a significant effect, if there was a true effect, is too low to reliably distinguish signal from noise (or situations where there is a true effect from situations where there is no true effect). Simonsohn (2015, p.&nbsp;561) calls this the <strong>small telescopes approach</strong>, and writes: “Imagine an astronomer claiming to have found a new planet with a telescope. Another astronomer tries to replicate the discovery using a larger telescope and finds nothing. Although this does not prove that the planet does not exist, it does nevertheless contradict the original findings, because planets that are observable with the smaller telescope should also be observable with the larger one.”</p>
<p>Although this approach to setting a smallest effect size of interest (SESOI) is arbitrary (why not 30% power, or 35%?) it suffices for practical purposes (and you are free to choose a power level you think is too low). The nice thing about this definition of a SESOI is that if you know the sample size of the original study, you can always calculate the effect size that study had 33% power to detect. You can thus always use this approach to set a smallest effect size of interest. If you fail to find support for an effect size the original study has 33% power to detect, it does not mean there is no true effect, and not even that the effect is too small to be of any theoretical or practical interest. But using the small telescopes approach is a good first step, since it will get the conversation started about which effects are meaningful and allows researchers who want to replicate a study to specify when they would consider the original claim falsified.</p>
<p>With the small telescopes approach, the SESOI is based only on the sample size in the original study. A smallest effect size of interest is set only for effects in the same direction. All effects smaller than this effect (including large effects in the opposite direction) are interpreted as a failure to replicate the original results. We see that the small telescopes approach is a <strong>one-sided equivalence test</strong>, where only the upper bound is specified, and the smallest effect size of interest is determined based on the sample size of the original study. The test examines if we can reject effects as large or larger than the effect the original study has 33% power to detect. It is a simple one-sided test, not against 0, but against a SESOI.</p>
<p>For example, consider our study above in which 20 guessers tried to estimate the number of coins. The results were analyzed with a two-sided one-sample <em>t</em>-test, using an alpha level of 0.05. To determine the effect size that this study had 33% power for, we can perform a sensitivity analysis. In a sensitivity analysis we compute the required effect size given the alpha, sample size, and desired statistical power. Note that Simonsohn uses a two-sided test in his power analyses, which we will follow here – if the original study reported a pre-registered directional prediction, the power analysis should be based on a one-sided test. In this case, the alpha level is 0.05, the total sample size is 20, and the desired power is 33%. We compute the effect size that gives us 33% power and see that it is a Cohen’s <em>d</em> of 0.358. This means we can set our smallest effect size of interest for the replication study to <em>d</em> = 0.358. If we can reject effects as large or larger than <em>d</em> = 0.358, we can conclude that the effect is smaller than anything the original study had 33% power for. The screenshot below illustrates the correct settings in G*Power, and the code in R is:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="fu">library</span>(<span class="st">"pwr"</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>pwr<span class="sc">::</span><span class="fu">pwr.t.test</span>(</span>
<span id="cb11-4"><a href="#cb11-4"></a>  <span class="at">n =</span> <span class="dv">20</span>, </span>
<span id="cb11-5"><a href="#cb11-5"></a>  <span class="at">sig.level =</span> <span class="fl">0.05</span>, </span>
<span id="cb11-6"><a href="#cb11-6"></a>  <span class="at">power =</span> <span class="fl">0.33</span>, </span>
<span id="cb11-7"><a href="#cb11-7"></a>  <span class="at">type =</span> <span class="st">"one.sample"</span>,</span>
<span id="cb11-8"><a href="#cb11-8"></a>  <span class="at">alternative =</span> <span class="st">"two.sided"</span></span>
<span id="cb11-9"><a href="#cb11-9"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     One-sample t test power calculation 

              n = 20
              d = 0.3577466
      sig.level = 0.05
          power = 0.33
    alternative = two.sided</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-smalltelpower" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="images/0deabffd850f7b63c16e41e0af9ae0b6.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.6: Screenshot illustrating a sensitivity power analysis in G*Power to compute the effect size an original study had 33% power to detect.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Determining the SESOI based on the effect size the original study had 33% power to detect has an additional convenient property. Imagine the true effect size is actually 0, and you perform a statistical test to see if the data is statistically smaller than the SESOI based on the small telescopes approach (which is called an inferiority test). If you increase the sample size by 2.5 times, you will have approximately 80% power for this one-sided equivalence test, assuming the true effect size is exactly 0 (e.g., <em>d</em> = 0). People who do a replication study can follow the small telescope recommendations, and very easily determine both the smallest effect size of interest, and the sample size needed to design an informative replication study, assuming the true effect size is 0 (but see the section above for a-priori power analyses where you want to test for equivalence, but do not expect a true effect size of 0).</p>
<p>The figure below, from Simonsohn (2015) illustrates the small telescopes approach using a real-life example. The original study by Zhong and Liljenquist (2006) had a tiny sample size of 30 participants in each condition and observed an effect size of <em>d</em> = 0.53, which was barely statistically different from zero. Given a sample size of 30 per condition, the study had 33% power to detect effects larger than <em>d</em> = 0.401. This “small effect” is indicated by the green dashed line. In R, the smallest effect size of interest is calculated using:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>pwr<span class="sc">::</span><span class="fu">pwr.t.test</span>(</span>
<span id="cb13-2"><a href="#cb13-2"></a>  <span class="at">n =</span> <span class="dv">30</span>, </span>
<span id="cb13-3"><a href="#cb13-3"></a>  <span class="at">sig.level =</span> <span class="fl">0.05</span>, </span>
<span id="cb13-4"><a href="#cb13-4"></a>  <span class="at">power =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>, </span>
<span id="cb13-5"><a href="#cb13-5"></a>  <span class="at">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb13-6"><a href="#cb13-6"></a>  <span class="at">alternative =</span> <span class="st">"two.sided"</span></span>
<span id="cb13-7"><a href="#cb13-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample t test power calculation 

              n = 30
              d = 0.401303
      sig.level = 0.05
          power = 0.3333333
    alternative = two.sided

NOTE: n is number in *each* group</code></pre>
</div>
</div>
<p>Note that 33% power is a rounded value, and the calculation uses 1/3 (or 0.3333333…).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-simonsohnexample" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="images/a4aa20a6e2dadfbaa82bc614d40693c7.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.7: Example used in Simonsohn (2015) of an original study and two replication studies.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We can see that the first replication by Gámez and colleagues also had a relatively small sample size (N = 47, compared to N = 60 in the original study), and was not designed to yield informative results when interpreted with a small telescopes approach. The confidence interval is very wide and includes the null effect (<em>d</em> = 0) and the smallest effect size of interest (<em>d</em> = 0.401). Thus, this study is inconclusive. We can’t reject the null, but we can also not reject effect sizes of 0.401 or larger that are still considered to be in line with the original result. The second replication has a much larger sample size, and tells us that we can’t reject the null, but we can reject the smallest effect size of interest, suggesting that the effect is smaller than what is considered an interesting effect based on the small telescopes approach.</p>
<p>Although the <em>small telescope</em> recommendations are easy to use, one should take care not to turn any statistical procedure into a heuristic. In our example above with the 20 referees, a Cohen’s <em>d</em> of 0.358 would be used as a smallest effect size of interest, and a sample size of 50 would be collected (2.5 times the original 20), but if someone would make the effort to perform a replication study, it would be relatively easy to collect a larger sample size. Alternatively, had the original study been extremely large, it would have had high power for effects that might not be practically significant, and we would not want to collect 2.5 times as many observations in a replication study. Indeed, as Simonsohn writes: “whether we need 2.5 times the original sample size or not depends on the question we wish to answer. If we are interested in testing whether the effect size is smaller than d33%, then, yes, we need about 2.5 times the original sample size no matter how big that original sample was. When samples are very large, however, that may not be the question of interest.” Always think about the question you want to ask, and design the study so that it provides an informative answer for a question of interest. Do not automatically follow a 2.5 times n heuristic, and always reflect on whether the use of a suggested procedure is appropriate in your situation.</p>
</section><section id="setting-the-smallest-effect-size-of-interest-to-the-minimal-statistically-detectable-effect" class="level2" data-number="9.12"><h2 data-number="9.12" class="anchored" data-anchor-id="setting-the-smallest-effect-size-of-interest-to-the-minimal-statistically-detectable-effect">
<span class="header-section-number">9.12</span> Setting the Smallest Effect Size of Interest to the Minimal Statistically Detectable Effect</h2>
<p>Given a sample size and alpha level, every test has a <a href="#sec-minimaldetectable">minimal statistically detectable effect</a>. For example, given a test with 86 participants in each group, and an alpha level of 5%, only <em>t</em>-tests which yield a <em>t</em> ≥ 1.974 will be statistically significant. In other words, <em>t</em> = 1.974 is the <strong>critical <em>t</em>-value</strong>. Given a sample size and alpha level, the critical <em>t</em>-value can be transformed into a <strong>critical <em>d</em>-value</strong>. As visualized in <a href="#fig-distpowerplot1">Figure&nbsp;<span>9.8</span></a>, with n = 50 in each group and an alpha level of 5% the critical <em>d</em>-value is 0.4. This means that only effects larger than 0.4 will yield a <em>p</em> &lt; $%. The critical <em>d</em>-value is influenced by the sample size per group, and the alpha level, but does not depend on the the true effect size.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-distpowerplot1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="images/dpplot50.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.8: Null and alternative distribution with Type 1 and Type 2 error indicating the smallest effect size that will be statistically significant with n = 50 per condition.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>It is possible to observe a statistically significant test result if the true effect size is <em>smaller</em> than the critical effect size. Due to random variation, it is possible to observe a larger value in a <em>sample</em> than is the true value in the population. This is the reason the statistical power of a test is never 0 in a null hypothesis significance test. As illustrated in <a href="#fig-distpowerplot2">Figure&nbsp;<span>9.9</span></a>, even if the true effect size is smaller than the critical value (i.e., if the true effect size is 0.2) we see from the distribution that we can expect some <em>observed effect sizes</em> to be larger than 0.4 when the <em>true population effect size</em> is <em>d</em> = 0.2 – if we compute the statistical power for this test, it turns out we can expect 16.77% of the <em>observed effect sizes</em> will be larger than 0.4, in the long run. That is not a lot, but it is something. This is also the reason why publication bias combined with underpowered research is problematic: It leads to a large <strong>overestimation of the true effect size</strong> when only observed effect sizes from statistically significant findings in underpowered studies end up in the scientific literature.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-distpowerplot2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="images/dpplot502.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.9: Null and alternative distribution with Type 1 and Type 2 error indicating the smallest effect size that will be statistically significant with n = 50 per condition.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We can use the minimal statistically detectable effect to set the SESOI for replication studies. If you attempt to replicate a study, one justifiable option when choosing the smallest effect size of interest (SESOI) is to use the smallest observed effect size that could have been statistically significant in the study you are replicating. In other words, you decide that effects that could not have yielded a <em>p</em>-value less than $% in an original study will not be considered meaningful in the replication study. The assumption here is that the original authors were interested in observing a significant effect, and thus were not interested in observed effect sizes that could not have yielded a significant result. It might be likely that the original authors did not consider which effect sizes their study had good statistical power to detect, or that they were interested in smaller effects but gambled on observing an especially large effect in the sample purely as a result of random variation. Even then, when building on earlier research that does not specify a SESOI, a justifiable starting point might be to set the SESOI to the smallest effect size that, when observed in the original study, <strong>could have been statistically significant</strong>. Not all researchers might agree with this (e.g., the original authors might say they actually cared just as much about an effect of <em>d</em> =0.001). However, as we try to change the field from the current situation where no one specifies what would falsify their hypothesis, or what their smallest effect size of interest is, this approach is one way to get started. In practice, as explained in the section on <a href="08-samplesizejustification.html#sec-posthocpower">post-hoc power</a>, due to the relation between <em>p</em> = 0.05 and 50% power for the observed effect size, this justification for a SESOI will mean that the SESOI is set to the effect size the original study had 50% power to detect for an independent <em>t</em>test. This approach is in some ways similar to the small telescopes approach by Simonsohn (2015), except that it will lead to a somewhat larger SESOI.</p>
<p>Setting a smallest effect size of interest for a replication study is a bit like a tennis match. Original authors serve and hit the ball across the net, saying ‘look, something is going on’. The approach to set the SESOI to the effect size that could have been significant in the original study is a return volley which allows you to say ‘there does not seem to be anything large enough that could have been significant in your own original study’ after performing a well-designed replication study with high statistical power to reject the SESOI. This is never the end of the match – the original authors can attempt to return the ball with a more specific statement about effects their theory predicts, and demonstrate such a smaller effect size is present. But the ball is back in their court, and if they want to continue to claim there is an effect, they will have to support their claim by new data.</p>
<p>Beyond replication studies, the minimal statistically detectable effect can also be computed based on the sample sizes that are typically used in a research field. For example, imagine a line of research in which a hypothesis has almost always been tested by performing a one-sample <em>t</em>-test, and where the sample sizes that are collected are always smaller than 100 observations. A one-sample <em>t</em>-test on 100 observations, using an alpha of .05 (two sided), has 80% power to detect an effect of <em>d</em> = 0.28 (as can be calculated in a sensitivity power analysis). In a new study, concluding that one can reliably reject the presence of effects more extreme than <em>d</em> = 0.28 suggests that sample sizes of 100 might not be enough to detect effects in such research lines. Rejecting the presence of effects more extreme than <em>d</em> = 0.28 does not test a theoretical prediction, but it contributes to the literature by answering a <strong>resource question</strong>. It suggests that future studies in this research line will need to change the design of their studies by substantially increasing the sample size. Setting the smallest effect size of interest based on this approach does not answer any theoretical question (after all, the SESOI is not based on any theoretical prediction). But this approach to specifying a smallest effect size of interest can make a useful contribution to the literature by informing peers that the effect is not large enough so that it can be reliably studied given the sample sizes researchers are typically collecting. It does not mean that the effect is not interesting, but it might be an indication that the field will need to coordinate data collection in the future, because the effect is too small to reliable study with the sample sizes that have been collected in the past.</p>
</section><section id="test-yourself" class="level2" data-number="9.13"><h2 data-number="9.13" class="anchored" data-anchor-id="test-yourself">
<span class="header-section-number">9.13</span> Test Yourself</h2>
<section id="questions-about-equivalence-tests" class="level3 webex-check webex-box" data-number="9.13.1"><h3 data-number="9.13.1" class="anchored" data-anchor-id="questions-about-equivalence-tests">
<span class="header-section-number">9.13.1</span> Questions about equivalence tests</h3>
<p><strong>Q1</strong>: When the 90% CI around a mean difference falls just within the equivalence range from -0.4 to 0.4, we can reject the smallest effect size of interest. Based on your knowledge about confidence intervals, when the equivalence range is changed to -0.3 – 0.3, what is needed for the equivalence test to be significant (assuming the effect size estimate and standard deviation remains the same)?</p>
<div class="cell" data-layout-align="center">
<div id="radio_DXYFQXIMNI" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_DXYFQXIMNI" value=""><span>A larger effect size.</span></label><label><input type="radio" autocomplete="off" name="radio_DXYFQXIMNI" value=""><span>A lower alpha level.</span></label><label><input type="radio" autocomplete="off" name="radio_DXYFQXIMNI" value="answer"><span>A larger sample size.</span></label><label><input type="radio" autocomplete="off" name="radio_DXYFQXIMNI" value=""><span>Lower statistical power.</span></label>
</div>
</div>
<p><strong>Q2</strong>: Why is it incorrect to conclude that there is no effect, when an equivalence test is statistically significant?</p>
<div class="cell" data-layout-align="center">
<div id="radio_KVHCSSATDM" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_KVHCSSATDM" value=""><span>An equivalence test is a statement about the data, not about the presence or absence of an effect. </span></label><label><input type="radio" autocomplete="off" name="radio_KVHCSSATDM" value=""><span>The result of an equivalence test could be a Type 1 error, and therefore, one should conclude that there is no effect, or a Type 1 error has been observed. </span></label><label><input type="radio" autocomplete="off" name="radio_KVHCSSATDM" value="answer"><span>An equivalence test rejects values as large or larger than the smallest effect size of interest, so the possibility that there is a small non-zero effect cannot be rejected.</span></label><label><input type="radio" autocomplete="off" name="radio_KVHCSSATDM" value=""><span>We conclude there is no effect when the equivalence test is non-significant, not when the equivalence test is significant.</span></label>
</div>
</div>
<p><strong>Q3</strong>: Researchers are interested in showing that students who use an online textbook perform just as well as students who use a paper textbook. If so, they can recommend teachers to allow students to choose their preferred medium, but if there is a benefit, they would recommend the medium that leads to better student performance. They randomly assign students to use an online textbook or a paper textbook, and compare their grades on the exam for the course (from the worst possible grade, 1, to the best possible grade, 10). They find that the both groups of students perform similarly, with for the paper textbook condition <em>m</em> = 7.35, <em>sd</em> = 1.15, <em>n</em> = 50, and the online textbook <em>m</em> = 7.13, <em>sd</em> = 1.21, <em>n</em> = 50). Let’s assume we consider any effect as large or larger than half a grade point (0.5) worthwhile, but any difference smaller than 0.5 too small to matter, and the alpha level is set at 0.05. What would the authors conclude? Copy the code below into R, replacing all zeroes with the correct numbers. Type <code>?tsum_TOST</code> for help with the function.</p>
<!-- ```{r eval = FALSE} -->
<!-- TOSTER::tsum_TOST(m1 = 7.35, -->
<!--                   sd1 = 1.15, -->
<!--                   n1 = 50, -->
<!--                   m2 = 7.13, -->
<!--                   sd2 = 1.21, -->
<!--                   n2 = 50, -->
<!--                   low_eqbound = -0.5, -->
<!--                   high_eqbound = 0.5, -->
<!--                   eqbound_type = "raw", -->
<!--                   alpha = 0.05) -->
<!-- ``` -->
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>TOSTER<span class="sc">::</span><span class="fu">tsum_TOST</span>(</span>
<span id="cb15-2"><a href="#cb15-2"></a>  <span class="at">m1 =</span> <span class="fl">0.00</span>,</span>
<span id="cb15-3"><a href="#cb15-3"></a>  <span class="at">sd1 =</span> <span class="fl">0.00</span>,</span>
<span id="cb15-4"><a href="#cb15-4"></a>  <span class="at">n1 =</span> <span class="dv">0</span>,</span>
<span id="cb15-5"><a href="#cb15-5"></a>  <span class="at">m2 =</span> <span class="fl">0.00</span>,</span>
<span id="cb15-6"><a href="#cb15-6"></a>  <span class="at">sd2 =</span> <span class="fl">0.00</span>,</span>
<span id="cb15-7"><a href="#cb15-7"></a>  <span class="at">n2 =</span> <span class="dv">0</span>,</span>
<span id="cb15-8"><a href="#cb15-8"></a>  <span class="at">low_eqbound =</span> <span class="sc">-</span><span class="fl">0.0</span>,</span>
<span id="cb15-9"><a href="#cb15-9"></a>  <span class="at">high_eqbound =</span> <span class="fl">0.0</span>,</span>
<span id="cb15-10"><a href="#cb15-10"></a>  <span class="at">eqbound_type =</span> <span class="st">"raw"</span>,</span>
<span id="cb15-11"><a href="#cb15-11"></a>  <span class="at">alpha =</span> <span class="fl">0.05</span></span>
<span id="cb15-12"><a href="#cb15-12"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="radio_AHUEJLCYWU" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_AHUEJLCYWU" value=""><span>We can <strong>reject</strong> an effect size of zero, and we can <strong>reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_AHUEJLCYWU" value=""><span>We can <strong>not reject</strong> an effect size of zero, and we can <strong>reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_AHUEJLCYWU" value=""><span>We can <strong>reject</strong> an effect size of zero, and we can <strong>not reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_AHUEJLCYWU" value="answer"><span>We can <strong>not reject</strong> an effect size of zero, and we can <strong>not reject</strong> the presence of effects as large or larger than the smallest effect size of interest.</span></label>
</div>
</div>
<p><strong>Q4</strong>: If we increase the sample size in question Q3 to 150 participants in each condition, and assuming the observed means and standard deviations would be exactly the same, which conclusion would we draw?</p>
<div class="cell" data-layout-align="center">
<div id="radio_CDIWCCTVYI" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_CDIWCCTVYI" value=""><span>We can <strong>reject</strong> an effect size of zero, and we can <strong>reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_CDIWCCTVYI" value="answer"><span>We can <strong>not reject</strong> an effect size of zero, and we can <strong>reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_CDIWCCTVYI" value=""><span>We can <strong>reject</strong> an effect size of zero, and we can <strong>not reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_CDIWCCTVYI" value=""><span>We can <strong>not reject</strong> an effect size of zero, and we can <strong>not reject</strong> the presence of effects as large or larger than the smallest effect size of interest.</span></label>
</div>
</div>
<p><strong>Q5</strong>: If we increase the sample size in question Q3 to 500 participants in each condition, and assuming the observed means and standard deviations would be exactly the same, which conclusion would we draw?</p>
<div class="cell" data-layout-align="center">
<div id="radio_VSLRLGDSWQ" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_VSLRLGDSWQ" value="answer"><span>We can <strong>reject</strong> an effect size of zero, and we can <strong>reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_VSLRLGDSWQ" value=""><span>We can <strong>not reject</strong> an effect size of zero, and we can <strong>reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_VSLRLGDSWQ" value=""><span>We can <strong>reject</strong> an effect size of zero, and we can <strong>not reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_VSLRLGDSWQ" value=""><span>We can <strong>not reject</strong> an effect size of zero, and we can <strong>not reject</strong> the presence of effects as large or larger than the smallest effect size of interest.</span></label>
</div>
</div>
<p>Sometimes the result of a test is <strong>inconclusive</strong>, as both the null hypothesis test, and the equivalence test, of not statistically significant. The only solution in such a case is to collect additional data. Sometimes both the null hypothesis test and the equivalence test are statistically significant, in which case the effect is <strong>statistically different from zero, but practically insignificant</strong> (based on the justification for the SESOI).</p>
<p><strong>Q6</strong>: We might wonder what the statistical power was for the test in Q3, assuming there was no true difference between the two groups (so a true effect size of 0). Using the new and improved <code>power_t_TOST</code> function in the TOSTER R package, we can compute the power using a sensitivity power analysis (i.e., entering the sample size per group of 50, the assumed true effect size of 0, the equivalence bounds, and the alpha level. Note that because the equivalence bounds were specified on a raw scale in Q3, we will also need to specify an estimate for the true standard deviation in the population. Let’s assume this true standard deviation is 1.2. Round the answer to two digits after the decimal. Type <code>?power_t_TOST</code> for help with the function. What was the power in Q3?</p>
<!-- ```{r eval = FALSE} -->
<!-- TOSTER::power_t_TOST( -->
<!--   n = 50, -->
<!--   delta = 0.0, -->
<!--   sd = 1.2, -->
<!--   low_eqbound = -0.5, -->
<!--   high_eqbound = 0.5, -->
<!--   alpha = 0.05, -->
<!--   type = "two.sample" -->
<!-- ) -->
<!-- ``` -->
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>TOSTER<span class="sc">::</span><span class="fu">power_t_TOST</span>(</span>
<span id="cb16-2"><a href="#cb16-2"></a>  <span class="at">n =</span> <span class="dv">00</span>,</span>
<span id="cb16-3"><a href="#cb16-3"></a>  <span class="at">delta =</span> <span class="fl">0.0</span>,</span>
<span id="cb16-4"><a href="#cb16-4"></a>  <span class="at">sd =</span> <span class="fl">0.0</span>,</span>
<span id="cb16-5"><a href="#cb16-5"></a>  <span class="at">low_eqbound =</span> <span class="sc">-</span><span class="fl">0.0</span>,</span>
<span id="cb16-6"><a href="#cb16-6"></a>  <span class="at">high_eqbound =</span> <span class="fl">0.0</span>,</span>
<span id="cb16-7"><a href="#cb16-7"></a>  <span class="at">alpha =</span> <span class="fl">0.05</span>,</span>
<span id="cb16-8"><a href="#cb16-8"></a>  <span class="at">type =</span> <span class="st">"two.sample"</span></span>
<span id="cb16-9"><a href="#cb16-9"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="radio_ESWTYIUUOS" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_ESWTYIUUOS" value=""><span>0.00</span></label><label><input type="radio" autocomplete="off" name="radio_ESWTYIUUOS" value=""><span>0.05</span></label><label><input type="radio" autocomplete="off" name="radio_ESWTYIUUOS" value="answer"><span>0.33</span></label><label><input type="radio" autocomplete="off" name="radio_ESWTYIUUOS" value=""><span>0.40</span></label>
</div>
</div>
<p><strong>Q7</strong>: Assume we would only have had 15 participants in each group in Q3, instead of 50. What would be the statistical power of the test with this smaller sample size (keeping all other settings as in Q6)? Round the answer to 2 digits.</p>
<div class="cell" data-layout-align="center">
<div id="radio_EXNGZMZORF" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_EXNGZMZORF" value="answer"><span>0.00</span></label><label><input type="radio" autocomplete="off" name="radio_EXNGZMZORF" value=""><span>0.05</span></label><label><input type="radio" autocomplete="off" name="radio_EXNGZMZORF" value=""><span>0.33</span></label><label><input type="radio" autocomplete="off" name="radio_EXNGZMZORF" value=""><span>0.40</span></label>
</div>
</div>
<p><strong>Q8</strong>: You might remember from discussions on statistical power for a null hypothesis significance test that the statistical power is never smaller than 5% (if the true effect size is 0, power is formally undefined, but we will observe at least 5% Type 1 errors, and the power increases when introducing a true effect). In a two-sided equivalence tests, power can be lower than the alpha level. Why?</p>
<div class="cell" data-layout-align="center">
<div id="radio_BUAQMUMZHB" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_BUAQMUMZHB" value=""><span>Because in an equivalence test the Type 1 error rate is not bounded at 5%.</span></label><label><input type="radio" autocomplete="off" name="radio_BUAQMUMZHB" value=""><span>Because in an equivalence test the null hypothesis and alternative hypothesis are reversed, and therefore the Type 2 error rate does not have a lower bound (just as the Type 1 error rate in NHST has no lower bound).</span></label><label><input type="radio" autocomplete="off" name="radio_BUAQMUMZHB" value="answer"><span>Because the confidence interval needs to fall between the lower and upper bound of the equivalence interval, and with small sample sizes, this probability can be close to zero (because the confidence interval is very wide).</span></label><label><input type="radio" autocomplete="off" name="radio_BUAQMUMZHB" value=""><span>Because the equivalence test is based on a confidence interval, and not on a <em>p</em>-value, and therefore power is not limited by the alpha level. </span></label>
</div>
</div>
<p><strong>Q9</strong>: A well designed study has high power to detect an effect of interest, but also to reject the smallest effect size of interest. Perform an a-priori power analysis for the situation described in Q3. Which sample size in <strong>each group</strong> needs to be collected to achieved a desired statistical power of 90% (or 0.9), assuming the true effect size is 0, and we still assume the true standard deviation is 1.2? Use the code below, and round up the sample size (as we cannot collect a partial observation).</p>
<!-- ```{r eval = FALSE} -->
<!-- TOSTER::power_t_TOST( -->
<!--   power = 0.90, -->
<!--   delta = 0.0, -->
<!--   sd = 1.2, -->
<!--   low_eqbound = -0.5, -->
<!--   high_eqbound = 0.5, -->
<!--   alpha = 0.05, -->
<!--   type = "two.sample" -->
<!-- ) -->
<!-- ``` -->
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a>TOSTER<span class="sc">::</span><span class="fu">power_t_TOST</span>(</span>
<span id="cb17-2"><a href="#cb17-2"></a>  <span class="at">power =</span> <span class="fl">0.00</span>,</span>
<span id="cb17-3"><a href="#cb17-3"></a>  <span class="at">delta =</span> <span class="fl">0.0</span>,</span>
<span id="cb17-4"><a href="#cb17-4"></a>  <span class="at">sd =</span> <span class="fl">0.0</span>,</span>
<span id="cb17-5"><a href="#cb17-5"></a>  <span class="at">low_eqbound =</span> <span class="sc">-</span><span class="fl">0.0</span>,</span>
<span id="cb17-6"><a href="#cb17-6"></a>  <span class="at">high_eqbound =</span> <span class="fl">0.0</span>,</span>
<span id="cb17-7"><a href="#cb17-7"></a>  <span class="at">alpha =</span> <span class="fl">0.05</span>,</span>
<span id="cb17-8"><a href="#cb17-8"></a>  <span class="at">type =</span> <span class="st">"two.sample"</span></span>
<span id="cb17-9"><a href="#cb17-9"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="radio_IYHXYTNZTE" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_IYHXYTNZTE" value=""><span>100</span></label><label><input type="radio" autocomplete="off" name="radio_IYHXYTNZTE" value="answer"><span>126</span></label><label><input type="radio" autocomplete="off" name="radio_IYHXYTNZTE" value=""><span>200</span></label><label><input type="radio" autocomplete="off" name="radio_IYHXYTNZTE" value=""><span>252</span></label>
</div>
</div>
<p><strong>Q10</strong>: Assume that when performing the power analysis for Q9 we did not expect the true effect size to be 0, but we actually expected a mean difference of 0.1 grade point. Which sample size in <strong>each group</strong> would we need to collect for the equivalence test, now that we expect a true effect size of 0.1? Change the variable <code>delta</code> in <code>power_t_TOST</code> to answer this question.</p>
<!-- ```{r eval = FALSE} -->
<!-- TOSTER::power_t_TOST( -->
<!--   power = 0.90, -->
<!--   delta = 0.1, -->
<!--   sd = 1.2, -->
<!--   low_eqbound = -0.5, -->
<!--   high_eqbound = 0.5, -->
<!--   alpha = 0.05, -->
<!--   type = "two.sample" -->
<!-- ) -->
<!-- ``` -->
<div class="cell" data-layout-align="center">
<div id="radio_JLTWXOBAEZ" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_JLTWXOBAEZ" value=""><span>117</span></label><label><input type="radio" autocomplete="off" name="radio_JLTWXOBAEZ" value="answer"><span>157</span></label><label><input type="radio" autocomplete="off" name="radio_JLTWXOBAEZ" value=""><span>314</span></label><label><input type="radio" autocomplete="off" name="radio_JLTWXOBAEZ" value=""><span>3118</span></label>
</div>
</div>
<p><strong>Q11</strong>: Change the equivalence range to -0.1 and 0.1 for Q9 (and set the expected effect size of <code>delta</code> to 0). To be able to reject effects outside such a very narrow equivalence range, you’ll need a large sample size. With an alpha of 0.05, and a desired power of 0.9 (or 90%), how many participants would you need in <strong>each group</strong>?</p>
<!-- ```{r eval = FALSE} -->
<!-- TOSTER::power_t_TOST( -->
<!--   power = 0.90, -->
<!--   delta = 0.0, -->
<!--   sd = 1.2, -->
<!--   low_eqbound = -0.1, -->
<!--   high_eqbound = 0.1, -->
<!--   alpha = 0.05, -->
<!--   type = "two.sample" -->
<!-- ) -->
<!-- ``` -->
<div class="cell" data-layout-align="center">
<div id="radio_YEHMYDJFRE" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_YEHMYDJFRE" value=""><span>117</span></label><label><input type="radio" autocomplete="off" name="radio_YEHMYDJFRE" value=""><span>157</span></label><label><input type="radio" autocomplete="off" name="radio_YEHMYDJFRE" value=""><span>314</span></label><label><input type="radio" autocomplete="off" name="radio_YEHMYDJFRE" value="answer"><span>3118</span></label>
</div>
</div>
<p>You can see it takes a very large sample size to have high power to reliably reject very small effects. This should not be surprising. After all, it also requires a very large sample size to <em>detect</em> small effects! This is why we typically leave it to a future meta-analysis to detect, or reject, the presence of small effects.</p>
<p><strong>Q12</strong>: You can do equivalence tests for all tests. The TOSTER package has functions for <em>t</em>-tests, correlations, differences between proportions, and meta-analyses. If the test you want to perform is not included in any software, remember that you can just use a 90% confidence interval, and test whether you can reject the smallest effect size of interest. Let’s perform an equivalence test for a meta-analysis. Hyde, Lindberg, Linn, Ellis, and Williams <span class="citation" data-cites="hyde_gender_2008">(<a href="references.html#ref-hyde_gender_2008" role="doc-biblioref">2008</a>)</span> report that effect sizes for gender differences in mathematics tests across the 7 million students in the US represent trivial differences, where a trivial difference is specified as an effect size smaller then <em>d</em> =0.1. The table with Cohen’s d and se is reproduced below:</p>
<table class="table">
<thead><tr class="header">
<th><strong>Grades</strong></th>
<th><strong>d + se</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Grade 2</td>
<td>0.06 +/- 0.003</td>
</tr>
<tr class="even">
<td>Grade 3</td>
<td>0.04 +/- 0.002</td>
</tr>
<tr class="odd">
<td>Grade 4</td>
<td>-0.01 +/- 0.002</td>
</tr>
<tr class="even">
<td>Grade 5</td>
<td>-0.01 +/- 0.002</td>
</tr>
<tr class="odd">
<td>Grade 6</td>
<td>-0.01 +/- 0.002</td>
</tr>
<tr class="even">
<td>Grade 7</td>
<td>-0.02 +/- 0.002</td>
</tr>
<tr class="odd">
<td>Grade 8</td>
<td>-0.02 +/- 0.002</td>
</tr>
<tr class="even">
<td>Grade 9</td>
<td>-0.01 +/- 0.003</td>
</tr>
<tr class="odd">
<td>Grade 10</td>
<td>0.04 +/- 0.003</td>
</tr>
<tr class="even">
<td>Grade 11</td>
<td>0.06 +/- 0.003</td>
</tr>
</tbody>
</table>
<p>For grade 2, when we perform an equivalence test with boundaries of <em>d</em> =-0.1 and <em>d</em> =0.1, using an alpha of 0.01, which conclusion can we draw? Use the TOSTER function TOSTmeta, and enter the alpha, effect size (ES), standard error (se), and equivalence bounds.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a>TOSTER<span class="sc">::</span><span class="fu">TOSTmeta</span>(</span>
<span id="cb18-2"><a href="#cb18-2"></a>  <span class="at">ES =</span> <span class="fl">0.00</span>,</span>
<span id="cb18-3"><a href="#cb18-3"></a>  <span class="at">se =</span> <span class="fl">0.000</span>,</span>
<span id="cb18-4"><a href="#cb18-4"></a>  <span class="at">low_eqbound_d =</span> <span class="sc">-</span><span class="fl">0.0</span>,</span>
<span id="cb18-5"><a href="#cb18-5"></a>  <span class="at">high_eqbound_d =</span> <span class="fl">0.0</span>,</span>
<span id="cb18-6"><a href="#cb18-6"></a>  <span class="at">alpha =</span> <span class="fl">0.05</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="radio_TAFNMHQSPO" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_TAFNMHQSPO" value="answer"><span>We can <strong>reject</strong> an effect size of zero, and we can <strong>reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_TAFNMHQSPO" value=""><span>We can <strong>not reject</strong> an effect size of zero, and we can <strong>reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_TAFNMHQSPO" value=""><span>We can <strong>reject</strong> an effect size of zero, and we can <strong>not reject</strong> the presence of effects as large or larger than the smallest effect size of interest. </span></label><label><input type="radio" autocomplete="off" name="radio_TAFNMHQSPO" value=""><span>We can <strong>not reject</strong> an effect size of zero, and we can <strong>not reject</strong> the presence of effects as large or larger than the smallest effect size of interest.</span></label>
</div>
</div>
</section><section id="questions-about-the-small-telescopes-approach" class="level3 webex-check webex-box" data-number="9.13.2"><h3 data-number="9.13.2" class="anchored" data-anchor-id="questions-about-the-small-telescopes-approach">
<span class="header-section-number">9.13.2</span> Questions about the small telescopes approach</h3>
<p><strong>Q13</strong>: What is the smallest effect size of interest based on the small telescopes approach, when the original study collected 20 participants in each condition of an independent <em>t</em>-test, with an <strong>alpha level of 0.05</strong>. Note that for this answer, it happens to depend on whether you enter the power as 0.33 or 1/3 (or 0.333). You can use the code below, which relies on the <code>pwr</code> package.</p>
<!-- ```{r} -->
<!-- pwr::pwr.t.test( -->
<!--   n = 20,  -->
<!--   sig.level = 0.05,  -->
<!--   power = 1/3,  -->
<!--   type = "two.sample", -->
<!--   alternative = "two.sided" -->
<!-- ) -->
<!-- ``` -->
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>pwr<span class="sc">::</span><span class="fu">pwr.t.test</span>(</span>
<span id="cb19-2"><a href="#cb19-2"></a>  <span class="at">n =</span> <span class="dv">0</span>, </span>
<span id="cb19-3"><a href="#cb19-3"></a>  <span class="at">sig.level =</span> <span class="fl">0.00</span>, </span>
<span id="cb19-4"><a href="#cb19-4"></a>  <span class="at">power =</span> <span class="dv">0</span>, </span>
<span id="cb19-5"><a href="#cb19-5"></a>  <span class="at">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb19-6"><a href="#cb19-6"></a>  <span class="at">alternative =</span> <span class="st">"two.sided"</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="radio_PQSNFKHWTS" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_PQSNFKHWTS" value=""><span><em>d</em> =0.25 (setting power to 0.33) or 0.26 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_PQSNFKHWTS" value=""><span><em>d</em> =0.33 (setting power to 0.33) or 0.34 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_PQSNFKHWTS" value="answer"><span><em>d</em> =0.49 (setting power to 0.33) or 0.50 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_PQSNFKHWTS" value=""><span><em>d</em> =0.71 (setting power to 0.33) or 0.72 (setting power to 1/3)</span></label>
</div>
</div>
<p><strong>Q14</strong>: Let’s assume you are trying to replicate a previous result based on a correlation in a two-sided test. The study had 150 participants. Calculate the SESOI using a small telescopes justification for a replication of this study that will use an alpha level of 0.05. Note that for this answer, it happens to depend on whether you enter the power as 0.33 or 1/3 (or 0.333). You can use the code below.</p>
<!-- ```{r, eval = FALSE} -->
<!-- pwr::pwr.r.test( -->
<!--   n = 150,  -->
<!--   sig.level = 0.05,  -->
<!--   power = 1/3,  -->
<!--   alternative = "two.sided") -->
<!-- ``` -->
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>pwr<span class="sc">::</span><span class="fu">pwr.r.test</span>(</span>
<span id="cb20-2"><a href="#cb20-2"></a>  <span class="at">n =</span> <span class="dv">0</span>, </span>
<span id="cb20-3"><a href="#cb20-3"></a>  <span class="at">sig.level =</span> <span class="dv">0</span>, </span>
<span id="cb20-4"><a href="#cb20-4"></a>  <span class="at">power =</span> <span class="dv">0</span>, </span>
<span id="cb20-5"><a href="#cb20-5"></a>  <span class="at">alternative =</span> <span class="st">"two.sided"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="radio_EBUELCZRNM" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_EBUELCZRNM" value="answer"><span><em>r</em> = 0.124 (setting power to 0.33) or 0.125 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_EBUELCZRNM" value=""><span><em>r</em> = 0.224 (setting power to 0.33) or 0.225 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_EBUELCZRNM" value=""><span><em>r</em> = 0.226 (setting power to 0.33) or 0.227 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_EBUELCZRNM" value=""><span><em>r</em> = 0.402 (setting power to 0.33) or 0.403 (setting power to 1/3)</span></label>
</div>
</div>
<p><strong>Q15</strong>: In the age of big data researchers often have access to large databases, and can run correlations on samples of thousands of observations. Let’s assume the original study in the previous question did not have 150 observations, but 15000 observations. We still use an alpha level of 0.05. Note that for this answer, it happens to depend on whether you enter the power as 0.33 or 1/3 (or 0.333). What is the SESOI based on the small telescopes approach?</p>
<div class="cell" data-layout-align="center">
<div id="radio_WOXBZMPGPQ" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_WOXBZMPGPQ" value="answer"><span><em>r</em> = 0.0124 (setting power to 0.33) or 0.0125 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_WOXBZMPGPQ" value=""><span><em>r</em> = 0.0224 (setting power to 0.33) or 0.0225 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_WOXBZMPGPQ" value=""><span><em>r</em> = 0.0226 (setting power to 0.33) or 0.0227 (setting power to 1/3)</span></label><label><input type="radio" autocomplete="off" name="radio_WOXBZMPGPQ" value=""><span><em>r</em> = 0.0402 (setting power to 0.33) or 0.0403 (setting power to 1/3)</span></label>
</div>
</div>
<p>Is this effect likely to be practically or theoretically significant? Probably not. This would be a situation where the small telescopes approach is not a very useful procedure to determine a smallest effect size of interest.</p>
<p><strong>Q16</strong>: Using the small telescopes approach, you set the SESOI in a replication study to <em>d</em> = 0.35, and set the alpha level to 0.05. After collecting the data in a well-powered replication study that was as close to the original study as practically possible, you find no significant effect, and you can reject effects as large or larger than <em>d</em> = 0.35. What is the correct interpretation of this result?</p>
<div class="cell" data-layout-align="center">
<div id="radio_KIHVMFSPLJ" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_KIHVMFSPLJ" value=""><span>There is no effect.</span></label><label><input type="radio" autocomplete="off" name="radio_KIHVMFSPLJ" value=""><span>We can statistically reject (using an alpha of 0.05) effects anyone would find theoretically meaningful.</span></label><label><input type="radio" autocomplete="off" name="radio_KIHVMFSPLJ" value=""><span>We can statistically reject (using an alpha of 0.05) effects anyone would find practically relevant.</span></label><label><input type="radio" autocomplete="off" name="radio_KIHVMFSPLJ" value="answer"><span>We can statistically reject (using an alpha of 0.05) effects the original study had 33% power to detect.</span></label>
</div>
</div>
</section><section id="questions-about-specifying-the-sesoi-as-the-minimal-statistically-detectable-effect" class="level3 webex-check webex-box" data-number="9.13.3"><h3 data-number="9.13.3" class="anchored" data-anchor-id="questions-about-specifying-the-sesoi-as-the-minimal-statistically-detectable-effect">
<span class="header-section-number">9.13.3</span> Questions about specifying the SESOI as the Minimal Statistically Detectable Effect</h3>
<p><strong>Q17</strong>: Open the online Shiny app that can be used to compute the minimal statistically detectable effect for two independent groups: https://shiny.ieis.tue.nl/d_p_power/. Three sliders influence what the figure looks like: The sample size per condition, the true effect size, and the alpha level. Which statement is true?</p>
<div class="cell" data-layout-align="center">
<div id="radio_AFQMVZBODL" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_AFQMVZBODL" value=""><span>The critical <em>d</em>-value is influenced by the sample size per group, the true effect size, but <strong>not</strong> by the alpha level.</span></label><label><input type="radio" autocomplete="off" name="radio_AFQMVZBODL" value="answer"><span>The critical <em>d</em>-value is influenced by the sample size per group, the alpha level, but <strong>not</strong> by the true effect size.</span></label><label><input type="radio" autocomplete="off" name="radio_AFQMVZBODL" value=""><span>The critical <em>d</em>-value is influenced by the alpha level, the true effect size, but <strong>not</strong> by the sample size per group.</span></label><label><input type="radio" autocomplete="off" name="radio_AFQMVZBODL" value=""><span>The critical <em>d</em>-value is influenced by the sample size per group, the alpha level, and by the true effect size.</span></label>
</div>
</div>
<p><strong>Q18</strong>: Imagine researchers performed a study with 18 participants in each condition, and performed a <em>t</em>-test using an alpha level of 0.01. Using the Shiny app, what is the smallest effect size that could have been statistically significant in this study?</p>
<div class="cell" data-layout-align="center">
<div id="radio_EWFSTLCDON" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_EWFSTLCDON" value=""><span><em>d</em> = 0.47</span></label><label><input type="radio" autocomplete="off" name="radio_EWFSTLCDON" value=""><span><em>d</em> = 0.56</span></label><label><input type="radio" autocomplete="off" name="radio_EWFSTLCDON" value="answer"><span><em>d</em> = 0.91</span></label><label><input type="radio" autocomplete="off" name="radio_EWFSTLCDON" value=""><span><em>d</em> = 1</span></label>
</div>
</div>
<p><strong>Q19</strong>: You expect the true effect size in your next study to be <em>d</em> = 0.5, and you plan to use an alpha level of 0.05. You collect 30 participants in each group for an independent <em>t</em>-test. Which statement is true?</p>
<div class="cell" data-layout-align="center">
<div id="radio_ZMCNOBFSPT" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_ZMCNOBFSPT" value=""><span>You have low power for all possible effect sizes.</span></label><label><input type="radio" autocomplete="off" name="radio_ZMCNOBFSPT" value=""><span>Observed effect sizes of <em>d</em> = 0.5 will never be statistically significant.</span></label><label><input type="radio" autocomplete="off" name="radio_ZMCNOBFSPT" value="answer"><span>Observed effect sizes of <em>d</em> = 0.5 will never be statistically significant.</span></label><label><input type="radio" autocomplete="off" name="radio_ZMCNOBFSPT" value=""><span>Observed effect sizes of <em>d</em> = 0.5 will be statistically significant.</span></label>
</div>
</div>
<p>The example we have used so far was based on performing an independent <em>t</em>-test, but the idea can be generalized. A shiny app for an <em>F</em>-test is available here: <a href="https://shiny.ieis.tue.nl/f_p_power/" class="uri">https://shiny.ieis.tue.nl/f_p_power/</a>. The effect size associated to the power of an <em>F</em>-test is partial eta squared (<span class="math inline">\(\eta_{p}^{2})\)</span>, which for a One-Way ANOVA (visualized in the Shiny app) equals eta-squared.</p>
<p>The distribution for eta-squared looks slightly different from the distribution of Cohen’s <em>d</em>, primarily because an <em>F</em>-test is a one-directional test (and because of this, eta-squared values are all positive, while Cohen’s <em>d</em> can be positive or negative). The light grey line plots the expected distribution of eta-squared when the null is true, with the red area under the curve indicating Type 1 errors, and the black line plots the expected distribution of eta-squared when the true effect size is η = 0.059. The blue area indicates the expected effect sizes smaller that the critical η of 0.04, which will not be statistically significant, and thus will be Type 2 errors.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-critf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="images/7f6d17dc07bdc9e95ea8944d78b16d7c.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9.10: Illustration of the critical <em>F</em>-value for two groups, 50 observations per group, and an alpha level of 0.05.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong>Q20</strong>: Set the number of participants (per condition) to 14, and the number of groups to 3. Using the Shiny app at <a href="https://shiny.ieis.tue.nl/f_p_power/" class="uri">https://shiny.ieis.tue.nl/f_p_power/</a> which effect sizes (expressed in partial eta-squared, as indicated on the vertical axis) can be statistically significant with n = 14 per group, and 3 groups?</p>
<div class="cell" data-layout-align="center">
<div id="radio_DZUUNWPZEX" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_DZUUNWPZEX" value=""><span>Only effects larger than 0.11</span></label><label><input type="radio" autocomplete="off" name="radio_DZUUNWPZEX" value="answer"><span>Only effects larger than 0.13</span></label><label><input type="radio" autocomplete="off" name="radio_DZUUNWPZEX" value=""><span>Only effects larger than 0.14</span></label><label><input type="radio" autocomplete="off" name="radio_DZUUNWPZEX" value=""><span>Only effects larger than 0.16</span></label>
</div>
</div>
<p>Every sample size and alpha level implies a minimal statistically detectable effect that can be statistically significant in your study. Looking at which observed effects you can detect is a useful way to make sure you could actually detect the smallest effect size you are interested in.</p>
<p><strong>Q21</strong>: Using the minimal statistically detectable effect, you set the SESOI in a replication study to <em>d</em> = 0.35, and set the alpha level to 0.05. After collecting the data in a well-powered replication study that was as close to the original study as practically possible, you find no significant effect, and you can reject effects as large or larger than <em>d</em> = 0.35. What is the correct interpretation of this result?</p>
<div class="cell" data-layout-align="center">
<div id="radio_UCZWDWQVOC" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_UCZWDWQVOC" value=""><span>There is no effect.</span></label><label><input type="radio" autocomplete="off" name="radio_UCZWDWQVOC" value=""><span>We can statistically reject (using an alpha of 0.05) effects anyone would find theoretically meaningful.</span></label><label><input type="radio" autocomplete="off" name="radio_UCZWDWQVOC" value=""><span>We can statistically reject (using an alpha of 0.05) effects anyone would find practically relevant.</span></label><label><input type="radio" autocomplete="off" name="radio_UCZWDWQVOC" value="answer"><span>We can statistically reject (using an alpha of 0.05) effects that could have been statistically significant in the original study.</span></label>
</div>
</div>
</section><section id="open-questions" class="level3" data-number="9.13.4"><h3 data-number="9.13.4" class="anchored" data-anchor-id="open-questions">
<span class="header-section-number">9.13.4</span> Open Questions</h3>
<ol type="1">
<li><p>What is meant with the statement ‘Absence of evidence is not evidence of absence’?</p></li>
<li><p>What is the goal of an equivalence test?</p></li>
<li><p>What is the difference between a nil null hypothesis and a non-nil null hypothesis?</p></li>
<li><p>What is a minimal effect test?</p></li>
<li><p>What conclusion can we draw if a null-hypothesis significance test and equivalence test are performed for the same data, and neither test is statistically significant?</p></li>
<li><p>When designing equivalence tests to have a desired statistical power, why do you need a larger sample size, the narrower the equivalence range is?</p></li>
<li><p>While for a null hypothesis significance test one always has some probability to observe a statistically significant result, it is possible to perform an equivalence test with 0% power. When would this happen?</p></li>
<li><p>Why is it incorrect to say there is ‘no effect’ when the equivalence test is statistically significant?</p></li>
<li><p>Specify one way in which the Bayesian ROPE procedure and an equivalence test are similar, and specify one way in which they are different.</p></li>
<li><p>What is the anchor based method to specify a smallest effect size of interest?</p></li>
<li><p>What is a cost-benefit approach to specify a smallest effect size of interest?</p></li>
<li><p>How can researchers use theoretical predictions to specify a smallest effect size of interest?</p></li>
<li><p>What is the idea behind the ‘small telescopes’ approach to equivalence testing?</p></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list" style="display: none">
<div id="ref-abelson_value_2003" class="csl-entry" role="listitem">
Abelson, P. (2003). The <span>Value</span> of <span>Life</span> and <span>Health</span> for <span>Public Policy</span>. <em>Economic Record</em>, <em>79</em>, S2–S13. <a href="https://doi.org/10.1111/1475-4932.00087">https://doi.org/10.1111/1475-4932.00087</a>
</div>
<div id="ref-altman_statistics_1995" class="csl-entry" role="listitem">
Altman, D. G., &amp; Bland, J. M. (1995). Statistics notes: <span>Absence</span> of evidence is not evidence of absence. <em>BMJ</em>, <em>311</em>(7003), 485. <a href="https://doi.org/10.1136/bmj.311.7003.485">https://doi.org/10.1136/bmj.311.7003.485</a>
</div>
<div id="ref-anderson_theres_2016" class="csl-entry" role="listitem">
Anderson, S. F., &amp; Maxwell, S. E. (2016). There’s more than one way to conduct a replication study: <span>Beyond</span> statistical significance. <em>Psychological Methods</em>, <em>21</em>(1), 1–12. <a href="https://doi.org/10.1037/met0000051">https://doi.org/10.1037/met0000051</a>
</div>
<div id="ref-anvari_using_2021" class="csl-entry" role="listitem">
Anvari, F., &amp; Lakens, D. (2021). Using anchor-based methods to determine the smallest effect size of interest. <em>Journal of Experimental Social Psychology</em>, <em>96</em>, 104159. <a href="https://doi.org/10.1016/j.jesp.2021.104159">https://doi.org/10.1016/j.jesp.2021.104159</a>
</div>
<div id="ref-ball_effects_2002" class="csl-entry" role="listitem">
Ball, K., Berch, D. B., Helmers, K. F., Jobe, J. B., Leveck, M. D., Marsiske, M., Morris, J. N., Rebok, G. W., Smith, D. M., &amp; Tennstedt, S. L. (2002). Effects of cognitive training interventions with older adults: A randomized controlled trial. <em>Jama</em>, <em>288</em>(18), 2271–2281.
</div>
<div id="ref-bauer_unifying_1996" class="csl-entry" role="listitem">
Bauer, P., &amp; Kieser, M. (1996). A unifying approach for confidence intervals and testing of equivalence and difference. <em>Biometrika</em>, <em>83</em>(4), 934–937.
</div>
<div id="ref-bem_feeling_2011" class="csl-entry" role="listitem">
Bem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. <em>Journal of Personality and Social Psychology</em>, <em>100</em>(3), 407–425. <a href="https://doi.org/10.1037/a0021524">https://doi.org/10.1037/a0021524</a>
</div>
<div id="ref-burriss_changes_2015" class="csl-entry" role="listitem">
Burriss, R. P., Troscianko, J., Lovell, P. G., Fulford, A. J. C., Stevens, M., Quigley, R., Payne, J., Saxton, T. K., &amp; Rowland, H. M. (2015). Changes in women’s facial skin color over the ovulatory cycle are not detectable by the human visual system. <em>PLOS ONE</em>, <em>10</em>(7), e0130093. <a href="https://doi.org/10.1371/journal.pone.0130093">https://doi.org/10.1371/journal.pone.0130093</a>
</div>
<div id="ref-button_minimal_2015" class="csl-entry" role="listitem">
Button, K. S., Kounali, D., Thomas, L., Wiles, N. J., Peters, T. J., Welton, N. J., Ades, A. E., &amp; Lewis, G. (2015). Minimal clinically important difference on the <span>Beck Depression Inventory</span> - <span>II</span> according to the patient’s perspective. <em>Psychological Medicine</em>, <em>45</em>(15), 3269–3279. <a href="https://doi.org/10.1017/S0033291715001270">https://doi.org/10.1017/S0033291715001270</a>
</div>
<div id="ref-cribbie_recommendations_2004" class="csl-entry" role="listitem">
Cribbie, R. A., Gruman, J. A., &amp; Arpin-Cribbie, C. A. (2004). Recommendations for applying tests of equivalence. <em>Journal of Clinical Psychology</em>, <em>60</em>(1), 1–10.
</div>
<div id="ref-delacre_why_2017" class="csl-entry" role="listitem">
Delacre, M., Lakens, D., &amp; Leys, C. (2017). Why <span>Psychologists Should</span> by <span>Default Use Welch</span>’s <span><em>t</em></span>-test <span>Instead</span> of <span>Student</span>’s <span><em>t</em></span>-test. <em>International Review of Social Psychology</em>, <em>30</em>(1). <a href="https://doi.org/10.5334/irsp.82">https://doi.org/10.5334/irsp.82</a>
</div>
<div id="ref-dienes_using_2014" class="csl-entry" role="listitem">
Dienes, Z. (2014). Using <span>Bayes</span> to get the most out of non-significant results. <em>Frontiers in Psychology</em>, <em>5</em>. <a href="https://doi.org/10.3389/fpsyg.2014.00781">https://doi.org/10.3389/fpsyg.2014.00781</a>
</div>
<div id="ref-ferguson_vast_2012" class="csl-entry" role="listitem">
Ferguson, C. J., &amp; Heene, M. (2012). A vast graveyard of undead theories publication bias and psychological science’s aversion to the null. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 555–561.
</div>
<div id="ref-ferguson_providing_2021" class="csl-entry" role="listitem">
Ferguson, C. J., &amp; Heene, M. (2021). Providing a lower-bound estimate for psychology’s <span>“crud factor”</span>: <span>The</span> case of aggression. <em>Professional Psychology: Research and Practice</em>, <em>52</em>(6), 620–626. https://doi.org/<a href="http://dx.doi.org/10.1037/pro0000386">http://dx.doi.org/10.1037/pro0000386</a>
</div>
<div id="ref-gosset_application_1904" class="csl-entry" role="listitem">
Gosset, W. S. (1904). <em>The <span>Application</span> of the "<span>Law</span> of <span>Error</span>" to the <span>Work</span> of the <span>Brewery</span></em> (1 vol 8; pp. 3–16). <span>Arthur Guinness &amp; Son, Ltd.</span>
</div>
<div id="ref-hauck_new_1984" class="csl-entry" role="listitem">
Hauck, D. W. W., &amp; Anderson, S. (1984). A new statistical procedure for testing equivalence in two-group comparative bioavailability trials. <em>Journal of Pharmacokinetics and Biopharmaceutics</em>, <em>12</em>(1), 83–91. <a href="https://doi.org/10.1007/BF01063612">https://doi.org/10.1007/BF01063612</a>
</div>
<div id="ref-hodges_testing_1954" class="csl-entry" role="listitem">
Hodges, J. L., &amp; Lehmann, E. L. (1954). Testing the <span>Approximate Validity</span> of <span>Statistical Hypotheses</span>. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <em>16</em>(2), 261–268. <a href="https://doi.org/10.1111/j.2517-6161.1954.tb00169.x">https://doi.org/10.1111/j.2517-6161.1954.tb00169.x</a>
</div>
<div id="ref-hoenig_abuse_2001" class="csl-entry" role="listitem">
Hoenig, J. M., &amp; Heisey, D. M. (2001). The abuse of power: The pervasive fallacy of power calculations for data analysis. <em>The American Statistician</em>, <em>55</em>(1), 19–24. <a href="https://doi.org/10.1198/000313001300339897">https://doi.org/10.1198/000313001300339897</a>
</div>
<div id="ref-hyde_gender_2008" class="csl-entry" role="listitem">
Hyde, J. S., Lindberg, S. M., Linn, M. C., Ellis, A. B., &amp; Williams, C. C. (2008). Gender <span>Similarities Characterize Math Performance</span>. <em>Science</em>, <em>321</em>(5888), 494–495. <a href="https://doi.org/10.1126/science.1160364">https://doi.org/10.1126/science.1160364</a>
</div>
<div id="ref-jaeschke_measurement_1989" class="csl-entry" role="listitem">
Jaeschke, R., Singer, J., &amp; Guyatt, G. H. (1989). Measurement of health status: <span>Ascertaining</span> the minimal clinically important difference. <em>Controlled Clinical Trials</em>, <em>10</em>(4), 407–415. <a href="https://doi.org/10.1016/0197-2456(89)90005-6">https://doi.org/10.1016/0197-2456(89)90005-6</a>
</div>
<div id="ref-king_point_2011" class="csl-entry" role="listitem">
King, M. T. (2011). A point of minimal important difference (<span>MID</span>): A critique of terminology and methods. <em>Expert Review of Pharmacoeconomics &amp; Outcomes Research</em>, <em>11</em>(2), 171–184. <a href="https://doi.org/10.1586/erp.11.9">https://doi.org/10.1586/erp.11.9</a>
</div>
<div id="ref-kruschke_bayesian_2013" class="csl-entry" role="listitem">
Kruschke, J. K. (2013). Bayesian estimation supersedes the t test. <em>Journal of Experimental Psychology: General</em>, <em>142</em>(2), 573–603. <a href="https://doi.org/10.1037/a0029146">https://doi.org/10.1037/a0029146</a>
</div>
<div id="ref-kruschke_doing_2014" class="csl-entry" role="listitem">
Kruschke, J. K. (2014). <em>Doing <span>Bayesian Data Analysis</span>, <span>Second Edition</span>: <span>A Tutorial</span> with <span>R</span>, <span>JAGS</span>, and <span>Stan</span></em> (2 edition). <span>Academic Press</span>.
</div>
<div id="ref-kruschke_bayesian_2017" class="csl-entry" role="listitem">
Kruschke, J. K., &amp; Liddell, T. M. (2017). The <span>Bayesian New Statistics</span>: <span>Hypothesis</span> testing, estimation, meta-analysis, and power analysis from a <span>Bayesian</span> perspective. <em>Psychonomic Bulletin &amp; Review</em>. <a href="https://doi.org/10.3758/s13423-016-1221-4">https://doi.org/10.3758/s13423-016-1221-4</a>
</div>
<div id="ref-lakens_equivalence_2017" class="csl-entry" role="listitem">
Lakens, D. (2017). Equivalence <span>Tests</span>: <span>A Practical Primer</span> for t <span>Tests</span>, <span>Correlations</span>, and <span>Meta-Analyses</span>. <em>Social Psychological and Personality Science</em>, <em>8</em>(4), 355–362. <a href="https://doi.org/10.1177/1948550617697177">https://doi.org/10.1177/1948550617697177</a>
</div>
<div id="ref-lakens_practical_2021" class="csl-entry" role="listitem">
Lakens, D. (2021). The practical alternative to the p value is the correctly used p value. <em>Perspectives on Psychological Science</em>, <em>16</em>(3), 639–648. <a href="https://doi.org/10.1177/1745691620958012">https://doi.org/10.1177/1745691620958012</a>
</div>
<div id="ref-levine_communication_2008" class="csl-entry" role="listitem">
Levine, T. R., Weber, R., Park, H. S., &amp; Hullett, C. R. (2008). A communication researchers’ guide to null hypothesis significance testing and alternatives. <em>Human Communication Research</em>, <em>34</em>(2), 188–209.
</div>
<div id="ref-mazzolari_myths_2022" class="csl-entry" role="listitem">
Mazzolari, R., Porcelli, S., Bishop, D. J., &amp; Lakens, D. (2022). Myths and methodologies: <span>The</span> use of equivalence and non-inferiority tests for interventional studies in exercise physiology and sport science. <em>Experimental Physiology</em>, <em>107</em>(3), 201–212. <a href="https://doi.org/10.1113/EP090171">https://doi.org/10.1113/EP090171</a>
</div>
<div id="ref-mcelreath_statistical_2016" class="csl-entry" role="listitem">
McElreath, R. (2016). <em>Statistical <span>Rethinking</span>: <span>A Bayesian Course</span> with <span>Examples</span> in <span>R</span> and <span>Stan</span></em> (Vol. 122). <span>CRC Press</span>.
</div>
<div id="ref-meehl_appraising_1990" class="csl-entry" role="listitem">
Meehl, P. E. (1990). Appraising and amending theories: <span>The</span> strategy of <span>Lakatosian</span> defense and two principles that warrant it. <em>Psychological Inquiry</em>, <em>1</em>(2), 108–141. <a href="https://doi.org/10.1207/s15327965pli0102_1">https://doi.org/10.1207/s15327965pli0102_1</a>
</div>
<div id="ref-mrozek_what_2002" class="csl-entry" role="listitem">
Mrozek, J. R., &amp; Taylor, L. O. (2002). What determines the value of life? A meta-analysis. <em>Journal of Policy Analysis and Management</em>, <em>21</em>(2), 253–270. <a href="https://doi.org/10.1002/pam.10026">https://doi.org/10.1002/pam.10026</a>
</div>
<div id="ref-murphy_testing_1999" class="csl-entry" role="listitem">
Murphy, K. R., &amp; Myors, B. (1999). Testing the hypothesis that treatments have negligible effects: <span class="nocase">Minimum-effect</span> tests in the general linear model. <em>Journal of Applied Psychology</em>, <em>84</em>(2), 234–248. <a href="https://doi.org/10.1037/0021-9010.84.2.234">https://doi.org/10.1037/0021-9010.84.2.234</a>
</div>
<div id="ref-murphy_statistical_2014" class="csl-entry" role="listitem">
Murphy, K. R., Myors, B., &amp; Wolach, A. H. (2014). <em>Statistical power analysis: A simple and general model for traditional and modern hypothesis tests</em> (Fourth edition). <span>Routledge, Taylor &amp; Francis Group</span>.
</div>
<div id="ref-nickerson_null_2000" class="csl-entry" role="listitem">
Nickerson, R. S. (2000). Null hypothesis significance testing: <span>A</span> review of an old and continuing controversy. <em>Psychological Methods</em>, <em>5</em>(2), 241–301. <a href="https://doi.org/10.1037//1082-989X.5.2.241">https://doi.org/10.1037//1082-989X.5.2.241</a>
</div>
<div id="ref-norman_truly_2004" class="csl-entry" role="listitem">
Norman, G. R., Sloan, J. A., &amp; Wyrwich, K. W. (2004). The truly remarkable universality of half a standard deviation: Confirmation through another look. <em>Expert Review of Pharmacoeconomics &amp; Outcomes Research</em>, <em>4</em>(5), 581–585.
</div>
<div id="ref-nunnally_place_1960" class="csl-entry" role="listitem">
Nunnally, J. (1960). The place of statistics in psychology. <em>Educational and Psychological Measurement</em>, <em>20</em>(4), 641–650. <a href="https://doi.org/10.1177/001316446002000401">https://doi.org/10.1177/001316446002000401</a>
</div>
<div id="ref-orben_crud_2020" class="csl-entry" role="listitem">
Orben, A., &amp; Lakens, D. (2020). Crud (<span>Re</span>)<span>Defined</span>. <em>Advances in Methods and Practices in Psychological Science</em>, <em>3</em>(2), 238–247. <a href="https://doi.org/10.1177/2515245920917961">https://doi.org/10.1177/2515245920917961</a>
</div>
<div id="ref-parkhurst_statistical_2001" class="csl-entry" role="listitem">
Parkhurst, D. F. (2001). Statistical significance tests: <span>Equivalence</span> and reverse tests should reduce misinterpretation. <em>Bioscience</em>, <em>51</em>(12), 1051–1057. <a href="https://doi.org/10.1641/0006-3568(2001)051%5B1051:SSTEAR%5D2.0.CO;2">https://doi.org/10.1641/0006-3568(2001)051[1051:SSTEAR]2.0.CO;2</a>
</div>
<div id="ref-popper_logic_2002" class="csl-entry" role="listitem">
Popper, K. R. (2002). <em><span>The logic of scientific discovery</span></em>. <span>Routledge</span>.
</div>
<div id="ref-quertemont_how_2011" class="csl-entry" role="listitem">
Quertemont, E. (2011). How to <span>Statistically Show</span> the <span>Absence</span> of an <span>Effect</span>. <em>Psychologica Belgica</em>, <em>51</em>(2), 109–127. <a href="https://doi.org/10.5334/pb-51-2-109">https://doi.org/10.5334/pb-51-2-109</a>
</div>
<div id="ref-rogers_using_1993" class="csl-entry" role="listitem">
Rogers, J. L., Howard, K. I., &amp; Vessey, J. T. (1993). Using significance tests to evaluate equivalence between two experimental groups. <em>Psychological Bulletin</em>, <em>113</em>(3), 553–565. https://doi.org/<a href="http://dx.doi.org/10.1037/0033-2909.113.3.553">http://dx.doi.org/10.1037/0033-2909.113.3.553</a>
</div>
<div id="ref-schuirmann_comparison_1987" class="csl-entry" role="listitem">
Schuirmann, D. J. (1987). A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. <em>Journal of Pharmacokinetics and Biopharmaceutics</em>, <em>15</em>(6), 657–680.
</div>
<div id="ref-schumi_through_2011" class="csl-entry" role="listitem">
Schumi, J., &amp; Wittes, J. T. (2011). Through the looking glass: Understanding non-inferiority. <em>Trials</em>, <em>12</em>(1), 106. <a href="https://doi.org/10.1186/1745-6215-12-106">https://doi.org/10.1186/1745-6215-12-106</a>
</div>
<div id="ref-schweder_confidence_2016" class="csl-entry" role="listitem">
Schweder, T., &amp; Hjort, N. L. (2016). <em>Confidence, <span>Likelihood</span>, <span>Probability</span>: <span>Statistical Inference</span> with <span>Confidence Distributions</span></em>. <span>Cambridge University Press</span>. <a href="https://doi.org/10.1017/CBO9781139046671">https://doi.org/10.1017/CBO9781139046671</a>
</div>
<div id="ref-seaman_equivalence_1998" class="csl-entry" role="listitem">
Seaman, M. A., &amp; Serlin, R. C. (1998). Equivalence confidence intervals for two-group comparisons of means. <em>Psychological Methods</em>, <em>3</em>(4), 403–411. https://doi.org/<a href="http://dx.doi.org.dianus.libr.tue.nl/10.1037/1082-989X.3.4.403">http://dx.doi.org.dianus.libr.tue.nl/10.1037/1082-989X.3.4.403</a>
</div>
<div id="ref-simonsohn_small_2015" class="csl-entry" role="listitem">
Simonsohn, U. (2015). Small telescopes: <span>Detectability</span> and the evaluation of replication results. <em>Psychological Science</em>, <em>26</em>(5), 559–569. <a href="https://doi.org/10.1177/0956797614567341">https://doi.org/10.1177/0956797614567341</a>
</div>
<div id="ref-viamonte_cost-benefit_2006" class="csl-entry" role="listitem">
Viamonte, S. M., Ball, K. K., &amp; Kilgore, M. (2006). A <span>Cost-Benefit Analysis</span> of <span>Risk-Reduction Strategies Targeted</span> at <span>Older Drivers</span>. <em>Traffic Injury Prevention</em>, <em>7</em>(4), 352–359. <a href="https://doi.org/10.1080/15389580600791362">https://doi.org/10.1080/15389580600791362</a>
</div>
<div id="ref-wellek_testing_2010" class="csl-entry" role="listitem">
Wellek, S. (2010). <em>Testing statistical hypotheses of equivalence and noninferiority</em> (2nd ed). <span>CRC Press</span>.
</div>
<div id="ref-westlake_use_1972" class="csl-entry" role="listitem">
Westlake, W. J. (1972). Use of <span>Confidence Intervals</span> in <span>Analysis</span> of <span>Comparative Bioavailability Trials</span>. <em>Journal of Pharmaceutical Sciences</em>, <em>61</em>(8), 1340–1341. <a href="https://doi.org/10.1002/JPS.2600610845">https://doi.org/10.1002/JPS.2600610845</a>
</div>
</div>
</section></section></main><!-- /main --><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script><script>

/* update total correct if #webex-total_correct exists */
update_total_correct = function() {
  console.log("webex: update total_correct");

  var t = document.getElementsByClassName("webex-total_correct");
  for (var i = 0; i < t.length; i++) {
    p = t[i].parentElement;
    var correct = p.getElementsByClassName("webex-correct").length;
    var solvemes = p.getElementsByClassName("webex-solveme").length;
    var radiogroups = p.getElementsByClassName("webex-radiogroup").length;
    var selects = p.getElementsByClassName("webex-select").length;

    t[i].innerHTML = correct + " of " + (solvemes + radiogroups + selects) + " correct";
  }
}

/* webex-solution button toggling function */
b_func = function() {
  console.log("webex: toggle hide");

  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* check answers */
check_func = function() {
  console.log("webex: check answers");

  var cl = this.parentElement.classList;
  if (cl.contains('unchecked')) {
    cl.remove("unchecked");
    this.innerHTML = "Hide Answers";
  } else {
    cl.add("unchecked");
    this.innerHTML = "Show Answers";
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  console.log("webex: check solveme");

  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "")
  }

  if (my_answer == "") {
    cl.remove("webex-correct");
    cl.remove("webex-incorrect");
  } else if (real_answers.includes(my_answer)) {
    cl.add("webex-correct");
    cl.remove("webex-incorrect");
  } else {
    cl.add("webex-incorrect");
    cl.remove("webex-correct");
  }

  // match numeric answers within a specified tolerance
  if(this.dataset.tol > 0){
    var tol = JSON.parse(this.dataset.tol);
    var matches = real_answers.map(x => Math.abs(x - my_answer) < tol)
    if (matches.reduce((a, b) => a + b, 0) > 0) {
      cl.add("webex-correct");
    } else {
      cl.remove("webex-correct");
    }
  }

  // added regex bit
  if (cl.contains("regex")){
    answer_regex = RegExp(real_answers.join("|"))
    if (answer_regex.test(my_answer)) {
      cl.add("webex-correct");
    }
  }

  update_total_correct();
}

/* function for checking select answers */
select_func = function(e) {
  console.log("webex: check select");

  var cl = this.classList

  /* add style */
  cl.remove("webex-incorrect");
  cl.remove("webex-correct");
  if (this.value == "answer") {
    cl.add("webex-correct");
  } else if (this.value != "blank") {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

/* function for checking radiogroups answers */
radiogroups_func = function(e) {
  console.log("webex: check radiogroups");

  var checked_button = document.querySelector('input[name=' + this.id + ']:checked');
  var cl = checked_button.parentElement.classList;
  var labels = checked_button.parentElement.parentElement.children;

  /* get rid of styles */
  for (i = 0; i < labels.length; i++) {
    labels[i].classList.remove("webex-incorrect");
    labels[i].classList.remove("webex-correct");
  }

  /* add style */
  if (checked_button.value == "answer") {
    cl.add("webex-correct");
  } else {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

window.onload = function() {
  console.log("webex onload");
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('webex-solution')) {
      buttons[i].onclick = b_func;
    }
  }

  var check_sections = document.getElementsByClassName("webex-check");
  console.log("check:", check_sections.length);
  for (var i = 0; i < check_sections.length; i++) {
    check_sections[i].classList.add("unchecked");

    let btn = document.createElement("button");
    btn.innerHTML = "Show Answers";
    btn.classList.add("webex-check-button");
    btn.onclick = check_func;
    check_sections[i].appendChild(btn);

    let spn = document.createElement("span");
    spn.classList.add("webex-total_correct");
    check_sections[i].appendChild(spn);
  }

  /* set up webex-solveme inputs */
  var solveme = document.getElementsByClassName("webex-solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off");
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";

    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;

    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;

    $(solveme[i]).after(" <span class='webex-icon'></span>");
  }

  /* set up radiogroups */
  var radiogroups = document.getElementsByClassName("webex-radiogroup");
  for (var i = 0; i < radiogroups.length; i++) {
    radiogroups[i].onchange = radiogroups_func;
  }

  /* set up selects */
  var selects = document.getElementsByClassName("webex-select");
  for (var i = 0; i < selects.length; i++) {
    selects[i].onchange = select_func;
    $(selects[i]).after(" <span class='webex-icon'></span>");
  }

  update_total_correct();
}

</script><script>
// open rdrr links externally ----

var exlinks = document.querySelectorAll("a[href^='https://rdrr.io']");
var exlink_func = function(){
  window.open(this.href);
  return false;
};
for (var i = 0; i < exlinks.length; i++) {
    exlinks[i].addEventListener('click', exlink_func, false);
}

// visible second sidebar in mobile ----

function move_sidebar() {
  var toc = document.getElementById("TOC");
  var small_sidebar = document.querySelector("#quarto-sidebar .sidebar-menu-container");
  var right_sidebar = document.getElementById("quarto-margin-sidebar");

  if (window.innerWidth < 768) {
    small_sidebar.append(toc);
  } else {
    right_sidebar.append(toc);
  }
}
move_sidebar();
window.onresize = move_sidebar;
</script><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./08-samplesizejustification.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sample Size Justification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./10-sequential.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequential Analysis</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">Lakens, D. (2022). Improving Your Statistical Inferences. Retrieved from https://lakens.github.io/statistical_inferences/. https://doi.org/10.5281/zenodo.6409077</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


</body></html>