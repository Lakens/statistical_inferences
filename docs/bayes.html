<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Bayesian statistics | Improving Your Statistical Inferences</title>
<meta name="author" content="Daniel Lakens">
<meta name="description" content="&quot;Logic!&quot; said the Professor half to himself. &quot;Why don't they teach logic at these schools? There are only three possibilities. Either your sister is telling lies, or she is mad, or she is telling...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 4 Bayesian statistics | Improving Your Statistical Inferences">
<meta property="og:type" content="book">
<meta property="og:description" content="&quot;Logic!&quot; said the Professor half to himself. &quot;Why don't they teach logic at these schools? There are only three possibilities. Either your sister is telling lies, or she is mad, or she is telling...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Bayesian statistics | Improving Your Statistical Inferences">
<meta name="twitter:description" content="&quot;Logic!&quot; said the Professor half to himself. &quot;Why don't they teach logic at these schools? There are only three possibilities. Either your sister is telling lies, or she is mad, or she is telling...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="pvalue.html"><span class="header-section-number">1</span> Using p-values to test a hypothesis</a></li>
<li><a class="" href="errorcontrol.html"><span class="header-section-number">2</span> Error control</a></li>
<li><a class="" href="likelihoods.html"><span class="header-section-number">3</span> Likelihoods</a></li>
<li><a class="active" href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></li>
<li><a class="" href="questions.html"><span class="header-section-number">5</span> Asking Statistical Questions</a></li>
<li><a class="" href="effectsize.html"><span class="header-section-number">6</span> Effect Sizes</a></li>
<li><a class="" href="confint.html"><span class="header-section-number">7</span> Confidence Intervals</a></li>
<li><a class="" href="power.html"><span class="header-section-number">8</span> Sample size justification</a></li>
<li><a class="" href="references.html"><span class="header-section-number">9</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/Lakens/statistical_inferences">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayes" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Bayesian statistics<a class="anchor" aria-label="anchor" href="#bayes"><i class="fas fa-link"></i></a>
</h1>
<blockquote>
<p>"Logic!" said the Professor half to himself. "Why don't they teach logic at these schools? There are only three possibilities. Either your sister is telling lies, or she is mad, or she is telling the truth. You know she doesn't tell lies and it is obvious that she is not mad. For the moment then and unless any further evidence turns up, we must assume that she is telling the truth."</p>
</blockquote>
<p><em><a href="https://gutenberg.ca/ebooks/lewiscs-thelionthewitchandthewardrobe/lewiscs-thelionthewitchandthewardrobe-00-h.html">The Lion, The Witch, and The Wardrobe. A Story for Children</a> by C. S. Lewis.</em></p>
<p>In the children's book The Lion, The Witch, and The Wardrobe, both Lucy and Edmund go through a wardrobe into a country called Narnia. Lucy tells their older brother and sister, Peter and Susan, about Narnia, but Edmund wants to keep it a secret, and tells Peter and Susan they were just pretending. Peter and Susan don't know what to believe - does Narnia exist, or not? They ask the Professor, who lives in the house with the wardrobe, for advice. The Professor asks Susan and Peter if in their past experience, Lucy or Edward has been more truthful, to which Peter answers "Up till now, I'd have said Lucy every time." The Professor then replies with the quote above. From the three possible options, it is very unlikely that Lucy is lying, as she has not done this in the past, and the Professor says it is clear just from talking to Lucy that she is not mad. Therefore, the most likely option is that Lucy is telling the truth. If new evidence is uncovered, these beliefs can be updated in the future. This approach to knowledge generation, where the prior probability of different hypotheses is quantified, and if possible updated in light of new data, is an example of <em>Bayesian inference</em>.</p>
<p>Although frequentist statistics is by far the dominant approach in science, it is important to have had at least rudimentary exposure to Bayesian statistics during any statistics training. Bayesian statistics is especially useful when inferences are made in cases where the data under investigation is unique, and there is no frequentist probability defined as the limit in many trials. For example, the question might not be how often Lucy lies on average, but whether Lucy is lying in this specific instance about the existence of Narnia. When we do research, we often start with a prior belief that a hypothesis is true. After collecting data, we can use this data to update our prior beliefs. Bayesian statistics allows you to update prior beliefs into posterior probabilities in a logically consistent manner. Before we have collected data, the <strong>prior odds</strong> of Hypothesis 1 (H1) over the null-hypothesis (H0) are P(H1)/P(H0), After we have collected data, we have the <strong>posterior odds</strong> P(H1|D)/P(H0|D), which you can read as the probability of H1, given the data, divided by the probability of H0, given the data. There are different approaches to Bayesian statistics. We will first discuss Bayes factors, and then Bayesian estimation.</p>
<div id="bayes-factors" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Bayes factors<a class="anchor" aria-label="anchor" href="#bayes-factors"><i class="fas fa-link"></i></a>
</h2>
<p>One approach in Bayesian statistics focuses on the comparison of different models that might explain the data (referred to as <strong>model comparison</strong>). In Bayesian statistics, the probability of data under a specified model (D|P(H0) is a number that expressed what is sometimes referred to as the absolute <strong>evidence</strong>, and more formally referred to as a marginal likelihood. The marginal likelihood uses prior probabilities to average the likelihood across the parameter space. For example, assume we have a simple model <em>M</em> that is
based on a single parameter, that can take on two values, X and Y, and that a-prior we believe the probability of both values is p(X) = 0.4 and p(Y) = 0.6. We collect data, and calculate the likelihood for both these parameter values, which is p(D|X) = 0.02 and p(D|Y) = 0.08. The marginal likelihood of our model <em>M</em> is then P(D|M) = 0.4 × 0.02 + 0.6 × 0.08 = 0.056. Most often, models have continuously varying parameters, and the marginal likelihood formula is based on an integral, but the idea remains the same.</p>
<p>A comparison of two models is based on the relative evidence the data provides for each models we are comparing. The relative evidence is calculated by dividing the marginal likelihood for one model by the marginal likelihood for another model, and this ratio of relative evidence based on these marginal likelihoods is called the <strong>Bayes factor</strong>. Bayes factors are the Bayesian equivalent of hypothesis tests <span class="citation">(<a href="references.html#ref-dienes_understanding_2008" role="doc-biblioref">Dienes, 2008</a>)</span>. The Bayes factor represents how much we have updated our beliefs, based on observing the data. We can express Bayes factors to indicate how much more likely H1 is given the data compared to H0 (often indicated by B10) or as how much more likely H0 has become compared to H1 (B01), and B10 = 1/B01. Similar to likelihood ratios of 1, a Bayes factor of 1 did not change our beliefs for one model compared to the other model. A very large Bayes factor for H1 over H0 has increased our belief in H1, and a Bayes Factor close for H1 over H0 to 0 has increased our belief in H0. If our prior belief in H1 was very, very low (e.g., your belief in unicorns) even a large Bayes factor that supports the presence of a unicorn might not yet convince you that unicorns are real – but you have updated your belief in unicorns, and now believe they are at least more likely then they were before (even if you still think unicorns are very unlikely to exist). The contribution of the Bayes Factor and the prior in calculating the posterior odds is clear in the following formula:</p>
<p><span class="math display">\[
\frac{P(H1|D)}{P(H0|D)} = \ \frac{P(D|H1)}{P(D|H0)}\  \times \ \frac{P(H1)}{P(H0)}
\]</span></p>
<p><span class="math display">\[
Posterior\ Probability = \ Bayes\ Factor\  \times \ Prior\ Probability
\]</span></p>
<p>A Bayesian analysis of data requires specifying the prior. Here, we will continue our example based on a binomial probability, such as a coin flip. In the likelihood example, we compared two point hypotheses (e.g., <em>p</em> = 0.5 vs. <em>p</em> = 0.8). In Bayesian statistics, parameters are considered to be random variables, and the uncertainty or degree of belief with respect to the parameters is quantified by <strong>probability distributions</strong>.</p>
<p>A binomial probability lies between 0 and 1. You could draw any probability density you want over 0 and 1, and turn it into a prior, but for good reasons (simplicity, mostly) a beta-prior is often used for binomial probabilities. The shape of the beta-prior depends on two parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Note that these are the same Greek letters as used for the Type 1 error rate and Type 2 error rate, but that is purely coincidental! The <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> in binomial probabilities are unrelated to error rates, and the use of the same letters is mainly due to a
lack of creativity among statisticians and the limited choice the alphabet gives us. It also does not help that <span class="math inline">\(\beta\)</span> is one of the parameters of the Beta distribution. Try to keep these different Beta’s apart! The probability density function is:</p>
<p><span class="math display">\[
\int_{}^{}{\left( x,\ \alpha,\ \beta \right) = \ \frac{1}{B(\alpha,\beta)}}x^{\alpha - 1}{(1 - x)}^{\beta - 1}
\]</span></p>
<p>where <em>B(<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>)</em> is the beta function. Understanding the mathematical basis of this function is beyond the scope of this chapter, but you can read more on <a href="https://en.wikipedia.org/wiki/Beta_distribution">Wikipedia</a> or Kruschke's book on Doing Bayesian Data Analysis <span class="citation">(<a href="references.html#ref-kruschke_doing_2014" role="doc-biblioref">J. Kruschke, 2014</a>)</span>. The beta-prior for a variety of values for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> can be seen in the figure below.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bayes1"></span>
<img src="04-bayes_files/figure-html/bayes1-1.png" alt="Four examples of Bayesian priors" width="100%"><p class="caption">
Figure 4.1: Four examples of Bayesian priors
</p>
</div>
<p>These beta densities reflect different types of priors. Let’s assume you are approached by a street merchant who tries to sell you a special coin with heads and tails that, when flipped, will almost always turn up heads. The <span class="math inline">\(\alpha\)</span> = 1, <span class="math inline">\(\beta\)</span> = 1 prior is what a newborn baby would have as a prior, without any idea of what to expect when you flip a coin, and thus every value of <em>p</em> is equally likely. The <span class="math inline">\(\alpha\)</span> = 1, <span class="math inline">\(\beta\)</span> = 1/2 prior is what a true believer would have as a prior. The sales merchant tells you the coin will turn up heads almost every time, and thus, you believe it will turn up heads almost every time. The <span class="math inline">\(\alpha\)</span> = 4, <span class="math inline">\(\beta\)</span> = 4, and the <span class="math inline">\(\alpha\)</span> = 100, <span class="math inline">\(\beta\)</span> = 100 priors are for slightly and extremely skeptical people. With an <span class="math inline">\(\alpha\)</span> = 4, <span class="math inline">\(\beta\)</span> = 4 prior, you expect the coin will be fair, but you are willing to believe a wide range of other true values is possible (the curve is centered on 0.5, but the curve is wide, allowing for very high and low values of <em>p</em>). With the <span class="math inline">\(\alpha\)</span> = 100, <span class="math inline">\(\beta\)</span> = 100 prior you are really convinced coins are fair, and believe there will be only a very slight bias, at most (the curve is again centered on 0.5, and a skeptic believes the <em>p</em> will lie between 0.4 and 0.6 – a much narrower range compared to the slightly skeptic individual).</p>
<p>Let’s assume the newborn baby, the true believer, the slightly skeptic and the extreme skeptic all buy the coin, flip it n = 20 times, and observe x = 10 heads. This outcome can be plotted as a binomial distribution with 10 heads out of 20 trials, or as a Beta(11, 11) distribution.</p>
<p>The newborn baby had a prior Beta distribution with <span class="math inline">\(\alpha\)</span> = 1 and <span class="math inline">\(\beta\)</span> = 1, which equals a binomial likelihood distribution for 0 heads out of 0 trials. The posterior is a Beta distribution with Beta(<span class="math inline">\(\alpha\)</span>*, <span class="math inline">\(\beta\)</span>*), where:</p>
<p><span class="math inline">\(\alpha\)</span>* = <span class="math inline">\(\alpha\)</span> + x = 1 + 10= 11</p>
<p><span class="math inline">\(\beta\)</span>* = <span class="math inline">\(\beta\)</span> + n – x = 1 + 20 – 10 = 11</p>
<p>Or calculating these values more directly from the <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> of the prior and
likelihood:</p>
<p><span class="math inline">\(\alpha\)</span>* = <span class="math inline">\(\alpha\)</span>prior + <span class="math inline">\(\alpha\)</span>likelihood – 1 = 1 + 11 - 1= 11</p>
<p><span class="math inline">\(\beta\)</span>* = <span class="math inline">\(\beta\)</span>prior + <span class="math inline">\(\beta\)</span>likelihood - 1 = 1 + 11 – 1 = 11</p>
<p>Thus, the posterior distribution for the newborn is a Beta(11,11) distribution. This equals a binomial likelihood function for 10 heads out of 20 trials, or Beta(11,11) distribution. In other words, the posterior distribution is identical to the likelihood function when a uniform prior is used.</p>
<p>Take a look at the Figure below. Given 10 heads out of 20 coin flips, we see the prior distribution of the newborn (the horizontal grey line), the likelihood (the blue dotted line) and the posterior (the black line).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bayes2"></span>
<img src="04-bayes_files/figure-html/bayes2-1.png" alt="Four examples of how different priors are updated based on data to the posterior." width="100%"><p class="caption">
Figure 4.2: Four examples of how different priors are updated based on data to the posterior.
</p>
</div>
<p>For the true believer the posterior distribution is not centered on the maximum likelihood of the observed data, but just a bit in the direction of the prior. The slightly skeptic and the strong skeptic end up with a much stronger belief in a fair coin after observing the data, but mainly because they already had a stronger prior that the coin was fair.</p>
</div>
<div id="updating-our-belief" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Updating our belief<a class="anchor" aria-label="anchor" href="#updating-our-belief"><i class="fas fa-link"></i></a>
</h2>
<p>Now that we have a distribution for the prior, and a distribution for the posterior, we can see in the graphs below for which values of <em>p</em> our belief has increased. Everywhere where the black line (of the posterior) is higher than the grey line (of the prior) our belief in that <em>p</em> has increased.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bayes4"></span>
<img src="04-bayes_files/figure-html/bayes4-1.png" alt="Plot for the prior, likelihood, and posterior." width="100%"><p class="caption">
Figure 4.3: Plot for the prior, likelihood, and posterior.
</p>
</div>
<p>The Bayes Factor is used to quantify this increase in relative evidence. Let’s calculate the Bayes Factor for the hypothesis that the coin is fair for the newborn. The Bayes Factor is simply the value of the posterior distribution at <em>p</em> = 0.5, divided by the value of the prior distribution at <em>p</em> = 0.5:</p>
<p>BF10 = Beta(<em>p</em> = 0.5, 11, 11)/Beta(<em>p</em> = 0.5, 1, 1) = 3.70/1 = 3.70</p>
<p>You can check this in an <a href="http://pcl.missouri.edu/bf-binomial">online Bayes Factor calculator</a> by Jeff Rouder and Richard Morey. At successes, fill in 10, at trials, fill in 20. We want to calculate the Bayes Factor for the point null value of <em>p</em> = 0.5, so fill in 0.5. The <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> for the prior are both 1, given the newborns prior of Beta(1,1). Clicking ‘submit query’ will give you the Bayes factor of 3.70.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gpower-screenshot"></span>
<img src="images/binombayesonline.png" alt="Screenshot of the online calculator for binomially distributed observations" width="100%"><p class="caption">
Figure 1.7: Screenshot of the online calculator for binomially distributed observations
</p>
</div>
<p>We can calculate and plot the Bayes Factor, and show the prior (grey), likelihood (dashed blue) and posterior (black). For the example of 20 flips, 10 heads, and the newborn prior, the plot looks like this:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bayes6"></span>
<img src="04-bayes_files/figure-html/bayes6-1.png" alt="Plot for the prior, likelihood, and posterior." width="100%"><p class="caption">
Figure 4.4: Plot for the prior, likelihood, and posterior.
</p>
</div>
<p>We see that for the newborn, <em>p</em> = 0.5 has become more probable, but so has <em>p</em> = 0.4. Now let’s assume the strong skeptic, who believes the coin is fair with a prior of Beta(100, 100), buys the coin and flips it 100 times. Surprisingly, the coin comes up heads 90 out of 100 flips. The plot of the prior, likelihood, and posterior now looks much more extreme, because we had a very informed prior, and extremely different data. We see the grey prior distribution, the dashed blue likelihood based on the data, and the posterior distribution in black. The Bayes Factor of 0 represents the substantial drop in belief that the coin is fair – indeed, this now seems an untenable hypothesis, even for the strong skeptic. It shows how data can update your belief. Where a newborn would now completely believe that the true <em>p</em> for the coin is somewhere around 0.9, the strong skeptic has more reason to believe the <em>p</em> is around 0.65, due to the strong prior conviction that the coin is fair. Given enough data, even this strong skeptic will become convinced that the coin will return heads most of the time as well.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bayes7"></span>
<img src="04-bayes_files/figure-html/bayes7-1.png" alt="Plot for the prior, likelihood, and posterior." width="100%"><p class="caption">
Figure 4.5: Plot for the prior, likelihood, and posterior.
</p>
</div>
<p>We can now also see the difference between a likelihood inference approach, and a Bayesian inference approach. In likelihood inference, you can compare different values of <em>p</em> for the same likelihood curve (e.g., <em>p</em> = 0.5 vs <em>p</em> = 0.8) and calculate the likelihood ratio. In Bayesian inference, you can compare the difference between the prior and the posterior for the same value of <em>p</em>, and calculate the Bayes Factor.</p>
<p>If you have never seen Bayes Factors before, you might find it difficult to interpret the numbers. As with any guideline (e.g., interpreting effect sizes as small, medium, and large) there is criticism on the use of benchmarks. On the other hand, you have to start somewhere in getting a feel for what Bayes Factors mean. A Bayes factor between 1 and 3 is considered ‘not worth more than a bare mention’, larger than 3 (or smaller than 1/3) is considered ‘substantial’, and larger than 10 (or smaller than 1/10) is considered ‘strong’. These labels refer to the increase in how much you believe a specific hypothesis, not in the posterior belief in that hypothesis. If you think extra-sensory perception is extremely implausible, a single study with a BF = 14 will increase your belief, but you will now think extra-sensory perception is pretty much extremely implausible.</p>
</div>
<div id="bayesest" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Bayesian Estimation<a class="anchor" aria-label="anchor" href="#bayesest"><i class="fas fa-link"></i></a>
</h2>
<p>The posterior distribution summarizes our belief about the expected number of heads when flipping a coin after seeing the data, by averaging over our prior beliefs and the data (or the likelihood). The mean of a Beta distribution can be calculated by <span class="math inline">\(\alpha\)</span>/(<span class="math inline">\(\alpha\)</span>+<span class="math inline">\(\beta\)</span>). We can thus easily calculate the mean of a posterior distribution, which is the expected value based on our prior beliefs and the data.</p>
<p>We can also calculate a <strong>credible interval</strong> around the mean, which is a Bayesian version of a confidence interval with a slightly different interpretation. Instead of the Frequentist interpretation where a parameter has one (unknown) true value, the Bayesian approach considers the data fixed, but allow the parameter to vary. In Bayesian approaches, probability distributions represent our degree of belief. When calculating a credible interval, one is saying ‘I believe it is 95% probable (given my prior and the data) that the true parameter falls within this credible interval’. A 95% credible interval is simply the area of the posterior distribution between the 0.025 and 0.975 quantiles.</p>
<p>A credible interval and a confidence interval are the same, when a uniform prior (e.g., Beta(1,1)) is used. In this case, credible interval is numerically identical to the confidence interval. Only the interpretation differs. Whenever an informed prior is used, the credible interval and confidence interval differ. If the chosen prior is not representative of the truth, the credible interval will not be representative of the truth, but it is always a correct formalization of your beliefs. For a single confidence interval, the probability that it contains the true population parameter is either 0 or 1. Only in the long run will 95% of confidence intervals contain the true population parameter. These are important differences between Bayesian credible intervals and Frequentist confidence intervals to keep in mind.</p>
<p>We can plot the mean for the posterior when 10 heads out of 20 coin flips are observed, given a uniform prior.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bayes8"></span>
<img src="04-bayes_files/figure-html/bayes8-1.png" alt="Plot for the mean of the posterior when 10 out of 20 heads are observed given a uniform prior." width="100%"><p class="caption">
Figure 4.6: Plot for the mean of the posterior when 10 out of 20 heads are observed given a uniform prior.
</p>
</div>
<p>We can also use the ‘binom’ package to calculate the posterior mean, credible interval, and <strong>highest density interval (HDI)</strong>. The highest density interval is an alternative to the credible interval that works better when the posterior beta distribution is skewed (and is identical when the posterior distribution is symmetrical. We won’t go into the calculations of the HDI here.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">binom</span><span class="op">)</span>

<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">20</span> <span class="co"># set total trials</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">10</span> <span class="co"># set successes</span>
<span class="va">aprior</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="co"># Set the alpha for the Beta distribution for the prior</span>
<span class="va">bprior</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="co"># Set the beta for the Beta distribution for the prior</span>

<span class="fu"><a href="https://rdrr.io/pkg/binom/man/binom.bayes.html">binom.bayes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, type <span class="op">=</span> <span class="st">"central"</span>, prior.shape1 <span class="op">=</span> <span class="va">aprior</span>, prior.shape2 <span class="op">=</span> <span class="va">bprior</span><span class="op">)</span></code></pre></div>
<pre><code>##   method  x  n shape1 shape2 mean     lower     upper  sig
## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05</code></pre>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/binom/man/binom.bayes.html">binom.bayes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, type <span class="op">=</span> <span class="st">"highest"</span>, prior.shape1 <span class="op">=</span> <span class="va">aprior</span>, prior.shape2 <span class="op">=</span> <span class="va">bprior</span><span class="op">)</span></code></pre></div>
<pre><code>##   method  x  n shape1 shape2 mean     lower     upper  sig
## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05</code></pre>
<p>The posterior mean is identical to the Frequentist mean, but this is only the case when the mean of the prior equals the mean of the likelihood <span class="citation">(<a href="references.html#ref-albers_credible_2018" role="doc-biblioref">Albers et al., 2018</a>)</span>. This chapter shows the essence of Bayesian inference, where we decide upon a prior distribution, collect data and calculate a marginal likelihood, and use these to calculate a posterior distribution. From this posterior distribution, we can estimate the mean and the 95% credible interval. For any specific hypothesis, we can calculate the relative evidence for a posterior model, compared to a prior model, through the Bayes Factor. There are many different flavors of Bayesian statistics, and the disagreements between Bayesians about what the best approach to statistical inferences is, is at least as great as the disagreements between frequentists and Bayesians, and many Bayesians dislike Bayes factors <span class="citation">(<a href="references.html#ref-mcelreath_statistical_2016" role="doc-biblioref">McElreath, 2016</a>)</span>. For example, some Bayesians dislike subjective priors as used in <strong>subjective Bayesian analysis</strong>, and instead prefer what is known as <strong>objective Bayesian analysis</strong> <span class="citation">(<a href="references.html#ref-berger_interplay_2004" role="doc-biblioref">Berger &amp; Bayarri, 2004</a>)</span>. In your research, you will most likely need other calculations than the binomial example we have used here, and a lot of Bayesian tests are now available in the free open source software package <a href="https://jasp-stats.org/">JASP</a>. The math and the priors become more complex, but the basic idea remains the same. You can use Bayesian statistics to quantify relative evidence, which can inform you how much we should believe, or update our beliefs, in theories.</p>
</div>
<div id="test-yourself-3" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Test Yourself<a class="anchor" aria-label="anchor" href="#test-yourself-3"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Q1</strong>: The true believer had a prior of Beta(1,0.5). After observing 10 heads out of 20 coin flips, what is the posterior distribution, given that α* = α + x and β* = β + n – x?</p>
<ol style="list-style-type: upper-alpha">
<li>Beta(10, 10)</li>
<li>Beta(11, 10.5)</li>
<li>Beta(10, 20)</li>
<li>Beta(11, 20.5)</li>
</ol>
<p><strong>Q2</strong>: The strong skeptic had a prior of Beta(100,100). After observing 50 heads out of 100 coin flips, what is the posterior distribution, given that α* = α + x and β* = β + n – x?</p>
<ol style="list-style-type: upper-alpha">
<li>Beta(50, 50)</li>
<li>Beta(51, 51)</li>
<li>Beta(150, 150)</li>
<li>Beta(151, 151)</li>
</ol>
<p>Copy the R script below into R. This script requires 5 input parameters (identical to the Bayes Factor calculator website used above). These are the hypothesis you want to examine (e.g., when evaluating whether a coin is fair, <em>p</em> = 0.5), the total number of trials (e.g., 20 flips), the number of successes (e.g., 10 heads), and the <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> values for the Beta distribution for the prior (e.g., <span class="math inline">\(\alpha\)</span> = 1 and <span class="math inline">\(\beta\)</span> = 1 for a uniform prior). Run the script. It will calculate the Bayes Factor, and plot the prior (grey), likelihood (dashed blue) and posterior (black).</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">H0</span> <span class="op">&lt;-</span> <span class="fl">0.5</span> <span class="co"># Set the point null hypothesis you want to calculate the Bayes Factor for</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">20</span> <span class="co"># set total trials</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">10</span> <span class="co"># set successes</span>
<span class="va">aprior</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="co"># Set the alpha for the Beta distribution for the prior</span>
<span class="va">bprior</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="co"># Set the beta for the Beta distribution for the prior</span>

<span class="va">alikelihood</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">+</span> <span class="fl">1</span> <span class="co"># Calculate the alpha for the Beta distribution for the likelihood</span>
<span class="va">blikelihood</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">-</span> <span class="va">x</span> <span class="op">+</span> <span class="fl">1</span> <span class="co"># Calculate the beta for the Beta distribution for the likelihood</span>
<span class="va">aposterior</span> <span class="op">&lt;-</span> <span class="va">aprior</span> <span class="op">+</span> <span class="va">alikelihood</span> <span class="op">-</span> <span class="fl">1</span> <span class="co"># Calculate the alpha for the Beta distribution for the posterior</span>
<span class="va">bposterior</span> <span class="op">&lt;-</span> <span class="va">bprior</span> <span class="op">+</span> <span class="va">blikelihood</span> <span class="op">-</span> <span class="fl">1</span> <span class="co"># Calculate the beta for the Beta distribution for the posterior</span>

<span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0.001</span><span class="op">)</span> <span class="co">#create probability range p from 0 to 1</span>
<span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">aprior</span>, <span class="va">bprior</span><span class="op">)</span>
<span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">alikelihood</span>, <span class="va">blikelihood</span><span class="op">)</span>
<span class="va">posterior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">aposterior</span>, <span class="va">bposterior</span><span class="op">)</span>

<span class="co"># Create plot</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">posterior</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">15</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"l"</span>, lwd <span class="op">=</span> <span class="fl">3</span>, xlab <span class="op">=</span> <span class="st">"p"</span>, ylab <span class="op">=</span> <span class="st">"Density"</span>, las <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">prior</span>, col <span class="op">=</span> <span class="st">"grey"</span>, lwd <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">likelihood</span>, lty <span class="op">=</span> <span class="fl">2</span>, lwd <span class="op">=</span> <span class="fl">3</span>, col <span class="op">=</span> <span class="st">"dodgerblue"</span><span class="op">)</span>
<span class="va">BF10</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">H0</span>, <span class="va">aposterior</span>, <span class="va">bposterior</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">H0</span>, <span class="va">aprior</span>, <span class="va">bprior</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">H0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">H0</span>, <span class="va">aposterior</span>, <span class="va">bposterior</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">19</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">H0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">H0</span>, <span class="va">aprior</span>, <span class="va">bprior</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"grey"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/segments.html">segments</a></span><span class="op">(</span><span class="va">H0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">H0</span>, <span class="va">aposterior</span>, <span class="va">bposterior</span><span class="op">)</span>, <span class="va">H0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">H0</span>, <span class="va">aprior</span>, <span class="va">bprior</span><span class="op">)</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Bayes Factor:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">BF10</span>, digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="04-bayes_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>We see that for the newborn, <em>p</em> = 0.5 has become more probable, but so has <em>p</em> = 0.4.</p>
<p><strong>Q3</strong>: Change the hypothesis in the first line from 0.5 to 0.675, and run the script. If you were testing the idea that this coin returns 67.5% heads, which statement is true?</p>
<ol style="list-style-type: upper-alpha">
<li>Your belief in this hypothesis, given the data, would have decreased.</li>
<li>Your belief in this hypothesis, given the data, would have stayed the same.</li>
<li>Your belief in this hypothesis, given the data, would have increased.</li>
</ol>
<p><strong>Q4</strong>: Change the hypothesis in the first line back to 0.5. Let’s look at the increase in the belief of the hypothesis <em>p</em> = 0.5 for the strong skeptic after 10 heads out of 20 coin flips. Change the <span class="math inline">\(\alpha\)</span> for the prior in line 4 to 100 and the <span class="math inline">\(\beta\)</span> for the prior in line 5 to 100. Run the script. Compare the Figure from R to the increase in belief for the newborn (in the plot on the previous page). Which statement is true?</p>
<ol style="list-style-type: upper-alpha">
<li>The belief in the hypothesis that <em>p</em> = 0.5, given the data, has <strong>increased</strong> for the strong skeptic, but <strong>not</strong> as much as it has for the newborn.</li>
<li>The belief in the hypothesis that <em>p</em> = 0.5, given the data, has <strong>increased</strong> for the strong skeptic, <strong>exactly as much</strong> as it has for the newborn.</li>
<li>The belief in the hypothesis that <em>p</em> = 0.5, given the data, has <strong>increased</strong> for the strong skeptic, and <strong>much more</strong> than it has for the newborn.</li>
<li>The belief in the hypothesis that <em>p</em> = 0.5, given the data, has <strong>decreased</strong> for the strong skeptic.</li>
</ol>
<p>Copy the R script below and run it. The script will plot the mean for the posterior when 10 heads out of 20 coin flips are observed, given a uniform prior (as in <a href="bayes.html#fig:bayes8">4.6</a>) . The script will also use the ‘binom’ package to calculate the posterior mean, credible interval, and <strong>highest density interval (HDI)</strong>. The highest density interval is an alternative to the credible interval that works better when the posterior beta distribution is skewed (and is identical when the posterior distribution is symmetrical. We won’t go into the calculations of the HDI here.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">20</span> <span class="co"># set total trials</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">10</span> <span class="co"># set successes</span>
<span class="va">aprior</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="co"># Set the alpha for the Beta distribution for the prior</span>
<span class="va">bprior</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="co"># Set the beta for the Beta distribution for the prior</span>

<span class="va">ymax</span> <span class="op">&lt;-</span> <span class="fl">10</span> <span class="co"># set max y-axis</span>

<span class="va">alikelihood</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">+</span> <span class="fl">1</span> <span class="co"># Calculate the alpha for the Beta distribution for the likelihood</span>
<span class="va">blikelihood</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">-</span> <span class="va">x</span> <span class="op">+</span> <span class="fl">1</span> <span class="co"># Calculate the beta for the Beta distribution for the likelihood</span>
<span class="va">aposterior</span> <span class="op">&lt;-</span> <span class="va">aprior</span> <span class="op">+</span> <span class="va">alikelihood</span> <span class="op">-</span> <span class="fl">1</span> <span class="co"># Calculate the alpha for the Beta distribution for the posterior</span>
<span class="va">bposterior</span> <span class="op">&lt;-</span> <span class="va">bprior</span> <span class="op">+</span> <span class="va">blikelihood</span> <span class="op">-</span> <span class="fl">1</span> <span class="co"># Calculate the beta for the Beta distribution for the posterior</span>

<span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0.001</span><span class="op">)</span> <span class="co"># create probability range p from 0 to 1</span>
<span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">aprior</span>, <span class="va">bprior</span><span class="op">)</span> <span class="co"># deterine prior distribution</span>
<span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">alikelihood</span>, <span class="va">blikelihood</span><span class="op">)</span> <span class="co"># determine likelihood distribution</span>
<span class="va">posterior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">aposterior</span>, <span class="va">bposterior</span><span class="op">)</span> <span class="co"># determine posterior distribution</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">posterior</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">ymax</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"l"</span>, lwd <span class="op">=</span> <span class="fl">3</span>, xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/bquote.html">bquote</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span>, ylab <span class="op">=</span> <span class="st">"Density"</span>, las <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="co"># draw posterior distribution</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">prior</span>, col <span class="op">=</span> <span class="st">"grey"</span>, lwd <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># draw prior distribution</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">likelihood</span>, lty <span class="op">=</span> <span class="fl">2</span>, lwd <span class="op">=</span> <span class="fl">3</span>, col <span class="op">=</span> <span class="st">"dodgerblue"</span><span class="op">)</span> <span class="co"># draw likelihood distribution</span>
<span class="va">LL</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">qbeta</a></span><span class="op">(</span><span class="fl">.025</span>, <span class="va">aposterior</span>, <span class="va">bposterior</span><span class="op">)</span> <span class="co"># calculate lower limit credible interval</span>
<span class="va">UL</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">qbeta</a></span><span class="op">(</span><span class="fl">.975</span>, <span class="va">aposterior</span>, <span class="va">bposterior</span><span class="op">)</span> <span class="co"># calculate upper limit credible interval</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="va">aposterior</span> <span class="op">/</span> <span class="op">(</span><span class="va">aposterior</span> <span class="op">+</span> <span class="va">bposterior</span><span class="op">)</span><span class="op">)</span> <span class="co"># draw line mean</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="va">LL</span>, col <span class="op">=</span> <span class="st">"grey"</span>, lty <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># draw line lower limit</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="va">UL</span>, col <span class="op">=</span> <span class="st">"grey"</span>, lty <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="co"># draw line upper limit</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/polygon.html">polygon</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="va">theta</span> <span class="op">&lt;</span> <span class="va">LL</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/rev.html">rev</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="va">theta</span> <span class="op">&lt;</span> <span class="va">LL</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">posterior</span><span class="op">[</span><span class="va">theta</span> <span class="op">&lt;</span> <span class="va">LL</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">theta</span> <span class="op">&lt;</span> <span class="va">LL</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"lightgrey"</span>, border <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/polygon.html">polygon</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="va">theta</span> <span class="op">&gt;</span> <span class="va">UL</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/rev.html">rev</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="va">theta</span> <span class="op">&gt;</span> <span class="va">UL</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">posterior</span><span class="op">[</span><span class="va">theta</span> <span class="op">&gt;</span> <span class="va">UL</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">theta</span> <span class="op">&gt;</span> <span class="va">UL</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"lightgrey"</span>, border <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Mean posterior:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="op">(</span><span class="va">aposterior</span> <span class="op">/</span> <span class="op">(</span><span class="va">aposterior</span> <span class="op">+</span> <span class="va">bposterior</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>, <span class="st">", 95% Credible Interval:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">LL</span>, digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>, <span class="st">";"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">UL</span>, digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="04-bayes_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="va">binom</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
  <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"binom"</span><span class="op">)</span>
<span class="op">}</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">binom</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/pkg/binom/man/binom.bayes.html">binom.bayes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, type <span class="op">=</span> <span class="st">"central"</span>, prior.shape1 <span class="op">=</span> <span class="va">aprior</span>, prior.shape2 <span class="op">=</span> <span class="va">bprior</span><span class="op">)</span></code></pre></div>
<pre><code>##   method  x  n shape1 shape2 mean     lower     upper  sig
## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05</code></pre>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/binom/man/binom.bayes.html">binom.bayes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, type <span class="op">=</span> <span class="st">"highest"</span>, prior.shape1 <span class="op">=</span> <span class="va">aprior</span>, prior.shape2 <span class="op">=</span> <span class="va">bprior</span><span class="op">)</span></code></pre></div>
<pre><code>##   method  x  n shape1 shape2 mean     lower     upper  sig
## 1  bayes 10 20     11     11  0.5 0.2978068 0.7021932 0.05</code></pre>
<p>The posterior mean is identical to the Frequentist mean, but this is only the case when the mean of the prior equals the mean of the likelihood.</p>
<p><strong>Q5</strong>: Assume the outcome of 20 coin flips had been 18 heads. Change x to 18 in line 2 and run the script. Remember that the mean of the prior Beta(1,1) distribution is α/(α+β), or 1/(1+1) = 0.5. The Frequentist mean is simply x/n, or 18/20=0.9. Which statement is true?</p>
<ol style="list-style-type: upper-alpha">
<li>The frequentist mean is <strong>higher</strong> than the mean of the posterior, because the mean of the posterior is <strong>closer</strong> to the mean of the prior distribution.</li>
<li>The frequentist mean is <strong>lower</strong> than the mean of the posterior, because the mean of the posterior is <strong>closer</strong> to the mean of the prior distribution.</li>
<li>The frequentist mean is <strong>higher</strong> than the mean of the posterior, because the mean of the posterior is <strong>further from</strong> to the mean of the prior distribution.</li>
<li>The frequentist mean is <strong>lower</strong> than the mean of the posterior, because the mean of the posterior is <strong>further from</strong> to the mean of the prior distribution.</li>
</ol>
<p><strong>Q6</strong>: What is, today, your best estimate of the probability that the sun rises every day? Assume you were born with an uniform Beta(1,1) prior. The sun can either rise, or it does not. Assume you have seen the sun every day since you were born, which means there has been a continuous string of successes for every day you have been alive. It is ok to estimate the days you have been alive by just multiplying your age by 365 days. What is your best estimate of the probability that the sun will rise?</p>
<p><strong>Q7</strong>: What would have been the best estimate from a Frequentist perspective?</p>
<p><strong>Q8</strong>: What do you think the goal of science is? Rozeboom <span class="citation">(<a href="references.html#ref-rozeboom_fallacy_1960" role="doc-biblioref">1960</a>)</span> has criticized Neyman-Pearson hypothesis testing by stating:</p>
<blockquote>
<p>But the primary aim of a scientific experiment is not to precipitate decisions, but to make an appropriate adjustment in the degree to which one accepts, or believes, the hypothesis or hypotheses being tested".</p>
</blockquote>
<p>Frick <span class="citation">(<a href="references.html#ref-frick_appropriate_1996" role="doc-biblioref">1996</a>)</span> has argued against Rozeboom, by stating:</p>
<blockquote>
<p>Rozeboom (1960) suggested that scientists should not be making decisions about claims, they should be calculating and updating the probability of these claims. However, this does not seem practical. If there were only a handful of potential claims in any given area of psychology, it would be feasible to assign them probabilities, to be constantly updating the probabilities, and to expect experimenters to keep track of these ever-changing probabilities. In fact, just the number of claims in psychology is overwhelming. It would probably be impossible for human beings to keep track of the probability for each claim, especially if these probabilities were constantly changing. In any case, scientists do not assign probabilities to claims. Instead, scientists act like the goal of science is to collect a corpus of claims that are considered to be established (Giere, 1972).</p>
</blockquote>
<p>When it comes to philosophy of science, there are no right or wrong answers. Reflect in 250 words on your thoughts about the two goals of science outlines by Rozeboom and Frick, and how these relate to your philosophy of science.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="likelihoods.html"><span class="header-section-number">3</span> Likelihoods</a></div>
<div class="next"><a href="questions.html"><span class="header-section-number">5</span> Asking Statistical Questions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayes"><span class="header-section-number">4</span> Bayesian statistics</a></li>
<li><a class="nav-link" href="#bayes-factors"><span class="header-section-number">4.1</span> Bayes factors</a></li>
<li><a class="nav-link" href="#updating-our-belief"><span class="header-section-number">4.2</span> Updating our belief</a></li>
<li><a class="nav-link" href="#bayesest"><span class="header-section-number">4.3</span> Bayesian Estimation</a></li>
<li><a class="nav-link" href="#test-yourself-3"><span class="header-section-number">4.4</span> Test Yourself</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/Lakens/statistical_inferences/blob/master/04-bayes.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/Lakens/statistical_inferences/edit/master/04-bayes.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniel Lakens. It was last built on 2022-02-24.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
