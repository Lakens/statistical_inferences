<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="description" content="This open educational resource contains information to improve statistical inferences, design better experiments, and report scientific research more transparently.">
<title>Improving Your Statistical Inferences - 17&nbsp; Replication Studies</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./16-confirmationbias.html" rel="prev">
<link href="./images/logos/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0MK2WTGRM3"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-0MK2WTGRM3', { 'anonymize_ip': true});
</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="include/booktem.css">
<link rel="stylesheet" href="include/style.css">
<link rel="stylesheet" href="include/webex.css">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17-replication.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Replication Studies</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Improving Your Statistical Inferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/Lakens/statistical_inferences" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Improving-Your-Statistical-Inferences.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Improving-Your-Statistical-Inferences.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <a href="https://twitter.com/intent/tweet?url=%7Curl%7C" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-pvalue.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Using <em>p</em>-values to test a hypothesis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-errorcontrol.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Error control</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-likelihoods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Likelihoods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesian statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-questions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Asking Statistical Questions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-effectsize.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Effect Sizes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-CI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Confidence Intervals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-samplesizejustification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sample Size Justification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-equivalencetest.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Equivalence Testing and Interval Hypotheses</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-sequential.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequential Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-meta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Meta-analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bias detection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-prereg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Preregistration and Transparency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-computationalreproducibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Computational Reproducibility</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-researchintegrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Research Integrity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-confirmationbias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Confirmation Bias and Organized Skepticism</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-replication.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Replication Studies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Change Log</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#why-replication-studies-are-important" id="toc-why-replication-studies-are-important" class="nav-link active" data-scroll-target="#why-replication-studies-are-important"><span class="header-section-number">17.1</span> Why replication studies are important</a></li>
  <li><a href="#direct-versus-conceptual-replications" id="toc-direct-versus-conceptual-replications" class="nav-link" data-scroll-target="#direct-versus-conceptual-replications"><span class="header-section-number">17.2</span> Direct versus conceptual replications</a></li>
  <li><a href="#analyzing-replication-studies." id="toc-analyzing-replication-studies." class="nav-link" data-scroll-target="#analyzing-replication-studies."><span class="header-section-number">17.3</span> Analyzing Replication Studies.</a></li>
  <li><a href="#replication-studies-or-lower-alpha-levels" id="toc-replication-studies-or-lower-alpha-levels" class="nav-link" data-scroll-target="#replication-studies-or-lower-alpha-levels"><span class="header-section-number">17.4</span> Replication studies or lower alpha levels?</a></li>
  <li><a href="#when-replication-studies-yield-conflicting-results" id="toc-when-replication-studies-yield-conflicting-results" class="nav-link" data-scroll-target="#when-replication-studies-yield-conflicting-results"><span class="header-section-number">17.5</span> When replication studies yield conflicting results</a></li>
  <li><a href="#why-are-replication-studies-so-rare" id="toc-why-are-replication-studies-so-rare" class="nav-link" data-scroll-target="#why-are-replication-studies-so-rare"><span class="header-section-number">17.6</span> Why are replication studies so rare?</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/Lakens/statistical_inferences/edit/master/17-replication.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/Lakens/statistical_inferences/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/Lakens/statistical_inferences/blob/master/17-replication.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-replication" class="quarto-section-identifier"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Replication Studies</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>In 2015 a team of 270 authors published the results of a research project where they replicated 100 studies <span class="citation" data-cites="opensciencecollaboration_estimating_2015">(<a href="references.html#ref-opensciencecollaboration_estimating_2015" role="doc-biblioref">Open Science Collaboration, 2015</a>)</span>. The original studies had all been published in three psychology journals in the year 2008. The authors of the replication project selected the last study of papers that could feasibly be replicated, performed a study with high power to detect the observed effect size, and attempted to design the best possible replication study. They stayed close to the original study where possible, but deviated where this was deemed necessary. For the original studies published in 2008 97 of the 100 studies were interpreted as significant. Given an estimated 92% power for the effect sizes observed in the original studies 97 <span class="math inline">\(\times\)</span> 0.92 = 89 of the replication studies could be expected to observe a significant effect, if the effects in the original studies were at least as large as reported. Yet, only 35 out of the 97 original studies that were significant replicated, for a replication rate of 36%. This result was a surprise for most researchers, and led to the realization that it is much more difficult to replicate findings than one might intuitively think. This result solidified the idea of a <em>replication crisis</em>, a sudden loss of confidence in the reliability of published results, which led to confusion and uncertainty about how scientists worked. Since 2015 the field of <strong>metascience</strong> has emerged to use empirical methods to study science itself and identify some of the causes of low replicability rates, and develop possible solutions to increase it.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-rpp" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-rpp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/rpp.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rpp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.1: Results of the Reproducibility Project:Psychology, with replications in green, and non-replications (based on <em>p</em> &gt; .05) in red.
</figcaption></figure>
</div>
</div>
</div>
<p>At the same time, the authors of the replication project acknowledged that a single replication of 100 studies is just the starting point of trying to understand why it was so difficult to replicate findings in the literature. They wrote in their conclusion:</p>
<blockquote class="blockquote">
<p>After this intensive effort to reproduce a sample of published psychological findings, how many of the effects have we established are true? Zero. And how many of the effects have we established are false? Zero. Is this a limitation of the project design? No.&nbsp;It is the reality of doing science, even if it is not appreciated in daily practice. Humans desire certainty, and science infrequently provides it. As much as we might wish it to be otherwise, a single study almost never provides definitive resolution for or against an effect and its explanation. The original studies examined here offered tentative evidence; the replications we conducted offered additional, confirmatory evidence. In some cases, the replications increase confidence in the reliability of the original results; in other cases, the replications suggest that more investigation is needed to establish the validity of the original findings. Scientific progress is a cumulative process of uncertainty reduction that can only succeed if science itself remains the greatest skeptic of its explanatory claims.</p>
</blockquote>
<p>A replication study is an experiment where the methods and procedures in a previous study are repeated by collecting new data. Typically, the term <strong>direct replication study</strong> is used when the methods and measures are as similar to the earlier study as possible. In a <strong>conceptual replication study</strong> a researcher intentionally introduces differences with the original study with the aim to test the generalization of the effect, either because they aim to systematically explore the impact of this change, or because they are not able to use the same methods and procedures. It is important to distinguish replication, where new data is collected, from <strong>reproducibility</strong>, where the same data is used to reproduce the reported results. In reproducibility checks the goal is to examine the presence of errors in the analysis files. Confusingly, the large-scale collaborative research project where 100 studies in psychology were replicated, and which is often considered an important contributor to the <strong>replication crisis</strong>, was called the Reproducibility Project: Psychology <span class="citation" data-cites="opensciencecollaboration_estimating_2015">(<a href="references.html#ref-opensciencecollaboration_estimating_2015" role="doc-biblioref">Open Science Collaboration, 2015</a>)</span>. It should have been called the Replication Project: Psychology. Our apologies.</p>
<p>The goal of direct replication studies is first of all to identify Type 1 or Type 2 errors in the literature. In any original study with an alpha of 5% and a statistical power of 80% there is a probability of making an erroneous claim. Direct replication studies (especially those with low error rates) have the goal to identify these errors in the scientific literature, which is an important part of having a reliable knowledge base. It is especially important given that the scientific literature is biased, which increases the probability that a published claim is a Type 1 error. Schmidt <span class="citation" data-cites="schmidt_shall_2009">(<a href="references.html#ref-schmidt_shall_2009" role="doc-biblioref">2009</a>)</span> also notes that claims can be erroneous because they were based on fraudulent data. Although direct replication studies can not identify fraud, they can point out erroneous claims due to fraud.</p>
<p>The second important goal of direct replications is to identify factors in the study that were deemed irrelevant, but were crucial in generating the observed data <span class="citation" data-cites="tunc_falsificationist_2023">(<a href="references.html#ref-tunc_falsificationist_2023" role="doc-biblioref">Tunç &amp; Tunç, 2023</a>)</span>. For example, researchers might discover that an original and replication study yielded different results because the experimenter in one study treated the participants in a much more friendly manner than the experimenter in the other study. Such details are typically not reported in the method section, as they are deemed irrelevant, but direct replication studies might identify such factors. This highlights how replication studies are never identical in the social sciences. They are designed to be similar in all ways that a researcher thinks will matter. But not all factors deemed irrelevant will by irrelevant (and vice versa).</p>
<p>The goal of conceptual replication studies is to examine the generalizability of effects <span class="citation" data-cites="sidman_tactics_1960">(<a href="references.html#ref-sidman_tactics_1960" role="doc-biblioref">Sidman, 1960</a>)</span>. Researchers intentionally vary factors in the experiment to examine if this leads to variability in the results. Sometimes this variability is theoretically predicted, and sometimes researchers simply want to see what will happen. I would define a direct replication as a study where a researcher has the goal to not introduce variability in the effect size compared to the original study, while in a conceptual replication variability is introduced intentionally with the goal to test the generalizability of the effect. This distinction means that it is possible that what one researcher aims to be a direct replication is seen by a peer as a conceptual replication. For example, if a researcher sees no reason why an effect tested in Germany would not be identical in The Netherlands, they will consider it a direct replication. A peer might believe there are theoretically relevant differences between the two countries that make this a conceptual replication, as they would interpret the study as intentionally introducing variability by not keeping an important factor constant. The only way to clarify which factors are deemed theoretically relevant is to write a <strong>constraints on generalizability</strong> statement in the discussion where researchers specify which contexts they theoretically expect the effect to replicate in, and where variability in the results would not be considered problematic for their original claim <span class="citation" data-cites="simons_constraints_2017">(<a href="references.html#ref-simons_constraints_2017" role="doc-biblioref">Simons et al., 2017</a>)</span>.</p>
<p>It should be clear that the goals of replication studies are rather modest. At the same time, they are essential for a well-functioning empirical science. They provide a tool to identify false positive or false negative results, and can reveal variability across contexts that might falsify theoretical predictions, or lead to the generation of new theories. Being able to systematically replicate and extend a basic effect is one of the most important ways in which scientists develop and test theories. Although there are other approaches to generating reliable knowledge, such as <strong>triangulation</strong> where the same theory is tested in different but complementary ways, in practice the vast majority of scientifically established claims are based on replicable effects.</p>
<p>Not all researchers agree that their science has inter-subjectively repeatable observations. In what is called the ‘crisis in social psychology’ Gergen <span class="citation" data-cites="gergen_social_1973">(<a href="references.html#ref-gergen_social_1973" role="doc-biblioref">1973</a>)</span> argued social psychology was not a cumulative science:</p>
<blockquote class="blockquote">
<p>It is the purpose of this paper to argue that social psychology is primarily an historical inquiry. Unlike the natural sciences, it deals with facts that are largely nonrepeatable and which fluctuate markedly over time. Principles of human interaction cannot readily be developed over time because the facts on which they are based do not generally remain stable. Knowledge cannot accumulate in the usual scientific sense because such knowledge does not generally transcend its historical boundaries.</p>
</blockquote>
<p>The belief that basic claims in psychology are not repeatable events lead to <strong>social constructivism</strong>. This approach did not become particularly popular, but it is useful to know of its existence. It is a fact that human behavior can change over time. It is also true that many psychological mechanisms have enabled accurate predictions for more than a century, and this is unlikely to change. Still, some researchers might believe they are studying unrepeatable events, and if so, they can state why they believe this to be the case. These researchers give up the aim to build theories upon which generalizable predictions can be made, and they will have to make a different argument for the value of their research. Luckily, one does not need to be a social constructivist to acknowledge that the world changes. We should not expect all direct replications to yield the same result. For example, in the classic ‘foot-in-the-door’ effect study, Freedman and Fraser <span class="citation" data-cites="freedman_compliance_1966">(<a href="references.html#ref-freedman_compliance_1966" role="doc-biblioref">1966</a>)</span> first called residents in a local community over the phone to ask them to answer some questions (the small request). If participants agreed, they were asked a larger request, which consisted of “five or six men from our staff coming into your home some morning for about 2 hours to enumerate and classify all the household products that you have. They will have to have full freedom in your house to go through the cupboards and storage places.” The idea that anyone would nowadays agree to such a request when called by a stranger over the telephone seems highly improbable. Repeating this procedure will not lead to more than 50% of respondents agreeing to this request. But the theories we build should be able to account for why some findings no longer replicate by specifying necessary conditions for the effect to be observed. This is the role of theory, and if researchers have good theories, they should be able to continue to make predictions, even if some aspects of the world change. As De Groot <span class="citation" data-cites="degroot_methodology_1969">(<a href="references.html#ref-degroot_methodology_1969" role="doc-biblioref">1969, p. 89</a>)</span> writes: “If one knows something to be true, he is in a position to predict; where prediction is impossible there is no knowledge”.</p>
<section id="why-replication-studies-are-important" class="level2" data-number="17.1"><h2 data-number="17.1" class="anchored" data-anchor-id="why-replication-studies-are-important">
<span class="header-section-number">17.1</span> Why replication studies are important</h2>
<p>Over the last half century, researchers have repeatedly observed that replication studies were rarely performed or published. In an editorial in the Journal of Personality and Social Psychology, Greenwald <span class="citation" data-cites="greenwald_editorial_1976">(<a href="references.html#ref-greenwald_editorial_1976" role="doc-biblioref">Greenwald, 1976</a>)</span> writes: “There may be a crisis in personality and social psychology, associated with the difficulty often experienced by researchers in attempting to replicate published work. A precise statement of the magnitude of this problem cannot be made, since most failures to replicate do not receive public report”. A similar concern about the replicability of findings is expressed by Epstein <span class="citation" data-cites="epstein_stability_1980">(<a href="references.html#ref-epstein_stability_1980" role="doc-biblioref">Epstein, 1980, p. 790</a>)</span>: “Not only are experimental findings often difficult to replicate when there are the slightest alterations in conditions, but even attempts at exact replication frequently fail.” Neher <span class="citation" data-cites="neher_probability_1967">(<a href="references.html#ref-neher_probability_1967" role="doc-biblioref">1967, p. 262</a>)</span> concludes: “The general adoption of independent replication as a requirement for acceptance of findings in the behavioral sciences will require the efforts of investigators, readers, and publishing editors alike. It seems clear that such a policy is both long overdue and crucial to the development of a sound body of knowledge concerning human behavior.” Lubin <span class="citation" data-cites="lubin_replicability_1957">(<a href="references.html#ref-lubin_replicability_1957" role="doc-biblioref">Lubin, 1957</a>)</span> suggests that, where relevant, manuscripts that demonstrate the replicability of findings should receive a higher publication priority. N. C. Smith <span class="citation" data-cites="smith_replication_1970">(<a href="references.html#ref-smith_replication_1970" role="doc-biblioref">1970, p. 974</a>)</span> notes how replication studies are neglected: “The review of the literature on replication and cross-validation research has revealed that psychologists in both research”disciplines” have tended to ignore replication research. Thus, one cannot help but wonder what the impact might be if every investigator repeated the study which he believed to be his most significant contribution to the field.” One problem in the past was the difficulty of describing the methods and analyses in sufficient detail to allow others to repeat the study as closely as possible <span class="citation" data-cites="mack_need_1951">(<a href="references.html#ref-mack_need_1951" role="doc-biblioref">Mack, 1951</a>)</span>. For example, Pereboom <span class="citation" data-cites="pereboom_fundamental_1971">(<a href="references.html#ref-pereboom_fundamental_1971" role="doc-biblioref">1971, p. 442</a>)</span> writes: “Related to the above is the common difficulty of communicating all important details of a psychological experiment to one’s audience. […] Investigators attempting to replicate the work of others are painfully aware of these informational gaps.” Open science practices, such as sharing <a href="14-computationalreproducibility.html">computationally reproducible</a> code and materials, are an important way to solve this problem, and a lot of progress has been made over the last decade to address these problems.</p>
<p>Many researchers have suggested that performing replication studies should be common practice. Lykken <span class="citation" data-cites="lykken_statistical_1968">(<a href="references.html#ref-lykken_statistical_1968" role="doc-biblioref">1968, p. 159</a>)</span> writes: “Ideally, all experiments would be replicated before publication but this goal is impractical”. Loevinger <span class="citation" data-cites="loevinger_information_1968">(<a href="references.html#ref-loevinger_information_1968" role="doc-biblioref">1968, p. 455</a>)</span> makes a similar point: “Most studies should be replicated prior to publication. This recommendation is particularly pertinent in cases where the results are in the predicted direction, but not significant, or barely so, or only by one-tailed tests”. Samelson <span class="citation" data-cites="samelson_watson_1980">(<a href="references.html#ref-samelson_watson_1980" role="doc-biblioref">1980, p. 623</a>)</span> notes in the specific context of Watson’s ‘Little Albert’ study: “Beyond this apparent failure of internal criticism of the data is another one that is even less debatable: the clear neglect of a cardinal rule of scientific method, that is, replication”.</p>
<p>The idea that replication is a ‘cardinal rule’ or ‘cornerstone’ of the scientific method follows directly from a methodological falsificationist philosophy of science. Popper <span class="citation" data-cites="popper_logic_2002">(<a href="references.html#ref-popper_logic_2002" role="doc-biblioref">2002</a>)</span> discusses how we increase our confidence in theories that make predictions that withstand attempts to falsify the theory. To be able to falsify predictions, predictions need to rule out certain observable data patterns. For example, if our theory predicts that people will be faster at naming the colour of words when their meaning matches the colour (e.g., “blue” written in blue instead of “blue” written in red), the observation that people are not faster (or even slower) would falsify our prediction. A problem is that given variability in observed data any possible data pattern will occur, exactly as often as dictated by chance. This means that in the long run, just based on chance, a study will show people are <em>slower</em> at naming the colour of words when their meaning matches the colour. This fluke will be observed by chance, even though our theory is correct. Popper realized this was a problem for his account of falsification, because it means that “probability statements will not be falsifiable”. After all, if all possible data patterns have a non-zero probability, even if they are extremely rare, they are not logically ruled out. This would make falsification impossible if we demanded that science works according to perfectly formal logical rules.</p>
<p>The solution is to admit that science does not work following perfectly formal rules. And yet, as Popper acknowledges, science still works. Therefore, instead of abandoning the idea of falsification, Popper proposes a more pragmatic approach to falsification. He writes: “It is fairly clear that this ‘practical falsification’ can be obtained only through a methodological decision to regard highly improbable events as ruled out — as prohibited.” The logical follow-up question is then “Where are we to draw the line? Where does this ‘high improbability’ begin?” Popper argues that even if any low probability event <em>can</em> occur, <em>they are not reproducible at will</em>. Any single study can reveal any possible effect, but a prediction should be considered falsified if we fail to see “the predictable and reproducible occurrence of systematic deviations”. This is why replication is considered a ‘cardinal rule’ in methodological falsificationism: When observations are probabilistic, only the replicable occurrence of low probability events can be taken as the falsification of a prediction. A single <em>p</em> &lt; 0.05 is not considered sufficient; only if close replication studies repeatedly observe a low probability event does Popper allow us to ‘practically falsify’ probabilistic predictions.</p>
</section><section id="direct-versus-conceptual-replications" class="level2" data-number="17.2"><h2 data-number="17.2" class="anchored" data-anchor-id="direct-versus-conceptual-replications">
<span class="header-section-number">17.2</span> Direct versus conceptual replications</h2>
<p>As Schmidt <span class="citation" data-cites="schmidt_shall_2009">(<a href="references.html#ref-schmidt_shall_2009" role="doc-biblioref">2009</a>)</span> writes “There is no such thing as an exact replication.” However, it is possible to 1) repeat an experiment where a researcher stays as closely as possible to the original study, 2) repeat an experiment where there it is likely there is some variation in factors that are deemed irrelevant, and 3) knowingly vary aspects of a study design. Popper agrees: “We can never repeat an experiment precisely — all we can do is to keep certain conditions constant, within certain limits.” One of the first extensive treatments of replication comes from Sidman <span class="citation" data-cites="sidman_tactics_1960">(<a href="references.html#ref-sidman_tactics_1960" role="doc-biblioref">1960</a>)</span>. He distinguishes direct replications from <strong>systematic replications</strong> (which I refer to here as conceptual replications). Sidman writes:</p>
<blockquote class="blockquote">
<p>Where direct replication helps to establish generality of a phenomenon among the members of a species, systematic replication can accomplish this and, at the same time, extend its generality over a wide range of different situations.</p>
</blockquote>
<p>If the same result is observed when systematically varying auxiliary assumptions we build confidence in the general nature of the finding, and therefore, in the finding itself. The more robust an effect is to factors that are deemed irrelevant, the less likely it is that the effect is caused by a confound introduced by one of these factors. If a prediction is confirmed across time, locations, in different samples of participants, by different experimenters, and with different measures of the same variable, then the likelihood of confounds underlying all these interrelated effects decreases. If conceptual replications are successful, they yield more information than a direct replication, because a conceptual replication generalizes the finding beyond the original context. However, this benefit is only present if the conceptual replication is successful. Sidman warns:</p>
<blockquote class="blockquote">
<p>But this procedure is a gamble. If systematic replication fails, the original experiment will still have to be redone, else there is no way of determining whether the failure to replicate stemmed from the introduction of new variables in the second experiment, or whether the control of relevant factors was inadequate in the first one.</p>
</blockquote>
<p>Sometimes there is no need to choose between a direct replication and a conceptual replication, as both can be performed. Researchers can perform <strong>replication and extension studies</strong> where an original study is replicated, but additional conditions are added that test a novel hypotheses. Sidman <span class="citation" data-cites="sidman_tactics_1960">(<a href="references.html#ref-sidman_tactics_1960" role="doc-biblioref">1960</a>)</span> refers to this as the <strong>baseline technique</strong> where an original effect is always part of the experimental design, and variations are tested against the baseline effect. Replication and extension studies are one of the best ways to build cumulative knowledge and develop strong scientific theories <span class="citation" data-cites="bonett_replicationextension_2012">(<a href="references.html#ref-bonett_replicationextension_2012" role="doc-biblioref">Bonett, 2012</a>)</span>.</p>
<p>It is often difficult to reach agreement on whether a replication study is a direct replication or a conceptual replication, because researchers can disagree about whether changes are theoretically relevant or not. Some authors have chosen to resolve this difficulty by defining a replication study as “a study for which any outcome would be considered diagnostic evidence about a claim from prior research” <span class="citation" data-cites="nosek_what_2020">(<a href="references.html#ref-nosek_what_2020" role="doc-biblioref">Nosek &amp; Errington, 2020</a>)</span>. However, this definition is too broad to be useful in practice. It has limited use when theoretical predictions are vague (which regrettably holds for most research lines in psychology), and it does not sufficiently acknowledge that it is important to specify falsifiable auxiliary hypotheses. The specification of falsifiable auxiliary hypothesis in replication studies lies at the core of the <strong>Systematic Replications Framework</strong> <span class="citation" data-cites="tunc_falsificationist_2023">(<a href="references.html#ref-tunc_falsificationist_2023" role="doc-biblioref">Tunç &amp; Tunç, 2023</a>)</span>. The idea is that researchers should specify the auxiliary hypotheses that are assumed to be relevant. In principle the number of auxiliary hypotheses is infinite, but as Uygun-Tunç and Tunç <span class="citation" data-cites="tunc_falsificationist_2023">(<a href="references.html#ref-tunc_falsificationist_2023" role="doc-biblioref">2023</a>)</span> write: “it is usually assumed that the exact color of the lab walls, the elevation of the lab above the sea level, the exact design of the chairs used by the subjects, the humidity of the room that the study takes place or many other minute details do not significantly influence the study outcomes.” These factors are relegated to the <strong>ceteris paribus clause</strong>, meaning that any differences on these factors are not considered relevant, and for all purposes studies can be treated as ‘all equal’ - even if the color of the walls differs between an original and a replication study. No replication study is exactly the same as the original study <span class="citation" data-cites="schmidt_shall_2009">(<a href="references.html#ref-schmidt_shall_2009" role="doc-biblioref">Schmidt, 2009</a>)</span>, and some differences in auxiliary hypotheses are meaningfully different. The challenge is to identify which auxiliary hypotheses explain failures to replicate an original study.</p>
<p>A direct replication study that yields no statistically significant effect can have three interpretations <span class="citation" data-cites="schmidt_shall_2009 tunc_falsificationist_2023">(<a href="references.html#ref-schmidt_shall_2009" role="doc-biblioref">Schmidt, 2009</a>; <a href="references.html#ref-tunc_falsificationist_2023" role="doc-biblioref">Tunç &amp; Tunç, 2023</a>)</span>. First, it is possible that the replication study yielded a Type 2 error. Second, it is possible that the original study was a Type 1 error. Third, some of the auxiliary assumptions that have been relegated to the ceteris paribus clause actually matter more than researchers might have thought. To resolve disagreements between the results of original and replication studies researchers should perform a set of studies that systematically varies those auxiliary hypotheses that are most crucial for a theoretical viewpoint. Resolving inconsistencies in science is an effortful process that can be facilitated by engaging in an <strong>adversarial collaboration</strong>, where two teams join forces to resolve inconsistencies <span class="citation" data-cites="mellers_frequency_2001">(<a href="references.html#ref-mellers_frequency_2001" role="doc-biblioref">Mellers et al., 2001</a>)</span>. Opposing teams can point out the most crucial auxiliary hypotheses to test, and severely test different theories.</p>
</section><section id="analyzing-replication-studies." class="level2" data-number="17.3"><h2 data-number="17.3" class="anchored" data-anchor-id="analyzing-replication-studies.">
<span class="header-section-number">17.3</span> Analyzing Replication Studies.</h2>
<p>There are multiple ways to statistically analyze a replication study <span class="citation" data-cites="anderson_there_2016">(<a href="references.html#ref-anderson_there_2016" role="doc-biblioref">Anderson &amp; Maxwell, 2016</a>)</span>. The most straightforward statistical approach is also the least common: Testing whether the effect sizes in the original and replication studies are statistically different from each other. Two effect sizes from an independent <em>t</em>-test can be tested against each other using:</p>
<p><span class="math display">\[
Z_{Diff} = \frac{{{\delta}}_{1}{- {\delta}}_{2}}{\sqrt{{V}_{\delta_{1}} + {V}_{\delta_{2}}}}
\]</span></p>
<p>where the difference between the two Cohen’s <em>d</em> effect sizes is divided by the standard error of the difference score, based on the square root of the combined variances of the effect sizes <span class="citation" data-cites="borenstein_introduction_2009">(<a href="references.html#ref-borenstein_introduction_2009" role="doc-biblioref">Borenstein, 2009</a>, formula 19.6 and 19.7)</span>. This formula provides a hint why researchers rarely test for differences between effect sizes. The standard error of the difference score is based on the variance in the original and replication study. If the original study has a small sample size, the variance will be large, and a test of the difference between effect sizes can have very low power.</p>
<p>Let’s take a look at an original non-preregistered study which observed an effect size of <em>d</em> = 0.6 in an independent <em>t</em>-test with 30 participants in each group. A preregistered replication study is performed which observed an effect size of 0 with 150 participants in each group. Testing the two effect sizes against each other can be achieved in three ways that are in principle identical (although the exact implementation, such as computing Cohen’s <em>d</em> or Hedges’ <em>g</em>, might yield slightly different <em>p</em>-values). The first is to compute the corresponding <em>p</em>-value for the <em>Z</em>-test in the formula above. The other two ways are implemented in the <code>metafor</code> package and consist of a heterogeneity analysis and a moderator analysis. These two approaches are mathematically identical. In the heterogeneity analysis we test if the variability in effect sizes (even though there are only two) is greater than expected based only on random variation.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">escalc</span>(<span class="at">n1i =</span> <span class="dv">30</span>, </span>
<span id="cb1-2"><a href="#cb1-2"></a>             <span class="at">n2i =</span> <span class="dv">30</span>, </span>
<span id="cb1-3"><a href="#cb1-3"></a>             <span class="at">di =</span> <span class="fl">0.6</span>, </span>
<span id="cb1-4"><a href="#cb1-4"></a>             <span class="at">measure =</span> <span class="st">"SMD"</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a>d2 <span class="ot">&lt;-</span> <span class="fu">escalc</span>(<span class="at">n1i =</span> <span class="dv">100</span>, </span>
<span id="cb1-6"><a href="#cb1-6"></a>             <span class="at">n2i =</span> <span class="dv">100</span>, </span>
<span id="cb1-7"><a href="#cb1-7"></a>             <span class="at">di =</span> <span class="fl">0.0</span>, </span>
<span id="cb1-8"><a href="#cb1-8"></a>             <span class="at">measure =</span> <span class="st">"SMD"</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a>metadata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">yi =</span> <span class="fu">c</span>(d1<span class="sc">$</span>yi, d2<span class="sc">$</span>yi), </span>
<span id="cb1-10"><a href="#cb1-10"></a>                       <span class="at">vi =</span> <span class="fu">c</span>(d1<span class="sc">$</span>vi, d2<span class="sc">$</span>vi), </span>
<span id="cb1-11"><a href="#cb1-11"></a>                       <span class="at">study =</span> <span class="fu">c</span>(<span class="st">"original"</span>, <span class="st">"replication"</span>))</span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co"># Test based on heterogeneity analysis</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>res_h <span class="ot">&lt;-</span> <span class="fu">rma</span>(yi, </span>
<span id="cb1-15"><a href="#cb1-15"></a>             vi, </span>
<span id="cb1-16"><a href="#cb1-16"></a>             <span class="at">data =</span> metadata, </span>
<span id="cb1-17"><a href="#cb1-17"></a>             <span class="at">method =</span> <span class="st">"FE"</span>)</span>
<span id="cb1-18"><a href="#cb1-18"></a>res_h</span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="co"># The moderator test would be: rma(yi, vi, mods = ~study, method = "FE", data = metadata, digits = 3)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Fixed-Effects Model (k = 2)

I^2 (total heterogeneity / total variability):   74.45%
H^2 (total variability / sampling variability):  3.91

Test for Heterogeneity:
Q(df = 1) = 3.9146, p-val = 0.0479

Model Results:

estimate      se    zval    pval    ci.lb   ci.ub    
  0.1322  0.1246  1.0607  0.2888  -0.1121  0.3765    

---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The result yields <em>p</em>-value of 0.048, which is statistically significant, and therefore allows us to statistically conclude that the two effect sizes differ from each other. We can also perform this test as a moderator analysis.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>res_mod <span class="ot">&lt;-</span> <span class="fu">rma</span>(yi, </span>
<span id="cb3-2"><a href="#cb3-2"></a>               vi, </span>
<span id="cb3-3"><a href="#cb3-3"></a>               <span class="at">mods =</span> <span class="sc">~</span>study,</span>
<span id="cb3-4"><a href="#cb3-4"></a>               <span class="at">method =</span> <span class="st">"FE"</span>,</span>
<span id="cb3-5"><a href="#cb3-5"></a>               <span class="at">data =</span> metadata,</span>
<span id="cb3-6"><a href="#cb3-6"></a>               <span class="at">digits =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This result yields the same <em>p</em>-value of 0.048. This same test was recently repackaged by Spence and Stanley <span class="citation" data-cites="spence_tempered_2024">(<a href="references.html#ref-spence_tempered_2024" role="doc-biblioref">2024</a>)</span> as a prediction interval, but this approach is just a test of the difference between effect sizes.</p>
<p>It is worth pointing out that in the example above the difference between the effect sizes was large, the replication study had a much larger sample size than the original study, but the differences was only just statistically significant. As we see in <a href="#fig-rep-1" class="quarto-xref">Figure&nbsp;<span>17.2</span></a> the original study had large uncertainty, which as explained above is part of the variance of the estimate of the difference between effect sizes. If the replication study had yielded an effect size that was even slightly in the positive direction (which should happen 50% of the time if the true effect size is 0) the difference would no longer have been statistically significant. In general, power is low when original studies have small sample sizes. Despite this limitation, directly testing the difference between effect sizes is the most intuitive and coherent approach to claiming that a study failed to replicate.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-rep-1" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-rep-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-replication_files/figure-html/fig-rep-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rep-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.2: Forest plot for an original study (N = 60, d = 0.6) and a replication study (N = 200, d = 0).
</figcaption></figure>
</div>
</div>
</div>
<p>Although a statistical difference between effect sizes is one coherent approach to decide whether a study has been replicated, researchers are sometimes interested in a different question: Was there a significant result in the replication study? In this approach to analyzing replication studies there is no direct comparison with the effect observed in the original study. The question is therefore not so much ‘is the original effect replicated?’ but ‘if we repeat the original study is a statistically significant effect observed?’. In other words, we are not testing whether an effect has been replicated, but whether a predicted effect has been observed. Another way of saying this is that we are not asking whether the observed effect replicated, but whether the original ordinal claim of the presence of an non-zero effect is replicated. In the example in <a href="#fig-rep-1" class="quarto-xref">Figure&nbsp;<span>17.2</span></a> the replication study has an effect size of 0, so there is no statistically significant effect, and the original effect did not replicate (in the sense that repeated the procedure did not yield a significant result).</p>
<p>Let’s take a step back, and consider which statistical test best reflects the question ‘did this study replicate’. On the one hand it seems reasonable to consider an effect ‘not replicated’ if there is a statistically significant difference between the effect sizes. However, this can mean that a non-significant effect in the replication studies leads to a ‘replication’ just because the effect size estimate is not statistically smaller than the original effect size. On the other hand, it seems reasonable to consider an effect replicated if it is statistically significant in the replication study. These two approaches can lead to a conflict, however. Some statistically significant effects are statistically smaller than the original study. Should these be considered ‘replicated’ or not? We might want to combine both statistical tests, and consider a study a replication if the effect is both statistically different from 0 (i.e., <em>p</em> &lt; .05 in a traditional significance test), and the difference in effect sizes is not statistically different from 0 (i.e., <em>p</em> &gt; .05 for a test of heterogeneity in a meta-analysis of both effect sizes). We can perform both tests, and only consider a finding replicated if both conditions are met. Logically, a finding should then be considered as a non-replication if the opposite is true (i.e., <em>p</em> &gt; .05 for significance test of the replication study, and <em>p</em> &lt; .05 for the test of heterogeneity).</p>
<p>However, due to the low power of the test for a difference between the effect sizes in the original and replication study, it is not easy to meet the bar of an informative result. In practice, many replication studies that do not yield a statistically significant result will also not show a statistically significant difference as part of the test of heterogeneity. In a preprint (that we never bothered to resubmit after we received positive reviews) Richard Morey and I <span class="citation" data-cites="morey_why_2016">(<a href="references.html#ref-morey_why_2016" role="doc-biblioref">Richard D. Morey &amp; Lakens, 2016</a>)</span> summarized this problem in our title “Why most of psychology is statistically unfalsiﬁable”. If we want to have an informative test for replication following the criteria outlined above, most tests will be uninformative, and lead to inconclusive results.</p>
<p>This problem becomes even bigger if we admit that we often consider some effects too small to matter. <span class="citation" data-cites="anderson_there_2016">Anderson &amp; Maxwell (<a href="references.html#ref-anderson_there_2016" role="doc-biblioref">2016</a>)</span> mention an additional approach to analyzing replication studies where we do not just test the difference in effect sizes between an original and replication study, but combine this with an equivalence test to examine if the difference - if any - is too small to matter. With very large sample sizes tiny differences between studies can become statistically significant, which would result in the conclusion that a study did not replicate, even if the difference is considered too small to matter.</p>
<p>Anderson also point out the importance of incorporating a <a href="09-equivalencetest.html#sec-sesoi">smallest effect size of interest</a> when interpreting whether a replication study yields a null effect. For example, <span class="citation" data-cites="mccarthy_registered_2018">McCarthy et al. (<a href="references.html#ref-mccarthy_registered_2018" role="doc-biblioref">2018</a>)</span> replicated a hostility priming study originally reported to have an effect size of <em>d</em> = 3.01. In a multi-lab replication they observed an effect very close to zero: <em>d</em> = 0.06, 95% CI = [0.01, 0.12]. This effect was statistically different from the original, but statistically significant. McCarthy and colleagues argue this effect, even if significant, is too small to matter, and conclude “Our results suggest that the procedures we used in this replication study are unlikely to produce an assimilative priming effect that researchers could practically and routinely detect. Indeed, to detect priming effects as small as the 0.08-scale-unit difference we observed (which works out to approximately d = 0.06, 95% CI = [0.01, 0.12]), a study would need 4,362 participants in each priming condition to have 80% power with an alpha set to .05.”</p>
<p>Should we consider the replication study with <em>d</em> = 0.06 a successful replication, just because the effect is significant in a very large sample? As explained in the chapter on <a href="#sec-equivalencetesting">equivalence testing</a>, effects very close to zero are often considered equivalent to the absence of an effect because the effect is practically insignificant, theoretically smaller than expected, or impossible to reliably investigate given the resources that researchers have available. As <span class="citation" data-cites="mccarthy_registered_2018">McCarthy et al. (<a href="references.html#ref-mccarthy_registered_2018" role="doc-biblioref">2018</a>)</span> argue, the very small effect they observed is no longer practically feasible to study, and psychologists would therefore consider such effects equivalent to a null result. It is recommended to specify a smallest effect of interest in replication studies, where possible together with the original author (see the example by <span class="citation" data-cites="morey_preregistered_2021">Richard D. Morey et al. (<a href="references.html#ref-morey_preregistered_2021" role="doc-biblioref">2021</a>)</span> below) and evaluate replication studies with an equivalence test against the smallest effect size of interest.</p>
<p>As noted above, although ideally we directly test the difference between effect sizes, the uncertainty in the original effect size estimate when sample sizes are small can make lead to a test with very low power. As we can never increase the sample size of original studies, and we also can not do science if we can not conclude claims in the literature can not be replicated, we will need to be pragmatic and develop alternative statistical procedures to claim an effect can not be replicated. As the large uncertainty in the original effect sizes are the main cause of the problem, a logical solution is to move to statistical approaches where this uncertainty is not part of the test. An intuitive approach would be to test if the effect size in the replication study is statistically smaller than the effect in the original study. In the example above we would test whether the 95% confidence interval around the effect size in the replication study excludes the effect in the original study (i.e., 0.6, or 0.592 when converted to Hedges’<em>g</em>). In essence this is a two-sided test against the original effect size. This is the equivalent of changing a independent <em>t</em>-test where two groups are compared to a one-sample <em>t</em>-test where the 95% confidence interval in one group is compared against the observed mean in the other group, ignoring uncertainty in this other group. We do not accept this approach when we compare two means, and we should not accept this approach when we compare two standardized mean differences from an original and replication study. The problem is that this test will too easily reject the null-hypothesis that the two effects are equal, because it ignores the variability in one of the two effect sizes.</p>
<p>An alternative approach is to test against a more conservative effect size, and consider an original finding not replicated if this conservative effect size can be rejected. In practice researchers are primarily interested in the question whether the effect size in the replication study is statistically smaller that the effect size in the original study. This questions is answered by an <em>inferiority test</em>, which is statistically significant if the 90% confidence interval around the effect size in the replication study does not contain the conservative effect size estimate. One implementation of an inferiority test is the <em>small telescopes</em> approach <span class="citation" data-cites="simonsohn_small_2015">(<a href="references.html#ref-simonsohn_small_2015" role="doc-biblioref">Simonsohn, 2015</a>)</span>. In the small telescopes approach a test is performed against the effect the original study had 33% power to detect. In the example in <a href="#fig-rep-1" class="quarto-xref">Figure&nbsp;<span>17.2</span></a> the original study had 33% power to detect and effect of <em>d</em> = 0.4.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>pwr<span class="sc">::</span><span class="fu">pwr.t.test</span>(</span>
<span id="cb4-2"><a href="#cb4-2"></a>  <span class="at">n =</span> <span class="dv">30</span>,</span>
<span id="cb4-3"><a href="#cb4-3"></a>  <span class="at">sig.level =</span> <span class="fl">0.05</span>,</span>
<span id="cb4-4"><a href="#cb4-4"></a>  <span class="at">power =</span> <span class="fl">0.33</span>,</span>
<span id="cb4-5"><a href="#cb4-5"></a>  <span class="at">type =</span> <span class="st">"two.sample"</span>,</span>
<span id="cb4-6"><a href="#cb4-6"></a>  <span class="at">alternative =</span> <span class="st">"two.sided"</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample t test power calculation 

              n = 30
              d = 0.3988825
      sig.level = 0.05
          power = 0.33
    alternative = two.sided

NOTE: n is number in *each* group</code></pre>
</div>
</div>
<p>The 95% confidence interval of the replication study shows effects larger than 0.277 can be rejected, so a 90% confidence interval would be able to reject even smaller effect sizes. Therefore, the small telescopes approach would allow researchers to conclude that the original effect could not be replicated. The small telescopes is popular especially when replicating small original studies, as the smaller the sample size, the larger the effect size the study had 33% power to detect, and this the easier it is to reject in a replication study. This is the opposite of what happens if we want to test the two effect sizes against each other, where smaller original studies reduce the power of the test.</p>
<p>A fair point of criticism of the small telescopes approach is that the convention to test against an effect the original study had 33% power to detect is completely arbitrary. Ideally researchers would specify the <em>smallest effect size of interest</em> and perform an inferiority test against the smallest effect that would actually matter. Specifying a smallest effect size of interest is difficult, however. In some cases researchers involved in the original study might specify the smallest effect they care about. For example, in a multi-lab replication study of the action-sentence compatibility effect the researchers who published the original study stated that they considered an effect of 50 milliseconds or less theoretically negligible <span class="citation" data-cites="morey_preregistered_2021">(<a href="references.html#ref-morey_preregistered_2021" role="doc-biblioref">Richard D. Morey et al., 2021</a>)</span>. The large sample size allowed the authors to conclusively reject the presence of effects as large or larger than were considered theoretically negligible. Another approach is to set the smallest effect size of interest based on theoretical predictions. Alternatively, researchers might set the smallest effect size of interest to be larger than the <a href="05-questions.html#sec-crud">crud factor</a>, which is the effect size in a literature due to theoretically uninteresting systematic noise <span class="citation" data-cites="orben_crud_2020">(<a href="references.html#ref-orben_crud_2020" role="doc-biblioref">Orben &amp; Lakens, 2020</a>)</span>. For example, <span class="citation" data-cites="ferguson_providing_2021">Ferguson &amp; Heene (<a href="references.html#ref-ferguson_providing_2021" role="doc-biblioref">2021</a>)</span> suggests a smallest effect size of interest of <em>r</em> = 0.1 (or <em>d</em> = 0.2) because such effect sizes can be observed for nonsensical variables, and are likely purely driven by (methodological) confounds.</p>
<p>Another approach that has been proposed is to combine the effect size of the original study and the replication study in a meta-analysis, and test whether the meta-analytic effect is statistically different from zero. This approach is interesting if there is no bias, but when there is bias the weakness of this approach outweighs its usefulness. As publication bias and selection bias <a href="06-effectsize.html#sec-effectinflated">inflated effect sizes</a> the meta-analytic effect size will also be inflated. This in turn inflates the probability that the meta-analytic effect size is statistically significant, when there is no effect.</p>
</section><section id="replication-studies-or-lower-alpha-levels" class="level2" data-number="17.4"><h2 data-number="17.4" class="anchored" data-anchor-id="replication-studies-or-lower-alpha-levels">
<span class="header-section-number">17.4</span> Replication studies or lower alpha levels?</h2>
<p>Statistically minded researchers sometimes remark that there is no difference in the Type 1 error probability when a single study with a lower alpha level is used to test a hypothesis (say 0.05 x 0.05 = 0.0025) compared to when the same hypothesis is tested in two studies, each at an alpha level of 0.05. This is correct, but in practice it is not possible to replace the function of replication studies to decrease the Type 1 error probability with directly lowering alpha levels in single studies. Lowering the alpha level to a desired Type 1 error probability would require that scientists 1) can perfectly predict how important a claim will be in the future, and 2) are able to reach consensus on the Type 1 error rate they collectively find acceptable for each claim. Neither is true in practice. First, it is possible that a claim becomes increasingly important in a research area, for example because a large number of follow-up studies cite the study and assume the claim is true. This increase in importance might convince the entire scientific community that it is worthwhile if the Type 1 error probability is reduced by performing a replication study, as the importance of the original claim has made a Type 1 error more costly <span class="citation" data-cites="isager_deciding_2023">(<a href="references.html#ref-isager_deciding_2023" role="doc-biblioref">Isager et al., 2023</a>)</span>. For claims no one builds on, no one will care about reducing the Type 1 error probability. How important a claim will be for follow-up research can not be predicted in advance.</p>
<p>The second reason to reduce the probability of Type 1 errors through replications is that there are individual differences between researchers in when they believe a finding is satisfactorily demonstrated. As Popper writes: “Every test of a theory, whether resulting in its corroboration or falsification, must stop at some basic statement or other which we <em>decide to accept</em>.” “This procedure has no natural end. Thus if the test is to lead us anywhere, nothing remains but to stop at some point or other and say that we are satisfied, for the time being.” Different researchers will have different thresholds for how satisfied they are we are with the error probability associated with a claim. Some researchers are happy to accept a claim when the Type 1 error probability is 10%, while more skeptical researchers would like to see it reduced to 0.1% before building on a finding. Using a lower alpha level for a single study does not give scientists the flexibility to lower Type 1 error probabilities as the need arises, while performing replication studies does. It is a good idea to think carefully about the desired Type 1 error rate for studies, and if Type 1 errors are costly, researchers can decide to use a lower alpha level than the default level of 0.05 <span class="citation" data-cites="maier_justify_2022">(<a href="references.html#ref-maier_justify_2022" role="doc-biblioref">Maier &amp; Lakens, 2022</a>)</span>. But replication studies will remain necessary in practice to further reduce the probability of a Type 1 error as claims increase in importance, or for researchers who are more skeptical about a claim.</p>
<p>There is another reason why it is more beneficial to use independent replication studies to reduce the error rate, compared to performing studies with a lower alpha level. If others are able to independently replicate the same effect, it becomes less likely that the original finding was due to systematic error. Systematic errors do not average out to zero in the long run (as random errors do). There can be many sources of systematic error in a study. One source is the measures that are used. For example, if a researcher uses a weighing scale that is limited to a maximum weight of 150 kilo, they might fail to identify an increase in weight, while different researchers who use a weighting scale with a higher maximum weight will identify the difference. An experimenter might be another source of systematic error. An observed effect might not be due to a manipulation, but due to the way the experimenter treats participants in different conditions. Other experimenters who repeat the manipulation, but do not show the same experimenter bias, would observe different results.</p>
<p>When a researcher repeats their own experiment this is referred to as a <strong>self-replication</strong>, while in an <strong>independent replication</strong> other researchers repeat the experiment. As explained above, self-replication can reduce the Type 1 error rate of a claim, while independent replication can in addition reduce the probability of a claim being caused by a systematic error. Both self-replication and independent replication are useful in science. For example, research collaborations such as the Large Hadron Collider at CERN prefer to not only replicate studies with the same detector, but also replicate studies across different detectors. The Large Hadron Collider has four detectors (ATLAS, CMS, ALICE, and LHCb). Experiments can be self-replicated in the same detector by collecting more data (referred to by physicists as a ‘replica’), but they can also be replicated in a different detector. As Junk and Lyons <span class="citation" data-cites="junk_reproducibility_2020">(<a href="references.html#ref-junk_reproducibility_2020" role="doc-biblioref">2020</a>)</span> note, in self-replications: “The statistical variations are expected to be different in the replica and the original, but the sources of systematic errors are expected to be unchanged.” One way to examine systematic errors is to perform the same experiment in different detectors. The detectors at CERN are not exact copies of each other, and by performing studies in two different detectors, the research collaboration increases its confidence in the reliability of the conclusions if a replication study yields the same observations, despite minor differences in the experimental set-up.</p>
<p>The value of an independent replication is not only the reduction in the probability of a Type 1 error, but at the same time the reduction in concerns about systematic error influencing the conclusion <span class="citation" data-cites="neher_probability_1967">(<a href="references.html#ref-neher_probability_1967" role="doc-biblioref">Neher, 1967</a>)</span>. As already highlighted by Mack <span class="citation" data-cites="mack_need_1951">(<a href="references.html#ref-mack_need_1951" role="doc-biblioref">1951</a>)</span>: “Indeed, the introduction of different techniques gives the replication the additional advantage of serving as a check on the validity of the original research”. Similarly, Lubin <span class="citation" data-cites="lubin_replicability_1957">(<a href="references.html#ref-lubin_replicability_1957" role="doc-biblioref">1957</a>)</span> notes: “Our confidence in a study will be a positive monotonic function of the extent to which replication designs are used which vary these supposedly irrelevant factors”. To conclude, although both self-replications (a replication by the same researchers) and independent replication (a replication by different researchers) reduce the probability of a false positive claim, independent replications have the added benefit of testing the generalizability of the findings across factors that are deemed irrelevant to observe the effect.</p>
<p>From a purely statistical level is it interesting to ask whether it is less costly in terms of the required sample size to perform a single study at an alpha level of 0.0025, or two studies at an alpha level of 0.05. If the sample size required to achieve a desired power is much lower for a single test at an alpha of 0.0025 this would be a reason to perform larger single studies to achieve a low Type 1 error rate instead of performing two smaller studies at an alpha of 0.05. For an independent <em>t</em>-test the more efficient approach depends on the power of the test and whether the test is two-sided or one-sided. In <a href="#fig-alphalevels1" class="quarto-xref">Figure&nbsp;<span>17.3</span></a> a ratio of 1 means the sample size required to reach 70% to 95% power for an alpha level of 0.0025 is identical to two single studies with an alpha level of 0.05, and the two approaches would be equally efficient for a two-sided independent <em>t</em>-test. We see the ratio is below 1 for studies with high power. For example, the total sample size required to detect an effect of <em>d</em> = 0.5 with an alpha level of 0.0025 and with 80% power is 244. With an alpha of 0.05 the total required sample size is 128 in each study, which makes the ratio 244/(128+128) = 0.95, and the number of observations saved is (2*128)-244 = 12.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-alphalevels1" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-alphalevels1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-replication_files/figure-html/fig-alphalevels1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alphalevels1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.3: Ratio of the sample size required to run one study at al alpha of 0.0025 or two studies at an alpha of 0.05 for a two-sided independent <em>t</em>-test.
</figcaption></figure>
</div>
</div>
</div>
<p>However, there are strong arguments in favor of <a href="05-questions.html#sec-onesided">directional tests</a> in general, and especially for replication studies. When we examine the same scenario for a one-sided test the ratios change, and one larger study at 0.0025 is slightly more efficient for smaller effect sizes when the desired statistical power is 90%, but for 80% it is more efficient to perform two tests at 0.05. For example, the total sample size required to detect an effect of d = 0.5 with an alpha level of 0.0025 and with 80% power is 218 With an alpha of 0.05 the total required sample size is 102, which makes the ratio 218/(102+102) = 1.07, and the number of observations saved (now when performing two studies) is (2*102)-218 = 14.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-alphalevels2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-alphalevels2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-replication_files/figure-html/fig-alphalevels2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alphalevels2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.4: Ratio of the sample size required to run one study at al alpha of 0.0025 or two studies at an alpha of 0.05 for a one-sided independent <em>t</em>-test.
</figcaption></figure>
</div>
</div>
</div>
<p>The statistical power in these calculations is identical for both scenarios, but one needs to reflect on how the two studies at an alpha of 0.05 will be analyzed. If the two studies are direct replications, a fixed effect meta-analysis of both effect sizes has the same power as a single study with twice the sample size, and therefore a meta-analysis at an alpha level of 0.0025 would be identical to a saningle study with an alpha of 0.0025. Altogether, the choice of whether to perform two studies at an alpha of 0.05 or a single study at 0.0025 depends on the directionality of the test and the desired power. There is not clear-cut advantage of a single study at an alpha level of 0.0025, and given the non-statistical benefits of independent replication studies, and the fact that replication studies should arguably always be directional tests, a case can be made to prefer the two study approach. This does require that scientists collaborate, and that all studies are shared regardless of the outcome (for example as Registered Reports).</p>
<!-- ## Why Prediction Markets or Machine Learning can not Replace Replications -->
<!-- As explained above, just as a single alpha level attached to a claim is not sufficient, as we need to be able to lower the Type 1 error probability as needed, so will other approaches to quantify the probability that a finding is not replicable not be able to replace replication studies if they can not be lowered as needed. The SCORE project examined 2 approaches: Machine Learning, and Predictions  -->
</section><section id="when-replication-studies-yield-conflicting-results" class="level2" data-number="17.5"><h2 data-number="17.5" class="anchored" data-anchor-id="when-replication-studies-yield-conflicting-results">
<span class="header-section-number">17.5</span> When replication studies yield conflicting results</h2>
<p>As mentioned above, there are three possible reasons for a non-replication: The replication has yielded a Type 1 error, the original study was a Type 1 error, or there is a difference between the two studies. The probability of a Type 2 error can be reduced by performing an a-priori power analysis, or even better, by performing a power analysis for an <a href="09-equivalencetest.html">equivalence test</a> against a smallest effect size of interest. Even then, there is always a probability that the result in a replication study is a false negative.</p>
<p>Some researchers strongly believe failures to replicate published findings can be explained by the presence of hitherto unidentified, or ‘hidden’, moderators <span class="citation" data-cites="stroebe_alleged_2014">(<a href="references.html#ref-stroebe_alleged_2014" role="doc-biblioref">Stroebe &amp; Strack, 2014</a>)</span>. There has been at least one example of researchers who were able to provide modest support for the idea that a previous failure to replicate a finding was due to how personally relevant a message in the study was <span class="citation" data-cites="luttrell_replicating_2017">(<a href="references.html#ref-luttrell_replicating_2017" role="doc-biblioref">Luttrell et al., 2017</a>)</span>. It is difficult to reliably identify moderator variables that explain failures to replicate published findings, but easy to raise them as an explanation when replication studies do not observe the same effect as the original study. Especially in the social sciences some of these potential moderators are practically impossible to test, such as the fact that society has changed over time. This is an age-old problem, already identified by Galileo in <a href="https://web.archive.org/web/https://web.stanford.edu/~jsabol/certainty/readings/Galileo-Assayer.pdf">The Assayer</a>, one of the first books on the scientific method. In this book, Galileo discusses the claim that Babylonians cooked eggs by whirling them in a sling, which turned out to be impossible to replicate, and writes:</p>
<blockquote class="blockquote">
<p>‘If we do not achieve an effect which others formerly achieved, it must be that we lack something in our operation which was the cause of this effect succeeding, and if we lack one thing only, then this alone can be the true cause. Now we do not lack eggs, or slings, or sturdy fellows to whirl them, and still they do not cook, but rather cool down faster if hot. And since we lack nothing except being Babylonians, then being Babylonian is the cause of the egg hardening.’</p>
</blockquote>
<p>At the same time, some failures to replicate <em>are</em> due to a difference in auxiliary hypotheses. In the most interesting study examining whether failures to replicate are due to differences in auxiliary assumptions, Ebersole and colleagues <span class="citation" data-cites="ebersole_many_2020">(<a href="references.html#ref-ebersole_many_2020" role="doc-biblioref">2020</a>)</span> performed additional replications of 10 studies that were replicated in the Reproducibility Project: Psychology (RP:P, <span class="citation" data-cites="opensciencecollaboration_estimating_2015">Open Science Collaboration (<a href="references.html#ref-opensciencecollaboration_estimating_2015" role="doc-biblioref">2015</a>)</span>). When the replication s were designed the authors of original studies were approaches for feedback on the design of the replication study. For each of these 10 studies the original authors had raised concerns, but these were not incorporated in the replication study. For example, authors raised the concern that the replication study included participants who had taken prior psychology or economics courses, or who had participated in prior psychology studies. The authors predicted that the effects should be larger in ‘naive’ participants. Other authors pointed out the possibility that the stimuli were not sufficiently pilot tested, the fact that data was collected in a different country, differences in the materials or stimuli, or they pointed out differences in screen resolution of the computer set-up. All these concerns involve predictions about the effects of auxiliary hypotheses, and a team of 172 researchers collaborated with original authors to examine these auxiliary hypotheses in Many Labs 5 <span class="citation" data-cites="ebersole_many_2020">(<a href="references.html#ref-ebersole_many_2020" role="doc-biblioref">Ebersole et al., 2020</a>)</span>. What makes this project especially interesting is that large replication studies were performed both of the RP:P version of the study, as of the revised protocol that addressed the concerns raised by the researchers.</p>
<p>The results of this project provide a nice illustration of how difficult it is to predict whether findings will replicate, until you try to replicate them <span class="citation" data-cites="miller_what_2009">(<a href="references.html#ref-miller_what_2009" role="doc-biblioref">Miller, 2009</a>)</span>. Two of the studies that did not replicate in the RP:P also did not replicate when a larger new sample was collected, but did replicate with the revised protocol. However, these replication effect sizes were arguably trivially small (Albarracin et al., Study 5 and Shnabel &amp; Nadler in <a href="#fig-manylabs5" class="quarto-xref">Figure&nbsp;<span>17.5</span></a>). A third study (Van Dijk et al) showed a similar pattern but only just failed to show a significant effect in the revised protocol. A fourth study (Albarracin et al., Study 7) was just significant in the original study, and the RP:P study found a very similar effect size which was just non-significant. A meta-analysis pooling these two studies would have yielded a significant meta-analytic effect. And yet, surprisingly, both the replication of the RP:P and the revised protocol based on feedback by the original authors yielded clear null results. A fifth study (Crosby et al) found the same pattern in the original and RP:P study as the second study. Neither the much larger RP:P replication, nor the replication based on the revised protocol yielded a significant result. And yet, the pattern of effect sizes is extremely similar in all four studies, and a meta-analysis across all studies reveals a small but statistically significant effect. In total six of the studies can clearly be regarded as a non-replication where the original authors’ concerns did not matter, as only the original study showed a significant effect, and none of the replication studies yielded a significant result.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-manylabs5" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-manylabs5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/manylabs5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-manylabs5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.5: Forest plot for the original study, RP:P replication study, the larger replication of the RP:P replication study, and the revised protocol based on author feedback, and a meta-analysis of all data, for all 10 studies in Many Labs 5
</figcaption></figure>
</div>
</div>
</div>
<p>Note that we can not conclude that the concerns the authors raised in the other four studies mattered. Despite the large sample sizes, only one statistical difference between the RP:P protocol and the revised protocol was observed (Payne et al.), and here the changes suggested by the authors led to an effect sizes were even <em>further</em> away from the effect size in the original study, if we test the effect sizes directlyagainst each other in a test for heterogeneity:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Payne, Burkley, &amp; Stokes (2008)</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="co"># Note that the CI in the figure is wide because there is considerable variability across the sites where data was collected, especially for the revised protocol.</span></span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a>r1 <span class="ot">&lt;-</span> <span class="fu">escalc</span>(<span class="at">ni =</span> <span class="dv">545</span>, </span>
<span id="cb6-5"><a href="#cb6-5"></a>             <span class="at">ri =</span> <span class="fl">0.05</span>, </span>
<span id="cb6-6"><a href="#cb6-6"></a>             <span class="at">measure =</span> <span class="st">"ZCOR"</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a>r2 <span class="ot">&lt;-</span> <span class="fu">escalc</span>(<span class="at">ni =</span> <span class="dv">558</span>, </span>
<span id="cb6-8"><a href="#cb6-8"></a>             <span class="at">ri =</span> <span class="sc">-</span><span class="fl">0.16</span>, </span>
<span id="cb6-9"><a href="#cb6-9"></a>             <span class="at">measure =</span> <span class="st">"ZCOR"</span>)</span>
<span id="cb6-10"><a href="#cb6-10"></a>metadata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">yi =</span> <span class="fu">c</span>(r1<span class="sc">$</span>yi, r2<span class="sc">$</span>yi), </span>
<span id="cb6-11"><a href="#cb6-11"></a>                       <span class="at">vi =</span> <span class="fu">c</span>(r1<span class="sc">$</span>vi, r2<span class="sc">$</span>vi), </span>
<span id="cb6-12"><a href="#cb6-12"></a>                       <span class="at">study =</span> <span class="fu">c</span>(<span class="st">"original"</span>, <span class="st">"replication"</span>))</span>
<span id="cb6-13"><a href="#cb6-13"></a></span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="co"># Test based on heterogeneity analysis</span></span>
<span id="cb6-15"><a href="#cb6-15"></a>res_h <span class="ot">&lt;-</span> <span class="fu">rma</span>(yi, </span>
<span id="cb6-16"><a href="#cb6-16"></a>             vi, </span>
<span id="cb6-17"><a href="#cb6-17"></a>             <span class="at">data =</span> metadata, </span>
<span id="cb6-18"><a href="#cb6-18"></a>             <span class="at">method =</span> <span class="st">"FE"</span>)</span>
<span id="cb6-19"><a href="#cb6-19"></a>res_h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Fixed-Effects Model (k = 2)

I^2 (total heterogeneity / total variability):   91.84%
H^2 (total variability / sampling variability):  12.26

Test for Heterogeneity:
Q(df = 1) = 12.2578, p-val = 0.0005

Model Results:

estimate      se     zval    pval    ci.lb   ci.ub    
 -0.0569  0.0302  -1.8854  0.0594  -0.1161  0.0023  . 

---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The other difference between the RP:P protocol and the revised protocol were not statistically significant, but in one other study the effect size was even further away from the original effect size after the changes to the protocol, and in the other two studies the effect size was more similar as to the original study. Altogether, it seems authors who raise concerns about replication studies do not have a high success rate in predicting which auxiliary hypotheses influence the observed effect size. This is a very important conclusion of Many Labs 5.</p>
</section><section id="why-are-replication-studies-so-rare" class="level2" data-number="17.6"><h2 data-number="17.6" class="anchored" data-anchor-id="why-are-replication-studies-so-rare">
<span class="header-section-number">17.6</span> Why are replication studies so rare?</h2>
<p>“It is difficult to deny that there is more thrill, and usually more glory, involved in blazing a new trail than in checking the pioneer’s work” <span class="citation" data-cites="mack_need_1951">(<a href="references.html#ref-mack_need_1951" role="doc-biblioref">Mack, 1951</a>)</span>. Throughout history, researchers have pointed out that the reward structures value novel research over replication studies <span class="citation" data-cites="koole_rewarding_2012 fishman_american_1982">(<a href="references.html#ref-fishman_american_1982" role="doc-biblioref">Fishman &amp; Neigher, 1982</a>; <a href="references.html#ref-koole_rewarding_2012" role="doc-biblioref">Koole &amp; Lakens, 2012</a>)</span>. Performing replication studies is a social dilemma: It is good for everyone if scientists perform replication studies, but it is better for an individual scientist to perform a novel study than a replication study. Exact numbers of how many replication studies are performed are difficult to get, as there is no complete database that keeps track of all replication studies (but see <a href="https://curatescience.org/">Curate Science</a>, <a href="https://replication.uni-goettingen.de/wiki/index.php/Main_Page">Replication WIKI</a> or the <a href="https://metaanalyses.shinyapps.io/replicationdatabase/">Replication Database</a>.</p>
<p>Although the reward structures have remained the same, there are some positive developments. At the start of the replication crisis failed replications of Bem’s pre-cognition study were desk-rejected by the editor of JPSP, Eliot Smith, who stated “This journal does not publish replication studies, whether successful or unsuccessful” and “We don’t want to be the Journal of Bem Replication” <span class="citation" data-cites="aldhous_journal_2011">(<a href="references.html#ref-aldhous_journal_2011" role="doc-biblioref">Aldhous, 2011</a>)</span>. This led to public outcry, and numerous journals have started to explicitly state they accept replication studies. An increasing number of studies are accepting Registered Report publications, which can also be replication studies, and Peer Community Inn: Registered Reports initiative is publishing replication studies. The APA has created specific <a href="https://apastyle.apa.org/jars/quant-table-6.pdf">reporting guidelines for replication studies</a>. Some science funders have developed <a href="https://www.nature.com/articles/nature.2016.20287">grants for replication research</a>. At the same time, replication studies are still rewarded less than novel work, which means researchers who want to build a career are still pushed towards novel research instead of replication studies. So despite positive developments, in many disciplines there is still some way to go before replication studies become a normal aspect of scientific research.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-aldhous_journal_2011" class="csl-entry" role="listitem">
Aldhous, P. (2011). <em>Journal rejects studies contradicting precognition</em>. New Scientist. <a href="https://www.newscientist.com/article/dn20447-journal-rejects-studies-contradicting-precognition/">https://www.newscientist.com/article/dn20447-journal-rejects-studies-contradicting-precognition/</a>
</div>
<div id="ref-anderson_there_2016" class="csl-entry" role="listitem">
Anderson, S. F., &amp; Maxwell, S. E. (2016). There’s more than one way to conduct a replication study: <span>Beyond</span> statistical significance. <em>Psychological Methods</em>, <em>21</em>(1), 1–12. <a href="https://doi.org/10.1037/met0000051">https://doi.org/10.1037/met0000051</a>
</div>
<div id="ref-bonett_replicationextension_2012" class="csl-entry" role="listitem">
Bonett, D. G. (2012). Replication-<span>Extension Studies</span>. <em>Current Directions in Psychological Science</em>, <em>21</em>(6), 409–412. <a href="https://doi.org/10.1177/0963721412459512">https://doi.org/10.1177/0963721412459512</a>
</div>
<div id="ref-borenstein_introduction_2009" class="csl-entry" role="listitem">
Borenstein, M. (Ed.). (2009). <em>Introduction to meta-analysis</em>. John Wiley &amp; Sons.
</div>
<div id="ref-degroot_methodology_1969" class="csl-entry" role="listitem">
de Groot, A. D. (1969). <em>Methodology</em> (Vol. 6). Mouton &amp; Co.
</div>
<div id="ref-ebersole_many_2020" class="csl-entry" role="listitem">
Ebersole, C. R., Mathur, M. B., Baranski, E., Bart-Plange, D.-J., Buttrick, N. R., Chartier, C. R., Corker, K. S., Corley, M., Hartshorne, J. K., IJzerman, H., Lazarević, L. B., Rabagliati, H., Ropovik, I., Aczel, B., Aeschbach, L. F., Andrighetto, L., Arnal, J. D., Arrow, H., Babincak, P., … Nosek, B. A. (2020). Many <span>Labs</span> 5: <span>Testing Pre-Data-Collection Peer Review</span> as an <span>Intervention</span> to <span>Increase Replicability</span>. <em>Advances in Methods and Practices in Psychological Science</em>, <em>3</em>(3), 309–331. <a href="https://doi.org/10.1177/2515245920958687">https://doi.org/10.1177/2515245920958687</a>
</div>
<div id="ref-epstein_stability_1980" class="csl-entry" role="listitem">
Epstein, S. (1980). The stability of behavior: <span>II</span>. <span>Implications</span> for psychological research. <em>American Psychologist</em>, <em>35</em>(9), 790–806. <a href="https://doi.org/10.1037/0003-066X.35.9.790">https://doi.org/10.1037/0003-066X.35.9.790</a>
</div>
<div id="ref-ferguson_providing_2021" class="csl-entry" role="listitem">
Ferguson, C. J., &amp; Heene, M. (2021). Providing a lower-bound estimate for psychology’s <span>“crud factor”</span>: <span>The</span> case of aggression. <em>Professional Psychology: Research and Practice</em>, <em>52</em>(6), 620–626. https://doi.org/<a href="http://dx.doi.org/10.1037/pro0000386">http://dx.doi.org/10.1037/pro0000386</a>
</div>
<div id="ref-fishman_american_1982" class="csl-entry" role="listitem">
Fishman, D. B., &amp; Neigher, W. D. (1982). American psychology in the eighties: <span>Who</span> will buy? <em>American Psychologist</em>, <em>37</em>(5), 533–546. <a href="https://doi.org/10.1037/0003-066X.37.5.533">https://doi.org/10.1037/0003-066X.37.5.533</a>
</div>
<div id="ref-freedman_compliance_1966" class="csl-entry" role="listitem">
Freedman, J. L., &amp; Fraser, S. C. (1966). Compliance without pressure: <span>The</span> foot-in-the-door technique. <em>Journal of Personality and Social Psychology</em>, <em>4</em>(2), 195–202. <a href="https://doi.org/10.1037/h0023552">https://doi.org/10.1037/h0023552</a>
</div>
<div id="ref-gergen_social_1973" class="csl-entry" role="listitem">
Gergen, K. J. (1973). Social psychology as history. <em>Journal of Personality and Social Psychology</em>, <em>26</em>, 309–320. <a href="https://doi.org/10.1037/h0034436">https://doi.org/10.1037/h0034436</a>
</div>
<div id="ref-greenwald_editorial_1976" class="csl-entry" role="listitem">
Greenwald, A. G. (Ed.). (1976). An editorial. <em>Journal of Personality and Social Psychology</em>, <em>33</em>(1), 1–7. <a href="https://doi.org/10.1037/h0078635">https://doi.org/10.1037/h0078635</a>
</div>
<div id="ref-isager_deciding_2023" class="csl-entry" role="listitem">
Isager, P. M., van Aert, R. C. M., Bahník, Š., Brandt, M. J., DeSoto, K. A., Giner-Sorolla, R., Krueger, J. I., Perugini, M., Ropovik, I., van ’t Veer, A. E., Vranka, M., &amp; Lakens, D. (2023). Deciding what to replicate: <span>A</span> decision model for replication study selection under resource and knowledge constraints. <em>Psychological Methods</em>, <em>28</em>(2), 438–451. <a href="https://doi.org/10.1037/met0000438">https://doi.org/10.1037/met0000438</a>
</div>
<div id="ref-junk_reproducibility_2020" class="csl-entry" role="listitem">
Junk, T., &amp; Lyons, L. (2020). Reproducibility and <span>Replication</span> of <span>Experimental Particle Physics Results</span>. <em>Harvard Data Science Review</em>, <em>2</em>(4). <a href="https://doi.org/10.1162/99608f92.250f995b">https://doi.org/10.1162/99608f92.250f995b</a>
</div>
<div id="ref-koole_rewarding_2012" class="csl-entry" role="listitem">
Koole, S. L., &amp; Lakens, D. (2012). Rewarding replications <span>A</span> sure and simple way to improve psychological science. <em>Perspectives on Psychological Science</em>, <em>7</em>(6), 608–614. <a href="https://doi.org/10.1177/1745691612462586">https://doi.org/10.1177/1745691612462586</a>
</div>
<div id="ref-loevinger_information_1968" class="csl-entry" role="listitem">
Loevinger, J. (1968). The "information explosion.". <em>American Psychologist</em>, <em>23</em>(6), 455–455. <a href="https://doi.org/10.1037/h0020800">https://doi.org/10.1037/h0020800</a>
</div>
<div id="ref-lubin_replicability_1957" class="csl-entry" role="listitem">
Lubin, A. (1957). Replicability as a publication criterion. <em>American Psychologist</em>, <em>12</em>, 519–520. <a href="https://doi.org/10.1037/h0039746">https://doi.org/10.1037/h0039746</a>
</div>
<div id="ref-luttrell_replicating_2017" class="csl-entry" role="listitem">
Luttrell, A., Petty, R. E., &amp; Xu, M. (2017). Replicating and fixing failed replications: <span>The</span> case of need for cognition and argument quality. <em>Journal of Experimental Social Psychology</em>, <em>69</em>, 178–183. <a href="https://doi.org/10.1016/j.jesp.2016.09.006">https://doi.org/10.1016/j.jesp.2016.09.006</a>
</div>
<div id="ref-lykken_statistical_1968" class="csl-entry" role="listitem">
Lykken, D. T. (1968). Statistical significance in psychological research. <em>Psychological Bulletin</em>, <em>70</em>, 151–159. <a href="https://doi.org/10.1037/h0026141">https://doi.org/10.1037/h0026141</a>
</div>
<div id="ref-mack_need_1951" class="csl-entry" role="listitem">
Mack, R. W. (1951). The <span>Need</span> for <span>Replication Research</span> in <span>Sociology</span>. <em>American Sociological Review</em>, <em>16</em>(1), 93–94. <a href="https://doi.org/10.2307/2087978">https://doi.org/10.2307/2087978</a>
</div>
<div id="ref-maier_justify_2022" class="csl-entry" role="listitem">
Maier, M., &amp; Lakens, D. (2022). Justify your alpha: <span>A</span> primer on two practical approaches. <em>Advances in Methods and Practices in Psychological Science</em>. <a href="https://doi.org/10.31234/osf.io/ts4r6">https://doi.org/10.31234/osf.io/ts4r6</a>
</div>
<div id="ref-mccarthy_registered_2018" class="csl-entry" role="listitem">
McCarthy, R. J., Skowronski, J. J., Verschuere, B., Meijer, E. H., Jim, A., Hoogesteyn, K., Orthey, R., Acar, O. A., Aczel, B., Bakos, B. E., Barbosa, F., Baskin, E., Bègue, L., Ben-Shakhar, G., Birt, A. R., Blatz, L., Charman, S. D., Claesen, A., Clay, S. L., … Yıldız, E. (2018). Registered <span>Replication Report</span> on <span>Srull</span> and <span>Wyer</span> (1979). <em>Advances in Methods and Practices in Psychological Science</em>, <em>1</em>(3), 321–336. <a href="https://doi.org/10.1177/2515245918777487">https://doi.org/10.1177/2515245918777487</a>
</div>
<div id="ref-mellers_frequency_2001" class="csl-entry" role="listitem">
Mellers, B., Hertwig, R., &amp; Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? <span>An</span> exercise in adversarial collaboration. <em>Psychological Science</em>, <em>12</em>(4), 269–275. <a href="https://doi.org/10.1111/1467-9280.00350">https://doi.org/10.1111/1467-9280.00350</a>
</div>
<div id="ref-miller_what_2009" class="csl-entry" role="listitem">
Miller, J. (2009). What is the probability of replicating a statistically significant effect? <em>Psychonomic Bulletin &amp; Review</em>, <em>16</em>(4), 617–640. <a href="https://doi.org/10.3758/PBR.16.4.617">https://doi.org/10.3758/PBR.16.4.617</a>
</div>
<div id="ref-morey_preregistered_2021" class="csl-entry" role="listitem">
Morey, Richard D., Kaschak, M. P., Díez-Álamo, A. M., Glenberg, A. M., Zwaan, R. A., Lakens, D., Ibáñez, A., García, A., Gianelli, C., Jones, J. L., Madden, J., Alifano, F., Bergen, B., Bloxsom, N. G., Bub, D. N., Cai, Z. G., Chartier, C. R., Chatterjee, A., Conwell, E., … Ziv-Crispel, N. (2021). A pre-registered, multi-lab non-replication of the action-sentence compatibility effect (<span>ACE</span>). <em>Psychonomic Bulletin &amp; Review</em>. <a href="https://doi.org/10.3758/s13423-021-01927-8">https://doi.org/10.3758/s13423-021-01927-8</a>
</div>
<div id="ref-morey_why_2016" class="csl-entry" role="listitem">
Morey, Richard D., &amp; Lakens, D. (2016). <em>Why most of psychology is statistically unfalsifiable</em>. <a href="https://raw.githubusercontent.com/richarddmorey/psychology_resolution/master/paper/response.pdf">https://raw.githubusercontent.com/richarddmorey/psychology_resolution/master/paper/response.pdf</a>
</div>
<div id="ref-neher_probability_1967" class="csl-entry" role="listitem">
Neher, A. (1967). Probability <span>Pyramiding</span>, <span>Research Error</span> and the <span>Need</span> for <span>Independent Replication</span>. <em>The Psychological Record</em>, <em>17</em>(2), 257–262. <a href="https://doi.org/10.1007/BF03393713">https://doi.org/10.1007/BF03393713</a>
</div>
<div id="ref-nosek_what_2020" class="csl-entry" role="listitem">
Nosek, B. A., &amp; Errington, T. M. (2020). What is replication? <em>PLOS Biology</em>, <em>18</em>(3), e3000691. <a href="https://doi.org/10.1371/journal.pbio.3000691">https://doi.org/10.1371/journal.pbio.3000691</a>
</div>
<div id="ref-opensciencecollaboration_estimating_2015" class="csl-entry" role="listitem">
Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. <em>Science</em>, <em>349</em>(6251), aac4716–aac4716. <a href="https://doi.org/10.1126/science.aac4716">https://doi.org/10.1126/science.aac4716</a>
</div>
<div id="ref-orben_crud_2020" class="csl-entry" role="listitem">
Orben, A., &amp; Lakens, D. (2020). Crud (<span>Re</span>)<span>Defined</span>. <em>Advances in Methods and Practices in Psychological Science</em>, <em>3</em>(2), 238–247. <a href="https://doi.org/10.1177/2515245920917961">https://doi.org/10.1177/2515245920917961</a>
</div>
<div id="ref-pereboom_fundamental_1971" class="csl-entry" role="listitem">
Pereboom, A. C. (1971). Some <span>Fundamental Problems</span> in <span>Experimental Psychology</span>: <span>An Overview</span>. <em>Psychological Reports</em>, <em>28</em>(2). <a href="https://doi.org/10.2466/pr0.1971.28.2.439">https://doi.org/10.2466/pr0.1971.28.2.439</a>
</div>
<div id="ref-popper_logic_2002" class="csl-entry" role="listitem">
Popper, K. R. (2002). <em>The logic of scientific discovery</em>. Routledge.
</div>
<div id="ref-samelson_watson_1980" class="csl-entry" role="listitem">
Samelson, F. (1980). J <span>B Watson</span>’s <span>Little Albert</span>, <span>Cyril Burt</span>’s twins, and the need for a critical science. <em>American Psychologist</em>, <em>35</em>(7), 619–625. <a href="https://doi.org/10.1037/0003-066X.35.7.619">https://doi.org/10.1037/0003-066X.35.7.619</a>
</div>
<div id="ref-schmidt_shall_2009" class="csl-entry" role="listitem">
Schmidt, S. (2009). Shall we really do it again? <span>The</span> powerful concept of replication is neglected in the social sciences. <em>Review of General Psychology</em>, <em>13</em>(2), 90–100. <a href="https://doi.org/10.1037/a0015108">https://doi.org/10.1037/a0015108</a>
</div>
<div id="ref-sidman_tactics_1960" class="csl-entry" role="listitem">
Sidman, M. (1960). <em>Tactics of <span>Scientific Research</span>: <span>Evaluating Experimental Data</span> in <span>Psychology</span></em> (New edition). Cambridge Center for Behavioral.
</div>
<div id="ref-simons_constraints_2017" class="csl-entry" role="listitem">
Simons, D. J., Shoda, Y., &amp; Lindsay, D. S. (2017). Constraints on <span>Generality</span> (<span>COG</span>): <span>A Proposed Addition</span> to <span>All Empirical Papers</span>. <em>Perspectives on Psychological Science</em>, <em>12</em>(6), 1123–1128. <a href="https://doi.org/10.1177/1745691617708630">https://doi.org/10.1177/1745691617708630</a>
</div>
<div id="ref-simonsohn_small_2015" class="csl-entry" role="listitem">
Simonsohn, U. (2015). Small telescopes: <span>Detectability</span> and the evaluation of replication results. <em>Psychological Science</em>, <em>26</em>(5), 559–569. <a href="https://doi.org/10.1177/0956797614567341">https://doi.org/10.1177/0956797614567341</a>
</div>
<div id="ref-smith_replication_1970" class="csl-entry" role="listitem">
Smith, N. C. (1970). Replication studies: <span>A</span> neglected aspect of psychological research. <em>American Psychologist</em>, <em>25</em>(10), 970–975. <a href="https://doi.org/10.1037/h0029774">https://doi.org/10.1037/h0029774</a>
</div>
<div id="ref-spence_tempered_2024" class="csl-entry" role="listitem">
Spence, J. R., &amp; Stanley, D. J. (2024). Tempered <span>Expectations</span>: <span>A Tutorial</span> for <span>Calculating</span> and <span>Interpreting Prediction Intervals</span> in the <span>Context</span> of <span>Replications</span>. <em>Advances in Methods and Practices in Psychological Science</em>, <em>7</em>(1), 25152459231217932. <a href="https://doi.org/10.1177/25152459231217932">https://doi.org/10.1177/25152459231217932</a>
</div>
<div id="ref-stroebe_alleged_2014" class="csl-entry" role="listitem">
Stroebe, W., &amp; Strack, F. (2014). The <span>Alleged Crisis</span> and the <span>Illusion</span> of <span>Exact Replication</span>. <em>Perspectives on Psychological Science</em>, <em>9</em>(1), 59–71. <a href="https://doi.org/10.1177/1745691613514450">https://doi.org/10.1177/1745691613514450</a>
</div>
<div id="ref-tunc_falsificationist_2023" class="csl-entry" role="listitem">
Tunç, D. U., &amp; Tunç, M. N. (2023). A <span>Falsificationist Treatment</span> of <span>Auxiliary Hypotheses</span> in <span>Social</span> and <span>Behavioral Sciences</span>: <span>Systematic Replications Framework</span>. <em>Meta-Psychology</em>, <em>7</em>. <a href="https://doi.org/10.15626/MP.2021.2756">https://doi.org/10.15626/MP.2021.2756</a>
</div>
</div>
</section></main><!-- /main --><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script><script>

/* update total correct if #webex-total_correct exists */
update_total_correct = function() {
  console.log("webex: update total_correct");

  var t = document.getElementsByClassName("webex-total_correct");
  for (var i = 0; i < t.length; i++) {
    p = t[i].parentElement;
    var correct = p.getElementsByClassName("webex-correct").length;
    var solvemes = p.getElementsByClassName("webex-solveme").length;
    var radiogroups = p.getElementsByClassName("webex-radiogroup").length;
    var selects = p.getElementsByClassName("webex-select").length;

    t[i].innerHTML = correct + " of " + (solvemes + radiogroups + selects) + " correct";
  }
}

/* webex-solution button toggling function */
b_func = function() {
  console.log("webex: toggle hide");

  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* check answers */
check_func = function() {
  console.log("webex: check answers");

  var cl = this.parentElement.classList;
  if (cl.contains('unchecked')) {
    cl.remove("unchecked");
    this.innerHTML = "Hide Answers";
  } else {
    cl.add("unchecked");
    this.innerHTML = "Show Answers";
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  console.log("webex: check solveme");

  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "")
  }

  if (my_answer == "") {
    cl.remove("webex-correct");
    cl.remove("webex-incorrect");
  } else if (real_answers.includes(my_answer)) {
    cl.add("webex-correct");
    cl.remove("webex-incorrect");
  } else {
    cl.add("webex-incorrect");
    cl.remove("webex-correct");
  }

  // match numeric answers within a specified tolerance
  if(this.dataset.tol > 0){
    var tol = JSON.parse(this.dataset.tol);
    var matches = real_answers.map(x => Math.abs(x - my_answer) < tol)
    if (matches.reduce((a, b) => a + b, 0) > 0) {
      cl.add("webex-correct");
    } else {
      cl.remove("webex-correct");
    }
  }

  // added regex bit
  if (cl.contains("regex")){
    answer_regex = RegExp(real_answers.join("|"))
    if (answer_regex.test(my_answer)) {
      cl.add("webex-correct");
    }
  }

  update_total_correct();
}

/* function for checking select answers */
select_func = function(e) {
  console.log("webex: check select");

  var cl = this.classList

  /* add style */
  cl.remove("webex-incorrect");
  cl.remove("webex-correct");
  if (this.value == "answer") {
    cl.add("webex-correct");
  } else if (this.value != "blank") {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

/* function for checking radiogroups answers */
radiogroups_func = function(e) {
  console.log("webex: check radiogroups");

  var checked_button = document.querySelector('input[name=' + this.id + ']:checked');
  var cl = checked_button.parentElement.classList;
  var labels = checked_button.parentElement.parentElement.children;

  /* get rid of styles */
  for (i = 0; i < labels.length; i++) {
    labels[i].classList.remove("webex-incorrect");
    labels[i].classList.remove("webex-correct");
  }

  /* add style */
  if (checked_button.value == "answer") {
    cl.add("webex-correct");
  } else {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

window.onload = function() {
  console.log("webex onload");
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('webex-solution')) {
      buttons[i].onclick = b_func;
    }
  }

  var check_sections = document.getElementsByClassName("webex-check");
  console.log("check:", check_sections.length);
  for (var i = 0; i < check_sections.length; i++) {
    check_sections[i].classList.add("unchecked");

    let btn = document.createElement("button");
    btn.innerHTML = "Show Answers";
    btn.classList.add("webex-check-button");
    btn.onclick = check_func;
    check_sections[i].appendChild(btn);

    let spn = document.createElement("span");
    spn.classList.add("webex-total_correct");
    check_sections[i].appendChild(spn);
  }

  /* set up webex-solveme inputs */
  var solveme = document.getElementsByClassName("webex-solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off");
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";

    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;

    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;

    $(solveme[i]).after(" <span class='webex-icon'></span>");
  }

  /* set up radiogroups */
  var radiogroups = document.getElementsByClassName("webex-radiogroup");
  for (var i = 0; i < radiogroups.length; i++) {
    radiogroups[i].onchange = radiogroups_func;
  }

  /* set up selects */
  var selects = document.getElementsByClassName("webex-select");
  for (var i = 0; i < selects.length; i++) {
    selects[i].onchange = select_func;
    $(selects[i]).after(" <span class='webex-icon'></span>");
  }

  update_total_correct();
}

</script><script>
// open rdrr links externally ----

var exlinks = document.querySelectorAll("a[href^='https://rdrr.io']");
var exlink_func = function(){
  window.open(this.href);
  return false;
};
for (var i = 0; i < exlinks.length; i++) {
    exlinks[i].addEventListener('click', exlink_func, false);
}

// visible second sidebar in mobile ----

function move_sidebar() {
  var toc = document.getElementById("TOC");
  var small_sidebar = document.querySelector("#quarto-sidebar .sidebar-menu-container");
  var right_sidebar = document.getElementById("quarto-margin-sidebar");

  if (window.innerWidth < 768) {
    small_sidebar.append(toc);
  } else {
    right_sidebar.append(toc);
  }
}
move_sidebar();
window.onresize = move_sidebar;
</script><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./16-confirmationbias.html" class="pagination-link" aria-label="Confirmation Bias and Organized Skepticism">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Confirmation Bias and Organized Skepticism</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Lakens, D. (2022). Improving Your Statistical Inferences. Retrieved from https://lakens.github.io/statistical_inferences/. https://doi.org/10.5281/zenodo.6409077</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/Lakens/statistical_inferences/edit/master/17-replication.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/Lakens/statistical_inferences/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/Lakens/statistical_inferences/blob/master/17-replication.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


</body></html>