<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 13 Bias detection | Improving Your Statistical Inferences</title>
<meta name="author" content="Daniel Lakens">
<meta name="description" content="Bias can be introduced throughout the research process. It is useful to prevent this or to detect it. Some researchers recommend a skeptical attitude towards any claim you read in the scientific...">
<meta name="generator" content="bookdown 0.25 with bs4_book()">
<meta property="og:title" content="Chapter 13 Bias detection | Improving Your Statistical Inferences">
<meta property="og:type" content="book">
<meta property="og:url" content="https://lakens.github.io/statistical_inferences/bias.html">
<meta property="og:description" content="Bias can be introduced throughout the research process. It is useful to prevent this or to detect it. Some researchers recommend a skeptical attitude towards any claim you read in the scientific...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 13 Bias detection | Improving Your Statistical Inferences">
<meta name="twitter:description" content="Bias can be introduced throughout the research process. It is useful to prevent this or to detect it. Some researchers recommend a skeptical attitude towards any claim you read in the scientific...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-0MK2WTGRM3"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-0MK2WTGRM3');
    </script><link rel="shortcut icon" href="images/favicon.ico">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="pvalue.html"><span class="header-section-number">1</span> Using p-values to test a hypothesis</a></li>
<li><a class="" href="errorcontrol.html"><span class="header-section-number">2</span> Error control</a></li>
<li><a class="" href="likelihoods.html"><span class="header-section-number">3</span> Likelihoods</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></li>
<li><a class="" href="questions.html"><span class="header-section-number">5</span> Asking Statistical Questions</a></li>
<li><a class="" href="effectsize.html"><span class="header-section-number">6</span> Effect Sizes</a></li>
<li><a class="" href="confint.html"><span class="header-section-number">7</span> Confidence Intervals</a></li>
<li><a class="" href="power.html"><span class="header-section-number">8</span> Sample size justification</a></li>
<li><a class="" href="equivalencetest.html"><span class="header-section-number">9</span> Equivalence Testing and Interval Hypotheses</a></li>
<li><a class="" href="prereg.html"><span class="header-section-number">10</span> Preregistration and Transparency</a></li>
<li><a class="" href="sequential.html"><span class="header-section-number">11</span> Sequential Analysis</a></li>
<li><a class="" href="meta.html"><span class="header-section-number">12</span> Meta-analysis</a></li>
<li><a class="active" href="bias.html"><span class="header-section-number">13</span> Bias detection</a></li>
<li><a class="" href="computationalreproducibility.html"><span class="header-section-number">14</span> Computational Reproducibility</a></li>
<li><a class="" href="references.html"><span class="header-section-number">15</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/Lakens/statistical_inferences">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bias" class="section level1" number="13">
<h1>
<span class="header-section-number">13</span> Bias detection<a class="anchor" aria-label="anchor" href="#bias"><i class="fas fa-link"></i></a>
</h1>
<p>Bias can be introduced throughout the research process. It is useful to prevent this or to detect it. Some researchers recommend a skeptical attitude towards any claim you read in the scientific literature. For example, the philosopher of science Deborah Mayo <span class="citation">(<a href="references.html#ref-mayo_statistical_2018" role="doc-biblioref">2018</a>)</span> writes: "Confronted with the statistical news flash of the day, your first question is: Are the results due to selective reporting, cherry picking, or any number of other similar ruses?". You might not make yourself very popular if this is the first question you ask a speaker at the next scientific conference you are attending, but at the same time it would be naïve to ignore the fact that researchers more or less intentionally introduce bias into their claims.</p>
<p>At the most extreme end of practices that introduce bias into scientific research is <strong>research misconduct</strong>: Making up data or results, or changing or omitting data or results such that the research isn’t accurately represented in the research record. For example, <a href="https://en.wikipedia.org/wiki/Andrew_Wakefield">Andrew Wakefield</a> authored a fraudulent paper in 1998 that claimed a link between the measles, mumps, and rubella (MMR) vaccine and autism. It was retracted in 2010, but only after it caused damage to trust in vaccines among some parts of the general population. The website Retraction Watch maintains a <a href="retractiondatabase.org">database</a> that tracks reasons why scientific papers are retracted, including data fabrication. It is unknown how often data fabrication occurs in practice, but some estimates suggest that almost 2% of scientists have fabricated, falsified or modified data or results at least once <span class="citation">(<a href="references.html#ref-fanelli_how_2009" role="doc-biblioref">Fanelli, 2009</a>)</span>.</p>
<p>A different category of mistakes are statistical reporting errors, which range from reporting incorrect degrees of freedom, to reporting <em>p</em> = 0.056 as <em>p</em> &lt; 0.05 <span class="citation">(<a href="references.html#ref-nuijten_prevalence_2015" role="doc-biblioref">Nuijten et al., 2015</a>)</span>. Although we should do our best to prevent errors, everyone makes them, and data and code sharing become more common, it will become easier to detect errors in the work of other researchers. As Dorothy Bishop <span class="citation">(<a href="references.html#ref-bishop_fallibility_2018" role="doc-biblioref">2018</a>)</span> writes: "As open science becomes increasingly the norm, we will find that everyone is fallible. The reputations of scientists will depend not on whether there are flaws in their research, but on how they respond when those flaws are noted."</p>
<p><a href="http://statcheck.io/">Statcheck</a> is software that automatically extracts statistics from articles and recomputes their <em>p</em>-values, as long as statistics are reported following guidelines from the American Psychological Association (APA). It checks if the reported statistics are internally consistent: Given the test statistics and degrees of freedom, is the reported <em>p</em>-value accurate? If it is, that makes it less likely that you have made a mistake (although it does not prevent coherent mistakes!) and if it is not, you should check if all the information in your statistical test is accurate. Statcheck is not perfect, and it will make Type 1 errors where it flags something as an error when it actually is not, but it is an easy to use tool to check your articles before you submit them for publication.</p>
<p>Some inconsistencies in data are less easy to automatically detect, but can be identified manually. For example, <span class="citation">N. J. L. Brown &amp; Heathers (<a href="references.html#ref-brown_grim_2017" role="doc-biblioref">2017</a>)</span> show that many papers report means that are not possible given the sample size (known as the <a href="http://nickbrown.fr/GRIM"><em>GRIM</em> test</a>). For example, Matti Heino noticed in a <a href="https://mattiheino.com/2016/11/13/legacy-of-psychology/">blog post</a> that three of the reported means in the table in a classic study by Festinger and Carlsmith are mathematically impossible. With 20 observations per condition, and a scale from -5 to 5, all means should end in a multiple of 1/20, or 0.05. The three means ending in X.X8 or X.X2 are not consistent with the reported sample size and scale. Of course, such inconsistencies can be due to failing to report that there was missing data for some of the questions, but the GRIM test has also been used to uncover <a href="https://en.wikipedia.org/wiki/GRIM_test">scientific misconduct</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:festinger"></span>
<img src="images/festinger_carlsmith.png" alt="Screenshot of the table reporting the main results from Festinger and Carlsmith, 1959" width="100%"><p class="caption">
Figure 13.1: Screenshot of the table reporting the main results from Festinger and Carlsmith, 1959
</p>
</div>
<div id="publication-bias" class="section level2" number="13.1">
<h2>
<span class="header-section-number">13.1</span> Publication bias<a class="anchor" aria-label="anchor" href="#publication-bias"><i class="fas fa-link"></i></a>
</h2>
<p>Publication bias is one of the biggest challenges that science faces. <strong>Publication bias</strong> is the practice of selectively submitting and publishing scientific research, often based on whether or not the results are ‘statistically significant’ or not. The scientific literature is dominated by these statistically significant results. At the same time, we know that many studies researchers perform do not yield significant results. When scientists only have access to significant results, but not to all results, they are lacking a complete overview of the evidence for a hypothesis. In extreme cases, selective reporting can lead to a situation where there are hundreds of statistically significant results in the published literature, but no true effect because there are even more non-significant studies that are not shared. This is known as the <strong>file-drawer problem</strong>, when non-significant results are hidden away in file-drawers (or nowadays, folders on your computer) and not available to the scientific community. Every scientist should work towards solving the publication bias, because it is extremely difficult to learn what is likely to be true as long as scientists do not share all their results.</p>
<p>Publication bias can only be fixed by making all your research results available to fellow scientists, irrespective of the <em>p</em>-value of the main hypothesis test. Registered Reports are one way to combat publication bias, as this type of scientic article is reviewed based on the introduction, method, and statistical analysis plan, before the data is collected. After peer review, the article can get an 'in principle acceptance', which means that as long as the research plan is followed, the article will be published, regardless of the results. Not surprisingly, as shown in Figure @ref(fig:fig.1.3.1), an analysis of the first Registered Reports revealed that 31 out of 71 (44%) observed positive results, compared to 146 out of 152 (96%) of comparable standard scientific articles published during the same time period <span class="citation">(<a href="references.html#ref-scheel_excess_2021" role="doc-biblioref">Scheel, Schijen, et al., 2021</a>)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:scheel"></span>
<img src="images/scheel.png" alt="Positive result rates for standard reports and Registered Reports. Error bars indicate 95% confidence intervals around the observed positive result rate." width="100%"><p class="caption">
Figure 13.2: Positive result rates for standard reports and Registered Reports. Error bars indicate 95% confidence intervals around the observed positive result rate.
</p>
</div>
<p>In the past, Registered Reports did not exist, and scientisits did not share all results <span class="citation">(<a href="references.html#ref-franco_publication_2014" role="doc-biblioref">Franco et al., 2014</a>)</span>, and as a consequence, we have to try to detect the extent to which publication bias impacts our ability to accurately evaluate the literature. Meta-analyses should always carefully examine the impact of publication bias on the meta-analytic effect size estimate - even though only an estimated 57% of meta-analyses in Psychological Bulletin report that they assessed publication bias <span class="citation">(<a href="references.html#ref-polanin_transparency_2020" role="doc-biblioref">Polanin et al., 2020</a>)</span>. Several techniques to detect publication bias have been developed, and this continues to be a very active field of research. All techniques are based on specific assumptions, which you should consider before applying a test <span class="citation">(<a href="references.html#ref-carter_correcting_2019" role="doc-biblioref">Carter et al., 2019</a>)</span>. There is no silver bullet: None of these techniques can fix publication bias. None of them can tell you with certainty what the true meta-analytic effect size is corrected for publication bias. The best these methods can do is detect publication bias caused by specific mechanisms, under specific conditions. Publication bias can be detected, but it can not be corrected.</p>
<p>In the chapter on <a href="likelihoods.html#likelihoods">likelihoods</a> we saw how mixed results are to be expected, and can be strong evidence for the alternative hypothesis. It is not only the case that mixed results should be expected, but exclusively observing statistically significant results, especially when the statistical power is low, is very surprising. With the commonly used lower limit for statistical power of 80%, a non-significant result in one out of five studies when there is a true effect. Some researchers have pointed out that <em>not</em> finding mixed results can be very unlikely (or ‘too good to be true’) in a set of studies <span class="citation">(<a href="references.html#ref-francis_frequency_2014" role="doc-biblioref">Francis, 2014</a>; <a href="references.html#ref-schimmack_ironic_2012" role="doc-biblioref">Schimmack, 2012</a>)</span>. We don’t have a very good feeling for what real patterns of studies look like, because we are continuously exposed to a scientific literature that does not reflect reality. Almost all multiple study papers in the scientific literature present only statistically significant results, even though this is unlikely.</p>
<p>The <a href="http://shiny.ieis.tue.nl/mixed_results_likelihood/">online Shiny app we used to compute binomial likelihoods</a> displays, if you scroll to the bottom of the page, binomial probabilities to find multiple significant findings given a specific assumption about the power of the tests. Francis <span class="citation">(<a href="references.html#ref-francis_frequency_2014" role="doc-biblioref">Francis, 2014</a>)</span> used these binomial likelihoods to calculate the test of excessive significance <span class="citation">(<a href="references.html#ref-ioannidis_exploratory_2007" role="doc-biblioref">Ioannidis &amp; Trikalinos, 2007</a>)</span> for 44 articles published in the journal Psychological Science between 2009 and 2012 that contained four studies or more. He found that for 36 of these articles, the likelihood of observing four significant results, given the average power computed based on the observed effect sizes, was less than 10%. Given his choice of an alpha of 0.10, this binomial probability is a hypothesis test, and allows the claims (at a 10% alpha level) that whenever the binomial probability of the number of statistically significant results is lower than 10%, the data is surprising, and we can reject the hypothesis that this is an unbiased set of studies. In other words, it is unlikely that this many significant results would be observed, suggesting that publication bias or other selection effects have played a role in these articles.</p>
<p>One of these 44 articles had been co-authored by myself <span class="citation">(<a href="references.html#ref-jostmann_weight_2009" role="doc-biblioref">Jostmann et al., 2009</a>)</span>. At this time, I knew little about statistical power and publication bias, and beyond accused of improper scientific conduct was stressful. And yet, the accusations were correct - we had selectively reported results, and selectively reported analyses that worked. Having received virtually no training on this topic, we educated ourselves, and uploaded an unpublished study to the website psychfiledrawer.org (which no longer exists) to share our filedrawer. Some years later, we assisted when Many Labs 3 included one of the studies we had published in the set of studies they were replicating <span class="citation">(<a href="references.html#ref-ebersole_many_2016" role="doc-biblioref">Ebersole et al., 2016</a>)</span>, and when a null result was observed, we wrote "We have had to conclude that there is actually no reliable evidence for the effect" <span class="citation">(<a href="references.html#ref-jostmann_short_2016" role="doc-biblioref">Jostmann et al., 2016</a>)</span>. I hope this educational materials prevents others from making a fool of themselves like we did.</p>
</div>
<div id="bias-detection-in-meta-analysis" class="section level2" number="13.2">
<h2>
<span class="header-section-number">13.2</span> Bias detection in meta-analysis<a class="anchor" aria-label="anchor" href="#bias-detection-in-meta-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>New methods to detect publication bias are continuously developed, and old methods become outdated (even though you can still see them appear in meta-analyses). One outdated method is known as <strong>fail-safe N</strong>. The idea was to calculate the number of non-significant results one would need to have in file-drawers before an observed meta-analytic effect size estimate would no longer be statistically different from 0. It is <a href="https://handbook-5-1.cochrane.org/chapter_10/10_4_4_3_fail_safe_n.htm">no longer recommended</a>, and Becker <span class="citation">(<a href="references.html#ref-becker_failsafe_2005" role="doc-biblioref">2005</a>)</span> writes "Given the other approaches that now exist for dealing with publication bias, the failsafe N should be abandoned in favor of other, more informative analyses". Currently, the only use fail-safe N has is as a tool to identify meta-analyses that are not state-of-the-art.</p>
<p>Before we can explain a second method (Trim-and-Fill), it’s useful to explain a common way to visualize meta-analyses, known as a <strong>funnel plot</strong>. In a funnel plot, the x-axis is used to plot the effect size of each study, and the y-axis is used to plot the ‘precision’ of each effect size. Typically, the y-axis is used to plot the standard error of each effect size estimate. The larger the study, the more precise the effect size estimate, the smaller the standard error, and thus the higher up in the funnel plot the study will be, and an infinitely precise study (with a standard error of 0) would be at the top of y-axis.</p>
<p>The script below simulates meta-analyses based nsims studies, and stores all the results needed need to examine bias detection tools. In the first section of the script, statistically significant results in the desired direction are simulated, and in the second part null results are generated, which are subsequently combined. The script generates a percentage of significant results as indicated by <code>pub.bias</code> - when set to 1, all results are significant. Currently, pu.bias is set to 0.05, as there is no true effect in the simulation (<code>m1</code> and <code>m2</code> are equal, so there is no difference between the groups), so only 5% false positives should be expected. Finally, the meta-analysis is perfomed, the results are printed, and a funnel plot is created.</p>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.metafor-project.org">metafor</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/olafmersmann/truncnorm">truncnorm</a></span><span class="op">)</span>

<span class="va">nsims</span> <span class="op">&lt;-</span> <span class="fl">100</span> <span class="co"># number of simulated experiments</span>
<span class="va">pub.bias</span> <span class="op">&lt;-</span> <span class="fl">0.05</span> <span class="co"># set percentage of significant results in the literature</span>

<span class="va">m1</span> <span class="op">&lt;-</span> <span class="fl">0</span> <span class="co"># too large effects will make non-significant results extremely rare</span>
<span class="va">sd1</span> <span class="op">&lt;-</span> <span class="fl">1</span>
<span class="va">m2</span> <span class="op">&lt;-</span> <span class="fl">0</span>
<span class="va">sd2</span> <span class="op">&lt;-</span> <span class="fl">1</span>
<span class="va">metadata.sig</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>m1 <span class="op">=</span> <span class="cn">NA</span>, m2 <span class="op">=</span> <span class="cn">NA</span>, sd1 <span class="op">=</span> <span class="cn">NA</span>, sd2 <span class="op">=</span> <span class="cn">NA</span>, 
                           n1 <span class="op">=</span> <span class="cn">NA</span>, n2 <span class="op">=</span> <span class="cn">NA</span>, pvalues <span class="op">=</span> <span class="cn">NA</span>, pcurve <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>
<span class="va">metadata.nonsig</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>m1 <span class="op">=</span> <span class="cn">NA</span>, m2 <span class="op">=</span> <span class="cn">NA</span>, sd1 <span class="op">=</span> <span class="cn">NA</span>, sd2 <span class="op">=</span> <span class="cn">NA</span>, 
                              n1 <span class="op">=</span> <span class="cn">NA</span>, n2 <span class="op">=</span> <span class="cn">NA</span>, pvalues <span class="op">=</span> <span class="cn">NA</span>, pcurve <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>

<span class="co"># simulate significant effects in the expected direction</span>
<span class="kw">if</span><span class="op">(</span><span class="va">pub.bias</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span><span class="op">{</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nsims</span><span class="op">*</span><span class="va">pub.bias</span><span class="op">)</span> <span class="op">{</span> <span class="co"># for each simulated experiment</span>
  <span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="co"># reset p to 1</span>
  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu">truncnorm</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/truncnorm/man/dtruncnorm.html">rtruncnorm</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">20</span>, <span class="fl">1000</span>, <span class="fl">100</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span> <span class="co"># n based on truncated normal</span>
  <span class="kw">while</span> <span class="op">(</span><span class="va">p</span> <span class="op">&gt;</span> <span class="fl">0.025</span><span class="op">)</span> <span class="op">{</span> <span class="co"># continue simulating as along as p is not significant</span>
    <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">m1</span>, sd <span class="op">=</span> <span class="va">sd1</span><span class="op">)</span> 
    <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">m2</span>, sd <span class="op">=</span> <span class="va">sd2</span><span class="op">)</span> 
    <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alternative <span class="op">=</span> <span class="st">"greater"</span>, var.equal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">$</span><span class="va">p.value</span>
  <span class="op">}</span>
  <span class="va">metadata.sig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">metadata.sig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
  <span class="va">metadata.sig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">metadata.sig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">4</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
  <span class="va">metadata.sig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">n</span>
  <span class="va">metadata.sig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">6</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">n</span>
  <span class="va">out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, var.equal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
  <span class="va">metadata.sig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">7</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">out</span><span class="op">$</span><span class="va">p.value</span>
  <span class="va">metadata.sig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">8</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"t("</span>, <span class="va">out</span><span class="op">$</span><span class="va">parameter</span>, <span class="st">")="</span>, <span class="va">out</span><span class="op">$</span><span class="va">statistic</span><span class="op">)</span>
<span class="op">}</span><span class="op">}</span>

<span class="co"># simulate non-significant effects (two-sided)</span>
<span class="kw">if</span><span class="op">(</span><span class="va">pub.bias</span> <span class="op">&lt;</span> <span class="fl">1</span><span class="op">)</span><span class="op">{</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nsims</span><span class="op">*</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">pub.bias</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span> <span class="co"># for each simulated experiment</span>
  <span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0</span> <span class="co"># reset p to 1</span>
  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu">truncnorm</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/truncnorm/man/dtruncnorm.html">rtruncnorm</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">20</span>, <span class="fl">1000</span>, <span class="fl">100</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span>
  <span class="kw">while</span> <span class="op">(</span><span class="va">p</span> <span class="op">&lt;</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">{</span> <span class="co"># continue simulating as along as p is significant</span>
    <span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">m1</span>, sd <span class="op">=</span> <span class="va">sd1</span><span class="op">)</span> <span class="co"># produce  simulated participants</span>
    <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">m2</span>, sd <span class="op">=</span> <span class="va">sd2</span><span class="op">)</span> <span class="co"># produce  simulated participants</span>
    <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, var.equal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">$</span><span class="va">p.value</span>
  <span class="op">}</span>
  <span class="va">metadata.nonsig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">metadata.nonsig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
  <span class="va">metadata.nonsig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">metadata.nonsig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">4</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
  <span class="va">metadata.nonsig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">n</span>
  <span class="va">metadata.nonsig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">6</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">n</span>
  <span class="va">out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, var.equal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
  <span class="va">metadata.nonsig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">7</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">out</span><span class="op">$</span><span class="va">p.value</span>
  <span class="va">metadata.nonsig</span><span class="op">[</span><span class="va">i</span>, <span class="fl">8</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"t("</span>, <span class="va">out</span><span class="op">$</span><span class="va">parameter</span>, <span class="st">")="</span>, <span class="va">out</span><span class="op">$</span><span class="va">statistic</span><span class="op">)</span>
<span class="op">}</span><span class="op">}</span>

<span class="co"># Combine significant and non-significant effects</span>
<span class="va">metadata</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">metadata.nonsig</span>, <span class="va">metadata.sig</span><span class="op">)</span>

<span class="co"># Use escalc to compute effect sizes</span>
<span class="va">metadata</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://wviechtb.github.io/metafor/reference/escalc.html">escalc</a></span><span class="op">(</span>n1i <span class="op">=</span> <span class="va">n1</span>, n2i <span class="op">=</span> <span class="va">n2</span>, m1i <span class="op">=</span> <span class="va">m1</span>, m2i <span class="op">=</span> <span class="va">m2</span>, sd1i <span class="op">=</span> <span class="va">sd1</span>, 
  sd2i <span class="op">=</span> <span class="va">sd2</span>, measure <span class="op">=</span> <span class="st">"SMD"</span>, data <span class="op">=</span> <span class="va">metadata</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span><span class="op">(</span><span class="va">metadata</span><span class="op">)</span>,<span class="op">]</span><span class="op">)</span>
<span class="co"># add se for PET-PEESE analysis</span>
<span class="va">metadata</span><span class="op">$</span><span class="va">sei</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">metadata</span><span class="op">$</span><span class="va">vi</span><span class="op">)</span>

<span class="co">#Perform meta-analysis</span>
<span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">metafor</span><span class="fu">::</span><span class="fu"><a href="https://wviechtb.github.io/metafor/reference/rma.uni.html">rma</a></span><span class="op">(</span><span class="va">yi</span>, <span class="va">vi</span>, data <span class="op">=</span> <span class="va">metadata</span><span class="op">)</span>
<span class="va">result</span>

<span class="co"># Print a Funnel Plot</span>
<span class="fu">metafor</span><span class="fu">::</span><span class="fu"><a href="https://wviechtb.github.io/metafor/reference/funnel.html">funnel</a></span><span class="op">(</span><span class="va">result</span>, level <span class="op">=</span> <span class="fl">0.95</span>, refline <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="va">result</span><span class="op">$</span><span class="va">b</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, lty <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="co">#  vertical line at meta-analytic ES</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">result</span><span class="op">$</span><span class="va">b</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, y <span class="op">=</span> <span class="fl">0</span>, cex <span class="op">=</span> <span class="fl">1.5</span>, pch <span class="op">=</span> <span class="fl">17</span><span class="op">)</span> <span class="co"># add point</span></code></pre></div>
<p>Let’s start by looking at what unbiased research looks like, be running the code, keeping pu.bias at 0.05, such that only 5% Type 1 errors enter the scientific literature.</p>
<pre><code>## 
## Random-Effects Model (k = 100; tau^2 estimator: REML)
## 
## tau^2 (estimated amount of total heterogeneity): 0.0000 (SE = 0.0018)
## tau (square root of estimated tau^2 value):      0.0006
## I^2 (total heterogeneity / total variability):   0.00%
## H^2 (total variability / sampling variability):  1.00
## 
## Test for Heterogeneity:
## Q(df = 99) = 91.7310, p-val = 0.6851
## 
## Model Results:
## 
## estimate      se     zval    pval    ci.lb   ci.ub 
##  -0.0021  0.0121  -0.1775  0.8591  -0.0258  0.0215    
## 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<p>When we examine the results of the meta-analysis, we see there are 100 studies in the meta-analysis (<code>k = 100</code>), and there is no statistically significant heterogeneity (<em>p</em> = 0.69, which is not too surprising as we programmed the simulation to have a true effect size of 0, there is no heterogeneity in effect sizes). We also get the results for the meta-analysis. The meta-analytic estimate is d = -0.002, which is very close to 0 (as it should be, because the true effect size is indeed 0). The standard error around this estimate is 0.012. With 100 studies, we have a very accurate estimate of the true effect size. The Z-value for the test against d = 0 is -0.177, and the <em>p</em>-value for this test is 0.86. We can not reject the hypothesis that the true effect size is 0. The CI around the effect size estimate (-0.026, 0.021) includes 0.</p>
<p>If we examine the funnel plot in Figure <a href="bias.html#fig:funnel1">13.3</a> we see each study represented as a dot. The larger the sample size, the higher up in the plot, and the smaller the sample size, the lower in the plot. The white pyramid represent the area within which a study is not statistically significant, because the observed effect size (x-axis) is not far enough removed from 0 such that the confidence interval around the observed effect size would exclude 0. The lower the standard error, the more narrow the confidence interval, and the smaller the effect sizes can can be statistically significant. At the same time, the smaller the standard error, the closer the effect size will be to the true effect size, so the less likely we will see effects far away from 0. We should expect 95% of the effect size estimates to fall within the funnel, if it is centered on the true effect size. We see only a few studies (five, to be exact) fall outside the white pyramid on the right side of the plot. These are the 5% significant results that we programmed in the simulation. Note that all 5 of these studies are false positives, as there is no true effect. If there was a true effect (e.g., d = 0.5, my changing <code>m1 &lt;- 0</code> in the simulation to <code>m1 &lt;- 0.5</code> the cloud of points would move to the right, and be centered on 0.5 instead of 0.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:funnel1"></span>
<img src="13-bias_files/figure-html/funnel1-1.png" alt="Funnel plot of unbiased null results." width="100%"><p class="caption">
Figure 13.3: Funnel plot of unbiased null results.
</p>
</div>
<p>We can now compare the unbiased meta-analysis above with a biased meta-analysis. We can simulate a situation with extreme publication bias. Building on the estimate by <span class="citation">Scheel, Schijen, et al. (<a href="references.html#ref-scheel_excess_2021" role="doc-biblioref">2021</a>)</span>, let's assume 96% of the results show positive results, by setting <code>pub.bias &lt;- 0.96</code> in the code. We keep both means at 0, so there still is not real effect, but we will end up with mainly Type 1 errors in the predicted direction in the final set of studies. We can simulate biased results, and perform the meta-analysis to see if the statistical inference might be misleading.</p>
<pre><code>## 
## Random-Effects Model (k = 100; tau^2 estimator: REML)
## 
## tau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0019)
## tau (square root of estimated tau^2 value):      0
## I^2 (total heterogeneity / total variability):   0.00%
## H^2 (total variability / sampling variability):  1.00
## 
## Test for Heterogeneity:
## Q(df = 99) = 77.6540, p-val = 0.9445
## 
## Model Results:
## 
## estimate      se     zval    pval   ci.lb   ci.ub 
##   0.2701  0.0125  21.6075  &lt;.0001  0.2456  0.2946  *** 
## 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<p>This time, the results of the meta-analysis paint a different picture. The meta-analytic estimate is d = 0.27, while the true effect size in the simulation is 0, so it is clearly upwardly biased due to selection of significant studies. The effect is not too large, but it is statistically significant, with a Z-value for the test against d = 0 of 21.607, and a <em>p</em>-value of 0. Now, we can reject the hypothesis that the true effect size is 0, even though this inference is misleading, due to bias in the set of studies we have meta-analyzed. The confidence interval around the effect size estimate (0.246, 0.295) does not include 0.</p>
<p>The biased nature of the set of studies we have analyzed becomes clear if we examine the funnel plot in Figure <a href="bias.html#fig:funnel2">13.4</a>. The pattern is quite peculiar. We see four unbiased null results, as we programmed into the simulation, but the remainder of the 96 studies are statistically significant, even though the null is true. We see most studies fall just on the edge of the white pyramid. Because <em>p</em>-values are uniformaly distributed under the null, the Type 1 errors we observe often have <em>p</em>-values in the range of 0.02 to 0.05, unlike what we would expect if there was a true effect. These just significant <em>p</em>-values fall just outside of the white pyramid. The large the study, the smaller the effect size that is significant. The fact that the effect sizes do not vary around a single true effect size (e.g., d = 0 or d = 0.5), but that effect sizes become smaller, the larger the sample size (or the smaller the standard error), is a strong indicator of bias. The vertical dotted line and black triangle at the top of the plot illustrate the observed (upwardly biased) meta-analytic effect size estimate.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:funnel2"></span>
<img src="13-bias_files/figure-html/funnel2-1.png" alt="Funnel plot of biased null results with mostly significant results." width="100%"><p class="caption">
Figure 13.4: Funnel plot of biased null results with mostly significant results.
</p>
</div>
<p>One might wonder if such extreme bias ever really emerges in scientific research. It does. In Figure <a href="bias.html#fig:carterbias">13.5</a> we see a funnel plot by <span class="citation">Carter &amp; McCullough (<a href="references.html#ref-carter_publication_2014" role="doc-biblioref">2014</a>)</span> who examined bias in 198 published studies testing the 'ego-depletion' effect, the idea that self-control relies on a limited resource. Do you notice any similarities to the extremely biased meta-analysis we simulated above? You might not be surprised that, even though before 2015 researchers thought there was a large and reliable literature demonstrating ego-depletion effects, a Registered Replication report yielded a non-significant effect size estimate <span class="citation">(<a href="references.html#ref-hagger_multilab_2016" role="doc-biblioref">Hagger et al., 2016</a>)</span>, and even when the original researchers tried to replicate their own work, they failed to observe a significant effect of ego-depletion <span class="citation">(<a href="references.html#ref-vohs_multisite_2021" role="doc-biblioref">Vohs et al., 2021</a>)</span>. Imagine the huge amount of wasted time, effort, and money on a literature that was completely based on bias in scientific research. Obviously, such research waste has ethical implications, and researchers need to take their responsibility to prevent such waste in the future.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:carterbias"></span>
<img src="images/carterfunnel.png" alt="Funnel plot from Carter &amp; McCullough (2014) vizualizing bias in 198 published tests of the ego-depletion effect." width="100%"><p class="caption">
Figure 13.5: Funnel plot from Carter &amp; McCullough (2014) vizualizing bias in 198 published tests of the ego-depletion effect.
</p>
</div>
<p>We can also see signs of bias in the forest plot for a meta-analysis. In Figure <a href="bias.html#fig:twoforestplot">13.6</a> two forest plots are plotted side by side. The left forest plot is based on unbiased data, the right forest plot is based on biased data. The forest plots are a bit big with 100 studies, but we see that in the left forest plot effects randomly vary around 0 as they should. On the right, beyond the first four studies, all confidence intervals magically just exclude an effect of 0.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:twoforestplot"></span>
<img src="13-bias_files/figure-html/twoforestplot-1.png" alt="Forest plot of unbiased meta-analysis (left) and biased meta-analysies (right)." width="100%"><p class="caption">
Figure 13.6: Forest plot of unbiased meta-analysis (left) and biased meta-analysies (right).
</p>
</div>
<p>When there is publication bias because researchers only publish statistically significant results (p &lt; alpha), and you calculate the effect size in a meta-analysis, the meta-analytic effect size estimate is <strong>higher</strong> when there is publication bias (where researchers publish only effects with p &lt; alpha) compared to when there is no publication bias. This is because publication bias filters out the smaller (non-significant) effect sizes. which are then not included in the computation of the meta-analytic effect size. This leads to a meta-analytic effect size estimate that is larger than the true population effect size. With strong publication bias, we know the meta-analytic effect size is inflated, but we don't know by how much. The true effect size could just be a bit smaller, but the true effect size could also be 0, such as in the case of the ego-deplation literature.</p>
</div>
<div id="trim-and-fill" class="section level2" number="13.3">
<h2>
<span class="header-section-number">13.3</span> Trim and Fill<a class="anchor" aria-label="anchor" href="#trim-and-fill"><i class="fas fa-link"></i></a>
</h2>
<p>Trim and fill is a technique that aims to augment a dataset by adding hypothetical ‘missing’ studies (that may be in the ‘file-drawer’). The procedure starts by removing (‘trimming’) small studies that bias the meta-analytic effect size, then estimates the true effect size, and ends with ‘filling’ in a funnel plot with studies that are assumed to be missing due to publication bias. In the Figure <a href="bias.html#fig:trimfill1">13.7</a>, you can see the same funnel plot as above, but now with added hypothetical studies (the unfilled circles which represent ‘imputed’ studies). If you look closely, you’ll see these points each have a mirror image on the opposite side of the meta-analytic effect size estimate (this is clearest in the lower half of the funnel plot). If we examine the result of the meta-analysis that includes these imputed studies, we see that trim and fill successfully alerts us to the fact that the meta-analysis is biased (if not, it would not add imputed studies) but it fails misearably in correcting the effect size estimate. In the funnel plot, we see the original (biased) effect size estimate indicated by the triangle, and the meta-analytic effect size estimate adjusted with the trim-and-fill method (indicated by the black circle). We see the meta-analytic effect size estimate is a bit lower, but given that the true effect size in the simulation was 0, the adjustment is clearly not sufficient.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:trimfill1"></span>
<img src="13-bias_files/figure-html/trimfill1-1.png" alt="Funnel plot with assumed missing effects added through trim-and-fill." width="100%"><p class="caption">
Figure 13.7: Funnel plot with assumed missing effects added through trim-and-fill.
</p>
</div>
<p>Trim-and-fill is not very good under many realistic publication bias scenarios. The method is criticized for its reliance on the strong assumption of symmetry in the funnel plot. When publication bias is based on the <em>p</em>-value of the study (arguably the most important source of publication bias in many fields) the trim-and-fill method does not perform well enough to yield a corrected meta-analytic effect size estimate that is close to the true effect size <span class="citation">(<a href="references.html#ref-peters_performance_2007" role="doc-biblioref">Peters et al., 2007</a>; <a href="references.html#ref-terrin_adjusting_2003" role="doc-biblioref">Terrin et al., 2003</a>)</span>. When the assumptions are met, it can be used as a <strong>sensitivity analysis.</strong> Researchers should not report the trim-and-fill corrected effect size estimate as a realistic estimate of the unbiased effect size.</p>
</div>
<div id="pet-peese" class="section level2" number="13.4">
<h2>
<span class="header-section-number">13.4</span> PET-PEESE<a class="anchor" aria-label="anchor" href="#pet-peese"><i class="fas fa-link"></i></a>
</h2>
<p>A novel class of solutions to publication bias is <strong>meta-regression</strong>. Instead of plotting a line through individual data-points, in meta-regression a line is plotted through data points that each represent a study. As with normal regression, the more data meta-regression is based on, the more precise the estimate is, and therefore, the more studies in a meta-analysis, the better meta-regression will work in practice. If the number of studies is small, all bias detection tests lose power, and this is something that one should keep in mind when using meta-regression. Furthermore, regression requires sufficient variation in the data, which in the case of meta-regression means a wide range of sample sizes (recommendations indicate meta-regression performs well if studies have a range from 15 to 200 participants in each group – which is not typical for most research areas in psychology). Meta-regression techniques try to estimate the population effect size if precision was perfect (so when the standard error = 0).</p>
<p>One meta-regression technique is known as PET-PEESE (<span class="citation">Stanley &amp; Doucouliagos (<a href="references.html#ref-stanley_meta-regression_2014" role="doc-biblioref">2014</a>)</span>; <span class="citation">Stanley et al. (<a href="references.html#ref-stanley_finding_2017" role="doc-biblioref">2017</a>)</span>). It consists of a ‘precision-effect-test’ (PET) which can be used in a Neyman-Pearson hypothesis testing framework to test whether the meta-regression estimate can reject an effect size of 0 based on the 95% CI around the PET estimate at the intercept SE = 0. Note that when the confidence interval is very wide due to a small number of observations, this test might have low power, and have an a-priori low probability of rejecting the null effect. The estimated effect size for PET is calculated with: <span class="math inline">\(d = β_0 + β_1SE_i + _ui\)</span> where d is the estimated effect size, SE is the standard error, and the equation is estimated using weighted least squares (WLS), with 1/SE2i as the weights. The PET estimate underestimates the effect size when there is a true effect. Therefore, the PET-PEESE procedure recommends first using PET to test whether the null can be rejected, and if so, then PEESE should be used to estimate the meta-analytic effect size. In PEESE, the standard error (used in PET) is replaced by the variance (i.e., the standard error squared), which <span class="citation">Stanley &amp; Doucouliagos (<a href="references.html#ref-stanley_meta-regression_2014" role="doc-biblioref">2014</a>)</span> find reduces the bias of the estimated meta-regression intercept.</p>
<p>PET-PEESE has limitations, as all bias detection techniques have. The biggest limitations are that it does not work well when there are few studies, all studies in a meta-analysis have small sample sizes, or when there is large heterogeneity in the meta-analysis <span class="citation">(@ <a href="references.html#ref-stanley_finding_2017" role="doc-biblioref">Stanley et al., 2017</a>)</span>. When these situations apply (and they will in practice), PET-PEESE might not be a good approach. Furthermore, there are some situations where there might be a correlation between sample size and precision, which in practice will often be linked to heterogeneity in the effect sizes included in a meta-analysis. For example, if true effects are different across studies, and people perform power analyses with accurate information about the expected true effect size, large effect sizes in a meta-analysis will have small sample sizes, and small effects will have large sample sizes. Meta-regression is, like normal regression, a way to test for an association, but you need to think about the causal mechanism behind the association.</p>
<p>Let’s explore how PET-PEESE meta-regression attempts to give us an unbiased effect size estimate, under specific assumptions of how publication bias is caused. In Figure <a href="bias.html#fig:petpeese">13.8</a> we once again see the funnel plot, not complemented with 2 additional lines through the plots. The vertical line at d = 0.27 is the meta-analytic effect size estimate, which is upwardly biased because we are averaging over statistically significant studies only. There are 2 additional lines, which are the meta-regression lines for PET-PEESE based on the formulas detailed previously. The straight diagonal line gives us the PET estimate at a SE of 0 (an infinite sample, at the top of the plot), indicated by the circle. The dotted line around this PET estimate is the 95% confidence interval for the estimate. In this case, the 95% CI contains 0, which means that based on the PET estimate of d = 0.02, we cannot reject a meta-analytic effect size of 0. Note that even with 100 studies, the 95% CI is quite wide. Meta-regression is, just like normal regression, only as accurate as the data we have data. This is one limitation of PET-PEESE meta-regression: With small numbers of studies in the meta-analysis, it has low accuracy. If we had been able to reject the null based on the PET estimate, we would then have used the PEESE estimate of d = 0.17 for the meta-analytic effect size, corrected for bias (while never knowing whether the model underlying the PEESE estimate corresponded to the true bias generating mechanisms in the meta-analysis, and thus if the meta-analytic estimate was accurate).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:petpeese"></span>
<img src="13-bias_files/figure-html/petpeese-1.png" alt="Funnel plot with PET_PEESE regression lines." width="100%"><p class="caption">
Figure 13.8: Funnel plot with PET_PEESE regression lines.
</p>
</div>
</div>
<div id="p-value-meta-analysis" class="section level2" number="13.5">
<h2>
<span class="header-section-number">13.5</span> P-value meta-analysis<a class="anchor" aria-label="anchor" href="#p-value-meta-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>In addition to a meta-analysis of effect sizes, it is possible to perform a meta-analysis of <em>p</em>-values. The first of such approaches is known as the <a href="https://en.wikipedia.org/wiki/Fisher%27s_method"><strong>Fisher's combined probability test</strong></a>, and more recent bias detection tests such as <em>p</em>-curve analysis <span class="citation">(<a href="references.html#ref-simonsohn_p-curve_2014" role="doc-biblioref">Simonsohn et al., 2014</a>)</span> and <em>p</em>-uniform* <span class="citation">(<a href="references.html#ref-aert_correcting_2018" role="doc-biblioref">Aert &amp; Assen, 2018</a>)</span> build on this idea. These two techniques are an example of selection model approaches to test and adjust for meta-analysis <span class="citation">(<a href="references.html#ref-iyengar_selection_1988" role="doc-biblioref">Iyengar &amp; Greenhouse, 1988</a>)</span>, where a model about the data generating process of the effect sizes is combined with a selection model of how publication bias impacts which effect sizes become part of the scientific literature.</p>
<p><em>P</em>-curve analysis tests whether the <em>p</em>-value distribution is flatter than what would be expected if the studies you analyze had 33% power (which suggests the distribution looks more like one expected when the null-hypothesis is true), or more right-skewed than a uniform <em>p</em>-value distribution (which suggests the studies might have examined a true effect and had at least some power). As an example, let’s consider Figure 3 from Simonsohn and colleagues <span class="citation">(<a href="references.html#ref-simonsohn_p-curve_2014" role="doc-biblioref">2014</a>)</span>. The authors compared 20 papers in the Journal of Personality and Social Psychology that used a covariate in the analysis, and 20 studies that did not use a covariate. The authors suspected that researchers might add a covariate in their analyses to try to find a <em>p</em>-value smaller than 0.05, when the first analysis they tried did not yield a significant effect.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pcurve"></span>
<img src="images/pcurve.png" alt="Figure 3 from Simonsohn et al (2014) showing a p-curve with and without bias." width="100%"><p class="caption">
Figure 13.9: Figure 3 from Simonsohn et al (2014) showing a p-curve with and without bias.
</p>
</div>
<p>The <em>p</em>-curve distribution of the observed <em>p</em>-values is represented by five points in the blue line. <em>P</em>-curve analysis is performed <em>only</em> on statistically significant results, based on the assumption that these are always published, and thus that this part of the <em>p</em>-value distribution contains all studies that were performed. The 5 points illustrate the percentage of <em>p</em>-values between 0 and 0.01, 0.01 and 0.02, 0.02 and 0.03, 0.03 and 0.04, and 0.04 and 0.05. In the figure on the right, you see a relatively normal right-skewed <em>p</em>-value distribution, with more low than high <em>p</em>-values. The <em>p</em>-curve analysis shows the blue line in the right figure is more right-skewed than the uniform red line (where the red line is the uniform <em>p</em>-value distribution expected if there was no effect). Simonsohn and colleagues summarize this pattern as an indication the set of studies has 'evidential value', but this terminolgy is somewhat misleading. The formally correct interpretation is that we can reject a <em>p</em>-value distribution as expected when the null-hypothesis was true in all studies included in the <em>p</em>-curve analysis. Rejecting a uniform <em>p</em>-value distribution does not automatically mean there is evidence for the theorized effect (e.g., the pattern could be caused by a mix of null effects and a small subset of studies that show an effect due to a methodological confound).</p>
<p>In the left figure we see the opposite pattern, with mainly high <em>p</em>-values around 0.05, and almost no <em>p</em>-values around 0.01. Because the blue line is significantly less right-skewed than the green line, the <em>p</em>-curve analysis suggests this set of studies is the result of selection bias, but was not generated by a set of sufficiently powered studies. P-curve analysis is a useful tool. But it is important to correctly interpret what a <em>p</em>-curve analysis can tell you. A right-skewed <em>p</em>-curve does not prove that there is no bias, or that the theoretical hypothesis is true. A flat <em>p</em>-curve does not prove that the theory is incorrect, but it does show that the studies that were meta-analyzed look more like the pattern that would be expected if the null hypothesis was true, and there was selection bias.</p>
<p>The script stores all the test statistics for the 100 simulated <em>t</em>-tests that are included in the meta-analysis. The first few rows look like:</p>
<pre><code>## t(136)=0.208132209831132
## t(456)=-1.20115958535433
## t(58)=0.0422284763301259
## t(358)=0.0775200850900646
## t(188)=2.43353676652346</code></pre>
<p>Print all test results with <code>cat(metadata$pcurve, sep = "\n")</code>, and go to the online <em>p</em>-curve app at <a href="http://www.p-curve.com/app4/" class="uri">http://www.p-curve.com/app4/</a>. Paste all the test results, and click the ‘Make the p-curve’ button. Note that the <em>p</em>-curve app will only yield a result when there are <em>p</em>-values smaller than 0.05 - if all test statistics yield a <em>p</em> &gt; 0.05, the <em>p</em>-curve cannot be computed, as these tests are ignored.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pcurveresult"></span>
<img src="images/pcurveresult.png" alt="Result of the p-curve analysis of the biased studies." width="100%"><p class="caption">
Figure 13.10: Result of the p-curve analysis of the biased studies.
</p>
</div>
<p>The distribution of <em>p</em>-values clearly looks like it comes from a uniform distribution (as it indeed does), and the statistical test indicates we can reject a <em>p</em>-value distribution as steep or steeper as would be generated by a set of studies with 33% power, <em>p</em> &lt; 0.0001. The app also provides an estimate of the average power of the tests that generated the observed <em>p</em>-value distribution, 5%, which is indeed correct. Therefore, we can conclude these studies, even though many effects are statistically significant, are more in line with selective reporting of Type 1 errors, than with a <em>p</em>-value distribution that should be expected if there was a true effect that was studied with sufficient statistical power. The theory might still be true, but the set of studies we have analyzed here do not provide support for the theory.</p>
<p>A similar meta-analytic technique is <em>p</em>-uniform*, which can also be used to estimate a bias-adjusted meta-analytic effect size estimate. Below, we see the output of the <em>p</em>-uniform* which estimates the bias-corrected effect size to be <em>d</em> = 0.0126, which is not statistically different from 0, <em>p</em> = 0.3857.</p>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">puniform</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/puniform/man/puniform.html">puniform</a></span><span class="op">(</span>m1i <span class="op">=</span> <span class="va">metadata</span><span class="op">$</span><span class="va">m1</span>, m2i <span class="op">=</span> <span class="va">metadata</span><span class="op">$</span><span class="va">m2</span>, n1i <span class="op">=</span> <span class="va">metadata</span><span class="op">$</span><span class="va">n1</span>, 
  n2i <span class="op">=</span> <span class="va">metadata</span><span class="op">$</span><span class="va">n2</span>, sd1i <span class="op">=</span> <span class="va">metadata</span><span class="op">$</span><span class="va">sd1</span>, sd2i <span class="op">=</span> <span class="va">metadata</span><span class="op">$</span><span class="va">sd2</span>, side <span class="op">=</span> <span class="st">"right"</span><span class="op">)</span></code></pre></div>
<pre><code>## 
## Method: P
## 
## Effect size estimation p-uniform
## 
##        est     ci.lb     ci.ub       L.0      pval      ksig
##     0.0126   -0.0811    0.0887   -0.2904    0.3857        96
## 
## ===
## 
## Publication bias test p-uniform
## 
##       L.pb    pval
##     7.9976   &lt;.001
## 
## ===
## 
## Fixed-effect meta-analysis
## 
##     est.fe     se.fe   zval.fe pval.fe  ci.lb.fe  ci.ub.fe     Qstat     Qpval
##     0.2701    0.0125   21.6025   &lt;.001    0.2456    0.2946   77.6031     0.945</code></pre>
<p>An even more novel technique that also meta-analyzes the <em>p</em>-values from individual studies is <em>z</em>-curve, which is basically a meta-analysis of observed power <span class="citation">(<a href="references.html#ref-bartos_z-curve20_2020" role="doc-biblioref">Bartoš &amp; Schimmack, 2020</a>; <a href="references.html#ref-brunner_estimating_2020" role="doc-biblioref">Brunner &amp; Schimmack, 2020</a>; <a href="references.html#ref-sotola_garbage_2022" role="doc-biblioref">Sotola, 2022</a>)</span>. Like a traditional meta-analysis, <em>z</em>-curve transforms observed test results (<em>p</em>-values) into <em>z</em>-scores. Using mixtures of normal distributions centered at means 0 to 6, <em>z</em>-curve aims to estimate the average power of the studies. The newest version of <em>z</em>-curve then calculates the <em>observed discovery rate</em> (ODR: the percentage of significant results, or the observed power), the <em>expected discovery rate</em> (EDR: the proportion of the area under the curve on the right side of the significance criterion) and the expected replication rate (ERR: the expected proportion of successfully replicated significant studies from all significant studies). <em>Z</em>-curve is able to correct for selection bias for positive results (under specific assumptions), and can estimate the EDR and ERR using only the significant <em>p</em>-values.</p>
<p>To examine the presence of bias, it is preferable to submit non-significant and significant <em>p</em>-values to a <em>z</em>-curve analysis, even if only the significant <em>p</em>-values are used to produce estimates. Publication bias can then be examined by comparing the ODR to the EDR. Since our simulated studies have bias, the <em>z</em>-curve analysis should be able to indicate this.</p>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Perform the z-curve analysis using the z-curve package</span>
<span class="va">z_res</span> <span class="op">&lt;-</span> <span class="fu">zcurve</span><span class="fu">::</span><span class="fu"><a href="https://fbartos.github.io/zcurve/reference/zcurve.html">zcurve</a></span><span class="op">(</span>p <span class="op">=</span> <span class="va">metadata</span><span class="op">$</span><span class="va">pvalues</span>, method <span class="op">=</span> <span class="st">"EM"</span>, bootstrap <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">z_res</span>, all <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<pre><code>## Call:
## zcurve::zcurve(p = metadata$pvalues, method = "EM", bootstrap = 1000)
## 
## model: EM via EM
## 
##               Estimate  l.CI   u.CI
## ERR              0.052 0.025  0.160
## EDR              0.053 0.050  0.119
## Soric FDR        0.947 0.389  1.000
## File Drawer R   17.987 7.399 19.000
## Expected N        1823   806   1920
## Missing N         1723   706   1820
## 
## Model converged in 38 + 205 iterations
## Fitted using 96 p-values. 100 supplied, 96 significant (ODR = 0.96, 95% CI [0.89, 0.99]).
## Q = -6.69, 95% CI[-23.63, 11.25]</code></pre>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">z_res</span>, annotation <span class="op">=</span> <span class="cn">TRUE</span>, CI <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="13-bias_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;"></div>
<p>We see that 96 out of 100 studies were significant, which makes the observed discovery rate (ODR), or observed power (across all these studies with different sample sizes) 0.96, 95% CI[0.89;0.99]. The expected discovery rate (EDR) is only 0.053, which differs statistically differ from the observed discovery rate, as indicated by the fact that the confidence interval of the EDR does not overlap with the ODR of 0.96. This means there is clear indication of selection bias based on the <em>z</em>-curve analysis. The expected replicability rate for these studies is only 0.052, which is in line with the expectation that we will only observe 5% Type 1 errors. Thus, even though we only entered significant <em>p</em>-values, <em>z</em>-curve analysis correctly suggests that we should not expect these results to replicate.</p>
</div>
<div id="conclusion" class="section level2" number="13.6">
<h2>
<span class="header-section-number">13.6</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h2>
<p>Publication bias is a big problem in science. It is present in almost all meta-analyses performed on the primary hypothesis test in scientific articles, because these articles are much more likely to be submitted and accepted for publication if this test is statistically significant. Meta-analytic effect size estimates that are not adjusted for bias will almost always overestimate the true effect size, but bias-adjusted effect sizes might still be misleading. After we messed up the scientific literature through publication bias, there is no way to know whether we are computed accurate meta-analytic effect sizes estimates from a literature. Publication bias inflates the effect size estimate to an unknown extent, and there have already have been several cases where the true effect size turned out to be zero. The publication bias tests in this chapter might not be able to provide certainty about the unbiased effect size, but they can function as a red flag to indicate when bias is present, and provide adjusted estimates that, if the underlying model of publication bias is correct, might be closer to the truth.</p>
<p>There is a lot of activity in the literature on tests for publication bias. There are many different tests, and you need to carefully check the assumptions of each test before applying it. Most tests don’t work well when there is large heterogeneity, and heterogeneity is quite likely. A meta-analysis should always examine whether there is publication bias, preferably using multiple publication bias tests, and therefpre it is useful to not just code effect sizes, but also test statistics or <em>p</em>-values. None of the bias detection techniques discussed in this assignment will be a silver bullet, but they will be better than naively interpreting the uncorrected effect size estimate from the meta-analysis.</p>
<p>For another open educational resource on tests for publication bias, see <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pub-bias.html">Doing Meta-Analysis in R</a>.</p>

<!-- To do: Reread, add references, and check if it all still works.  -->
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="meta.html"><span class="header-section-number">12</span> Meta-analysis</a></div>
<div class="next"><a href="computationalreproducibility.html"><span class="header-section-number">14</span> Computational Reproducibility</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bias"><span class="header-section-number">13</span> Bias detection</a></li>
<li><a class="nav-link" href="#publication-bias"><span class="header-section-number">13.1</span> Publication bias</a></li>
<li><a class="nav-link" href="#bias-detection-in-meta-analysis"><span class="header-section-number">13.2</span> Bias detection in meta-analysis</a></li>
<li><a class="nav-link" href="#trim-and-fill"><span class="header-section-number">13.3</span> Trim and Fill</a></li>
<li><a class="nav-link" href="#pet-peese"><span class="header-section-number">13.4</span> PET-PEESE</a></li>
<li><a class="nav-link" href="#p-value-meta-analysis"><span class="header-section-number">13.5</span> P-value meta-analysis</a></li>
<li><a class="nav-link" href="#conclusion"><span class="header-section-number">13.6</span> Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/Lakens/statistical_inferences/blob/master/13-bias.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/Lakens/statistical_inferences/edit/master/13-bias.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniel Lakens. It was last built on 2022-03-25.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a></p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
