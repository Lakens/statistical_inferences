<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Likelihoods | Improving Your Statistical Inferences</title>
<meta name="author" content="Daniël Lakens">
<meta name="description" content="In addition to frequentist and Bayesian approaches to statistical inferences, likelihoods provide a third approach to statistical inferences (Dienes, 2008; Pawitan, 2001). Like Bayesian...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 3 Likelihoods | Improving Your Statistical Inferences">
<meta property="og:type" content="book">
<meta property="og:url" content="https://lakens.github.io/statistical_inferences/likelihoods.html">
<meta property="og:image" content="https://lakens.github.io/statistical_inferences/images/logo.png">
<meta property="og:description" content="In addition to frequentist and Bayesian approaches to statistical inferences, likelihoods provide a third approach to statistical inferences (Dienes, 2008; Pawitan, 2001). Like Bayesian...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Likelihoods | Improving Your Statistical Inferences">
<meta name="twitter:description" content="In addition to frequentist and Bayesian approaches to statistical inferences, likelihoods provide a third approach to statistical inferences (Dienes, 2008; Pawitan, 2001). Like Bayesian...">
<meta name="twitter:image" content="https://lakens.github.io/statistical_inferences/images/logo.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Open%20Sans-0.4.1/font.css" rel="stylesheet">
<link href="libs/_Fira%20Code-0.4.1/font.css" rel="stylesheet">
<link href="libs/_Montserrat-0.4.1/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-0MK2WTGRM3"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-0MK2WTGRM3');
    </script><link rel="shortcut icon" href="images/favicon.ico">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="include/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction</a></li>
<li><a class="" href="pvalue.html"><span class="header-section-number">1</span> Using p-values to test a hypothesis</a></li>
<li><a class="" href="errorcontrol.html"><span class="header-section-number">2</span> Error control</a></li>
<li><a class="active" href="likelihoods.html"><span class="header-section-number">3</span> Likelihoods</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></li>
<li><a class="" href="questions.html"><span class="header-section-number">5</span> Asking Statistical Questions</a></li>
<li><a class="" href="effectsize.html"><span class="header-section-number">6</span> Effect Sizes</a></li>
<li><a class="" href="confint.html"><span class="header-section-number">7</span> Confidence Intervals</a></li>
<li><a class="" href="power.html"><span class="header-section-number">8</span> Sample Size Justification</a></li>
<li><a class="" href="equivalencetest.html"><span class="header-section-number">9</span> Equivalence Testing and Interval Hypotheses</a></li>
<li><a class="" href="sequential.html"><span class="header-section-number">10</span> Sequential Analysis</a></li>
<li><a class="" href="meta.html"><span class="header-section-number">11</span> Meta-analysis</a></li>
<li><a class="" href="bias.html"><span class="header-section-number">12</span> Bias detection</a></li>
<li><a class="" href="prereg.html"><span class="header-section-number">13</span> Preregistration and Transparency</a></li>
<li><a class="" href="computationalreproducibility.html"><span class="header-section-number">14</span> Computational Reproducibility</a></li>
<li><a class="" href="integrity.html"><span class="header-section-number">15</span> Research Integrity</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/Lakens/statistical_inferences">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Improving%20Your%20Statistical%20Inferences&amp;rft.rights=CC-BY-NC-SA&amp;rft.description=This%20open%20educational%20resource%20contains%20information%20to%20improve%20statistical%20inferences%2C%20design%20better%20experiments%2C%20and%20report%20scientific%20research%20more%20transparently.&amp;rft.identifier=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.6409077&amp;rft.aufirst=Dani%C3%ABl&amp;rft.aulast=Lakens&amp;rft.au=Dani%C3%ABl%20Lakens&amp;rft.date=2022&amp;rft.language=en"></span>
<div id="likelihoods" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Likelihoods<a class="anchor" aria-label="anchor" href="#likelihoods"><i class="fas fa-link"></i></a>
</h1>
<p>In addition to frequentist and Bayesian approaches to statistical inferences, likelihoods provide a third approach to statistical inferences <span class="citation">(<a href="references.html#ref-dienes_understanding_2008" role="doc-biblioref">Dienes, 2008</a>; <a href="references.html#ref-pawitan_all_2001" role="doc-biblioref">Pawitan, 2001</a>)</span>. Like <a href="bayes">Bayesian approaches</a>, which will be discussed in the next chapter, likelihoodists are interested in quantifying a measure of relative evidence when comparing two models or hypotheses. Unlike Bayesians, likelihoodists are not too enthusiastic about the idea of incorporating prior information into their statistical inferences. As the likelihoodists Taper and Lele <span class="citation">(<a href="references.html#ref-taper_philosophy_2011" role="doc-biblioref">2011</a>)</span> write:</p>
<blockquote>
<p>It is not that we believe that Bayes' rule or Bayesian mathematics is flawed, but that from the axiomatic foundational definition of probability Bayesianism is doomed to answer questions irrelevant to science. We do not care what you believe, we barely care what we believe, what we are interested in is what you can show.</p>
</blockquote>
<p>Likelihoodists are interested in a measure of relative evidence,. Unlike the Fisherian frequentist approach, where only <span class="math inline">\(H_0\)</span> is specified, and lower <em>p</em>-values that are less compatible with the null model are interpreted as evidence against the null, likelihoodists specify a null and an alternative model, and quantify the relative likelihood of the data under both models. The Neyman-Pearson approach, in which <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> are specified, aims at decisions about how to act, and does not aim to quantify evidence. At the same time, likelihood functions are an important part of both frequentist and Bayesian approaches. In the Neyman-Pearson approach, likelihoods play an important role through the Neyman-Pearson lemma, which shows that the likelihood ratio test is the most powerful test of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_1\)</span>, and is useful in determining the critical value that is used to reject a hypothesis. In Bayesian approaches, the likelihood is combined with a prior to compute a posterior probability distribution.</p>
<p>We can use likelihood functions to make inferences about unknown quantities. Let’s imagine you flip a coin 10 times, and it turns up heads 8 times. What is the true probability (which is sometimes indicated by the Greek letter <span class="math inline">\(\theta\)</span> (theta), but we will use <em>p</em> in this chapter) of this coin landing on heads?</p>
<p>The <strong>binomial probability</strong> of observing <em>k</em> successes in <em>n</em> studies is:</p>
<p><span class="math display">\[
Pr\left( k;n, p \right) = \frac{n!}{k!\left( n - k \right)!}p^{k}{(1 - p)}^{n - k}
\]</span></p>
<p>where <em>p</em> is the probability of a success, <em>k</em> is the observed number of successes, and <em>n</em> is the number of trials. The first term indicates the number of possible combinations of results (e.g., you could start out with eight successes, end with eight successes, or any of the other possible combinations), which is multiplied by the probability of observing one success in each of the trials, which is then multiplied by the probability of observing no success in the remaining trials.</p>
<p>Let’s assume you expect this is a fair coin. What is the binomial probability of observing 8 heads out of 10 coin flips, when <em>p</em> = 0.5? The answer is:</p>
<p><span class="math display">\[
Pr\left(8;10, 0.5 \right) = \frac{10!}{8!\left( 10 - 8 \right)!}*0.5^{8}*{(1 - 0.5)}^{10 - 8}
\]</span>
In R this probability is computed as as:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Special.html">factorial</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Special.html">factorial</a></span><span class="op">(</span><span class="fl">8</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Special.html">factorial</a></span><span class="op">(</span><span class="fl">10</span><span class="op">-</span><span class="fl">8</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fl">0.5</span><span class="op">^</span><span class="fl">8</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fl">0.5</span><span class="op">)</span><span class="op">^</span><span class="op">(</span><span class="fl">10</span><span class="op">-</span><span class="fl">8</span><span class="op">)</span></code></pre></div>
<p>or by using the function:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">2</span>, size <span class="op">=</span> <span class="fl">10</span>, prob <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></code></pre></div>
<p>Let’s assume we don’t have any other information about this coin. (You might believe most coins are fair; such priors will be discussed when we talk about <a href="bayes.html#bayes">Bayesian statistics</a> in the next chapter). The equation <em>Pr(k;n,p)</em> gives the probability of observing <em>k</em> successes from <em>n</em> trials when a coin’s probability of success is <em>p</em>.</p>
<p>When computing a probability, we assume the model to be known, and compute the probability of observing a specific outcome. But based on the data we have observed, we can ask the reversed question: which value of <em>p</em> will make the observed data <strong>most likely</strong>? When computing a likelihood, we assume the data to be known, and make an inference about the most likely parameter for the model. To answer this question, we can plug in the values for <em>k</em> and <em>n</em> and find which value of <em>p</em> maximizes this function. <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a> called this maximum likelihood estimation (this is considered one of the most important developments in 20th century statistics, and Fisher published his first paper on this in 1912 as a third year undergraduate when he was 22 <span class="citation">(<a href="references.html#ref-aldrich_r_1997" role="doc-biblioref">Aldrich, 1997</a>)</span>). Since <em>p</em> can be any value between 0 and 1, we can plot all values in what is known as the <em>likelihood function</em>, so we can see the maximum more easily.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like1"></span>
<img src="03-likelihoods_files/figure-html/like1-1.png" alt="Binomial likelihood function for 8 successes in 10 trials." width="100%"><p class="caption">
Figure 3.1: Binomial likelihood function for 8 successes in 10 trials.
</p>
</div>
<p>The likelihood is plotted for all possible values of <em>p</em> (from 0 to 1). It should not be surprising that given the data we have observed, the most likely value for the true parameter is 8 out of 10, or <em>p</em> = 0.8, with a likelihood of 0.30 (the highest point on the y-axis). In this example, <em>p</em> = 0.8 is called the <strong>maximum likelihood estimator</strong>. It is important to know that the likelihood itself has no meaning in isolation. In this sense, it differs from a probability. But we can compare likelihoods of the same function across different values of <em>p</em>. You can read off any other value for any other p, and see that given the observed data, low values of <em>p</em> (e.g., 0.2) are not very likely.</p>
<p>There is a subtle difference between a probability and a likelihood. In colloquial language, you can use both terms to mean the same thing, but in statistics both terms used for different sides of the same coin. Note how the equation for Pr involves both information about the data (<em>k</em>, <em>n</em>) and information about the parameter (<em>p</em>). To compute a <strong>probability</strong>, we view <em>p</em> as fixed (for instance, for a fair coin, we plug in <em>p</em> = 0.5) and then estimate the probability of different outcomes (<em>k</em>, <em>n</em>). The resulting function is the probability mass function. To compute the <strong>likelihood</strong>, we instead view the observed data as fixed (e.g., observing 5 heads out of 10 coin tosses), and we view Pr as a function of <em>p</em>, estimating the value that maximizes the likelihood of a particular sample.</p>
<p>Likelihoods are an example of statistical inference: We have observed some data, and we use this data to draw an inference about different population parameters. More formally, the likelihood function is the (joint) density function evaluated at the observed data. Likelihood functions can be calculated for many different models (binomial distributions, normal distributions, see <span class="citation">Millar (<a href="references.html#ref-millar_maximum_2011" role="doc-biblioref">2011</a>)</span>). This approach is called <strong>likelihoodist statistics</strong>, or <strong>likelihoodism</strong>, and it is distinct from frequentist and Bayesian approaches to statistics, as it directly uses the likelihood function to make inferences.</p>
<p>When a mix of heads and tails has been observed, the likelihood curve rises and falls, as it is not possible that the coin can only come up heads or tails (after all, both have already been observed). If only heads or 0 heads are observed, the likelihood curve peaks at the far left or right of the x-axis. When we plot the likelihood curves for 0 heads in 10 coin flips, the likelihood curve looks like Figure <a href="likelihoods.html#fig:like2">3.2</a>.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like2"></span>
<img src="03-likelihoods_files/figure-html/like2-1.png" alt="Binomial likelihood function for 0 successes in 10 trials." width="100%"><p class="caption">
Figure 3.2: Binomial likelihood function for 0 successes in 10 trials.
</p>
</div>
<p>Likelihoods can easily be combined. Imagine we have two people flipping the same coin independently. One person observes 8 heads out of 10 flips, and the other observes 4 heads out of 10 flips. You might believe that this should give the same likelihood curve as one person flipping a coin 20 times, and observing 12 heads, and indeed, it does. In the plot below, all likelihood curves are standardized by dividing the curve by the maximum of each likelihood curve. This is why all curves now have a maximum of 1, and we can more easily compare different likelihood curves.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like3"></span>
<img src="03-likelihoods_files/figure-html/like3-1.png" alt="Combining likelihoods." width="100%"><p class="caption">
Figure 3.3: Combining likelihoods.
</p>
</div>
<p>The curve on left is for 4 out of 10 heads, the one on the right is for 8 out of 10 heads. The black dotted curve in the middle is for 12 out of 20 heads. The grey curve, directly beneath the 12 out of 20 heads curve, is calculated by multiplying the likelihood curves: <span class="math inline">\(L(p_{combined}) = L(p = 0.8) / L(p = 0.4)\)</span>.</p>
<p>In Figure <a href="likelihoods.html#fig:like4">3.4</a> we see likelihood curves for 10, 100, and 1000 coin flips, which yield 5, 50, and 500 heads, respectively. The likelihood curves are again standardized to make them more easily comparable. As the sample size increases, the curves become more narrow (the dashed line is for <em>n</em> = 10, the dotted line is for <em>n</em> = 100, and the solid line is for <em>n</em> = 1000). This means that as the sample size increases, our data become increasingly less likely under population parameters further removed from the observed number of heads. In other words, we have collected increasingly strong evidence for <em>p</em> = 0.5, compared to most other possible population parameters.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like4"></span>
<img src="03-likelihoods_files/figure-html/like4-1.png" alt="Likelihood function for 5/10, 50/100 and 500/1000 heads in coin flips." width="100%"><p class="caption">
Figure 3.4: Likelihood function for 5/10, 50/100 and 500/1000 heads in coin flips.
</p>
</div>
<div id="likelihood-ratios" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Likelihood ratios<a class="anchor" aria-label="anchor" href="#likelihood-ratios"><i class="fas fa-link"></i></a>
</h2>
<p>We can use the likelihood function to compare possible values of p. For example, we might believe the coin we flipped was fair, even though we flipped eight out of ten heads. A fair coin will have <em>p</em> = 0.5, while we observed <em>p</em> = 0.8. The likelihood function allows us to compute the relative likelihood for different possible parameters. How much more likely is our observed data under the hypothesis that this is an unfair coin that will on average turn up heads 80% of the time, compared to the alternative theory that this is a fair coin which should turn up heads 50% of the time?</p>
<p>We can calculate the likelihood ratio:</p>
<p><span class="math display">\[
\frac{L(p = 0.8)}{L(p = 0.5)}
\]</span></p>
<p>Which is 0.302/0.044 = 6.87. In the plot, both circles show the points on
the likelihood curve for L(<em>p</em> = 0.5) and L(<em>p</em> = 0.8).</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like5"></span>
<img src="03-likelihoods_files/figure-html/like5-1.png" alt="Computing a likelihood ratio for p = 0.5 relative to p = 0.8 when observing p = 0.8." width="100%"><p class="caption">
Figure 3.5: Computing a likelihood ratio for <em>p</em> = 0.5 relative to <em>p</em> = 0.8 when observing <em>p</em> = 0.8.
</p>
</div>
<p>We can subjectively interpret this likelihood ratio, which tells us that our observed data is 6.87 times more likely under the hypothesis that this coin is unfair and will turn up heads 80% of the time, than under the hypothesis that this is a fair coin. How convincing is this? Let’s round the likelihood ratio to 7, and imagine two bags of marbles. One bag contains 7 blue marbles. The second contains 7 marbles, each one a different color of the rainbow, so violet, indigo, blue, green, yellow, orange, and red. Someone randomly picks one of the two bags, draws a marble, and shows it to you. The marble is blue: How certain are you this marble came from the bag with all blue marbles, compared to the bag with rainbow coloured marbles? This is how strongly the likelihood ratio tells us to believe our data were generated by an unfair coin that turns up heads 80% of the time, relative to a fair coin, given that we have observed 8 heads in 10 tosses. After this explanation, intended to not make you rely too much on benchmarks, it might still be useful to know that <span class="citation">Royall (<a href="references.html#ref-royall_statistical_1997" role="doc-biblioref">1997</a>)</span> considered likelihood ratios of 8 as moderately strong evidence, and likelihood ratios of 32 as strong evidence.</p>
<p>Note that likelihood ratios give us the relative evidence for one specified hypothesis, over another specified hypothesis. The likelihood ratio can be calculated for any two hypothesized values. For example, in Figure <a href="likelihoods.html#fig:like6">3.6</a> below, the likelihood ratio is calculated that compares the hypothesis for a fair coin (<em>p</em> = 0.5) with the alternative hypothesis that the coin comes up heads 80% of the time (<em>p</em> = 0.8), when we have observed 4 heads out of 10 coin flips. We see that the observed data are 0.2050/0.0055=37.25 times more likely (ignoring rounding differences – and try to calculate these numbers by hand using the formula provided earlier) under the hypothesis that this is a fair coin than under the hypothesis that this is a coin that turns up heads 80% of the time.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like6"></span>
<img src="03-likelihoods_files/figure-html/like6-1.png" alt="Computing a likelihood ratio for p = 0.5 relative to p = 0.8 when observing p = 0.4." width="100%"><p class="caption">
Figure 3.6: Computing a likelihood ratio for <em>p</em> = 0.5 relative to <em>p</em> = 0.8 when observing <em>p</em> = 0.4.
</p>
</div>
<p>A likelihood ratio of 1 means the data are equally likely under both hypotheses. Values further away from 1 indicate that the data are more likely under one hypothesis than the other. The ratio can be expressed in favor of one hypothesis over the other (for example L(<em>p</em> = 0.5)/L(<em>p</em> = 0.8) or vice versa (L(<em>p</em> = 0.8)/L(<em>p</em> = 0.5). This means the likelihood ratio of 37.25 for <span class="math inline">\(H_0\)</span> relative to <span class="math inline">\(H_1\)</span> is equivalent to a likelihood ratio of 1/37.25 = 0.02685 for <span class="math inline">\(H_1\)</span> relative to <span class="math inline">\(H_0\)</span>. Likelihood ratios range from 0 to infinity, and the closer to zero or infinity, the stronger the relative evidence for one hypothesis over the other. We will see in the chapter on <a href="bayes.html#bayes">Bayesian statistics</a> that likelihood ratios are in this sense very similar (and a special case of) a Bayes Factor.</p>
<p>Likelihoods are relative evidence. Just because the data are more likely under one possible value of <em>p</em> than another value of <em>p</em> doesn’t mean that the data have come from either of these two distributions. Other values might generate even higher likelihood values. For example, consider the situation where we flip a coin 100 times, and observe 50 heads. We compare <em>p</em> = 0.3 versus <em>p</em> = 0.8, and find that the likelihood ratio is 803462, implying that there is 803461 times more evidence in the data for <em>p</em> = 0.3 than for <em>p</em> = 0.8. That might sound pretty conclusive evidence for <em>p</em> = 0.3. But it is only relative evidence for <em>p</em> = 0.3 compared to <em>p</em> = 0.8. If we look at the likelihood function, we clearly see that, not surprisingly, <em>p</em> = 0.5 is the value that maximizes the likelihood function. Just because one hypothesis is more likely than another hypothesis, does not mean that there isn't a third hypothesis that is even more likely.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like7"></span>
<img src="03-likelihoods_files/figure-html/like7-1.png" alt="Computing a likelihood ratio for p = 0.3 relative to p = 0.8 when observing p = 0.5 in 100 coin flips." width="100%"><p class="caption">
Figure 3.7: Computing a likelihood ratio for <em>p</em> = 0.3 relative to <em>p</em> = 0.8 when observing <em>p</em> = 0.5 in 100 coin flips.
</p>
</div>
</div>
<div id="mixedresults" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Likelihood of mixed results in sets of studies<a class="anchor" aria-label="anchor" href="#mixedresults"><i class="fas fa-link"></i></a>
</h2>
<p>Science is a cumulative process, and we should evaluate lines of research, not single studies. One big problem in the scientific literature is that nonsignificant results are often never published <span class="citation">(<a href="references.html#ref-fanelli_positive_2010" role="doc-biblioref">Fanelli, 2010</a>; <a href="references.html#ref-franco_publication_2014" role="doc-biblioref">Franco et al., 2014</a>)</span>. At the same time, because the statistical power of hypothesis tests are never 100% (and often much lower), it is a mathematical reality that it is unlikely (or “too good to be true”) that a set of multiple studies yields exclusively significant results. <span class="citation">(<a href="references.html#ref-francis_frequency_2014" role="doc-biblioref">Francis, 2014</a>; <a href="references.html#ref-schimmack_ironic_2012" role="doc-biblioref">Schimmack, 2012</a>)</span>. We can use binomial likelihoods to examine how likely it is to observe mixed results, and understand when mixed results are nevertheless strong evidence for the presence of an effect. The following is largely based on <span class="citation">Lakens &amp; Etz (<a href="references.html#ref-lakens_too_2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>The probability of observing a significant or nonsignificant result in a study depends on the Type 1 error rate (<span class="math inline">\(\alpha\)</span>), the statistical power of the test (1-<span class="math inline">\(\beta\)</span>), and the probability that the null hypothesis is true <span class="citation">(<a href="references.html#ref-wacholder_assessing_2004" role="doc-biblioref">Wacholder et al., 2004</a>)</span>. There are four possible outcomes of a study: a true positive, a false positive, a true negative, and a false negative. When <span class="math inline">\(H_0\)</span> is true, the probability of observing a false positive depends on the <span class="math inline">\(\alpha\)</span> level or the Type 1 error rate (e.g., 5%). When <span class="math inline">\(H_1\)</span> is true, the probability of observing a true positive depends on the statistical power of the performed test (where an often recommended minimum is 80%), which in turn depends on the <span class="math inline">\(\alpha\)</span> level, the true effect size, and the sample size. With an <span class="math inline">\(\alpha\)</span> level of 5%, and when <span class="math inline">\(H_0\)</span> is true, a false positive will occur with a 5% probability (as long as error rates are controlled, e.g., in preregistered studies) and true negative will occur with a 95% probability. When a test has 80% power, and <span class="math inline">\(H_1\)</span> is true, a true positive has a probability of 80%, and a false negative has a probability of 20%.</p>
<p>If we perform multiple studies, we can calculate the binomial probability that we will observe a specific number of significant and non-significant findings <span class="citation">(<a href="references.html#ref-ioannidis_exploratory_2007" role="doc-biblioref">Ioannidis &amp; Trikalinos, 2007</a>)</span>. We can calculate the probability of finding exactly two significant results out of three studies assuming the null hypothesis is true. When <span class="math inline">\(H_0\)</span> is true, the probability of significant results equals the <span class="math inline">\(\alpha\)</span> level, and thus when the <span class="math inline">\(\alpha\)</span> level is carefully controlled (e.g., in preregistered studies) the probability of observing a significant result (p) = 0.05. That is, when k = 2, n = 3, and p = .05, the binomial probability function tells us that the probability of finding exactly two significant results in three studies is 0.007 (0.05 × 0.05 × 0.95 = 0.002375, and there are three orders in which two of the three results can be observed, so 0.002375 × 3 = 0.007).</p>
<p>To calculate the likelihood assuming <span class="math inline">\(H_1\)</span> is true, we need to make an assumption about the power in each study. Let’s provisionally assume all studies were powered at 80% and thus <em>p</em> = .80. The probability of observing exactly two significant results in three studies, assuming a power of 0.8, is 0.384 (0.8 × 0.8 × 0.2 = 0.128, and with three orders in which two of the three results can be significant, 0.128 × 3 = 0.384). In other words, if you set out to perform 3 studies, your hypothesis is correct, and you test your hypothesis with 80% power, there is a 38.4% probability of observing 2 out of 3 significant results, and a 9.6% probability to observe 1 out of 3 significant results (and for an extremely unlucky individual, a 0.8% probability of not finding any significant results in three studies, even though there is a true effect). Unless power is extremely high, mixed results should be expected in sets of studies.</p>
<p>Both likelihoods at <em>p</em> = .05 and <em>p</em> = .80 are highlighted in Figure <a href="likelihoods.html#fig:like8">3.8</a> by the circles on the dotted vertical lines.We can use the likelihood of the data assuming <span class="math inline">\(H_0\)</span> or <span class="math inline">\(H_1\)</span> is true to calculate the likelihood ratio, 0.384/0.007 = 53.89, which tells us the observed outcome of exactly two significant results out of three studies is 53.89 times more likely when <span class="math inline">\(H_1\)</span> is true and studies had 80% power, than when <span class="math inline">\(H_0\)</span> is true and studies have a carefully controlled 5% Type 1 error rate. Likelihood ratios of 8 and 32 have been proposed as benchmarks of moderately strong and strong evidence, respectively <span class="citation">(<a href="references.html#ref-royall_statistical_1997" role="doc-biblioref">Royall, 1997</a>)</span>, which implies that finding two significant results out of the three studies could be considered strong evidence for <span class="math inline">\(H_1\)</span>, assuming 80% power. A Shiny app to perform these calculations is available <a href="https://shiny.ieis.tue.nl/mixed_results_likelihood/">here</a>.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like8"></span>
<img src="03-likelihoods_files/figure-html/like8-1.png" alt="Computing a likelihood ratio for two out of three significant results, assuming an alpha of 5% and 80% power." width="100%"><p class="caption">
Figure 3.8: Computing a likelihood ratio for two out of three significant results, assuming an alpha of 5% and 80% power.
</p>
</div>
<p>In sets of studies, the likelihood ratio in favor of <span class="math inline">\(H_1\)</span> versus <span class="math inline">\(H_0\)</span> after observing a mix of significant and nonsignificant findings can become surprisingly large. Even though the evidence appears to be mixed, there is actually strong evidence in favor of a true effect. For example, when a researcher performs six studies with 80% power and a 5% alpha level and finds three significant outcomes and three nonsignificant outcomes, the cumulative likelihood ratio is convincingly large at 38-to-1 in favor of <span class="math inline">\(H_1\)</span> to consider the set of studies strong evidence for a true effect. Intuitively, researchers
might not feel convinced by a set of studies where three out of six results were statistically significant. But if we do the math, we see that such a set of studies can be very strong evidence in favor of a true effect. A better understanding of these probabilities might be an important step in mitigating the negative effects of publication bias.</p>
<p>Hopefully, researchers become more inclined to submit nonsignificant findings for publication when they have a better understanding of the evidential value in lines of research with mixed results. Publishing all performed studies in lines of research will reduce publication bias, and increase the informational value of the data in the scientific literature. Expecting all studies in lines of research to be statistically significant is not reasonable, and it is important that researchers develop more realistic expectations if they are to draw meaningful inferences from lines of research. We don’t have a very good feeling for what real patterns of studies look like, because we are continuously exposed to a scientific literature that does not reflect reality. Almost all multiple study papers in the scientific literature present only statistically significant results, even though this is unlikely given the power of these studies, and the probability that we would only study correct predictions <span class="citation">(<a href="references.html#ref-scheel_excess_2021" role="doc-biblioref">Scheel, Schijen, et al., 2021</a>)</span>. Educating researchers about binomial probabilities and likelihood ratios is a straightforward way to develop more realistic expectations about what research lines that contain evidential value in favor of <span class="math inline">\(H_1\)</span> actually look like.</p>
</div>
<div id="likettest" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Likelihoods for <em>t</em>-tests<a class="anchor" aria-label="anchor" href="#likettest"><i class="fas fa-link"></i></a>
</h2>
<p>So far we have computed likelihoods for binomial probabilities, but likelihoods can be computed for any statistical model <span class="citation">(<a href="references.html#ref-glover_likelihood_2004" role="doc-biblioref">Glover &amp; Dixon, 2004</a>; <a href="references.html#ref-pawitan_all_2001" role="doc-biblioref">Pawitan, 2001</a>)</span>. For example, we can compute the relative likelihood of observing a <em>t</em>-value under the null and an alternative hypothesis (Figure <a href="likelihoods.html#fig:like9">3.9</a>). Of course, the observed data is most likely if we assume the observed effect equals the true effect, but examining the likelihood reveals that there are many alternative hypotheses that are relatively more likely than the null hypothesis. This also holds when observing nonsignificant results, which can be more likely under an alternative hypothesis of interest, than under the null hypothesis. This is a reason why it is incorrect to say that there is no effect when <em>p</em> &gt; <span class="math inline">\(\alpha\)</span> (see <a href="pvalue.html#misconception1"><em>p</em>-value misconception 1</a>).</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:like9"></span>
<img src="03-likelihoods_files/figure-html/like9-1.png" alt="Likelihood ratio for observed t-value under \(H_0\) and \(H_1\)." width="100%"><p class="caption">
Figure 3.9: Likelihood ratio for observed <em>t</em>-value under <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span>.
</p>
</div>
</div>
<div id="test-yourself-2" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Test Yourself<a class="anchor" aria-label="anchor" href="#test-yourself-2"><i class="fas fa-link"></i></a>
</h2>
<div id="questions-about-likelihoods" class="section level3" number="3.4.1">
<h3>
<span class="header-section-number">3.4.1</span> Questions about likelihoods<a class="anchor" aria-label="anchor" href="#questions-about-likelihoods"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Q1</strong>: Let’s assume you expect this is a fair coin. What is the binomial probability of observing 8 heads out of 10 coin flips, when p = 0.5? (You can use the functions in the chapter, or compute it by hand).</p>
<ol style="list-style-type: upper-alpha">
<li>0.044</li>
<li>0.05</li>
<li>0.5</li>
<li>0.8</li>
</ol>
<p><strong>Q2</strong>: The likelihood curve rises up and falls down, except at the extremes, when 0 heads or only heads are observed. Copy the code below (remember that you can click the 'clipboard' icon on the top right of the code section to copy all the code to your clipboard), and plot the likelihood curves for 0 heads (x &lt;- 0) out of 10 flips (n &lt;- 10) by running the script. What does the likelihood curve look like?</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span> <span class="co"># set total trials</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="co"># set successes</span>
<span class="va">H0</span> <span class="op">&lt;-</span> <span class="fl">0.5</span> <span class="co"># specify one hypothesis you want to compare</span>
<span class="va">H1</span> <span class="op">&lt;-</span> <span class="fl">0.4</span> <span class="co"># specify another hypothesis you want to compare</span>
<span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H0</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H1</span><span class="op">)</span> <span class="co"># Returns the H0/H1 likelihood ratio</span>
<span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H1</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H0</span><span class="op">)</span> <span class="co"># Returns the H1/H0 likelihood ratio</span>

<span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, len <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="co"># create probability variable from 0 to 1</span>
<span class="va">like</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">theta</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">like</span>, type <span class="op">=</span> <span class="st">"l"</span>, xlab <span class="op">=</span> <span class="st">"p"</span>, ylab <span class="op">=</span> <span class="st">"Likelihood"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">H0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H0</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">H1</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H1</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/segments.html">segments</a></span><span class="op">(</span><span class="va">H0</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H0</span><span class="op">)</span>, <span class="va">x</span> <span class="op">/</span> <span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H0</span><span class="op">)</span>, lty <span class="op">=</span> <span class="fl">2</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/segments.html">segments</a></span><span class="op">(</span><span class="va">H1</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H1</span><span class="op">)</span>, <span class="va">x</span> <span class="op">/</span> <span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H1</span><span class="op">)</span>, lty <span class="op">=</span> <span class="fl">2</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/segments.html">segments</a></span><span class="op">(</span><span class="va">x</span> <span class="op">/</span> <span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H0</span><span class="op">)</span>, <span class="va">x</span> <span class="op">/</span> <span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H1</span><span class="op">)</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Likelihood Ratio H0/H1:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H0</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H1</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>, <span class="st">" Likelihood Ratio H1/H0:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H1</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">H0</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<ol style="list-style-type: upper-alpha">
<li>The likelihood curve is a horizontal line.</li>
<li>The script returns an error message: it is not possible to plot the likelihood curve for 0 heads.</li>
<li>The curve starts at its highest point at p = 0, and then the likelihood decreases as p increases.</li>
<li>The curve starts at its lowest point at p = 0, and then the likelihood increases as p increases.</li>
</ol>
<p><strong>Q3</strong>: Get a coin out of your wallet. Flip it 13 times, and count the number of heads. Using the code above, calculate the likelihood of your observed results under the hypothesis that your coin is fair, compared to the hypothesis that the coin is not fair. Set the number of successes (x) to the number of heads you observed. Change <span class="math inline">\(H_1\)</span> to the number of heads you have observed (or leave it to 0 if you didn’t observe any heads at all!). You can just use 4/13, or enter 0.3038. Leave <span class="math inline">\(H_0\)</span> at 0.5. Run the script to calculate the likelihood ratio. What is the likelihood ratio of a fair compared to a non-fair coin (or <span class="math inline">\(H_0\)</span>/<span class="math inline">\(H_1\)</span>) that flips heads as often as you have observed, based on the observed data? Round your answer to 2 digits after the decimal.</p>
<p><strong>Q4</strong>: Earlier we mentioned that with increasing sample sizes, we had collected stronger relative evidence. Let’s say we would want to compare L(p = 0.4) with L(p = 0.5). What is the likelihood ratio if <span class="math inline">\(H_1\)</span> is 0.4, <span class="math inline">\(H_0\)</span> is 0.5, and you flip 5 heads in 10 trials? From the two possible ways to calculate the likelihood ratio (<span class="math inline">\(H_1\)</span>/<span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_0\)</span>/<span class="math inline">\(H_1\)</span>), report the likelihood that is ≥ 1, and round to 2 digits after the decimal point.</p>
<p><strong>Q5</strong>: What is the likelihood ratio if <span class="math inline">\(H_1\)</span> is 0.4, <span class="math inline">\(H_0\)</span> is 0.5, and you flip 50 heads in 100 trials? From the two possible ways to calculate the likelihood ratio (<span class="math inline">\(H_1\)</span>/<span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_0\)</span>/<span class="math inline">\(H_1\)</span>), report the likelihood that is ≥ 1, and round to 2 digits after the decimal point.</p>
<p><strong>Q6</strong>: What is the likelihood ratio if <span class="math inline">\(H_1\)</span> is 0.4, <span class="math inline">\(H_0\)</span> is 0.5, and you flip 500 heads in 1000 trials? From the two possible ways to calculate the likelihood ratio (<span class="math inline">\(H_1\)</span>/<span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_0\)</span>/<span class="math inline">\(H_1\)</span>), report the likelihood that is ≥ 1, and round to 2 digits after the decimal point.</p>
<p><strong>Q7</strong>: When comparing two hypotheses (p = X vs p = Y), a likelihood ratio of:</p>
<ol style="list-style-type: upper-alpha">
<li>0.02 means that there is not enough evidence in the data for either of the two hypotheses.</li>
<li>5493 means that hypothesis p = X is most supported by the data.</li>
<li>5493 means that hypothesis p = X is much more supported by the data than p = Y.</li>
<li>0.02 means that the hypothesis that the data are 2% more likely under the hypothesis that p = X than under the hypothesis that p = Y.</li>
</ol>
</div>
<div id="questions-about-mixed-results" class="section level3" number="3.4.2">
<h3>
<span class="header-section-number">3.4.2</span> Questions about mixed results<a class="anchor" aria-label="anchor" href="#questions-about-mixed-results"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Q8:</strong> Which statement is correct when you perform 3 studies?</p>
<ol style="list-style-type: upper-alpha">
<li>When <span class="math inline">\(H_1\)</span> is true, alpha = 0.05, and power = 0.80, it is almost as likely to observe one or more non-significant results (48.8%) as it is to observe only significant result (51.2%).</li>
<li>When alpha = 0.05 and power = 0.80, it is extremely rare that you will find 3 significant results (0.0125%), regardless of whether <span class="math inline">\(H_0\)</span> is true or <span class="math inline">\(H_1\)</span> is true.</li>
<li>When alpha = 0.05 and power = 0.80, 2 out of 3 statistically significant results is the most likely outcome of all possible outcomes (0 out of 3, 1 out of 3, 2 out of 3, or 3 out of 3), and occurs 38.4% of the time when <span class="math inline">\(H_1\)</span> is true.</li>
<li>When alpha = 0.05 and power = 0.80, the probability of finding at least one false positive (a significant result when <span class="math inline">\(H_0\)</span> is true) in three studies is 5%.</li>
</ol>
<p><strong>Q9:</strong> Sometimes in lines of three studies, you’ll find a significant effect in one study, but there is no effect in the other two related studies. Assume the two related studies were not exactly the same in every way (e.g., you have changed the manipulation, or the procedure, or some of the questions). It could be that the two other studies did not work because of minor differences that had some effect you do not fully understand yet. Or it could be that the single significant result was a Type 1 error, and <span class="math inline">\(H_0\)</span> was true in all three studies. Which statement below is correct, assuming a 5% Type 1 error rate and 80% power?</p>
<ol style="list-style-type: upper-alpha">
<li>All else being equal, the probability of a Type 1 error in one of three studies is 5% when there is no true effect in all three studies, and the probability of finding exactly 1 in three significant effects, assuming 80% power in all three studies, is 80%, which is substantially more likely.</li>
<li>All else being equal, the probability of a Type 1 error in one of three studies is 13.5% when there is no true effect in all three studies, and the probability of finding exactly 1 in three significant effects, assuming 80% power in all three studies (and thus a true effect), is 9.6%, which is slightly, but not substantially less likely.</li>
<li>All else being equal, the probability of a Type 1 error in one of three studies is 85.7% when there is no true effect in all three studies, and the probability of finding exactly 1 in three significant effects, assuming 80% power in all three studies (and thus a true effect) (and thus a true effect), is 0.8%, which is substantially less likely.</li>
<li>It is not possible to know the probability you will observe a Type 1 error if you perform 3 studies.</li>
</ol>
<p>The idea that most studies have 80% power is slightly optimistic. <strong>Examine the correct answer to the previous question across a range of power values</strong> (e.g., 50% power, and 30% power).</p>
<p><strong>Q10:</strong> Several papers suggest it is a reasonable assumption that the power in the psychological literature might be around 50%. Set the number of studies to 4, the number of successes also to 4, and the assumed power slider to 50%, and look at the table at the bottom of the app. How likely is it to observe 4 significant results in 4 studies, assuming there is a true effect?</p>
<ol style="list-style-type: upper-alpha">
<li>6.25%</li>
<li>12.5%</li>
<li>25%</li>
<li>37.5%</li>
</ol>
<p>Imagine you perform 4 studies, and 3 show a significant result. <strong>Change these numbers in the online app. Leave the power at 50%</strong>. The output in the text tells you:</p>
<p><em>When the observed results are equally likely under <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span>, the likelihood ratio is 1. Benchmarks to interpret Likelihood Ratios suggest that when 1&lt;LR&lt;8 there is weak evidence, when 8&lt;LR&lt;32 there is moderate evidence, and when LR&gt;32, there is strong evidence.</em></p>
<p><em>The data are more likely under the alternative hypothesis than the null hypothesis with a likelihood ratio of 526.32</em></p>
<p>These calculations show that, assuming you have observed three significant results out of four studies, and assuming each study had 50% power, it is 526 times more likely to have observed these data when the alternative hypothesis is true, than when the null hypothesis is true. In other words, it is 526 times more likely to find a significant effect in three studies when you have 50% power, than to find three Type 1 errors in a set of four studies.</p>
<p><strong>Q11</strong>: Maybe you don’t think 50% power is a reasonable assumption. How low can the power be (rounded to 2 digits), for the likelihood to remain higher than 32 in favor of <span class="math inline">\(H_1\)</span> when observing 3 out of 4 significant results?</p>
<ol style="list-style-type: upper-alpha">
<li>5% power</li>
<li>17% power</li>
<li>34% power</li>
<li>44% power</li>
</ol>
<p>The main take home message of these calculations is to understand that 1) mixed results are supposed to happen, and 2) mixed results can contain strong evidence for a true effect, across a wide range of plausible power values. The app also tells you how much evidence, in a rough dichotomous way, you can expect. This is useful for our educational goal. But when you want to evaluate results from multiple studies, the formal way to do so is by performing a meta-analysis.</p>
<p>The above calculations make a very important assumption: The Type 1 error rate is controlled at 5%. If you try out many different tests in each study, and only report the result that yielded a p &lt; 0.05, these calculations no longer hold.</p>
<p><strong>Q12</strong>: Go back to the default settings of 2 out of 3 significant results, but now set the Type 1 error rate to 20%, to reflect a modest amount of p-hacking. Under these circumstances, what is the <strong>highest</strong> likelihood in favor of <span class="math inline">\(H_1\)</span> you can get if you explore all possible values for the true power?</p>
<ol style="list-style-type: upper-alpha">
<li>Approximately 1</li>
<li>Approximately 4.63</li>
<li>Approximately 6.70</li>
<li>Approximately 62.37</li>
</ol>
<p>As the scenario above shows, <em>p</em>-hacking makes studies extremely uninformative.
<strong>If you inflate the error rate, you quickly destroy the evidence in the data.</strong> You can no longer determine whether the data is more likely when there is no effect, than when there is an effect. Sometimes researchers complain that people who worry about <em>p</em>-hacking and try to promote better Type 1 error control are missing the point, and that other things (better measurement, better theory, etc.) are more important. I fully agree that these aspects of scientific research are at least as important as better error control. But better measures and theories will require decades of work. Better error control can be accomplished today, if researchers would stop inflating their error rates by flexibly analyzing their data. And as this assignment shows, inflated rates of false positives very quickly make it difficult to learn what is true from the data we collect. Because of the relative ease with which this part of scientific research can be improved, and because we can achieve this today (and not in a decade) I think it is worth stressing the importance of error control, and publish more realistic looking sets of studies.</p>
<p><strong>Q13</strong>: Some ‘prestigious’ journals (which, when examined in terms of scientific quality such as reproducibility, reporting standards, and policies concerning data and material sharing, are quite low quality despite their prestige) only publish manuscripts with a large number of studies, which should all be statistically significant. If we assume an average power in psychology of 50%, only 3.125% of 5 study articles should contain exclusively significant results. If you pick up a random issue from such a prestigious journal, and see 10 articles, each reporting 5 studies, and all manuscripts have exclusively significant results, would you trust the reported findings more, or less, than when all these articles had reported mixed results? Why?</p>
<p><strong>Q14</strong>: Unless you will power all your studies at 99.99% for the rest of your career (which would be slightly inefficient, but great if you don’t like insecurity), you will observe mixed results in lines of research. How do you plan to deal with mixed results in lines of research?</p>
</div>
<div id="open-questions-2" class="section level3" number="3.4.3">
<h3>
<span class="header-section-number">3.4.3</span> Open Questions<a class="anchor" aria-label="anchor" href="#open-questions-2"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li><p>What is the difference between a probability and a likelihood?</p></li>
<li><p>Why is it important to remember that a likelihood ratio is relative evidence?</p></li>
<li><p>If we compare 2 hypotheses, H0 and H1, and the likelihood ratio of H1 compared to H0 is 77, what does this mean?</p></li>
<li><p>What are benchmarks for medium and strong evidence according to Royall?</p></li>
<li><p>How can it be that a likelihood ratio is 200, but both hypotheses are incorrect?</p></li>
<li><p>If we perform multiple studies and find 2 out of 3 studies show a significant results, how can this actually be strong evidence for H1?</p></li>
</ol>
</div>
</div>
</div>
<script>
$( document ).ready(function() {
  var cite = ' ';
  var license = ' <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="blank"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a>';

  $("footer div.row div:eq(1) p").html(
    license + cite
  );

  function move_sidebar() {
    var w = window.innerWidth;
    if (w < 992) {
      $("#toc").appendTo($("#main-nav"));
    } else {
      $("#toc").appendTo($("div.sidebar-chapter"));
    }
  }
  move_sidebar();
  window.onresize = move_sidebar;
});
</script><div class="chapter-nav">
<div class="prev"><a href="errorcontrol.html"><span class="header-section-number">2</span> Error control</a></div>
<div class="next"><a href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#likelihoods"><span class="header-section-number">3</span> Likelihoods</a></li>
<li><a class="nav-link" href="#likelihood-ratios"><span class="header-section-number">3.1</span> Likelihood ratios</a></li>
<li><a class="nav-link" href="#mixedresults"><span class="header-section-number">3.2</span> Likelihood of mixed results in sets of studies</a></li>
<li><a class="nav-link" href="#likettest"><span class="header-section-number">3.3</span> Likelihoods for t-tests</a></li>
<li>
<a class="nav-link" href="#test-yourself-2"><span class="header-section-number">3.4</span> Test Yourself</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#questions-about-likelihoods"><span class="header-section-number">3.4.1</span> Questions about likelihoods</a></li>
<li><a class="nav-link" href="#questions-about-mixed-results"><span class="header-section-number">3.4.2</span> Questions about mixed results</a></li>
<li><a class="nav-link" href="#open-questions-2"><span class="header-section-number">3.4.3</span> Open Questions</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/Lakens/statistical_inferences/blob/master/03-likelihoods.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/Lakens/statistical_inferences/edit/master/03-likelihoods.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Improving Your Statistical Inferences</strong>" was written by Daniël Lakens. It was last built on 2022-09-28.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
