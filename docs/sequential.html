<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Sequential Analysis | Improving Your Statistical Inferences</title>
<meta name="author" content="Daniël Lakens">
<meta name="description" content="Repeatedly analyzing incoming data while data collection is in progress has many advantages. Researchers can stop the data collection at an interim analysis when they can reject the null...">
<meta name="generator" content="bookdown 0.25 with bs4_book()">
<meta property="og:title" content="Chapter 10 Sequential Analysis | Improving Your Statistical Inferences">
<meta property="og:type" content="book">
<meta property="og:url" content="https://lakens.github.io/statistical_inferences/sequential.html">
<meta property="og:image" content="https://lakens.github.io/statistical_inferences/images/logo.png">
<meta property="og:description" content="Repeatedly analyzing incoming data while data collection is in progress has many advantages. Researchers can stop the data collection at an interim analysis when they can reject the null...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 10 Sequential Analysis | Improving Your Statistical Inferences">
<meta name="twitter:description" content="Repeatedly analyzing incoming data while data collection is in progress has many advantages. Researchers can stop the data collection at an interim analysis when they can reject the null...">
<meta name="twitter:image" content="https://lakens.github.io/statistical_inferences/images/logo.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Open%20Sans-0.4.0/font.css" rel="stylesheet">
<link href="libs/_Fira%20Code-0.4.0/font.css" rel="stylesheet">
<link href="libs/_Montserrat-0.4.0/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-0MK2WTGRM3"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-0MK2WTGRM3');
    </script><link rel="shortcut icon" href="images/favicon.ico">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Improving Your Statistical Inferences</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction</a></li>
<li><a class="" href="pvalue.html"><span class="header-section-number">1</span> Using p-values to test a hypothesis</a></li>
<li><a class="" href="errorcontrol.html"><span class="header-section-number">2</span> Error control</a></li>
<li><a class="" href="likelihoods.html"><span class="header-section-number">3</span> Likelihoods</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">4</span> Bayesian statistics</a></li>
<li><a class="" href="questions.html"><span class="header-section-number">5</span> Asking Statistical Questions</a></li>
<li><a class="" href="effectsize.html"><span class="header-section-number">6</span> Effect Sizes</a></li>
<li><a class="" href="confint.html"><span class="header-section-number">7</span> Confidence Intervals</a></li>
<li><a class="" href="power.html"><span class="header-section-number">8</span> Sample size justification</a></li>
<li><a class="" href="equivalencetest.html"><span class="header-section-number">9</span> Equivalence Testing and Interval Hypotheses</a></li>
<li><a class="active" href="sequential.html"><span class="header-section-number">10</span> Sequential Analysis</a></li>
<li><a class="" href="meta.html"><span class="header-section-number">11</span> Meta-analysis</a></li>
<li><a class="" href="bias.html"><span class="header-section-number">12</span> Bias detection</a></li>
<li><a class="" href="prereg.html"><span class="header-section-number">13</span> Preregistration and Transparency</a></li>
<li><a class="" href="computationalreproducibility.html"><span class="header-section-number">14</span> Computational Reproducibility</a></li>
<li><a class="" href="references.html"><span class="header-section-number">15</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/Lakens/statistical_inferences">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Improving%20Your%20Statistical%20Inferences&amp;rft.rights=CC-BY-NC-SA&amp;rft.description=This%20open%20educational%20resource%20contains%20information%20to%20improve%20statistical%20inferences%2C%20design%20better%20experiments%2C%20and%20report%20scientific%20research%20more%20transparently.&amp;rft.identifier=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.6409077&amp;rft.aufirst=Dani%C3%ABl&amp;rft.aulast=Lakens&amp;rft.au=Dani%C3%ABl%20Lakens&amp;rft.date=2022&amp;rft.language=en"></span>
<div id="sequential" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Sequential Analysis<a class="anchor" aria-label="anchor" href="#sequential"><i class="fas fa-link"></i></a>
</h1>
<p>Repeatedly analyzing incoming data while data collection is in progress has many advantages. Researchers can stop the data collection at an interim analysis when they can reject the null hypothesis or the smallest effect size of interest, even if they would be willing to collect more data if needed, or if the results show there is an unexpected problem with the study (e.g., participants misunderstand the instructions or questions). One could easily argue that psychological researchers have an ethical obligation to repeatedly analyze accumulating data, given that continuing data collection whenever the desired level of confidence is reached, or whenever it is sufficiently clear that the expected effects are not present, is a waste of the time of participants and the money provided by taxpayers. In addition to this ethical argument, designing studies that make use of sequential analyses can be more efficient than when data is only analyzed a single time, when the maximum sample size a researcher is willing to collect has been reached.</p>
<p>Sequential analyses should not be confused with <a href="errorcontrol.html#optionalstopping"><strong>optional stopping</strong></a>, which was discussed in the chapter on error control. In optional stopping, researchers use an unadjusted alpha level (e.g., 5%) to repeatedly analyze the data as it comes in, which can substantially inflate the Type 1 error rate. The critical difference with <strong>sequential analysis</strong> is that the Type 1 error rate is controlled. By lowering the alpha level at each look at the data, the overall Type I error rate can be controlled, much like a Bonferroni correction is used to prevent inflation of the Type 1 error rate for multiple comparisons. Indeed, the Bonferroni correction is a valid (but conservative) approach to control the error rate in sequential analyses <span class="citation">(<a href="references.html#ref-wassmer_group_2016" role="doc-biblioref">Wassmer &amp; Brannath, 2016</a>)</span>.</p>
<p>In sequential analysis a researcher designs a study such that they are able to perform <strong>interim analyses</strong>, say when 25%, 50%, and 75% of the data is collected. At each interim analysis a test is performed at a corrected alpha level, so that over all planned analyses the desired Type 1 error rate is maintained. Sequential analyses are commonly used in medical trials, where quickly discovering an effective treatment can be a matter of life and death. If at an interim analysis researchers decide a new drug is effective, they want to terminate the trial, and give the working drug to patients in the control condition to improve their health, or even save their lives. For example, the safety and efficacy of the Pfizer–BioNTech COVID-19 vaccine used an experimental design where they planned to analyze the data 5 times, and controlled the overall Type 1 error rate by lowering the alpha level for each interim analysis.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:interim"></span>
<img src="images/vaccinetrial.png" alt='Screenshot of the &lt;a href="https://www.nejm.org/doi/suppl/10.1056/NEJMoa2034577/suppl_file/nejmoa2034577_protocol.pdf"&gt;planned interim analyses&lt;/a&gt; examining the safety and Efficacy of the BNT162b2 mRNA Covid-19 Vaccine.' width="100%"><p class="caption">
Figure 10.1: Screenshot of the <a href="https://www.nejm.org/doi/suppl/10.1056/NEJMoa2034577/suppl_file/nejmoa2034577_protocol.pdf">planned interim analyses</a> examining the safety and Efficacy of the BNT162b2 mRNA Covid-19 Vaccine.
</p>
</div>
<p>The use of sequential analyses is only slowly becoming more popular in many scientific disciplines, but sequential analysis techniques have a long history. As early as 1929, Dodge and Romig realized that analyzing the data sequentially was more efficient than doing so once <span class="citation">(<a href="references.html#ref-dodge_method_1929" role="doc-biblioref">Dodge &amp; Romig, 1929</a>)</span>. Wald, who popularized the idea of sequential tests of hypotheses in <span class="citation">(<a href="references.html#ref-wald_sequential_1945" role="doc-biblioref">1945</a>)</span>, performed his work during the second world war. He was only allowed to publish his findings after the war had ended, as he explains in a historical note:</p>
<blockquote>
<p>Because of the substantial savings in the expected number of observations effected by the sequential probability ratio test, and because of the simplicity of this test procedure in practical applications, the National Defense Research Committee considered these developments sufficiently useful for the war effort to make it desirable to keep the results out of the reach of the enemy, at least for a certain period of time. The author was, therefore, requested to submit his findings in a restricted report which was dated September, 1943.</p>
</blockquote>
<p>Sequential analyses are well-established procedures, and have been developed in great detail over the last decades <span class="citation">(<a href="references.html#ref-jennison_group_2000" role="doc-biblioref">Jennison &amp; Turnbull, 2000</a>; <a href="references.html#ref-proschan_statistical_2006" role="doc-biblioref">Proschan et al., 2006</a>; <a href="references.html#ref-wassmer_group_2016" role="doc-biblioref">Wassmer &amp; Brannath, 2016</a>)</span>. Here, we will explain the basics of how to control error rates in group sequential analyses, and perform a-priori power analysis and compare when sequential designs will be more or less efficient than fixed designs. Before we discuss these topics, it is useful to clarify some terminology. A <strong>look</strong> means analyzing all the data collected up to a specific point. E.g., you look after 50, 100, and 150 observations, and analyze all the data that has been collected up to that point. After 50 and 100 observations we perform an <strong>interim analysis</strong>, and after 150 observations we perform the <strong>final analysis</strong>, after which we always stop. Not all looks have to occur in practice. If the analysis reveals a statistically significant result at look 1, data collection can be terminated. We can stop because we reject <span class="math inline">\(H_0\)</span> (e.g., in a null hypothesis significance test), or because we reject <span class="math inline">\(H_1\)</span> (e.g., in an equivalence test). We can also stop for <strong>curtailment</strong> or for <strong>futility</strong>: It is either impossible, or very unlikely for the final analysis to yield p &lt; alpha. The <strong>overall alpha level</strong> in a sequential design differs from the alpha level at each look (also called <strong>stage</strong>). For example, if we want an overall Type I error rate of 5% for a two-sided test with 3 looks, the alpha level for each look could be 0.0221 (if we decide to use a correction for the alpha level proposed by Pocock <span class="citation">(<a href="references.html#ref-pocock_group_1977" role="doc-biblioref">Pocock, 1977</a>)</span>. In this chapter we will focus on group sequential designs, where data is collected in multiple groups, but other sequential approaches exist, as explained in the chapter on <a href="power.html#sequentialsamplesize">sample size justification</a>.</p>
<div id="choosing-alpha-levels-for-sequential-analyses." class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Choosing alpha levels for sequential analyses.<a class="anchor" aria-label="anchor" href="#choosing-alpha-levels-for-sequential-analyses."><i class="fas fa-link"></i></a>
</h2>
<p>If one would analyze the data at multiple looks without correcting the alpha level, the Type 1 error rate would inflate <span class="citation">(<a href="references.html#ref-armitage_repeated_1969" role="doc-biblioref">Armitage et al., 1969</a>)</span>. As Armitage and colleagues show, with equally spaced looks, the alpha level inflates to 0.142 after 5 looks, 0.374 after 100 looks, and 0.530 after 1000 looks. Looking at the data twice is conceptually similar to deciding if a result is significant if one of two dependent variables shows a statistically significant effect. However, an important difference is that in the case of sequential analyses the multiple tests are not independent, but dependent. A test at look to combines the old data collected at look 1 with the new data at look 2. This means the Type 1 error rate inflates less quickly compared to independent tests, and we will see below this enables more efficient and flexible solutions to controlling error rates.</p>
<p>When controlling the Type 1 error rate in sequential analyses, a decision needs to be made about how to spend the alpha level across all looks at the data. For example, when a researcher plans a study with one interim look and one final look at the data, boundary critical Z-values need to be set of the first look (at <em>n</em> out of <em>N</em> observations) and the second look (at <em>N</em> observations). These two critical values, <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> (for the first and the second analysis) need to be chosen such that the probability (Pr) that the null hypothesis is rejected either when in the first analysis the critical <span class="math inline">\(Z_n\)</span> ≥ <span class="math inline">\(c_1\)</span> or (when <span class="math inline">\(Z_n\)</span> &lt; c1 in the first analysis) when <span class="math inline">\(Z_N\)</span> ≥ <span class="math inline">\(c_2\)</span> in the second analysis. In formal terms, for a directional test:</p>
<p><span class="math display">\[Pr\{Z_n \geq c_1\} + Pr\{Zn &lt; c_1, Z_N \geq c_2\} = \alpha\]</span></p>
<p>With more than one interim analysis, additional critical values have to be determined following the same rationale. If you combine multiple looks at the data with multiple comparisons, you would correct the alpha level twice, once for multiple comparisons, and then for multiple looks. Because the alpha level is corrected, it does not matter which statistical test you perform at each look, all that matters is that the <em>p</em>-value is compared to the corrected alpha level. The corrections discussed below are valid for any design where the data is normally distributed, and where each group of observations is independent of the previous group.</p>
</div>
<div id="the-pocock-correction" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> The Pocock correction<a class="anchor" aria-label="anchor" href="#the-pocock-correction"><i class="fas fa-link"></i></a>
</h2>
<p>The first decision researchers need to make is how they want to correct the Type I error rate across looks. Four common approaches are the Pocock correction, the O'Brien-Fleming correction, The Haybittle &amp; Peto correction, and the Wang and Tsiatis approach. Users are also free to specify their own preferred way to spend the alpha level across looks.</p>
<p>The Pocock correction is the simplest way to correct the alpha level for multiple looks. Conceptually, it is very similar to the Bonferroni correction. The Pocock correction has been created such that the alpha level is identical for each look at the data, resulting in constant critical values (expressed as <em>z</em> values) <span class="math inline">\(u_k = c\)</span> to reject the null hypothesis, <span class="math inline">\(H_0\)</span>, at look <span class="math inline">\(k\)</span>. The following code uses the package <code>rpact</code> to design a study for a sequential analysis:</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.rpact.org">rpact</a></span><span class="op">)</span>
<span class="va">design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  kMax <span class="op">=</span> <span class="fl">2</span>,
  typeOfDesign <span class="op">=</span> <span class="st">"P"</span>,
  sided <span class="op">=</span> <span class="fl">2</span>,
  alpha <span class="op">=</span> <span class="fl">0.05</span>,
  beta <span class="op">=</span> <span class="fl">0.1</span>
<span class="op">)</span>
<span class="va">design</span></code></pre></div>
<pre><code>## Design parameters and output of group sequential design:
## 
## User defined parameters:
##   Type of design                               : Pocock 
##   Maximum number of stages                     : 2 
##   Stages                                       : 1, 2 
##   Significance level                           : 0.0500 
##   Type II error rate                           : 0.1000 
##   Test                                         : two-sided 
## 
## Derived from user defined parameters:
##   Information rates                            : 0.500, 1.000 
## 
## Default parameters:
##   Two-sided power                              : FALSE 
##   Tolerance                                    : 0.00000001 
## 
## Output:
##   Cumulative alpha spending                    : 0.02939, 0.05000 
##   Critical values                              : 2.178, 2.178 
##   Stage levels (one-sided)                     : 0.01469, 0.01469</code></pre>
<p>The output tells us we have design a study with 2 looks (one interim, one final) using the Pocock spending function. The last line returns one-sided alpha levels. The <code>rpact</code> package focuses on Confirmatory Adaptive Clinical Trial Design and Analysis. In clinical trials, researchers mostly test directional predictions, and thus, the default setting is to perform a one-sided test. In clinical trials it is common to use a 0.025 significance level for one-sided tests, but in many other fields, 0.05 is a more common default. We can get the two-sided alpha levels by multilpying the one-sided alpha levels by two:</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span><span class="op">$</span><span class="va">stageLevels</span> <span class="op">*</span> <span class="fl">2</span></code></pre></div>
<pre><code>## [1] 0.02938579 0.02938579</code></pre>
<p>We can check the output against the <a href="https://en.wikipedia.org/wiki/Pocock_boundary">Wikipedia page for the Pocock correction</a> where we indeed see that with 2 looks at the data the alpha level for each look is 0.0294. The Pocock correction is slightly more efficient than using a Bonferroni correction (in which case the alpha levels would be 0.025), because of the dependency in the data (at the second look, the data analyzed at the first look is again part of the analysis).</p>
<p><code>rpact</code> makes it easy to plot the boundaries (based on the critical values) for each look. We see the critical values are higher than the 1.96 we would use for a fixed design with a 5% alpha level, namely <em>Z</em> = 2.178. Whenever we observe a test statistical that is more extreme than the critical value, we can reject the null hypothesis.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:boundplot1"></span>
<img src="10-sequential_files/figure-html/boundplot1-1.png" alt="Plot of critical boundaries at each look for a 2 look design with a Pocock correction." width="100%"><p class="caption">
Figure 10.2: Plot of critical boundaries at each look for a 2 look design with a Pocock correction.
</p>
</div>
<p>The analysis can also be performed in the rpact <a href="https://rpact.shinyapps.io/public/">shiny app</a> which also allows users to create all plots through simple menu options, and download a complete report of the analyses (e.g., for a preregistration document).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:rpactshiny"></span>
<img src="images/rpact1.png" alt="Screenshot of rpact Shiny app." width="100%"><p class="caption">
Figure 10.3: Screenshot of rpact Shiny app.
</p>
</div>
</div>
<div id="comparing-spending-functions" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Comparing Spending Functions<a class="anchor" aria-label="anchor" href="#comparing-spending-functions"><i class="fas fa-link"></i></a>
</h2>
<p>We can vizualize the corrections for different types of designs for each of 3 looks (2 interim looks and one final look) in the same plot (see Figure <a href="sequential.html#fig:fourspendingfunctions">10.4</a>). The plot below shows the Pocock, O’Brien-Fleming, Haybittle-Peto, and Wang-Tsiatis correction with <span class="math inline">\(\Delta\)</span> = 0.25. We see that researchers can choose different approaches to spend their alpha level across looks. Researchers can choose to spend their alpha conservatively (keeping most of the alpha for the last look), or more liberally (spending more alpha at the earlier looks, which increases the probability of stopping early for many true effect sizes).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fourspendingfunctions"></span>
<img src="10-sequential_files/figure-html/fourspendingfunctions-1.png" alt="Four different alpha spending functions (O'Brien-Fleming, Pocock, Haybittle-Peto, Wang-Tsiatis) for 3 looks." width="100%"><p class="caption">
Figure 10.4: Four different alpha spending functions (O'Brien-Fleming, Pocock, Haybittle-Peto, Wang-Tsiatis) for 3 looks.
</p>
</div>
<p>We can see that the O'Brien and Fleming correction is much more conservative at the first look, and close to the uncorrected critical value of 1.96 (the black dashed line - for two-sided tests all critical values are mirrored in the negative direction) at the last look: 3.471, 2.454, and 2.004. The Pocock correction has the same critical value at each look (2.289, 2.289, and 2.289), the Haybittle and Peto correction has the same critical value at each look but the last (3, 3, and 1.975), while the critical values decrease for each look with the Wang and Tsiatis correction (2.741, 2.305, and 2.083).</p>
<p>Being conservative during early looks is sensible if you mainly want to monitor the results for unexpected developments. A Pocock correction is more useful when there is substantial uncertainty in both in whether an effect is present, and how large the effect size is, as it gives a higher probability of stopping the experiment early if the effects are large. Because the statistical power of a test depends on the alpha level, lowering the alpha level at the final look means that the statistical power is lower compared to a fixed design, and that to achieve a desired power, the sample size of a study needs to be increased to maintain the same statistical power at the last look. This increase in sample size can be compensated by stopping data collection early, in which case a sequential design is more efficient than a fixed design. Because the alpha at the last look for O’Brien-Fleming or Haybittle-Peto designs are very similar to the statistical power for a fixed design with only one look, the required sample size is also very similar. The Pocock correction requires a larger increase in the maximum sample size to achieve the desired power compared to a fixed design.</p>
<p>Corrected alpha levels can be computed to many digits, but this quickly reaches a level of precision that is meaningless in real life. The observed Type I error rate for all tests you will do in your lifetime is not noticeably different if you set the alpha level at 0.0194, 0.019, or 0.02 (see the concept of ‘<a href="https://en.wikipedia.org/wiki/Significant_figures">significant digits</a>'. Even as we calculate and use alpha thresholds up to many digits in sequential tests, the messiness of most research makes these alpha levels have <a href="https://en.wikipedia.org/wiki/False_precision">false precision</a>. Keep this in mind when interpreting your data.</p>
</div>
<div id="alpha-spending-functions" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> Alpha spending functions<a class="anchor" aria-label="anchor" href="#alpha-spending-functions"><i class="fas fa-link"></i></a>
</h2>
<p>The approaches to specify the shape of decision boundaries across looks discussed so far have an important limitation <span class="citation">(<a href="references.html#ref-proschan_statistical_2006" role="doc-biblioref">Proschan et al., 2006</a>)</span>. They require a pre-specified number of looks (e.g., 4), and the sample size for the interim looks need to be pre-specified as well (e.g., after 25%, 50%, 75%, and 100% of observations). It is logistically not always feasible to stop the data collection exactly at 25% of the planned total sample size. An important contribution to the sequential testing literature was made by Lan and DeMets <span class="citation">(<a href="references.html#ref-lan_discrete_1983" role="doc-biblioref">1983</a>)</span> who introduced the alpha spending approach to correct the alpha level. In this approach the cumulative Type I error rate spent across the looks is pre-specified through a function (the <em>alpha spending function</em>) in order to control the overall significance level <span class="math inline">\(\alpha\)</span> at the end of the study.</p>
<p>The main benefit of these alpha spending functions is that error rates at interim analyses can be controlled, while neither the number nor the timing of the looks needs to be specified in advance. This makes alpha spending approaches much more flexible than earlier approaches to controlling the Type 1 error in group sequential designs. When using an alpha spending function it is important that the decision to perform an interim analysis is not based on collected data, as this can still increase the Type I error rate. As long as this assumption is met, it is possible to update the alpha levels at each look during a study.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="10-sequential_files/figure-html/unnamed-chunk-5-1.png" alt="Comparison of Pocock and O'Brien-Fleming correction, and Pocock-like and O'Brien-Fleming like alpha spending function, for 5 looks." width="100%"><p class="caption">
Figure 10.5: Comparison of Pocock and O'Brien-Fleming correction, and Pocock-like and O'Brien-Fleming like alpha spending function, for 5 looks.
</p>
</div>
</div>
<div id="updating-boundaries-during-a-study" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Updating boundaries during a study<a class="anchor" aria-label="anchor" href="#updating-boundaries-during-a-study"><i class="fas fa-link"></i></a>
</h2>
<p>Although alpha spending functions control the Type I error rate even when there are deviations from the pre-planned number of looks, or their timing, this does require recalculating the boundaries used in the statistical test based on the amount of information that has been observed. Let us assume a researcher designs a study with three equally spaced looks at the data (two interim looks, one final look), using a Pocock-type alpha spending function, where results will be analyzed in a two-sided <em>t</em>-test with an overall desired Type I error rate of 0.05, and a desired power of 0.9 for a Cohen’s <em>d</em> of 0.5. An a-priori power analysis (which we will explain in the next section) shows that we achieve the desired power in our sequential design if we plan to look after 65.4, 130.9, and 196.3 observations in each condition. Since we cannot collect partial participants, we should round these numbers up, and because we have 2 independent groups, we will collect 66 observations for look 1 (33 in each condition), 132 at the second look (66 in each condition) and 198 at the third look (99 in each condition). The code below computes the alpha levels at each look (or stage) for a two-sided test:</p>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>kMax <span class="op">=</span> <span class="fl">3</span>, 
                                   typeOfDesign <span class="op">=</span> <span class="st">"asP"</span>,
                                   sided <span class="op">=</span> <span class="fl">2</span>, 
                                   alpha <span class="op">=</span> <span class="fl">0.05</span>, 
                                   beta <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>
<span class="va">design</span><span class="op">$</span><span class="va">stageLevels</span> <span class="op">*</span> <span class="fl">2</span></code></pre></div>
<pre><code>## [1] 0.02264162 0.02173822 0.02167941</code></pre>
<p>Now imagine that due to logistical issues, we do not manage to analyze the data until we have collected data from 76 observations (38 in each condition) instead of the planned 66 observations. Such logistical issues are common in practice, and one of the main reasons alpha spending functions for group sequential designs were developed. Our first look at the data does not occur at the planned time of collecting 33.3% of the total sample, but at 76/198 = 38.4% of the planned sample. We can recalculate the alpha level we should use for each look at the data, based on the current look, and planned future looks. Instead of using the alpha levels 0.0226, 0.0217, and 0.0217 at the three respective looks (as calculated above, and note how in the Pocock-style alpha spending function the alpha levels are almost, but not exactly, the same at each look, unlike the Pocock correction where the are identical at each look). We can adjust the information rates by explicitly specifying them using <code>informationRates</code> in the code below. The first look now occurs at 76/198 of the planned sample. The second look is still planned to occur at 2/3 of the sample, and the final look at the planned maximum sample size.</p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  typeOfDesign <span class="op">=</span> <span class="st">"asP"</span>, 
  informationRates <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">76</span><span class="op">/</span><span class="fl">198</span>, <span class="fl">2</span><span class="op">/</span><span class="fl">3</span>, <span class="fl">1</span><span class="op">)</span>, 
  alpha <span class="op">=</span> <span class="fl">0.05</span>, 
  sided <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="va">design</span><span class="op">$</span><span class="va">stageLevels</span> <span class="op">*</span> <span class="fl">2</span></code></pre></div>
<pre><code>## [1] 0.02532710 0.02043978 0.02164755</code></pre>
<p>The updated alpha levels are 0.0253 for the current look, 0.0204 for the second look, and 0.0216 for the final look. The alpha level we will use for the first look is therefore not 0.0226 (as originally planned) but the slightly higher 0.0253. The second look will now use a slightly lower alpha of 0.0204 instead of 0.0217. The differences are small, but the fact that there is a formal method to control the alpha level that provides the flexibility to look at different times than originally planned is extremely useful.</p>
<p>It is also possible to correct the alpha level if the final look at the data changes, for example because you are not able to collect the intended sample size, or because due to unforeseen circumstances you collect more data than planned. This is nowadays increasingly common as people preregister their studies, or publish using Registered Reports. Sometimes they end up with slightly more data than planned, which raises the question is they should analyze the planned sample size, or all the data. Analyzing all the collected data prevents wasting responses from participants, and uses all the information available, but it increases the flexibility in the data analysis (as researchers can now choose to analyze either the data from the planned sample, or all the data they have collected). Alpha spending functions solve this conundrum by allowing researchers to analyze all the data, while updating the alpha level that is used to control the overall alpha level.</p>
<p>If more data is collected than was planned, we can no longer use the alpha spending function that was chosen (i.e., the Pocock spending function), and instead have to provide a <strong>user-defined alpha spending function</strong> by updating the timing and alpha spending function to reflect the data collection as it actually occurred up to the final look. Assuming the second look in our earlier example occurred as originally planned at 2/3 of the data we planned to collect, but the last look occurred at 206 participants instead of 198, we can compute an updated alpha level for the last look. Given the current total sample size, we need to recompute the alpha levels for the earlier looks, which now occurred at 76/206 = 0.369, 132/206 = 0.641, and for the last look at 206/206 = 1.</p>
<p>The first and second look occurred with the adjusted alpha levels we computed after the first adjustment (alpha levels of 0.0253 and 0.0204). We have already spent part of our total alpha at the first two looks. We can look at the "Cumulative alpha spent' in the results from the design we specified above, and see how much of our Type I error rate we spent so far:</p>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span><span class="op">$</span><span class="va">alphaSpent</span></code></pre></div>
<pre><code>## [1] 0.02532710 0.03816913 0.05000000</code></pre>
<p>We see we have spent 0.0253 after look 1, and 0.0382 after look 2. We also know we want to spend the remainder of our Type I error rate at the last look, for a total of 0.05.</p>
<p>Our actual alpha spending function is no longer captured by the Pocock spending function after collecting more data than planned, so instead specify a user defined spending function. We can perform these calculations using the code below by specifying the <code>userAlphaSpending</code> information, after choosing the <code>asUser</code> design:</p>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  typeOfDesign <span class="op">=</span> <span class="st">"asUser"</span>, 
  informationRates <span class="op">=</span>
  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">72</span><span class="op">/</span><span class="fl">206</span>, <span class="fl">132</span><span class="op">/</span><span class="fl">206</span>, <span class="fl">1</span><span class="op">)</span>, 
  alpha <span class="op">=</span> <span class="fl">0.05</span>, 
  sided <span class="op">=</span> <span class="fl">2</span>, 
  userAlphaSpending <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.0253</span>, <span class="fl">0.0382</span>, <span class="fl">0.05</span><span class="op">)</span>
<span class="op">)</span>
<span class="va">design</span><span class="op">$</span><span class="va">stageLevels</span> <span class="op">*</span> <span class="fl">2</span></code></pre></div>
<pre><code>## [1] 0.02530000 0.01987072 0.02075796</code></pre>
<p>The alpha levels for looks in the past do not correspond with the alpha levels we used, but the final alpha level (0.0208) gives the alpha level we should use for our final analysis based on a sample size that is larger than what we planned to collect. The difference with the alpha level we would have used if we collected the planned sample size is really small (0.0216 vs. 0.0208), in part because we did not miss the planned sample size by a lot. Such small differences in alpha levels will not really be noticeable in practice, but it is very useful that there is a formally correct solution to deal with collecting more data than planned, while controlling the Type 1 error rate. If you use sequential designs, you can use these corrections whenever you overshoot the sample size you planned to collect in a preregistration.</p>
</div>
<div id="sample-size-for-sequential-designs" class="section level2" number="10.6">
<h2>
<span class="header-section-number">10.6</span> Sample Size for Sequential Designs<a class="anchor" aria-label="anchor" href="#sample-size-for-sequential-designs"><i class="fas fa-link"></i></a>
</h2>
<p>Sequential designs require somewhat more participants than a fixed design at the final look, depending on how much the alpha level at this look is lowered due to the correction for multiple comparisons. Due to early stopping, sequential designs will on average require less participants. Let's first examine how many participants we would need in a fixed design, where we only analyze our data once. We have an alpha level of 0.05, and a Type 2 (beta) error of 0.1 - in other words, the desired power is 90%. We will perform one test, and assuming a normal distribution our critical Z-score would be 1.96, for an alpha level of 5%.</p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  kMax <span class="op">=</span> <span class="fl">1</span>,
  typeOfDesign <span class="op">=</span> <span class="st">"P"</span>,
  sided <span class="op">=</span> <span class="fl">2</span>,
  alpha <span class="op">=</span> <span class="fl">0.05</span>,
  beta <span class="op">=</span> <span class="fl">0.1</span>
<span class="op">)</span>

<span class="va">power_res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getSampleSizeMeans.html">getSampleSizeMeans</a></span><span class="op">(</span>
  design <span class="op">=</span> <span class="va">design</span>,
  groups <span class="op">=</span> <span class="fl">2</span>,
  alternative <span class="op">=</span> <span class="fl">0.5</span>, 
  stDev <span class="op">=</span> <span class="fl">1</span>, 
  allocationRatioPlanned <span class="op">=</span> <span class="fl">1</span>,
  normalApproximation <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>

<span class="va">power_res</span></code></pre></div>
<pre><code>## Design plan parameters and output for means:
## 
## Design parameters:
##   Critical values                              : 1.96 
##   Two-sided power                              : FALSE 
##   Significance level                           : 0.0500 
##   Type II error rate                           : 0.1000 
##   Test                                         : two-sided 
## 
## User defined parameters:
##   Alternatives                                 : 0.5 
## 
## Default parameters:
##   Mean ratio                                   : FALSE 
##   Theta H0                                     : 0 
##   Normal approximation                         : FALSE 
##   Standard deviation                           : 1 
##   Treatment groups                             : 2 
##   Planned allocation ratio                     : 1 
## 
## Sample size and output:
##   Number of subjects fixed                     : 170.1 
##   Number of subjects fixed (1)                 : 85 
##   Number of subjects fixed (2)                 : 85 
##   Lower critical values (treatment effect scale) : -0.303 
##   Upper critical values (treatment effect scale) : 0.303 
##   Local two-sided significance levels          : 0.0500 
## 
## Legend:
##   (i): values of treatment arm i</code></pre>
<p>We see we need 85 participants in each group, (or 86, since the sample size is actually 85.03 and the required number of observations is rounded up, and so we need 172 participants in total. Other power analysis software, such as G*Power, sould yield the same required sample size. We can now examine our design above with 2 looks, a Pocock-like alpha spending function, a 2 sided test with an alpha of 0.05. We will look 2 times, and expect a true effect of <em>d</em> = 0.5 (which we enter by specifying an alternative of 0.5, and a stDev of 1).</p>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">seq_design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  kMax <span class="op">=</span> <span class="fl">2</span>,
  typeOfDesign <span class="op">=</span> <span class="st">"asP"</span>,
  sided <span class="op">=</span> <span class="fl">2</span>,
  alpha <span class="op">=</span> <span class="fl">0.05</span>,
  beta <span class="op">=</span> <span class="fl">0.1</span>
  <span class="op">)</span>

<span class="co"># Compute the sample size we need</span>
<span class="va">power_res_seq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getSampleSizeMeans.html">getSampleSizeMeans</a></span><span class="op">(</span>
  design <span class="op">=</span> <span class="va">seq_design</span>,
  groups <span class="op">=</span> <span class="fl">2</span>,
  alternative <span class="op">=</span> <span class="fl">0.5</span>, 
  stDev <span class="op">=</span> <span class="fl">1</span>, 
  allocationRatioPlanned <span class="op">=</span> <span class="fl">1</span>,
  normalApproximation <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>

<span class="va">power_res_seq</span></code></pre></div>
<pre><code>## Design plan parameters and output for means:
## 
## Design parameters:
##   Information rates                            : 0.500, 1.000 
##   Critical values                              : 2.157, 2.201 
##   Futility bounds (non-binding)                : -Inf 
##   Cumulative alpha spending                    : 0.03101, 0.05000 
##   Local one-sided significance levels          : 0.01550, 0.01387 
##   Two-sided power                              : FALSE 
##   Significance level                           : 0.0500 
##   Type II error rate                           : 0.1000 
##   Test                                         : two-sided 
## 
## User defined parameters:
##   Alternatives                                 : 0.5 
## 
## Default parameters:
##   Mean ratio                                   : FALSE 
##   Theta H0                                     : 0 
##   Normal approximation                         : FALSE 
##   Standard deviation                           : 1 
##   Treatment groups                             : 2 
##   Planned allocation ratio                     : 1 
## 
## Sample size and output:
##   Reject per stage [1]                         : 0.6022 
##   Reject per stage [2]                         : 0.2978 
##   Early stop                                   : 0.6022 
##   Maximum number of subjects                   : 188.9 
##   Maximum number of subjects (1)               : 94.5 
##   Maximum number of subjects (2)               : 94.5 
##   Number of subjects [1]                       : 94.5 
##   Number of subjects [2]                       : 188.9 
##   Expected number of subjects under H0         : 186 
##   Expected number of subjects under H0/H1      : 172.7 
##   Expected number of subjects under H1         : 132.1 
##   Lower critical values (treatment effect scale) [1] : -0.451 
##   Lower critical values (treatment effect scale) [2] : -0.323 
##   Upper critical values (treatment effect scale) [1] : 0.451 
##   Upper critical values (treatment effect scale) [2] : 0.323 
##   Local two-sided significance levels [1]      : 0.03101 
##   Local two-sided significance levels [2]      : 0.02774 
## 
## Legend:
##   (i): values of treatment arm i
##   [k]: values at stage k</code></pre>
<p>The sample size per condition at the first look is 47.24, and at the second look it is 94.47, which means we are now collecting 190 instead of 172 participants. This is a consequence of lowering our alpha level at each look (from 0.05 to 0.028). To compensate for the lower alpha level, we need to increase the sample size of the study to achieve the same power.</p>
<p>However, the maximum sample size is not the expected sample size for this design, because of the possibility that we can stop data collection at an earlier look in the sequential design. In the long run, if <em>d</em> = 0.5, and we use an Pocock-like alpha spending function, and ignoring upward rounding because we can only collect a complete number of observations, we will sometimes collect 96 participants and stop after the first look, and the remaining time continue to 190 participants. As we see in the rows 'Reject per stage' the data collection is expected to stop after the first look in 0.6 of the studies because we have observed a significant result. The remainder of the time (1 - 0.6 = 0.4)</p>
<p>This means that, assuming there is a true effect of <em>d</em> = 0.5, the <em>expected</em> sample size on average is the probability of stopping at each look, multiplied by the number of observations we collect at each look, so 0.6 * 96 + 0.3 * 190 = 133.39. The <code>rpact</code> package returns 132.06 under "Expected number of subjects under <span class="math inline">\(H_1\)</span>" - the small difference is due to the fact that <code>rpact</code> does not round the number of observations up, although it should). So, assuming the true effect is <em>d</em> = 0.5, in any single study we might need to collect slightly more data than in a fixed design (where we would collect 172), but on average we will need to collect less observations in a sequential design.</p>
<p>Because power is a curve, and the true effect size is unknown, it is useful to plot power across a range of possible effect sizes, so that we can explore the expected sample size, in the long run, if we use a sequential design, for different true effect sizes.</p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Use getPowerMeans and set max N to 188 based on analysis above</span>
<span class="va">sample_res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getPowerMeans.html">getPowerMeans</a></span><span class="op">(</span>
  design <span class="op">=</span> <span class="va">seq_design</span>,
  groups <span class="op">=</span> <span class="fl">2</span>,
  alternative <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">0.01</span><span class="op">)</span>, 
  stDev <span class="op">=</span> <span class="fl">1</span>, 
  allocationRatioPlanned <span class="op">=</span> <span class="fl">1</span>,
  maxNumberOfSubjects <span class="op">=</span> <span class="fl">190</span>, 
  normalApproximation <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">sample_res</span>, type <span class="op">=</span> <span class="fl">6</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:powerseq"></span>
<img src="10-sequential_files/figure-html/powerseq-1.png" alt="Power curve for a sequential design with 2 looks." width="100%"><p class="caption">
Figure 10.6: Power curve for a sequential design with 2 looks.
</p>
</div>
<p>The blue line in Figure <a href="sequential.html#fig:powerseq">10.6</a> indicates the expected number of observations we need to collect. Not surprisingly, when the true effect size is 0, we will almost always continue data collection to the end. We will only stop if we observe a Type 1 error, which is rare, and thus the expected number of observations is very close to the maximum sample size we are willing to collect. On the other side of the graph we see the scenario for when the true effect size is <em>d</em> = 1. With such a large effect size, we will have high power at our first look, and we will almost always be able to stop at the first look. The red line indicates the power at the final look, and the green line indicates the probability of stopping early.</p>
<p>The Pocock correction leads to a substantially lower alpha level at the last look, which requires an increase in sample size to compensate. As we saw before, the O'Brien-Fleming spending function does not require such a severe reduction in the alpha level at the last look. As the power analysis below shows, with 2 looks, this design would not need an increase in sample size at all in practice.</p>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">seq_design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  kMax <span class="op">=</span> <span class="fl">4</span>,
  typeOfDesign <span class="op">=</span> <span class="st">"asOF"</span>,
  sided <span class="op">=</span> <span class="fl">2</span>,
  alpha <span class="op">=</span> <span class="fl">0.05</span>,
  beta <span class="op">=</span> <span class="fl">0.1</span>
  <span class="op">)</span>

<span class="co"># Compute the sample size we need</span>
<span class="va">power_res_seq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getSampleSizeMeans.html">getSampleSizeMeans</a></span><span class="op">(</span>
  design <span class="op">=</span> <span class="va">seq_design</span>,
  groups <span class="op">=</span> <span class="fl">2</span>,
  alternative <span class="op">=</span> <span class="fl">0.5</span>, 
  stDev <span class="op">=</span> <span class="fl">1</span>, 
  allocationRatioPlanned <span class="op">=</span> <span class="fl">1</span>,
  normalApproximation <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">power_res_seq</span><span class="op">)</span></code></pre></div>
<pre><code>## Sample size calculation for a continuous endpoint
## 
## Sequential analysis with a maximum of 4 looks (group sequential design), overall 
## significance level 5% (two-sided).
## The sample size was calculated for a two-sample t-test, H0: mu(1) - mu(2) = 0, 
## H1: effect = 0.5, standard deviation = 1, power 90%.
## 
## Stage                                          1       2       3       4 
## Information rate                             25%     50%     75%    100% 
## Efficacy boundary (z-value scale)          4.333   2.963   2.359   2.014 
## Overall power                             0.0035  0.2579  0.6853  0.9000 
## Expected number of subjects                132.2 
## Number of subjects                          43.3    86.6   129.9   173.2 
## Cumulative alpha spent                   &lt;0.0001  0.0031  0.0193  0.0500 
## Two-sided local significance level       &lt;0.0001  0.0030  0.0183  0.0440 
## Lower efficacy boundary (t)               -1.493  -0.656  -0.419  -0.308 
## Upper efficacy boundary (t)                1.493   0.656   0.419   0.308 
## Exit probability for efficacy (under H0) &lt;0.0001  0.0030  0.0162 
## Exit probability for efficacy (under H1)  0.0035  0.2544  0.4274 
## 
## Legend:
##   (t): treatment effect scale</code></pre>
<p>This design meets the desired power when we collect 88 participants - exactly as many as when we would <em>not</em> look at the data once. We basically get a free look at the data, with the expected number of participants (assuming <em>d</em> = 0.5) dropping to 132.19. Increasing the number of looks to 4 comes at only a very small required increase in the number of participants to maintain the same statistical power, but further descreases the expected sample size. Especially for a conservative a-priori power analysis, or when performing an a-priori power analysis for a smallest effect size of interest, and there is a decent probability that the true effect size is larger, using sequential analysis is a very attractive option.</p>
</div>
<div id="stopping-for-futility" class="section level2" number="10.7">
<h2>
<span class="header-section-number">10.7</span> Stopping for futility<a class="anchor" aria-label="anchor" href="#stopping-for-futility"><i class="fas fa-link"></i></a>
</h2>
<p>So far, the sequential designs we have discussed would only stop at an interim analysis if we can reject <span class="math inline">\(H_0\)</span>. A well-designed study also takes into account the possibility that there is no effect, as we discussed in the chapter on <a href="equivalencetest.html#equivalencetest">equivalence testing</a>. In the sequential analysis literature, stopping to reject the presence of the smallest effect size of interest is called <strong>stopping for futility</strong>. In the most extreme case, it could be impossible after an interim analysis that the final analysis will yield a statistically significant result. To illustrate this in a hypothetical scenario, imagine that after collecting 182 out of 192 observations, the observed mean difference between two independent conditions is 0.1, while the study was designed with the idea that the smallest effect deemed worthwhile is a mean difference of 0.5. If the primary dependent variable is measured on a 7 point Likert scale, it might be that even if every of the remaining 5 participants in the control condition answers 1, and every of the remaining participants in the experimental condition answers 7, the effect size after 192 observations will not yield <em>p</em> &lt; <span class="math inline">\(\alpha\)</span>. If the goal of your study was to detect whether there was an effect of at least a mean difference of 0.5, at this point a researcher knows that goal will not be reached. Stopping a study at an interim analysis because the final result cannot yield a significant effect is called <em>non-stochastic curtailment</em>.</p>
<p>In less extreme but more common situations, it might still be possible for the study to observe a significant effect, but the probability might be very small. The probability of finding a significant result, given the data that have been observed up to an interim analysis, is called <strong>conditional power</strong>. Performing the conditional power analysis on the effect size that was originally expected might be too optimistic, but it is also undesirable to use the observed effect size, which typically has quite some uncertainty. One proposal is to update adjust the expected effect size based on the observed data. If a Bayesian updating procedure is used, this is called <strong>predictive power</strong> <span class="citation">(<a href="references.html#ref-spiegelhalter_monitoring_1986" role="doc-biblioref">D. J. Spiegelhalter et al., 1986</a>)</span>. It is possible to use <strong>adaptive designs</strong> that allow researchers to increase the final number of observations based on an interim analysis without inflating the Type 1 error rate (see <span class="citation">Wassmer &amp; Brannath (<a href="references.html#ref-wassmer_group_2016" role="doc-biblioref">2016</a>)</span>).</p>
<p>Alternatively, if the observed effect size is smaller than expected, one might want to stop for futility. As an illustration of a simple stopping rule for futility, imagine a researcher who will stop for futility whenever the observed effect size is is either zero, or in the opposite direction as was predicted. In Figure <a href="sequential.html#fig:futility1">10.7</a> the red line indicates critical values to declare a significant effect. In essence, this means that if the observed <em>z</em>-score for the interim test is either 0 or negative, data collection will be terminated. This can be specified by adding <code>futilityBounds = c(0, 0)</code> to the specification of the sequential design. One can choose in advance to stop whenever the criteria to stop for futility have been met, (i.e., a binding futility rule), but it is typically recommended to allow the possibility to continue data collection (i.e., a non-binding futility rule, specified by setting <code>bindingFutility = FALSE</code>).</p>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  sided <span class="op">=</span> <span class="fl">1</span>,
  alpha <span class="op">=</span> <span class="fl">0.05</span>,
  beta <span class="op">=</span> <span class="fl">0.1</span>,
  typeOfDesign <span class="op">=</span> <span class="st">"asP"</span>,
  futilityBounds <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>,
  bindingFutility <span class="op">=</span> <span class="cn">FALSE</span>
<span class="op">)</span></code></pre></div>
<p>In Figure <a href="sequential.html#fig:futility1">10.7</a> we see a sequential design where data collection is stopped to reject <span class="math inline">\(H_0\)</span> when the observed <em>z</em>-score is larger than the values indicated by the red line, computed based on a Pocock-like alpha spending function (as in Figure <a href="sequential.html#fig:fourspendingfunctions">10.4</a>. In addition, data collection will stop when at an interim analysis a <em>z</em>-score lower than or equal to 0 is observed, as indicated by the blue line. At the last look, the red and blue lines meet, because we will either reject <span class="math inline">\(H_0\)</span> at the critical value, or fail to reject <span class="math inline">\(H_0\)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:futility1"></span>
<img src="10-sequential_files/figure-html/futility1-1.png" alt="Pocock-type boundaries for 3 looks to stop when rejecting $H_0$ (red line) or to stop for futility (blue line) when the observed effect is in the opposite direction." width="100%"><p class="caption">
Figure 10.7: Pocock-type boundaries for 3 looks to stop when rejecting <span class="math inline">\(H_0\)</span> (red line) or to stop for futility (blue line) when the observed effect is in the opposite direction.
</p>
</div>
<p>Manually specifying the futility bounds is not ideal, as we risk stopping data collection because we fail to reject <span class="math inline">\(H_0\)</span>, when there is a high probability of a Type 2 error. It is better to set the futility bounds by directly controlling the Type 2 error across looks at the data. Just as we are willing to distribute our Type I error rate across interim analyses, we can distribute our Type II error rate across looks, and decide to stop for futility when we fail to reject the effect size of interest with a desired Type 2 error rate.</p>
<p>When a study is designed such that the null hypothesis significance test has 90% power to detect an effect of <em>d</em> = 0.5, 10% of the time <span class="math inline">\(H_0\)</span> will not be rejected, when it should. In these 10% of cases where we make a Type 2 error, the conclusion will be that an effect of 0.5 is not present, when in reality, there is an effect of <em>d</em> = 0.5 (or larger). In an equivalence against a smallest effect size of interest of <em>d</em> = 0.5, the conclusion that an effect of 0.5 or larger is not present, when in reality, there is an effect of <em>d</em> = 0.5 (or larger) is called a Type 1 error: We incorrectly conclude the effect is practically equivalent to zero. Therefore, what is a Type 2 error in NHST when <span class="math inline">\(H_0\)</span> is <em>d</em> = 0 and <span class="math inline">\(H_1\)</span> = <em>d</em> = 0.5 is a Type 1 error in an equivalence test where <span class="math inline">\(H_0\)</span> is <em>d</em> = 0.5 and <span class="math inline">\(H_1\)</span> is <em>d</em> = 0 <span class="citation">(<a href="references.html#ref-jennison_group_2000" role="doc-biblioref">Jennison &amp; Turnbull, 2000</a>)</span>. Controlling the Type 2 error in a sequential design can therefore be seen as controlling the Type 1 error for an equivalence test against the effect size the study is powered for. If we design a study to have a 5% Type 1 error rate and equally low Type 2 error rate (e.g., 5%, or 95% power), the study is an informative test for the presence or the absence of an effect of interest.</p>
<p>If the true effect size is (close to) 0, sequential designs that stop for futility are more efficient than designs that do not stop for futility. Adding futility bounds based on beta-spending functions reduces power, which needs to be compensated by increasing the sample size, but this can be compensated by the fact that studies can stop earlier for futility, which can make designs more efficient. When specifying a smallest effect size of interest is not possible, researchers might not want to incorporate stopping for futility into the study design. To control the Type 2 error rate across looks, a <strong>beta-spending function</strong> needs to be chosen, such as a Pocock type beta spending function, an O'Brien-Fleming type beta spending function, or a user defined beta spending function (the beta-spending function does not need to be the same as the alpha-spending function). In <code>rpact</code> beta-spending functions can only be chosen for directional (one-sided) tests.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:futility2"></span>
<img src="10-sequential_files/figure-html/futility2-1.png" alt="Pocock-type boundaries for 3 looks to stop when rejecting $H_0$ (red line) or to stop for futility (blue line) based on a Pocock-type beta-spending function." width="100%"><p class="caption">
Figure 10.8: Pocock-type boundaries for 3 looks to stop when rejecting <span class="math inline">\(H_0\)</span> (red line) or to stop for futility (blue line) based on a Pocock-type beta-spending function.
</p>
</div>
<p>With a beta-spending function the expected number of subjects under <span class="math inline">\(H_1\)</span> will increase, so if the alternative hypothesis is true, designing a study to be able to stop for futility comes at a cost. However, it is possible that <span class="math inline">\(H_0\)</span> is true, and when it is, stopping for futility reduces the expected sample size. In Figure <a href="sequential.html#fig:powerseq2">10.9</a> you can see the probability of stopping (the green line) is now also high when the true effect size is 0, as we will now stop for futility, and if we do, the expected sample size (the blue line) is lower compared to <a href="sequential.html#fig:powerseq">10.6</a>. It is important to design studies that have a high informational value to reject the presence of a meaningful effect at the final analysis, but whether stopping for futility early is an option you want to build into a study is a choice that requires considering the probability that the null hypothesis is true and the (perhaps small) increase in the sample size.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:powerseq2"></span>
<img src="10-sequential_files/figure-html/powerseq2-1.png" alt="Power curve for a sequential design with 2 looks with stopping for futility." width="100%"><p class="caption">
Figure 10.9: Power curve for a sequential design with 2 looks with stopping for futility.
</p>
</div>
</div>
<div id="reporting-the-results-of-a-sequential-analysis" class="section level2" number="10.8">
<h2>
<span class="header-section-number">10.8</span> Reporting the results of a sequential analysis<a class="anchor" aria-label="anchor" href="#reporting-the-results-of-a-sequential-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>Group sequential designs have been developed to efficiently test hypotheses using the Neyman-Pearson approach for statistical inference, where the goal is to decide how to act, while controlling error rates in the long run. Group sequential designs do not have the goal to quantify the strength of evidence, or provide accurate estimates of the effect size <span class="citation">(<a href="references.html#ref-proschan_statistical_2006" role="doc-biblioref">Proschan et al., 2006</a>)</span>. Nevertheless, after having reached a conclusion about whether a hypothesis can be rejected or not, researchers will often want to also interpret the effect size estimate when reporting results.</p>
<p>A challenge when interpreting the observed effect size in sequential designs is that whenever a study is stopped early when <span class="math inline">\(H_0\)</span> is rejected there is a risk that the data analysis was stopped because, due to random variation, a large effect size was observed at the time of the interim analysis. This means that the observed effect size at these interim analyses over-estimates the true effect size. As <span class="citation">Schönbrodt et al. (<a href="references.html#ref-schonbrodt_sequential_2017" role="doc-biblioref">2017</a>)</span> show, a meta-analysis of studies that used sequential designs will yield an accurate effect size, because studies that stop early have smaller sample sizes, and are weighed less, which is compensated by the smaller effect size estimates in those sequential studies that reach the final look, and are weighed more because of their larger sample size. However, researchers might want to interpret effect sizes from single studies before a meta-analysis can be performed, and in this case, reporting an adjusted effect size estimate can be useful. Although sequential analysis software only allows one to compute adjusted effect size estimates for certain statistical tests, we recommend to report both the adjusted effect size where possible, and to always also report the unadjusted effect size estimate for future meta-analyses.</p>
<p>A similar issue is at play when reporting <em>p</em> values and confidence intervals. When a sequential design is used, the distribution of a <em>p</em> value that does not account for the sequential nature of the design is no longer uniform when <span class="math inline">\(H_0\)</span> is true. A <em>p</em> value is the probability of observing a result <em>at least as extreme</em> as the result that was observed, given that <span class="math inline">\(H_0\)</span> is true. It is no longer straightforward to determine what 'at least as extreme' means a sequential design <span class="citation">(<a href="references.html#ref-cook_p-value_2002" role="doc-biblioref">T. D. Cook, 2002</a>)</span>. The most widely recommended procedure to determine what "at least as extreme" means is to order the outcomes of a series of sequential analyses in terms of the look at which the study was stopped, where earlier stopping is more extreme than later stopping, and where studies with higher <em>z</em> values are more extreme, when different studies are stopped at the same time <span class="citation">(<a href="references.html#ref-proschan_statistical_2006" role="doc-biblioref">Proschan et al., 2006</a>)</span>. This is referred to as <em>stagewise ordering</em>, which treats rejections at earlier looks as stronger evidence against <span class="math inline">\(H_0\)</span> than rejections later in the study <span class="citation">(<a href="references.html#ref-wassmer_group_2016" role="doc-biblioref">Wassmer &amp; Brannath, 2016</a>)</span>. Given the direct relationship between a <em>p</em> value and a confidence interval, confidence intervals for sequential designs have also been developed.</p>
<p>Reporting adjusted <em>p</em> values and confidence intervals, however, might be criticized. After a sequential design, a correct interpretation from a Neyman-Pearson framework is to conclude that <span class="math inline">\(H_0\)</span> is rejected, the alternative hypothesis is rejected, or that the results are inconclusive. The reason that adjusted <em>p</em> values are reported after sequential designs is to allow readers to interpret them as a measure of evidence. <span class="citation">Dupont (<a href="references.html#ref-dupont_sequential_1983" role="doc-biblioref">1983</a>)</span> provides good arguments to doubt that adjusted <em>p</em> values provide a valid measure of the strength of evidence. Furthermore, a strict interpretation of the Neyman-Pearson approach to statistical inferences also provides an argument against interpreting <em>p</em> values as measures of evidence <span class="citation">(<a href="references.html#ref-lakens_why_2022" role="doc-biblioref">Lakens, 2022a</a>)</span>. Therefore, it is recommended, if researchers are interested in communicating the evidence in the data for <span class="math inline">\(H_0\)</span> relative to the alternative hypothesis, to report likelihoods or Bayes factors, which can always be reported and interpreted after the data collection has been completed. Reporting the unadjusted <em>p</em>-value in relation to the alpha level communicates the basis to reject hypotheses, although it might be important for researchers performing a meta-analysis based on <em>p</em>-values (e.g., a <em>p</em>-curve or <em>z</em>-curve analysis, as explained in the chapter on <a href="bias.html#bias">bias detection</a>) that these are sequential <em>p</em>-values. Adjusted confidence intervals are useful tools to evaluate the observed effect estimate relative to its variability at an interim or the final look at the data. Note that the adjusted parameter estimates are only available in statistical software for a few commonly used designs in pharmaceutical trials, such as comparisons of mean differences between groups, or survuval analysis</p>
<p>Below, we see the same sequential design we started with with 2 looks and a Pocock-type alpha spending function. After completing the study with the planned sample size of 95 participants per condition (where we collect 48 participants at look 1, and the remaining 47 at look 2), we can now enter the observed data using the function <code>getDataset</code>. The means and standard deviations are entered for each stage, so at the second look, only the data from the second 95 participants in each condition are used to compute the means (1.51 and 1.01) and standard deviations (1.03 and 0.96).</p>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">seq_design</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  kMax <span class="op">=</span> <span class="fl">2</span>,
  typeOfDesign <span class="op">=</span> <span class="st">"asP"</span>,
  sided <span class="op">=</span> <span class="fl">2</span>,
  alpha <span class="op">=</span> <span class="fl">0.05</span>,
  beta <span class="op">=</span> <span class="fl">0.1</span>
<span class="op">)</span>

<span class="va">dataMeans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDataset.html">getDataset</a></span><span class="op">(</span>
  n1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">48</span>, <span class="fl">47</span><span class="op">)</span>, 
  n2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">48</span>, <span class="fl">47</span><span class="op">)</span>, 
  means1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.12</span>, <span class="fl">1.51</span><span class="op">)</span>, <span class="co">#for directional test, means 1 &gt; means 2</span>
  means2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.03</span>, <span class="fl">1.01</span><span class="op">)</span>,
  stDevs1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.98</span>, <span class="fl">1.03</span><span class="op">)</span>, 
  stDevs2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.06</span>, <span class="fl">0.96</span><span class="op">)</span>
  <span class="op">)</span>

<span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getAnalysisResults.html">getAnalysisResults</a></span><span class="op">(</span>
  <span class="va">seq_design</span>, 
  equalVariances <span class="op">=</span> <span class="cn">TRUE</span>,
  dataInput <span class="op">=</span> <span class="va">dataMeans</span>
  <span class="op">)</span>

<span class="va">res</span></code></pre></div>
<pre><code>## [PROGRESS] Stage results calculated [0.0399 secs] 
## [PROGRESS] Conditional power calculated [0.0319 secs] 
## [PROGRESS] Conditional rejection probabilities (CRP) calculated [0.001 secs] 
## [PROGRESS] Repeated confidence interval of stage 1 calculated [0.5685 secs] 
## [PROGRESS] Repeated confidence interval of stage 2 calculated [0.5425 secs] 
## [PROGRESS] Repeated confidence interval calculated [1.11 secs] 
## [PROGRESS] Repeated p-values of stage 1 calculated [0.2304 secs] 
## [PROGRESS] Repeated p-values of stage 2 calculated [0.2404 secs] 
## [PROGRESS] Repeated p-values calculated [0.4717 secs] 
## [PROGRESS] Final p-value calculated [0.001 secs] 
## [PROGRESS] Final confidence interval calculated [0.0758 secs]</code></pre>
<pre><code>## Analysis results (means of 2 groups, group sequential design):
## 
## Design parameters:
##   Information rates                            : 0.500, 1.000 
##   Critical values                              : 2.157, 2.201 
##   Futility bounds (non-binding)                : -Inf 
##   Cumulative alpha spending                    : 0.03101, 0.05000 
##   Local one-sided significance levels          : 0.01550, 0.01387 
##   Significance level                           : 0.0500 
##   Test                                         : two-sided 
## 
## User defined parameters: not available
## 
## Default parameters:
##   Normal approximation                         : FALSE 
##   Direction upper                              : TRUE 
##   Theta H0                                     : 0 
##   Equal variances                              : TRUE 
## 
## Stage results:
##   Cumulative effect sizes                      : 0.0900, 0.2928 
##   Cumulative (pooled) standard deviations      : 1.021, 1.013 
##   Stage-wise test statistics                   : 0.432, 2.435 
##   Stage-wise p-values                          : 0.333390, 0.008421 
##   Overall test statistics                      : 0.432, 1.993 
##   Overall p-values                             : 0.33339, 0.02384 
## 
## Analysis results:
##   Assumed standard deviation                   : 1.013 
##   Actions                                      : continue, accept 
##   Conditional rejection probability            : 0.007317, NA 
##   Conditional power                            : NA, NA 
##   Repeated confidence intervals (lower)        : -0.36630, -0.03306 
##   Repeated confidence intervals (upper)        : 0.5463, 0.6187 
##   Repeated p-values                            : &gt;0.5, 0.08195 
##   Final stage                                  : 2 
##   Final p-value                                : NA, 0.06662 
##   Final CIs (lower)                            : NA, -0.02007 
##   Final CIs (upper)                            : NA, 0.5734 
##   Median unbiased estimate                     : NA, 0.2814</code></pre>
<p>Imagine we have performed a study planned to have at most 2 equally spaced looks at the data, where we perform a two-sided test with an alpha of 0.05, and we use a Pocock type alpha spending function, and we observe mean differences between the two conditions of at the last look. Based on a Pocock-like alpha spending function with two equally spaced looks the alpha level for a two-sided <em>t</em>-test is 0.03101, and 0.02774. We can thus reject <span class="math inline">\(H_0\)</span> after look 2. But we would also like to report an effect size, and adjusted <em>p</em> values and confidence intervals.</p>
<p>The results show that the action after look 1 was to continue data collection, and that we could reject <span class="math inline">\(H_0\)</span> at the second look. The unadjusted mean difference is provided in the row "Overall effect size" and at the final look this was 0.293. The adjusted mean difference is provided in the row "Median unbiased estimate" and is lower, and the adjusted confidence interval is in the row "Final confidence interval", giving the result 0.281, 95% CI [-0.02, 0.573].</p>
<p>The unadjusted <em>p</em> values for a one-sided tests are reported in the row "Overall p-value". The actual <em>p</em> values for our two-sided test would be twice as large, so 0.6668, 0.0477. The adjusted <em>p</em>-value at the final look is provided in the row "Final p-value" and it is 0.06662.</p>
</div>
<div id="test-yourself-8" class="section level2" number="10.9">
<h2>
<span class="header-section-number">10.9</span> Test Yourself<a class="anchor" aria-label="anchor" href="#test-yourself-8"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Q1</strong>: Sequential analyses can increase the efficiency of the studies you perform. Which statement is true for a sequential design in which researchers only stop if <span class="math inline">\(H_0\)</span> can be rejected (and did not specify a rule to stop for futility)?</p>
<ol style="list-style-type: upper-alpha">
<li>Sequential analyses will reduce the sample size of every study you will perform.</li>
<li>Sequential analyses will on average reduce the sample size of studies you will perform.</li>
<li>Sequential analyses will on average reduce the sample size of studies you will perform, as long as there is a true effect (when a rule to stop for futility has not been specified).</li>
<li>Sequential analyses will on average require the same sample size as fixed designs, but offer more flexibility.</li>
</ol>
<p><strong>Q2</strong>: What is the difference between sequential analysis and optional stopping?</p>
<ol style="list-style-type: upper-alpha">
<li>The only difference is that a sequential analysis is transparently reporting, while optional stopping is typically not disclosed in a paper.</li>
<li>In sequential analysis the Type 1 error rate is controlled, while in optional stopping the Type 1 error rate is inflated.</li>
<li>In optional stopping data collection is only terminated when a significant result has been observed, while in sequential analysis data collection can also stop when the absence of a meaningful effect has been established.</li>
<li>In sequential analysis it is not possible to design a study where you analyze the data after every participant, while you can do this in optional stopping.</li>
</ol>
<p><strong>Q3</strong>: What is the defining feature of the Pocock correction?</p>
<ol style="list-style-type: upper-alpha">
<li>It uses a very conservative alpha level for early looks, and the alpha level at the last look is close to the unadjusted alpha level in a fixed design.</li>
<li>It uses the same alpha level at each look (or almost the same alpha level at each look, when using an Pocock-like alpha spending function).</li>
<li>It uses a critical value of 3 at each interim analysis, and spends the remaining Type 1 error rate at the last look.</li>
<li>It has a parameter that can be chosen such that the Type 1 error rate is spent more conservatively or more liberally in early interim analyses.</li>
</ol>
<p><strong>Q4</strong>: A benefit of the O’Brien-Fleming correction is that the alpha level at the last look is close to the alpha level. Why is this a benefit?</p>
<ol style="list-style-type: upper-alpha">
<li>It means that the sample size based on an a-priori power analysis (which depends on the alpha level) is close to the sample size in a fixed design, while allowing additional looks at the data.</li>
<li>It means the Type 1 error rate is not inflated only a little bit, compared to a fixed design.</li>
<li>It means the Type 1 error rate is only a bit more conservative, compared to a fixed design.</li>
<li>It means that the sample size based on an a-priori power analysis (which depends on the alpha level) is always identical to the sample size in a fixed design, while allowing additional looks at the data.</li>
</ol>
<p><strong>Q5</strong>: A researcher uses a sequential design for a study with 5 looks at the data, with a desired overall alpha level of 0.05 for a two-sided test, and chooses a <strong>Pocock correction</strong>. After continuing data collect to the third look, the researcher observes a <em>p</em>-value of 0.011. Which statement is true? Note: remember that <code>rpact</code> returns one-sided alpha levels. You can use the following code by replacing 0 and specifying the typeOfDesign:</p>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span> <span class="op">&lt;-</span> <span class="fu">rpact</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignGroupSequential.html">getDesignGroupSequential</a></span><span class="op">(</span>
  kMax <span class="op">=</span> <span class="fl">0</span>,
  typeOfDesign <span class="op">=</span> <span class="st">""</span>,
  sided <span class="op">=</span> <span class="fl">0</span>,
  alpha <span class="op">=</span> <span class="fl">0.0</span>
<span class="op">)</span>
<span class="va">design</span></code></pre></div>
<ol style="list-style-type: upper-alpha">
<li>The researcher can reject the null hypothesis and can terminate data collection.</li>
<li>The researcher fails to reject the null hypothesis and needs to continue the data collection.</li>
</ol>
<p><strong>Q6</strong>: A researcher uses a sequential design for a study with 5 looks at the data, with a desired overall alpha level of 0.05, and chooses an <strong>O’Brien-Fleming correction</strong>. After continuing data collect to the third look, the researcher observes a <em>p</em>-value of 0.011. Which statement is true (you can use the same code as for Q5)?</p>
<ol style="list-style-type: upper-alpha">
<li>The researcher can reject the null hypothesis and can terminate data collection.</li>
<li>The researcher fails to reject the null hypothesis and needs to continue the data collection.</li>
</ol>
<p><strong>Q7</strong>: For the design in Q5 (using the Pocock correction), what is the sample size required to achieve 80% power (the default – you can change the default by specifying a different value than <code>beta = 0.2</code> in the <code>getDesignGroupSequential</code> function) for an effect size of <em>d</em> = 0.5 (which equals a mean difference of 0.5 with a standard deviation of 1). You can use the code below.</p>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">power_res</span> <span class="op">&lt;-</span> <span class="fu">rpact</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getSampleSizeMeans.html">getSampleSizeMeans</a></span><span class="op">(</span>
  design <span class="op">=</span> <span class="va">design</span>,
  groups <span class="op">=</span> <span class="fl">2</span>,
  alternative <span class="op">=</span> <span class="fl">0.5</span>, 
  stDev <span class="op">=</span> <span class="fl">1</span>, 
  allocationRatioPlanned <span class="op">=</span> <span class="fl">1</span>,
  normalApproximation <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>

<span class="va">power_res</span></code></pre></div>
<ol style="list-style-type: upper-alpha">
<li>64 (32 in each independent group).</li>
<li>128 (64 in each independent group)</li>
<li>154 (77 in each independent group)</li>
<li>158 (79 in each independent group)</li>
</ol>
<p><strong>Q8</strong>: For the design in Q5, what is the sample size required to achieve 80% power for an effect size of <em>d</em> = 0.5 for a fixed design with only one look? First update the design (by changing the <code>kMax</code> to 1) and then re-run the function <code>getSampleSizeMeans</code>.</p>
<ol style="list-style-type: upper-alpha">
<li>64 (32 in each independent group).</li>
<li>128 (64 in each independent group)</li>
<li>154 (77 in each independent group)</li>
<li>158 (79 in each independent group)</li>
</ol>
<p>We see the sample size increases quite a bit because of the choice for the Pocock correction, and the number of looks (5, which lead to a low alpha level at the final look). The ratio of the maximum sample size for a sequential design and the sample size for a fixed design is known as the <strong>inflation factor</strong>, which is independent of the effect size. Although a-priori power analyses have not been programmed for all types of tests, the inflation factor can be used to compute the increased number of observations that is required relative to a fixed design for any test. Researchers can perform an a-priori power analysis for a fixed design with any tool they would normally use, and multiply the total number of observations with the inflation factor to determine the required sample size for a sequential design. The inflation factor can be retrieved using the <code>getDesignCharacteristics</code> function.</p>
<p><strong>Q9</strong>: First, re-run the code to create a sequential design with 5 looks at the data used in Q5. Then, run the code below, and find the inflation factor. What is the inflation factor, or the required increase in the sample size for a sequential design with 5 looks using the Pocock correction, compared to a fixed design? Note that <code>rpact</code> does not round up the number of observations for per group to whole numbers when calculating the inflation factor.</p>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">rpact</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/rpact/man/getDesignCharacteristics.html">getDesignCharacteristics</a></span><span class="op">(</span><span class="va">design</span><span class="op">)</span></code></pre></div>
<ol style="list-style-type: upper-alpha">
<li>The inflation factor is 1</li>
<li>The inflation factor is 1.0284</li>
<li>The inflation factor is 1.2286</li>
<li>The inflation factor is 1.2536</li>
</ol>
<p><strong>Q10</strong>: We see the inflation factor is quite large, and there is a certain probability that we will have to collect more observations than using a fixed design. Re-run the code for Q7 (for the Pocock design with 5 looks). We see that on average, if there is a true effect of 0.5, we will be more efficient than in a fixed design. What is the expected number of subjects under <span class="math inline">\(H_1\)</span>, as provided by <code>rpact</code>?</p>
<ol style="list-style-type: upper-alpha">
<li>101.9</li>
<li>104.3</li>
<li>125.3</li>
<li>152.8</li>
</ol>
<p>We see the sequential design will on average be more efficient than a fixed design, but the decision about the trade-off between the specific sequential design used, and whether the possible benefit is worth the risk of collecting additional data, must be made on a case-by-case basis.</p>
<p><strong>Q11</strong>: First, re-run the code to create a sequential design with 5 looks at the data used in Q6 (so using the O’Brien-Fleming correction). Then, run the code below, and find the inflation factor for this design. What is the inflation factor?</p>
<ol style="list-style-type: upper-alpha">
<li>The inflation factor is 1</li>
<li>The inflation factor is 1.0284</li>
<li>The inflation factor is 1.2286</li>
<li>The inflation factor is 1.2536</li>
</ol>
<p><strong>Q12</strong>: It is also possible to stop for futility (or to reject the presence of a specific effect of interest). Researchers should decide between binding and non-binding beta-spending functions, but they do not need to decide between binding and non-binding alpha spending functions. If a researcher observed a statistically significant result at an interim analysis, but decides not to stop the data collection, but continue the data collection (for example to get a more precise effect size estimate) what are the consequences?</p>
<ol style="list-style-type: upper-alpha">
<li>The Type 1 error rate will inflate, and the Type 2 error rate will inflate.</li>
<li>The Type 1 error rate will inflate, and the Type 2 error rate will not inflate.</li>
<li>The Type 1 error rate will not inflate, and the Type 2 error rate will inflate.</li>
<li>The Type 1 error rate will not inflate, and the Type 2 error rate will not inflate.</li>
</ol>
<p><strong>Q13</strong>: In the plot below you see the <em>t</em>-score boundaries for a sequential design to stop to reject <span class="math inline">\(H_0\)</span> (the red line) and to reject <span class="math inline">\(H_1\)</span> (the blue line). At the second interim look, you perform a test, and observe a <em>t</em>-value of 2. Which decision would you make?</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:futilityq13"></span>
<img src="10-sequential_files/figure-html/futilityq13-1.png" alt="Example of O'Brien-Fleming-type boundaries for 3 looks to stop when rejecting $H_0$ (red line) or to stop for futility (blue line) with a 5% Type 1 and Type 2 error." width="100%"><p class="caption">
Figure 10.10: Example of O'Brien-Fleming-type boundaries for 3 looks to stop when rejecting <span class="math inline">\(H_0\)</span> (red line) or to stop for futility (blue line) with a 5% Type 1 and Type 2 error.
</p>
</div>
<ol style="list-style-type: upper-alpha">
<li>You can reject <span class="math inline">\(H_0\)</span> and stop data collection.</li>
<li>You can reject <span class="math inline">\(H_1\)</span> and stop data collection.</li>
<li>You reject both <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> and stop data collection.</li>
<li>You fail to reject both <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> and continue data collection.</li>
</ol>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="equivalencetest.html"><span class="header-section-number">9</span> Equivalence Testing and Interval Hypotheses</a></div>
<div class="next"><a href="meta.html"><span class="header-section-number">11</span> Meta-analysis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#sequential"><span class="header-section-number">10</span> Sequential Analysis</a></li>
<li><a class="nav-link" href="#choosing-alpha-levels-for-sequential-analyses."><span class="header-section-number">10.1</span> Choosing alpha levels for sequential analyses.</a></li>
<li><a class="nav-link" href="#the-pocock-correction"><span class="header-section-number">10.2</span> The Pocock correction</a></li>
<li><a class="nav-link" href="#comparing-spending-functions"><span class="header-section-number">10.3</span> Comparing Spending Functions</a></li>
<li><a class="nav-link" href="#alpha-spending-functions"><span class="header-section-number">10.4</span> Alpha spending functions</a></li>
<li><a class="nav-link" href="#updating-boundaries-during-a-study"><span class="header-section-number">10.5</span> Updating boundaries during a study</a></li>
<li><a class="nav-link" href="#sample-size-for-sequential-designs"><span class="header-section-number">10.6</span> Sample Size for Sequential Designs</a></li>
<li><a class="nav-link" href="#stopping-for-futility"><span class="header-section-number">10.7</span> Stopping for futility</a></li>
<li><a class="nav-link" href="#reporting-the-results-of-a-sequential-analysis"><span class="header-section-number">10.8</span> Reporting the results of a sequential analysis</a></li>
<li><a class="nav-link" href="#test-yourself-8"><span class="header-section-number">10.9</span> Test Yourself</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/Lakens/statistical_inferences/blob/master/10-sequential.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/Lakens/statistical_inferences/edit/master/10-sequential.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>Lakens, D. (2022). Improving Your Statistical Inferences. https://doi.org/10.5281/zenodo.6409077. Built on 2022-04-03</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a></p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
