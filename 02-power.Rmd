--- 
knit: "bookdown::preview_chapter"
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(ggrepel)
library(grid)
library(magrittr)
library(kableExtra)
library(tidyverse)
library(pwr)
library(Superpower)
library(ggplot2)
library(patchwork)
library(MASS)
library(viridis)
library(TOSTER)
library(BEST)
library(metafor)
library(MOTE)
library(binom)
library(MASS)
knitr::opts_chunk$set(error = FALSE, warning = FALSE, message = FALSE)
```

# Sample size justification {#power}

When you perform an experiment, you want it to provide an answer to your research question that is as informative as possible. However, since all scientists are faced with resource limitations, you need to balance the cost of collecting each additional datapoint against the increase in information that datapoint provides. In economics, this is known as the Value of Information [@eckermann_value_2010]. Calculating the value of information is notoriously difficult. You need to specify the costs and benefits of possible outcomes of the study, and quantifying a utility function for scientific research is not easy. 

Because of the difficulty of quantifying the value of information, scientists use less formal approaches to justify the amount of data they set out to collect. That is, if they provide a justification for the number of observations to begin with. Even though in some fields a justification for the number of observations is required when submitting a grant proposal to a science funder, a research proposal to an ethical review board, or a manuscript for submission to a journal. In some research fields, the number of observations is *stated*, but not *justified*. This makes it difficult to evaluate how informative the study was. Referees can't just assume the number of observations is sufficient to provide an informative answer to your research question, so leaving out a justification for the number of observations is not best practice, and a reason reviewers can criticize your submitted article.

Below, several possible justifications for the number of observations are discussed. First, you might be able to **collect data from the entire population**. Second, you might justify the sample size based on **feasibility**. Since we always have resource constraints, it is good to know about feasibility justifications, even though they are rarely discussed in statistics textbooks. Third, you might want to perform an a-priori power analysis. We will discuss all these situations in turn. Before we do so, we need to get some terminology out of the way. 

The **sample size** should not be confused with the number of participants. When single observations are collected from participants in a study, the number of observations equals the number of participants. In these cases, we can use a sample size justification and a justification for the number of participants interchangeably. When multiple observations from single individuals can be collected, the sample size is the number of observation, but not the number of participants.  

**Statistical power** is the probability of a test to yield a statistically significant result if the alternative hypothesis is true [@aberson_applied_2019; @cohen_statistical_1988]. Power depends on the Type 1 error rate (α), the true effect size in the population, and the number of observations. Because the true effect size is typically unknown, it makes most sense to speak about the power function. In Figure \@ref(fig:power-1) you see a power curve for an independent *t*-test, with an alpha level of 0.05. We see that as the effect size (in Cohen's d) increases, power increases. If the effect you study has an effect size of 0.5, you would have almost 70% power with 50 observations in each independent group (indicated by the red dot). If the true effect size is smaller, power is lower, and as the effect size is larger, power is larger. 


```{r, power-1, fig.margin=FALSE, echo=FALSE, fig.cap="Power curve for an independent *t*-test as a function of the true effect size."}
N <- 50
d <- 0.5
p_upper <- 0.05
ncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d
plot_power_d <- (function(d, N, p_upper) {
  ncp <- d * (N * N / (N + N))^0.5 # formula to calculate t from d from Dunlap, Cortina, Vaslow, & Burke, 1996, Appendix B
  t <- qt(1 - (p_upper / 2), df = (N * 2) - 2)
  1 - (pt(t, df = N * 2 - 2, ncp = ncp) - pt(-t, df = N * 2 - 2, ncp = ncp))
})
par(bg = "aliceblue")
plot(-10,
  xlab = expression(paste("Cohen's ", delta)), ylab = "Power", axes = FALSE,
  main = substitute(paste("Power for independent t-test with N = 50 per group")), xlim = c(0, 2), ylim = c(0, 1)
)
abline(v = seq(0, 2, 0.2), h = seq(0, 1, 0.1), col = "lightgray", lty = 1)
axis(side = 1, at = seq(0, 2, 0.2), labels = seq(0, 2, 0.2))
axis(side = 2, at = seq(0, 1, 0.2), labels = seq(0, 1, 0.2))
curve(plot_power_d(d = x, N = N, p_upper = p_upper), 0, 2, type = "l", lty = 1, lwd = 2, ylim = c(0, 1), xlim = c(0, 2), add = TRUE)
points(x = d, y = (1 + pt(qt(p_upper / 2, 2 * N - 2, 0), 2 * N - 2, ncp) - pt(qt(1 - p_upper / 2, 2 * N - 2, 0), 2 * N - 2, ncp)), cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5))

```

**Power analysis** is the practice of computing the effect size, desired power, sample size, or alpha level from the other three values. Computing the required sample size is commonly referred to as **a-priori** power analysis, computing the smallest effect size you have the desired power for is called **sensitivty** power analysis, computing the alpha level is called **criterion** power analysis, and computing the achieved power is called **post-hoc** power analysis. We will discuss all these different types of power analysis below. With this terminology out of the way, let's examine different ways to justify the sample size.

## Measuring the Entire Population

In some instances it might be possible to collect data from the entire population under examination. For example, you might be interested in the average amount of time people spend on the moon, whenever they visit it. Since there are only 12 people who have ever been on the moon, we can calculate the average time people have spent on the moon by collecting data from the entire population. Whenever it is possible to measure the entire population, the justification for the sample size is that you collected all the data that is available. This is the simplest, and most straightforward, justification for the sample size. However, it is also quite rare that we are able to measure the entire population. 

## Feasibility

A common reason why a specific number of observations is collected is because collecting more data was not feasible. Note that all decisions for the sample size we collect in a study are based on the resources we have available in some way. A **feasibility justification** makes these resource limitations the primary reason for the sample size that is collected. Because we always have resource limitations in science, even when feasibility is not our primary justification for the number of observations we plan to collect, it is always a secondary reason. Despite the omnipresence of resource limitations, the topic often receives very little attention in texts on experimental design. This might make it feel like a feasibility justification is not appropriate, and you should perform an a-priori power analysis or plan for a desired precision instead. But feasibility always plays a secondary role, and therefore regardless of which justification for the sample size you provide, you will almost always need to include a feasibility justification as well.

Time and money are the two main resource limitations a scientist faces. Our master students write their thesis in 6 months, and therefore their data collection is necessarily limited in whatever can be collected in 6 months, minus the time needed to formulate a research question, design an experiment, analyze the data, and write up the thesis. A PhD student at our department would have 4 years to complete their thesis, but is also expected to complete multiple research lines in this time. In addition to limitations on time, we have limited financial resources. Although nowadays it is possible to collect data online quickly, if you offer participants a decent pay (as you should) most researchers have the financial means to collect thousands of datapoints.

A feasibility justification puts the limited resources at the center of the justification for the sample size that will be collected. For example, one might argue that 120 observations is the most that can be collected in the three weeks a master student has available to collect data, when each observation takes an hour to collect. A PhD student might collect data until the end of the academic year, and then needs to write up the results over the summer to stay on track to complete the thesis in time.

A feasibility justification thus *starts* with the expected number of observations (N) that a researcher expects to be able to collect. The challenge is to evaluate whether collecting N observations is worthwhile. The answer should sometimes be that data collection is *not* worthwhile. For example, assume I plan to manipulate the mood of participants using funny cartoons and then measure the effect of mood on some dependent variable - say the amount of money people donate to charity. I should expect an effect size around d = 0.31 for the mood manipulation [@joseph_manipulation_2020], and seems unlikely that the effect on donations will be larger than the effect size of the manipulation. If I can only collect mood data from 30 participants in total, how do we decide if this study will be informative?

First of all, more data is always better than no data, so in an absolute sense, all additional data that is collected is better than not collecting data. However, in line with the idea that we need to take into account costs and benefits, it is possible that the cost of data collection outweigh the benefits. To determine this, one needs to think about what the benefits of having the data are. The benefits are clearest when we know for certain that someone is going to make a decision, with or without data. If this is the case, then any data you collect will reduce the error rates of a well-calibrated decision process, even if only ever so slightly. In these cases, the value of information might be positive, as long as the reduction in error rates is more beneficial than the costs of data collection.

Another way in which a small dataset can be valuable is if its existence makes it possible to combine several small datasets into a meta-analysis. This argument in favor of collecting a small dataset requires 1) that you share the results in a way that a future meta-analyst can find them, and 2) that there is a decent probability that someone will perform a meta-analysis in the future which inclusion criteria would contain your study, because a sufficient number of small studies exist. The uncertainty about whether there will ever be such a meta-analysis should be weighed against the costs of data collection. One way to increase the probability of a future meta-analysis is if you commit to performing this yourself in the future. For example, you might plan to repeat a study for the next 12 years in a class you teach, with the expectation that a meta-analysis of 360 participants would be sufficient to achieve around 90% power for d = 0.31. If it is not plausible you will collect all the required data by yourself, you can attempt to set up a collaboration, where fellow researchers in your field commit to collecting similar data, with identical measures, over the next years. If it is not likely sufficient data will emerge over time, we will not be able to draw informative conclusions from the data, and it might be more beneficial to not collect the data to begin with, and examine an alternative research question with a larger effect size instead.

Even if you believe over time sufficient data will emerge, you will most likely compute statistics after collecting a small sample size. Before embarking on a study where your main justification for the sample size is based on feasibility, you can expect. I propose that a feasibility justification for the sample size is always accompanied by three statistics, detailed in the following three sections.

### The smallest effect size that can be statistically significant

In Figure \@ref(fig:power-effect1) the distribution of Cohen’s d given 15 participants per group is plotted when the true effect size is 0 (or the null-hypothesis is true), and when the true effect size is d = 0.5. The blue area is the Type 2 error rate (the probability of not finding p < α, when there is a true effect, and α = 0.05).

```{r, power-effect1, fig.margin=FALSE, echo=FALSE, fig.cap="Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 15 per group."}
N <- 15
d <- 0.5
p_upper <- 0.05
ncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d
crit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)
low_x <- min(-1 - crit_d)
high_x <- max(d + 1 + crit_d)
# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2) # calculate distribution of d based on t-distribution
# Set max Y for graph
y_max <- max(d_dist) + 1
# create plot
par(bg = "aliceblue")
d <- round(d, 2)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = substitute(paste("Cohen's ", delta == 0.5)), ylab = "Density", main = substitute(paste("Distribution of Cohen's ", delta == 0.5, ", N = 15 per group")))
# abline(v = seq(low_x,high_x,0.1), h = seq(0,0.5,0.1), col = "lightgray", lty = 1)
#    axis(side = 1, at = seq(low_x,high_x,0.1), labels = FALSE)
lines(x, d_dist, col = "black", type = "l", lwd = 2)
# add d = 0 line
d_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = 0) * sqrt(N / 2)
lines(x, d_dist, col = "grey", type = "l", lwd = 1)
# Add Type 1 error rate right
crit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)
y <- seq(crit_d, 10, length = 10000)
z <- (dt(y * sqrt(N / 2), df = (N * 2) - 2) * sqrt(N / 2)) # determine upperbounds polygon
polygon(c(crit_d, y, 10), c(0, z, 0), col = rgb(1, 0, 0, 0.5))
# Add Type 1 error rate left
crit_d <- -abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)
y <- seq(-10, crit_d, length = 10000)
z <- (dt(y * sqrt(N / 2), df = (N * 2) - 2) * sqrt(N / 2)) # determine upperbounds polygon
polygon(c(y, crit_d, crit_d), c(z, 0, 0), col = rgb(1, 0, 0, 0.5))
# Add Type 2 error rate
crit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)
y <- seq(-10, crit_d, length = 10000)
z <- (dt(y * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2)) # determine upperbounds polygon
polygon(c(y, crit_d, crit_d), c(0, z, 0), col = rgb(0, 0, 1, 0.5))
segments(crit_d, 0, crit_d, y_max - 0.8, col = "black", lwd = 2)
text(crit_d, y_max - 0.5, paste("Effects larger than d = ", round(crit_d, 2), "will be statistically significant"), cex = 1)
```

You might seen such graphs before. The only thing I have done is to transform the *t*-value distribution that is commonly used in these graphs, and calculated the distribution for Cohen’s d. This is a straightforward transformation, but instead of presenting the critical *t*-value the figure provides the critical *d*-value. For a two-sided independent *t*-test, this is calculated as: 

```{r, eval = FALSE}
qt(1-(a / 2), (n1 + n2) - 2) * sqrt(1/n1 + 1/n2)
```

where 'a' is the alpha level (e.g., 0.05) and N is the sample size in each independent group. For the example above, where alpha is 0.05 and n = 15: 

```{r, eval = TRUE}
qt(1-(0.05 / 2), (15 * 2) - 2) * sqrt(1/15 + 1/15)
```

The critical *t*-value (2.0484071) is also provided in commonly used power analysis software such as G\*Power. We can compute the critical Cohen's d from the *t*-value and sample size using: $d = t{\sqrt{1/n_1 + 1/n_2}}$.

```{r gcrit1, fig.margin=FALSE, echo=FALSE, fig.cap="The critical *t*-value is provided by G\\*Power software."}
knitr::include_graphics("images/gpowcrit.png")
```

When you will test an association between variables with a correlation, G\*Power will directly provide you with the critical effect size. When you compute a correlation based on a two-sided test, your alpha level is 0.05, and you have 30 observations, only effects larger than r = 0.361 will be statistically significant. In other words, the effect needs to be quite large to even have the mathematical possibility of becoming statistically significant.

```{r gcrit2, fig.margin=FALSE, echo=FALSE, fig.cap="The critical r is provided by G\\*Power software."}
knitr::include_graphics("images/gpowcrit2.png")
```

The critical effect size gives you information about the smallest effect size that, if observed, would by statistically significant. If you observe a smaller effect size, the *p*-value will be larger than your significance threshold. You always have some probability of observing effects larger than the critical effect size. After all, even if the null hypothesis is true, 5% of your tests will yield a significant effect. But what you should ask yourself is whether the effect sizes that could be statistically significant are realistically what you would expect to find. If this is not the case, it should be clear that there is no use in performing a significance test. Mathematically, when the critical effect size is larger than effects you expect, your statistical power will be less than 50% (see the section on [observed power](#observedpower)). If you perform a statistical test with less than 50% power, your single study is not very informative.

### Compute the width of the confidence interval around the effect size

The second statistic to report alongside a feasibility justification is the width of the 95% confidence interval around the effect size. As we saw in the section on [confidence intervals](#confint) they represent a range that is wide enough so that in the long run in 95% of repetitions of the same experiment the true population parameter falls within each confidence interval calculated around the observed effect size. Cumming -@cumming_understanding_2013 calls the difference between the observed effect size and the upper 95% confidence interval (or the lower 95% confidence interval) the margin of error (MOE).

```{r MOTE-CI-1, echo = TRUE}
# Compute the effect size d and 95% CI
res <- d.ind.t(m1 = 0, m2 = 0, sd1 = 1, sd2 = 1, n1 = 15, n2 = 15, a = .05)
# Print the result
res$estimate
# Compute the average Margin of Error (MOE)
(res$dhigh - res$dlow)/2
# For non-zero effects the CI is based on the non-central t
# The MOE is then asymmetric (but differences are very small)
(res$d - res$dlow)
(res$dhigh - res$d)

```

If we compute the 95% CI for an effect size of 0, we see that with 15 observations in each condition of an independent *t*-test the 95% CI ranges from -0.72 to 0.72. The MOE is half the width of the 95% CI, 0.72. This clearly shows we have a very imprecise estimate. A Bayesian estimator who uses an uninformative prior would compute a credible interval with the same upper and lower bound [@albers_credible_2018], and might conclude they personally believe there is a 95% chance the true effect size lies in this interval. A frequentist would reason more hypothetically: If the observed effect size in the data I plan to collect is 0, I could only reject effects more extreme than d = 0.72 in an equivalence test with a 5% alpha level (even though if such a test would be performed, power might be low, depending on the true effect size). Regardless of the statistical philosophy you plan to rely on when analyzing the data, both of these conclusions are pretty much trivial. Effect sizes in this range are findings such as "People become aggressive when they are provoked", "People prefer their own group to other groups", and "Romantic partners resemble one another in physical attractiveness" [@richard_one_2003]. If you can only reject the presence of effects that are so large people would clearly notice them, there is typically little need to collect data, as we do not learn something we didn't already know.  

We see this the MOE is almost, but not exactly, the same as the critical effect size d we observed above (d = 0.7479725). The reason for this is that the 95% confidence interval is calculated based on the *t*distribution. If the true effect size is not zero, the confidence interval is calculated based on the non-central *t*distribution, and the 95% CI is asymmetric. Figure \@ref(fig:noncentralt) vizualizes three *t*-distributions, one symmetric at 0, and two asymmetric distributions with a noncentrality parameter of 2 and 3. The asymmetry is most clearly visible in very small samples (the distribution in the plot have 5 degrees of freedom) but remain noticeablewhen calculating confidence intervals and statistical power. For example, for a true effect size of d = 0.5 the 95% CI is $d_s$ = 0.50, 95\\% CI [-0.23, 1.22]. The MOE based on the lower bound is 0.7317584 and based on the upper bound is 0.7231479. If we compute the 95% CI around the critical effect size (d = 0.7479725) we see the 95% CI ranges from exactly 0.00 to 1.48. As explained in the section on the relation between a [confidence interval and a *p*-value](#relatCIp), if the 95% CI excludes zero, the test is statistically significant. In this case the lowerbound of the confidence interval exactly touches 0, which means we would observe a *p* = 0.05 if we exactly observed the critical effect size.

```{r noncentralt, fig.margin = FALSE, echo = FALSE, fig.cap="Central (black) and 2 non-central (red and blue) *t*-distributions."}
x <- seq(-12, 12, length = 10000)
y <- dt(x, df = 5, 0)
plot(x, y, type = "l", col = "black", lwd = 3, xlab = "t", ylab = "", main = "Central and non-central t-distributions")
lines(x, dt(x, df = 5, 2), col = "#E69F00", type = "l", lwd = 3)
lines(x, dt(x, df = 5, 3), col = "#56B4E9", type = "l", lwd = 3)
legend(-12, 0.38, # places a legend at the appropriate place
       c("t(5, ncp = 0)", "t(5, ncp = 2)", "t(5, ncp = 3)"), # puts text in the legend
       lty = c(1, 1), # gives the legend appropriate symbols (lines)
       lwd = c(2.5, 2.5, 2.5), col = c("black", "#E69F00", "#56B4E9")
) # format lines and colors
```

Where computing the critical effect size can make it clear that a *p*-value is of little interest, computing the 95% CI around the effect size can make it clear that the effect size estimate is of little value. It will often be so uncertain, and the range of effect sizes you will not be able to reject if there is no effect is so large, the effect size estimate is not very useful. This is also the reason why performing a pilot study to estimate an effect size for an a-priori power analysis is not a sensible strategy [@albers_when_2018; @leon_role_2011]. However, it is possible that the sample size is large enough to exclude some effect sizes that are still a-priori plausible. For example, with 50 observations in each independent group, you have 82% [power for an equivalence test](#powerequiv) with bounds of -0.6 and 0.6, and if effect larger than 0.6 can be rejected, this might be sufficient to tentatively start to question claims of even larger effects in the literature.

### Plot a sensitivity power analysis {#sensitivitypower}

```{r gsens0, fig.margin=FALSE, echo=FALSE, fig.cap="Sensitivity power analysis in G\\*Power software."}
knitr::include_graphics("images/sensitivity0.png")
```

In a **sensitivity power analysis** the sample size and the alpha level are fixed, and you compute the effect size you have the desired statistical power to detect. For example, in Figure \@ref(fig:gsens0) the sample size in each group is set to 15, the alpha level is 0.05, and the desired power is set to 90%. The sensitivity power analysis shows we have 90% power to detect an effect of d = 1.23. Perhaps you feel a power of 90% is a bit high, and you would be happy with 80% power. In Figure \@ref(fig:gsens1) we see that if we desire 80% power, the effect size should be d = 1.06. The smaller the true effect size, the lower the power we have. This plot should again remind us not to put too much faith in a significance test when are sample size is small, since for 15 observations in each condition, statistical power is very low for anything but extremely large effect sizes.

```{r gsens1, fig.margin=FALSE, echo=FALSE, fig.cap="Plot of the effect size against the desired power when n = 15 per group and alpha = 0.05."}
knitr::include_graphics("images/sensitivity1.png")
```

If we look at the effect size that we would have 50% power for, we see it is d = 0.7411272. This is very close to our critical effect size of d = 0.7479725 (the smallest effect size that, if observed, would be significant). The difference is due to the non-central *t*-distribution (see Figure \@ref(fig:noncentralt). If the distribution was symmetric, observing an effect size exactly on the critical value would mean half of the distirbution is smaller than the critical effect size, and half of the distribution is larger, and we would have exactly 50% power for the critical effect size. Because the distribution is not symmetrical, we need to find the critical effect size for which it *is* true that half the distribution falls below it, and half the distribution falls above it. This value can't be calculated directly, and requires an iterative procedure that optimizes the values of the non-centrality parameter such that power is exactly 50% [@smithson_confidence_2003]. The `pwr` package in R can calculate this effect size in a sensitivity analysis where we enter the sample size (per group), the alpha level, and the desired power for a two-sided independent *t*-test, which will return d = 0.7411272. We can check this true effect size would indeed give with 50% power.

```{r, echo = TRUE}
# Sensitivity power analysis
pwr::pwr.t.test(n = 15, 
                sig.level = 0.05, 
                power = 0.5, 
                type = "two.sample",
                alternative = "two.sided")

# d = 0.7411272 - let's check this indeed gives 50% power
# We compute the non-centrality parameter delta as 2.0296604
# delta <- 0.7411272 * sqrt(15/2)

# For a 2-sided test power is the probability of finding a result that is
# 1) larger than the critical value of 2.0484071, which is: 0.4999563
# 2) We need to add the tiny probability that we find a significant result in
# the opposite direction. This will rarely happen: 0.00004372648
# Together these two probabilities make exactly 50%:
# 1-pt(2.0484071, 28, 2.0296604) + pt(-2.0484071, 28, 2.0296604)
```

### Reporting a feasibility justification.

To summarize, I recommend addressing the following components in a feasibility sample size justification. Addressing these points explicitly will allow you to evaluate for yourself if collecting the data will have scientific value. If not, there might be other reasons to collect the data. For example, at our department, students often collect data as part of their education. However, if the primary goal of data collection is educational, the sample size that is collected can be very small. It is often educational to collect data from a small number of participants to experience what data collection looks like in practice, but there is often no educational value in collecting data from more than 10 participants. Despite the small sample size, we often require students to report statistical analyses as part of their education, which is fine as long as it is clear the numbers that are calculated can not meaningfully be interpreted. Table \@ref(tab:table-pow-rec) should help to evaluate if the interpretation of statistical tests has any value, or not.

```{r table-pow-rec, echo = FALSE}

table_pow_rec <- data.frame(c("Will a future meta-analysis be performed?", "Will a decision be made, regardless of the amount of data that is available?", "What is the critical effect size?", "What is the width of the confidence interval?", "Which effect sizes would you have decent power to detect?"), c("Consider the plausibility that sufficient highly similar studies will be performed in the future to, eventually, make a met-analysis possible", "If it is known that a decision will be made, with or without data, then any data you collect will reduce error rates.", "Report and interpret the critical effect size, with a focus on whether a hypothesis test would even be significant for expected effect sizes. If not, indicate you will not interpret the data based on *p*-values.", "Report and interpret the width of the confidence interval. What will an estimate with this much uncertainty be useful for? If the null hypothesis is true, would rejecting effects outside of the confidence interval be worthwhile (ignoring you might have low power to actually test against these values)?", "Report a sensitivity power analysis, and report the effect sizes you could detect across a range of desired power levels (e.g., 80%, 90%, and 95%), or plot a sensitivity curve of effect sizes against desired power."))
colnames(table_pow_rec) <- c("What to address", "Example?")

knitr::kable(table_pow_rec, caption = "Overview of recommendations when reporting a sample size justification based on feasibility.")
```

If the study is not performed for educational purposes, but the goal is answer a research question, the feasibility justification might indicate that there is no value in collecting the data. If it wasn't possible to conclude that one should not proceed with the data collection, there is no use of justifying the sample size. There should be cases where it is unlikely there will ever be enough data to perform a meta-analysis (for example because of a lack of general interest in the topic), the information will not be used to make any decisions, and the statistical tests do not allow you to test a hypothesis or estimate an effect size estimate with any useful accuracy. It should be a feasibility justification - not a feasibility excuse. If there is no good justification to collect the maximum number of observations that is feasible, performing the study nevertheless is a waste of participants time, and/or a waste of money if data collection has associated costs. Collecting data without a good justification why the planned sample size will yield worthwhile information has an ethical component. As Button and colleagues @button_power_2013 write: "Low power therefore has an ethical dimension — unreliable research is inefficient and wasteful. This applies to both human and animal research". Think carefully if you can defend data collection based on a feasibility justification. Sometimes data collection is just not feasible, and we should accept this.

## A-priori power analysis

When designing a study where the goal is to observe a statistically significant effect, researchers often want to make sure their sample size is large enough to have sufficient power to detect effects they expect, or effects they are interested in observing. This is done by performing an *a-priori* power analysis. Given a specified effect size, alpha level, and desired power, an a-priori power analysis will inform you about the sample size you need to collect. In Figure \@ref(fig:power-2) you see how the statistical power increases as the number of observations (per group) in an independent *t*-test with an alpha level of 0.05 increases.

```{r, power-2, fig.margin=FALSE, echo=FALSE, fig.cap="Power curve for an independent *t*-test as a function of the sample size."}
N <- 50
d <- 0.5
p_upper <- 0.05
ncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d
plot_power_custom <- (function(d, N, p_upper) {
  ncp <- d * (N * N / (N + N))^0.5 # formula to calculate t from d from Dunlap, Cortina, Vaslow, & Burke, 1996, Appendix B
  t <- qt(1 - (p_upper / 2), df = (N * 2) - 2)
  1 - (pt(t, df = N * 2 - 2, ncp = ncp) - pt(-t, df = N * 2 - 2, ncp = ncp))
})
par(bg = "aliceblue")
plot(-10,
  xlab = "sample size (per group)", ylab = "Power", axes = FALSE,
  main = expression(paste("Power for independent t-test and ", delta, " = ", 0.5)), xlim = c(0, N * 2), ylim = c(0, 1)
)
abline(v = seq(0, N * 2, (2 * N) / 10), h = seq(0, 1, 0.1), col = "lightgray", lty = 1)
axis(side = 1, at = seq(0, 2 * N, (2 * N) / 10), labels = seq(0, 2 * N, (2 * N) / 10))
axis(side = 2, at = seq(0, 1, 0.2), labels = seq(0, 1, 0.2))
curve(plot_power_custom(d=d, N=x, p_upper=p_upper), 3, 2*N, type="l", lty=1, lwd=2, ylim=c(0,1), xlim=c(0,N), add=TRUE)
points(x = N, y = (1 + pt(qt(p_upper / 2, 2 * N - 2, 0), 2 * N - 2, ncp) - pt(qt(1 - p_upper / 2, 2 * N - 2, 0), 2 * N - 2, ncp)), cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5))

```

A-priori power calculations are performed under the assumption that there is an effect. In practice, it is of course also possible that there is no effect (e.g., d = 0). If there is no true effect, and you want to use formally correct language, power is *undefined*. Statistical power is a concept that can only be computed assuming there is a true effect. If there is no true effect, you will still observe significant effects, but these ar *Type 1 errors*, or false positives. These occur at your chosen alpha level (e.g., 5% of the time). If you perform a hypothesis test, there are four possible outcomes:

1. False positives or Type 1 errors, indicated by α (you observe a significant test result when H0 is true)
2. False negatives or Type 2 errors, indicated by β (you observe a non-significant result when H1 is true)
3. True negatives, indicated by 1-α (a non-significant result when H0 is true) 
4. True positives, indicated by 1-β (a significant test result when H1 is true)

```{r NHST-outcomes, fig.margin=TRUE, echo=FALSE, fig.cap="Four possible outcomes in a null hypothesis significance test."}
knitr::include_graphics("images/2.1.1.png")
```

The goal of an a-priori power analysis is to increase the sample size up to the level that the desired power, the probability of finding a significant result if there is a true effect, or 1-β, is at a desired level for an effect size one is interested in detecting, given a specific alpha level.

In Figure \@ref(fig:power-3) you see two distributions. The left (grey) distribution is centered at 0. This is our model for the null hypothesis. If the null hypothesis is true we can find statistically significant results if the effect size is extreme enough (in a two-sided test either in the positive or negative direction), but these would be Type 1 errors (the red areas under the curve). 

The right (black) distribution is centered at an effect of d = 0.5. This is our model for the alternative hypothesis, where we expect a true effect in the population of d = 0.5. Even though there is a true effect, we will not always find a statistically significant result. This happens when, due to random variation, the observed effect size is too close to 0 to be statistically significant. These results would be false negative results (the blue area under the curve).

```{r, power-3, fig.margin=FALSE, echo=FALSE, fig.cap="Null and alternative distribution, assuming d = 0.5, alpha = 0.05, and N = 86 per group."}
N <- 86
d <- 0.5
p_upper <- 0.05
ncp <- (d * sqrt(N / 2)) # Calculate non-centrality parameter d
crit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)
low_x <- min(-1 - crit_d)
high_x <- max(d + 1 + crit_d)
# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2) # calculate distribution of d based on t-distribution
# Set max Y for graph
y_max <- max(d_dist) + 1
# create plot
par(bg = "aliceblue")
d <- round(d, 2)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = substitute(paste("Cohen's ", delta == 0.5)), ylab = "Density", main = substitute(paste("Distribution of Cohen's ", delta == 0.5, ", N = 86 per group")))
# abline(v = seq(low_x,high_x,0.1), h = seq(0,0.5,0.1), col = "lightgray", lty = 1)
#    axis(side = 1, at = seq(low_x,high_x,0.1), labels = FALSE)
lines(x, d_dist, col = "black", type = "l", lwd = 2)
# add d = 0 line
d_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = 0) * sqrt(N / 2)
lines(x, d_dist, col = "grey", type = "l", lwd = 1)
# Add Type 1 error rate right
crit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)
y <- seq(crit_d, 10, length = 10000)
z <- (dt(y * sqrt(N / 2), df = (N * 2) - 2) * sqrt(N / 2)) # determine upperbounds polygon
polygon(c(crit_d, y, 10), c(0, z, 0), col = rgb(1, 0, 0, 0.5))
# Add Type 1 error rate left
crit_d <- -abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)
y <- seq(-10, crit_d, length = 10000)
z <- (dt(y * sqrt(N / 2), df = (N * 2) - 2) * sqrt(N / 2)) # determine upperbounds polygon
polygon(c(y, crit_d, crit_d), c(z, 0, 0), col = rgb(1, 0, 0, 0.5))
# Add Type 2 error rate
crit_d <- abs(qt(p_upper / 2, (N * 2) - 2)) / sqrt(N / 2)
y <- seq(-10, crit_d, length = 10000)
z <- (dt(y * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2)) # determine upperbounds polygon
polygon(c(y, crit_d, crit_d), c(0, z, 0), col = rgb(0, 0, 1, 0.5))

```

If we want to increase the statistical power, we are trying to reduce the size of the blue area. One way to do this would be to increase the effect size. This would shift the entire distribution to the right, and reduce the size of the blue area. Although there are ways in controlled experiments to increase the standardized effect size (e.g., by reducing statistical variation in the data), often the effect size is what it is. All we can do is increase the sample size. This will make the distribution around 0.5 more narrow, and this reduces the blue area. You can check this in an online shiny app that [reproduces the plot](http://shiny.ieis.tue.nl/d_p_power/).

### Performing a power analysis. 

There is a wide range of software tools that can help you to perform an a-priori power analysis. Sometimes statistical power can be analytically computed based on closed formulas, and in other situations power can be computed by performing simulations. If we simulate thousands of studies, and count the percentage of studies that are statistically significant, we have an estimate of the statistical power of the test. The code below computes the statistical power (using the `power.t.test` in base R) assuming an effect size of d = 0.5, and alpha level of 0.05, and a desired power of 90%. 

```{r, echo = TRUE}
power.t.test(delta = 0.5, 
             sig.level = 0.05,
             power = 0.9,
             type = "two.sample",
             alternative = "two.sided")
```

The output tells us we need to collect 85.03 observations. Because observations do not come in decimals (we can hardly cut 0.03 from a participant) we need to collect 86 observations. We could also get this answer through simulations. 

```{r, echo = TRUE, cache = TRUE}
set.seed(600746) #set a seed for reproducible simulations
p <- numeric(100000) #set up empty variable to store all simulated p-values

#Run simulation
for(i in 1:100000){ #for each simulated experiment
  x <- rnorm(n = 85, mean = 0, sd = 1) #Simulate data group 1
  y <- rnorm(n = 85, mean = 0.5, sd = 1) #Simulate data group 2
  z <- t.test(x, y) #perform the t-test
  p[i] <- z$p.value #get the p-value and store it
}

#Calculate power: sum significant p-values, divide by 100000 simulations
(sum(p < 0.05)/100000) #power
```

We see that after 100000 simulations with 85 participants, the estimated power is 0.90015 with 85 participants in each group. The analytic solution tells us power is 0.899894. The more simulations you perform, the more accurate the power estimate will be (as we see, 100000 simulations gives very accurate results).

The way you perform the power analysis depends on the software you use. There are excellent software packages for power analysis, such as [PASS](https://www.ncss.com/software/pass/) or [SAS](https://stats.idre.ucla.edu/sas/seminars/proc-power/), but these solutions are rather expensive. There are also freeware solutions, such as the widely used [G\*Power](https://gpower.hhu.de), [MorePower](https://wiki.usask.ca/pages/viewpageattachments.action?pageId=420413544) by Campbell and Thompson, and [PANGEA](https://jakewestfall.shinyapps.io/pangea/) by Jake Westfall. 
There is also a range of options in R. The default `stats` package has power functions for *t*-tests, proportions, and ANOVA, the `pwr` package has a wider range of options, `pwr2ppl` by Chris Aberson accompanies his excellent book on power analyses [@aberson_applied_2019], `powerlmm` by Kristoffer Magnusson performs power analyses for [two- and three-level linear mixed models](https://github.com/rpsychologist/powerlmm), and [Superpower](https://aaroncaldwell.us/SuperpowerBook/) created by Aaron Caldwell and myself does power analyses for complex ANOVA designs.

It takes time to learn to use this software correctly. You will need to sit down for a few hours and go through vignettes or read the accompanying publications, before you know what to do. In our online manual for Superpower, we compare Superpower against G\*Power, `pwr`, `pwr2ppl` and MorePower. Each software package will have slightly different design philosophies. In G\*Power you are expected to enter the standardized effect size d, while in MorePower we enter the mean difference and the standard deviation, and in Superpower we need to enter the means and standard deviations for each condition. In general I recommend to always think about the raw pattern of means you expect, before you perform a power analysis. This typically leads to more realistic effect sizes estimates than the use of standardized effect sizes (where researchers all too often 'expect' a d = 0.5 just because that is the default value in G\*Power).

```{r powerex1, fig.margin=FALSE, echo=FALSE, fig.cap="Example of a power analysis in G\\*Power."}
knitr::include_graphics("images/powerex1.png")
```

```{r powerex2, fig.margin=FALSE, echo=FALSE, fig.cap="Example of a power analysis in MorePower."}
knitr::include_graphics("images/powerex2.png")
```

```{r, echo = TRUE}
pwr::pwr.t.test(d = 0.5, 
                sig.level = 0.05,
                power = 0.8,
                type = "two.sample",
                alternative = "two.sided")
```

```{r, powerex3, fig.margin = FALSE, fig.cap="Example of a power analysis plot in Superpower.", echo = TRUE, cache = TRUE}

design_result <- Superpower::ANOVA_design(design = "2b",
                                          n = 86,
                                          mu = c(0.5, 0),
                                          sd = c(1, 1),
                                          plot = FALSE)

# Plot power curve (from 5 to 200)
Superpower::plot_power(design_result, max_n = 200, verbose = FALSE)

```

### Some advice when using G\*Power.

G\*Power is one of the most widely used software tools for power analysis [@bakker_recommendations_2020]. The option for power analysis for a Pearson's correlation coefficient is under the Exact test family (Correlation: Bivariate normal model). The "Correlation: Point biserial model" option under the *t*-tests family is for correlations where one variable is dichotomous. The difference is small, but the required sample size is typically a few observations larger for Pearson's correlation coefficient. 

```{r gpowcor, fig.margin=FALSE, echo=FALSE, fig.cap="The options for a power analysis for Pearson's correlation(above) and the point biserial correlation (when one variable is dichotomous)."}
knitr::include_graphics("images/gpowcor.png")
```

Although the effect size for an independent *t*-test and dependent *t*-test are often both referred to as Cohen's d, they differ, and are calculated in different ways. Cohen himself distinguished between $d_s$ and $d_z$ [@lakens_calculating_2013]. You should not enter Cohen's d for a power analysis for a dependent *t*-test (nor should you directly compared Cohen's d from a between design with Cohen's d for a within design, nor should you use the benchmarks Cohen provided for a small (0.2), medium (0.5), and large (0.8) effect be used for Cohen's $d_z$). Make sure you are entering the correct effect size. Cohen's $d_z$ can be calculated from the *t*-value for a dependent *t*-test and the sample size as follows: 

$$d_z = {\frac{t}{\sqrt{n}}} $$

```{r gpowdzd, fig.margin=FALSE, echo=FALSE, fig.cap="Power for a dependent and independent *t*-test require entering Cohen's dz and d, respectively."}
knitr::include_graphics("images/gpowdzd.png")
```

The third issue that researchers often miss is that G\*Power has a very unfortunate default setting for power analyses for within subject ANOVA tests. In Figure \@ref(fig:gpowdwithin2) we see on the left how we can directly calculate Cohen's $f$ (the effect size one needs to enter for ANOVA power analyses) from partial eta squared. A medium effect size for $\eta^2_p$ of 0.588 equals a Cohen's $f$ of 0.25. If we specify an expected correlation between dependent variables of 0.8, have 3 repeated measurements, one condition, and want to achieve 95% power with an alpha level of 0.05, we need a sample size of 19. However, if out value for $\eta^2_p$ comes from the scientific literature or statistical software such as SPSS, this effect size measure already has the correlation between observation incorporated. If we would simply enter it in G\*Power, we take into account the correlation twice, which leads to a massively smaller sample size. Instead, we need to click on the "Options" button and check the radiobutton before "As in SPSS". We see that "Effect size f" changes into "Effect size f(U)", and the box "Corr among rep measures" has disappeared on the right. This is because the correlation no longer needs to be entered separately - it is already taken into account in the $\eta^2_p$ as SPSS computes it. Most importantly, we now see that the sample size we need to achieve 95% power has changed to 127. This is a big difference, and I have seen people make this mistake very often. If you use G\*power for power analyses for ANOVA designs, I recommend always double checking the Options setting (and maybe repeat your analysis in software such as Superpower just to double check). 

```{r gpowdwithin2, fig.margin=FALSE, echo=FALSE, fig.cap="Power analysis for repeated ANOVA in G\\*Power by default expects a partial eta squared effect size that does not take the correlation between measurements into account."}
knitr::include_graphics("images/gpowwithin2.png")
```

### Reporting an a-priori power analysis.

As with all the analyses you rely on in your work, make sure the power analysis is computationally reproducible. If you performed power analyses in R, you can store the script. In G\*Power, you can save the several power analyses you performed consecutively in the app under the 'protocol of power analysis' tab by saving them as a .rtf file or printing them as a pdf file. 

```{r gpowprotocol, fig.margin = FALSE, echo = FALSE, fig.cap="G\\*Power allows you to easily save all details about the power analysis you performed."}
knitr::include_graphics("images/gpowprotocol.png")
```

If you use the [Superpower Shiny app](https://arcstats.io/shiny/anova-exact/) for more complex ANOVA designs, you can print a pdf file with all detailed about the power analysis you performed. Sharing the code or a printout of the power analysis will capture all required information about the power analysis you performed, which is often preferable over attempting to communicate all this information verbally.

```{r superpowerreport, fig.margin = FALSE, echo = FALSE, fig.cap="Superpower allows you to print a report with all information about the power analysis you performed."}
knitr::include_graphics("images/superpowerreport.png")
```

Providing a reproducible report is sufficient to clarify the power calculation, but not the reasons behind the power calculations. Your decision for the desired Type 1 and Type 2 error rate needs a justification, and most importantly, the effect size you have powered for needs to be justified. If your effect size estimate is based on the existing literature, provide a full citation, and preferably a direct quote from the article. If your effect size is based on an empirical estimate from the scientific literature, you will need to address 1) uncertainty, and 2) bias [@anderson_sample-size_2017; @taylor_bias_1996]. If your effect size is based on a smallest effect size of interest, this value should not just be stated, but [justified](#justifysesoi) (e.g., based on theoretical predictions, practical implications, or effect sizes observed or detectable in the literature). 

**should this be addressed here, or in the chapter on bias? Maybe in the chapter on bias**.

```{r table-pow-rec-2, echo = FALSE}

table_pow_rec_2 <- data.frame(c("List all primary analyses you plan to do.", "Specify the alpha level for each analysis", "What is the desired power?", "For each power analysis, specify the effect size metric, the effect size, and the justification for powering for the effect size.", "Make sure the power analysis is reproducible."), c("Following JARS guidelines, specify all primary analyses for which you want to control Type 1 and Type 2 error rates", "List and justify the chosen alpha level for each analysis. Make sure to correct for multiple comparisons where needed.", "List and justify the desired power for each analysis", "Report the effect size metric (e.g., Cohen's $d_z$,  Cohen's f), the effect size (e.g., 0.3). and the justification for the effect size.", "Include the code used to run the power analysis, or print a report containing the details about the power analyses you performed."))
colnames(table_pow_rec_2) <- c("What to address?", "How to address it?")

knitr::kable(table_pow_rec_2, caption = "Overview of recommendations when reporting an a-prior power analysis.")
```


## Compromise power analysis

Sometimes researchers are brave enough *not* to fix their alpha level to 0.05. There are two situations where it is sensible to use a different alpha level, even if you want to follow current conventions. The first situation is when you are fortunate enough to be able to collect so many observations that the statistical power for your test is very high. For example, imagine you have access to 2000 employees who are all required to answer questions during a yearly evaluation in a company where you are testing an intervention that should reduce subjectively reported stress levels. You are quite confident that an effect smaller than d = 0.2 is not large enough to be subjectively noticeable for individuals [@jaeschke_measurement_1989]. With an alpha level of 0.05 you would have a statistical power of 0.994, or a Type 2 error rate of 0.006. This means that for a smallest effect size of interest of d = 0.2 we are 8.3 times more likely to make a Type 1 error than a Type 2 error. 

This is a reversal of the normal balance of Type 1 and Type 2 error rates, where the minimum desired power is 0.8, and a Type 1 error (0.05) is 4 times as unlikely as a Type 2 error (20%). The default use of 80% power (or a 20% Type 2, or beta ($\beta$) error) is based on nothing more than a personal preference by @cohen_statistical_1988, who writes: 

>>It is proposed here as a convention that, when the investigator has no other basis for setting the desired power value, the value .80 be used. This means that $\beta$ is set at .20. This arbitrary but reasonable value is offered for several reasons (Cohen, 1965, pp. 98-99). The chief among them takes into consideration the implicit convention for $\alpha$ of .05. The $\beta$ of .20 is chosen with the idea that the general relative seriousness of these two kinds of errors is of the order of .20/.05, i.e., that Type I errors are of the order of four times as serious as Type II errors. This .80 desired power convention is offered with the hope that it will be ignored whenever an investigator can find a basis in his substantive concerns in his specific research investigation to choose a value ad hoc.

We see that conventions are built on conventions: the norm to aim for 80% power is built on the norm to set the alpha level at 5%. What we should take away from Cohen is not that we should aim for 90% power, but that we should justify our error rates based on the relative seriousness of each error. If our Type 1 error is 4 times as serious as a Type 2 error, and our Type 2 error is very low for all effects we care about, we should adjust our Type 1 error rate [@erdfelder_gpower_1996]. G\*Power allows us to perform such a **compromise power analysis**, as illustrated in In Figure \@ref(fig:gpowcompromise) 

```{r gpowcompromise, fig.margin = FALSE, echo = FALSE, fig.cap="Compromise power analysis in G\\*Power."}
knitr::include_graphics("images/compromise1.png")
```

Of course you do not need to follow conventions. If you believe Type 1 errors and Type 2 errors are equally problematic (which is often at least as defensible as arguing a Type 1 error is exactly 4 times as bad as a Type 2 error) you would weigh both types of errors equally, and use α = 0.01785. 

As Faul, Erdfelder, Lang, and Buchner -@faul_gpower_2007 write:

>>Of course, compromise power analyses can easily result in unconventional significance levels greater than $\alpha$ = .05 (in the case of small samples or effect sizes) or less than $\alpha$ = .001 (in the case of large samples or effect sizes). However, we believe that the benefit of balanced Type I and Type II error risks often offsets the costs of violating significance level conventions

This brings us to the second situation where we want to deviate from α = 0.05, while still following the underlying convention proposed by Cohen to weigh a Type 1 error 4 times as serious as a Type 2 error: The situation where power is lower than 80%. Although it is highly undesireable to make decisions when error rates are so very high, if one finds oneself in a situation where a decision must be made based on little data, @winer_statistical_1962 writes: 

>>The frequent use of the .05 and .01 levels of significance is a matter of convention having little scientific or logical basis. When the power of tests is likely to be low under these levels of significance, and when Type 1 and Type 2 errors are of approximately equal importance, the .30 and .20 levels of significance may be more appropriate than the .05 and .01 levels. 

If we plan to perform a two-sided *t*-test, and can collect at most 50 observations in each independent group, and we want to have 90% power, and expect a population effect size of 0.5, we would have 70% power if we set our alpha level to 0.05. If we can not increase our sample size beyond 50 per group, and we need to make a decision based on the data we will collect, we could set our alpha level to 0.065 to have a Type 1 error rate 4 times as high as our Type 2 error rate (0.262). OR, we can choose to weigh both types of error equally, and set the alpha level to 0.149. The choice of α and β can be further improved by taking priors probabilities of H0 and H1 into account [@miller_quest_2019] In the end, as Neyman and Pearson @neyman_problem_1933 write: "The use of these statistical tools in any given case, in determining just how the balance should be struck, must be left to the investigator."

## How to justify your sample size based on a power analysis.

One challenge in power analysis is that you never know the true effect size. This leads to the ‘sample size samba’ [@schulz_sample_2005]. Researchers go back and forth between the effect size they expect, and the sample size they are willing to collect, until they 'expect' the effect size that, in an a-priori power analysis, leads to the sample size they are willing to collect. This practice obviously makes a power analysis a useless procedure. 
One might be tempted to perform a small power analysis to estimate the effect size, and use this effect size estimate in an a-priori power analysis. Regrettably, this is not a recommended solution. First, effect size estimates from small pilot studies are highly uncertain , which means the estimate can easily be much smaller or larger than the population effect size (which can be seen by the width of the confidence interval around the effect size estimate). Second, such a procedure inevitably leads to bias, because you will only perform studies when the pilot provided an effect size estimates that, when entered in an a-priori power analysis, yielded a sample size that was feasible to collect. Since you only follow up on pilot studies when the effect size estimate is sufficiently large, this inevitably leads to 'follow-up bias' [@albers_when_2018]. 

What can be done? The recommended best practice is to not enter the effect size you expect, but the smallest effect size you would still be interested in. By doing this, you will be able to design a study that has sufficient power for the smallest effect you find worthwhile to observe, even when you do not know what the true effect size is. By determining the required sample size for the smallest effect size of interest, you can guarantee you have designed an informative study. However, in case the true effect size is larger than your smallest effect size of interest, you might collect many more participants than required. This can be solved by performing sequential analyses, where you analyze the data intermittently, while controlling the Type 1 error rate for multiple looks at the data. Ways to justify your smallest effect size of interest and how to perform sequential analyses are addressed elsewhere in this book.

## Observed (post-hoc) power analysis {#observedpower}

Observed power (or post-hoc power) is the statistical power of the test you have performed, based on the effect size estimate from your data. **Observed power** differs from the **true power** of your test, because the true power depends on the (true)unknown) population effect size, while observed power is computed based on the (unlikely) assumption that the effect size you observed in your sample is the true effect size. SPSS (which still hasn't included useful statistics such as effect sizes for *t*-tests) provides users with the option to report observed power by clicking a checkbox. You should never calculate the observed power. Observed power, computed based on the observed effect size, is a useless statistical concept. Despite this fact, editors and reviewers often ask authors to perform post-hoc power analysis to make a statement about the statistical power a study had. You should never comply with this request. The correct approach to evaluate the statistical power of a study after the data is collected is a [sensitivity power analysis](#sensitivitypower). 

Observed (or post-hoc) power and *p*-values are directly related [@hoenig_abuse_2001]. For a *Z*-test where the *p*-value is exactly 0.05, the perfectly symmetric alternative distribution falls exactly on top of the critical value. This means that half the distribution falls to the right of the critical value, and half falls on the left, and we have exactly 50% power.

<!-- ```{r obs-power-plot-1, echo = FALSE, fig.width = 8, fig.height = 8, fig.cap="Relationship between p-values and power for a Z-test."} -->
<!-- # For simplicity, take a one-sided test -->
<!-- # compute z value from p: -->
<!-- # p_val = 0.05 -->
<!-- # z <- qnorm(1-p_val) -->
<!-- # z -->
<!-- #  -->
<!-- # # compute p-value from z -->
<!-- # 1-pnorm(z) -->

<!-- # computing the Type 1 error rate: -->
<!-- # old, two-sided: 1-pnorm(q = 1.959964) + pnorm(q = -1.959964) -->

<!-- # Moving the mean from 0.  -->
<!-- # Because the normal distribution is symmetric, if we observe a p-value on top of the critical value (p = 0.05) we have 50% power. -->

<!-- # pnorm(q = 1.644854, mean = 1.644854) -->

<!-- # We can plot this across a range of observe p-values -->

<!-- plot_obs_pow <- (function(alpha_level, p_val) { -->
<!--   1 - pnorm(q = qnorm(1-alpha_level), mean = qnorm(1-p_val)) -->
<!-- }) -->


<!-- par(bg = "aliceblue", pty = "s") -->
<!-- plot(-10, -->
<!--   xlab = "p-value", ylab = "Observed power", axes = FALSE, -->
<!--   main = substitute(paste("Relationship between p-value and observed power")), xlim = c(0, 1), ylim = c(0, 1) -->
<!-- ) -->
<!-- abline(v = seq(0, 1, 0.1), h = seq(0, 1, 0.1), col = "lightgray", lty = 1) -->
<!-- axis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) -->
<!-- axis(side = 2, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) -->
<!-- curve(plot_obs_pow(alpha_level = 0.05, p_val = x), 0, 1, add = TRUE, lwd = 2) -->
<!-- points(x = 0.05, y = 0.5, cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5)) -->
<!-- abline(v = 0.05, h = 0.5, col = rgb(1, 0, 0, 0.5), lty = 1) -->


<!-- ``` -->

For an independent *t*-test the noncentral *t*-distribution is not perfectly symmetric. We can compute observe power in R based on the formula below, where `alpha_level` is the alpha level, `n`is the sample size in each group, and `p_val` is the observed *p*-value. In Figure \@ref(fig:obs-power-plot-2) we see that for a *p* = 0.05 observed power is ever so slightly larger than 50%, but the observed power value is very close. 

```{r obs_power-plot-2, echo = TRUE, eval = FALSE}
1 - pt(q = qt(1 - alpha_level/2, 2 * n - 2), 
       df = 2 * n - 2, 
       ncp = qt(1 - p_val/2, 2 * n - 2))
```

Plotting observe power for the *p*-value across the range from 0 to 1, we see that observed power is completely determined, and provides no additional value beyond, reporting the *p*-value. If your *p*-value is non-significant, observed power will be less than approximately 50% in a *Z*-test and *t*-test. As Lenth -@lenth_post_2007 shows, observed power is also completely determined by the observed *p*-value for *F*-tests, although the relationship between *p* = 0.05 and 50% power no longer holds. 

```{r, echo = FALSE, fig.width = 8, fig.height = 8, fig.cap="Relationship between p-values and power for an independent t-test."}

# # compute t value from p and N:
# p_val = 0.05
# n = 100
# z <- qt(1 - p_val/2, 2 * n - 2)
# z
# 
# # compute p-value from z
# 1-pt(z, 2 * n - 2) + pt(-z, 2 * n - 2)
# 
# # Moving the mean from 0. 
# # Because the normal distribution is symmetric, if we observe a p-value on top of the ncp (which we get from G*power) we have 50% power.
# 
# 1 - pt(q = 1.972017, df = 2 * n - 2, ncp = 1.9694098)

# We can plot this across a range of observe p-values

plot_obs_pow_t <- (function(alpha_level, n, p_val) {
  1 - pt(q = qt(1 - alpha_level/2, 2 * n - 2), df = 2 * n - 2, ncp = qt(1 - p_val/2, 2 * n - 2))
})


par(bg = "aliceblue", pty = "s")
plot(-10,
  xlab = "p-value", ylab = "Observed power", axes = FALSE,
  main = substitute(paste("Relationship between p-value and observed power for an independe t-test with n = 100")), xlim = c(0, 1), ylim = c(0, 1)
)
abline(v = seq(0, 1, 0.1), h = seq(0, 1, 0.1), col = "lightgray", lty = 1)
axis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1))
axis(side = 2, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1))
curve(plot_obs_pow_t(alpha_level = 0.05, n = 10, p_val = x), 0, 1, add = TRUE, lwd = 2)
points(x = 0.05, y = 0.5, cex = 2, pch = 19, col = rgb(1, 0, 0, 0.5))
abline(v = 0.05, h = 0.5, col = rgb(1, 0, 0, 0.5), lty = 1)


```

### What to do if your editor asks for post-hoc power?

Editors sometimes ask researchers to report post-hoc power analyses when authors report a test result of *p* > 0.05, and when authors want to conclude there is no effect. In such situations, editors would like to distinguish between true negatives (concluding there is no effect, when there is no effect) and false negatives (concluding there is no effect, when there actually is an effect, or a Type 2 error). As the preceding explanation of post-hoc power illustrates, reporting post-hoc power is nothing more than a different way of reporting the *p*-value, and will therefore not answer the question editors want to know [@hoenig_abuse_2001; @lenth_post_2007; @yuan_post_2005].

What you should do instead is report a [sensitivity power analysis](#sensitivitypower), preferably by plotting power across a wide range of possible true effect sizes. Many of the recommendations in Table \@ref(tab:table-pow-rec) apply if one wants to evaluate power after the data has been collected. The best solution is, not surprisingly, to design a study that will yield informative results regardless of whether the alternative hypothesis is true or the null hypothesis is true, by performing an a-priori power analysis both for the NHST and for an equivalence test. 

## Why Within-Subject Designs Typically Require Fewer Observations than Between-Subject Designs

One widely recommended approach to increase power is using a within subject design. Indeed, you need fewer observations to detect a mean difference between two conditions in a within-subjects design (in a dependent *t*-test) than in a between-subjects design (in an independent t-test). The reason is straightforward, but not always explained, and even less often expressed in the easy equation below. The sample size needed in within-designs (NW) relative to the sample needed in between-designs (NB), assuming normal distributions, is (from Maxwell & Delaney, 2004, p. 561, formula 45): 

NW = NB (1-ρ)/2

The “/2” part of the equation is due to the fact that in a two-condition within design every participant provides two data-points. The extent to which this reduces the sample size compared to a between-subject design depends on the correlation between the two dependent variables, as indicated by the (1-ρ) part of the equation. If the correlation is 0, a within-subject design simply needs half as many observations as a between-subject design (e.g., 64 instead 128 observations). The higher the correlation, the larger the relative benefit of within designs, and whenever the correlation is negative (up to -1) the relative benefit disappears. Note than when the correlation is -1, you need 128 observations in a within-design and 128 observations in a between-design, but in a within-design you will need to collect two measurements from each participant, making a within design more work than a between-design. However, negative correlations between dependent variables in psychology are rare, and perfectly negative correlations will probably never occur.

So what does the correlation do so that it increases the power of within designs, or reduces the number of observations you need? Let’s see what effect the correlation has on power by simulating and plotting correlated data. In the R script below, I’m simulating two measurements of IQ scores with a specific sample size (i.e., 10000), mean (i.e., 100 vs 106), standard deviation (i.e., 15), and correlation between the two measurements. The script generates three plots.

We will start with a simulation where the correlation between measurements is 0. First, we see the two normally distributed IQ measurements, with means of 100 and 106, and standard deviations of 15 (due to the large sample size, the numbers equal the input in the simulation, although small variation might still occur). 

```{r, plot-1, fig.margin=FALSE, echo=FALSE, fig.cap="Distributions of two dependent groups with means 100 and 106 and a standard deviation of 15."}
set.seed(419)
#Set color palette
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

n <- 10000 # set the sample size for each group
mx <- 100 # set the mean in group 1
sdx <- 15 # set the standard deviation in group 1
my <- 106 # set the mean in group 2
sdy <- 15 # set the standard deviation in group 2
cor.true <- 0.0 # set true correlation
# randomly draw data
cov.mat <- matrix(c(1.0, cor.true, cor.true, 1.0), nrow = 2, byrow = T)
mu <- c(0, 0)
mat <- mvrnorm(n, Sigma = cov.mat, mu = mu, empirical = FALSE)
x <- mat[, 1] * sdx + mx
y <- mat[, 2] * sdy + my
dif <- x - y
dataset <- data.frame(x, y)
DV <- c(x, y) # combine the two samples into a single variable
IV <- as.factor(c(rep("1", n), rep("2", n))) # create the independent variable (1 and 2)
datasetplot <- data.frame(IV, DV) # create a dataframe (to make the plot)

# plot graph two groups
p1 <- ggplot(datasetplot, aes(DV, fill = as.factor(IV))) +
  geom_histogram(alpha = 0.4, binwidth = 2, position = "identity", colour = "black", aes(y = ..density..)) +
  scale_fill_manual(values = cbbPalette, name = "Condition") +
  stat_function(fun = dnorm, args = c(mean = mx, sd = sdx), size = 1, color = "#E69F00", lty = 2) +
  stat_function(fun = dnorm, args = c(mean = my, sd = sdy), size = 1, color = "#56B4E9", lty = 2) +
  xlab("IQ") +
  ylab("number of people") +
  ggtitle("Data") +
  theme_bw(base_size = 20) +
  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) +
  geom_vline(xintercept = mean(x), colour = "black", linetype = "dashed", size = 1) +
  geom_vline(xintercept = mean(y), colour = "black", linetype = "dashed", size = 1) +
  coord_cartesian(xlim = c(50, 150)) +
  scale_x_continuous(breaks = c(50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150)) +
  annotate("text", x = 70, y = 0.02, label = paste("Mean X = ", round(mean(x)), "\n", "SD = ", round(sd(x)), sep = "")) +
  annotate("text", x = 130, y = 0.02, label = paste("Mean Y = ", round(mean(y)), "\n", "SD = ", round(sd(y)), sep = "")) +
  theme(plot.title = element_text(hjust = 0.5))


# plot data differences
p2 <- ggplot(as.data.frame(dif), aes(dif)) +
  geom_histogram(colour = "black", fill = "grey", aes(y = ..density..), binwidth = 2) +
  #  geom_density(fill=NA, colour="black", size = 1) +
  xlab("IQ dif") +
  ylab("number of people") +
  ggtitle("Data") +
  theme_bw(base_size = 20) +
  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) +
  geom_vline(xintercept = mean(dif), colour = "gray20", linetype = "dashed") +
  coord_cartesian(xlim = c(-80, 80)) +
  scale_x_continuous(breaks = c(seq(-80, 80, 10))) +
  annotate("text", x = mean(dif), y = 0.01, label = paste("Mean = ", round(mean(dif)), "\n", "SD = ", round(sd(dif)), sep = "")) +
  theme(plot.title = element_text(hjust = 0.5))

# Plot correlation
p3 <- ggplot(dataset, aes(x = x, y = y)) +
  geom_point(size = 2) + # Use hollow circles
  geom_smooth(method = lm, colour = "#E69F00", size = 1, fill = "#56B4E9") + # Add linear regression line
  coord_cartesian(xlim = c(40, 160), ylim = c(40, 160)) +
  scale_x_continuous(breaks = c(seq(40, 160, 20))) +
  scale_y_continuous(breaks = c(seq(40, 160, 20))) +
  xlab("IQ twin 1") +
  ylab("IQ twin 2") +
  ggtitle(paste("Correlation = ", round(cor(x, y), digits = 2), sep = "")) +
  theme_bw(base_size = 20) +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  coord_fixed(ratio = 1) +
  theme(plot.title = element_text(hjust = 0.5))


p1
```
In the scatter plot, we can see that the correlation between the measurements is indeed 0.
```{r, plot-2, fig.margin=FALSE, echo=FALSE, fig.cap="Correlation between two dependent groups."}
p3
```
Now, let’s look at the distribution of the mean differences. The mean difference is -6 (in line with the simulation settings), and the standard deviation is 21. This is also as expected. The standard deviation of the difference scores is √2 times as large as the standard deviation in each measurement, and indeed, 15*√2 = 21.21, which is rounded to 21. This situation where the correlation between measurements is zero equals the situation in an independent t-test, where the correlation between measurements is not taken into account. 

```{r, plot-3, fig.margin=FALSE, echo=FALSE, fig.cap="Distributions of difference scores between two dependent groups."}
p2
```

Now let’s increase the correlation between dependent variables to 0.7.

Nothing has changed when we plot the means:


```{r, plot-4, fig.margin=FALSE, echo=FALSE, fig.cap="Distributions of two independent groups with means 100 and 106 and a standard deviation of 15."}
set.seed(419)
#Set color palette
cbbPalette<-c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

n<-10000  #set the sample size for each group
mx<-100  #set the mean in group 1
sdx<-15  #set the standard deviation in group 1
my<-106  #set the mean in group 2
sdy<-15  #set the standard deviation in group 2
cor.true <- 0.0 #set true correlation
#randomly draw data
cov.mat <- matrix(c(1.0, cor.true, cor.true, 1.0), nrow = 2, byrow = T)
mu <- c(0,0)
mat <- mvrnorm(n, Sigma = cov.mat, mu = mu, empirical = FALSE)
x<-mat[,1]*sdx+mx
y<-mat[,2]*sdy+my
dif<-x-y
dataset<-data.frame(x,y)
DV<-c(x,y) #combine the two samples into a single variable
IV<-as.factor(c(rep("1", n), rep("2", n))) #create the independent variable (1 and 2) 
datasetplot<-data.frame(IV,DV) #create a dataframe (to make the plot)
t.test(x, y, alternative = "two.sided", paired = FALSE, var.equal = TRUE, conf.level = 0.95) #t-test

#plot graph two groups
p1 <- ggplot(datasetplot, aes(DV, fill = as.factor(IV)))  + 
  geom_histogram(alpha=0.4, binwidth=2, position="identity", colour="black", aes(y = ..density..)) +
  scale_fill_manual(values=cbbPalette, name = "Condition") +
  stat_function(fun=dnorm, args=c(mean=mx,sd=sdx), size=1, color="#E69F00", lty=2) +
  stat_function(fun=dnorm, args=c(mean=my,sd=sdy), size=1, color="#56B4E9", lty=2) +
  xlab("IQ") + ylab("number of people")  + ggtitle("Data") + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + 
  geom_vline(xintercept=mean(x), colour="black", linetype="dashed", size=1) + 
  geom_vline(xintercept=mean(y), colour="black", linetype="dashed", size=1) + 
  coord_cartesian(xlim=c(50,150)) + scale_x_continuous(breaks=c(50,60,70,80,90,100,110,120,130,140,150)) +
  annotate("text", x = 70, y = 0.02, label = paste("Mean X = ",round(mean(x)),"\n","SD = ",round(sd(x)),sep="")) +
  annotate("text", x = 130, y = 0.02, label = paste("Mean Y = ",round(mean(y)),"\n","SD = ",round(sd(y)),sep="")) +
  theme(plot.title = element_text(hjust = 0.5))


#plot data differences
p2 <- ggplot(as.data.frame(dif), aes(dif))  + 
  geom_histogram(colour="black", fill="grey", aes(y=..density..), binwidth=2) +
  #  geom_density(fill=NA, colour="black", size = 1) +
  xlab("IQ dif") + ylab("number of people")  + ggtitle("Data") + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), axis.text.y = element_blank(), panel.grid.minor.x = element_blank()) + 
  geom_vline(xintercept=mean(dif), colour="gray20", linetype="dashed") + 
  coord_cartesian(xlim=c(-80,80)) + scale_x_continuous(breaks=c(seq(-80, 80, 10))) +
  annotate("text", x = mean(dif), y = 0.01, label = paste("Mean = ",round(mean(dif)),"\n","SD = ",round(sd(dif)),sep="")) +
  theme(plot.title = element_text(hjust = 0.5))

#Plot correlation
p3 <- ggplot(dataset, aes(x=x, y=y)) +
  geom_point(size=2) +    # Use hollow circles
  geom_smooth(method=lm, colour="#E69F00", size = 1, fill = "#56B4E9") + # Add linear regression line
  coord_cartesian(xlim=c(40,160), ylim=c(40,160)) +
  scale_x_continuous(breaks=c(seq(40, 160, 20))) + scale_y_continuous(breaks=c(seq(40, 160, 20))) +
  xlab("IQ twin 1") + ylab("IQ twin 2")  + ggtitle(paste("Correlation = ",round(cor(x,y),digits=2),sep="")) + theme_bw(base_size=20) + 
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  coord_fixed(ratio = 1)  +
  theme(plot.title = element_text(hjust = 0.5))


p1
```

The correlation between measurements is now strongly positive:

```{r, plot-5, fig.margin=FALSE, echo=FALSE, fig.cap="Correlation between two dependent groups."}
p3
```

The important difference lies in the standard deviation of the difference scores. The SD = 11 instead of 21 in the simulation above. Because the standardized effect size is the difference divided by the standard deviation, the effect size (Cohen’s dz in within designs) is larger in this test than in the test above.

```{r, plot-6, fig.margin=FALSE, echo=FALSE, fig.cap="Difference scores between two dependent groups."}
p2
```

if you set the correlation to a negative value, the standard deviation of the difference scores actually increases. 

I like to think of dependent variables in within-designs as dance partners. If they are well-coordinated (or highly correlated), one person steps to the left, and the other person steps to the left the same distance. If there is no coordination (or no correlation), when one dance partner steps to the left, the other dance partner is just as likely to move to the wrong direction as to the right direction. Such a dance couple will take up a lot more space on the dance floor.

You see that the correlation between dependent variables is an important aspect of within designs. I recommend explicitly reporting the correlation between dependent variables in within designs (e.g., participants responded significantly slower (M = 390, SD = 44) when they used their feet than when they used their hands (M = 371, SD = 44, r = .953), t(17) = 5.98, p < 0.001, Hedges' g = 0.43, Mdiff = 19, 95% CI [12; 26]). 

Since most dependent variables in within designs in psychology are positively correlated, within designs will greatly increase the power you can achieve given the sample size you have available. Use within-designs when possible, but weigh the benefits of higher power against the downsides of order effects or carryover effects that might be problematic in a within-subject design. Maxwell and Delaney's book (Chapter 11) has a good discussion of this topic.

<!-- You can use this Shiny app to play around with different means, sd's, and correlations, and see the effect of the distribution of the difference scores. -->

<!-- ```{r shiny2, echo=F} -->
<!-- knitr::include_app('http://shiny.ieis.tue.nl/within_between/', height = '4000px') -->
<!-- ``` -->


## Improving the power of your tests

HEre are some things you should immediately implement if you use hypothesis tests, and data collection is costly.

**1) Use directional tests where relevant.** 

Just following statements such as ‘we predict X is larger than Y’ up with a logically consistent test of that claim (e.g., a one-sided t-test) will easily give you an increase of 10% power in any well-designed study. If you feel you need to give effects in both directions a non-zero probability, then at least use lopsided tests.

**2) Use sequential analysis whenever possible.**

It’s like optional stopping, but then without the questionable inflation of the false positive rate. Sequential analyses greatly increase the efficiency of tests [@lakens_performing_2014].

**3) Increase your alpha level.** 

As we wrote in our Justify Your Alpha paper as an argument to not require an alpha level of 0.005: “without (1) increased funding, (2) a reward system that values large-scale collaboration and (3) clear recommendations for how to evaluate research with sample size constraints, lowering the significance threshold could adversely affect the breadth of research questions examined.” If you *have* to make a decision, and the data you can feasibly collect is limited, consider performing a compromise power analysis.

**4) Use within designs where possible.**

Especially when measurements are strongly correlated, this can lead to a substantial increase in power, as explained above.

**5) Remove statistical variation where possible**

The smaller the variation, the larger the standardized effect size (because we are dividing the raw effect by a smaller denominator) and thus the higher the power given the same number of observations. For an overview of different approaches to reduce the variance, see @allison_power_1997. They discuss:

1. Better ways to screen participants for studies where participants need to be screened before participation.
2. Assigning participants unequally to conditions (if the control condition is much cheaper than the experimental condition, for example). 
3. Using multiple measurements to increase measurement reliability (or use well-validated measures, if I may add).
4. Smart use of (preregistered, I’d recommend) covariates.

Another approach they do not mention is to, where possible, collect multiple observations from the same participant. This can also increase power, especially if there is variation at the individual level, and you analyze data with hierarchical models.

**6) Use Bayesian statistics with informed priors.**

Regrettably, almost all approaches to statistical inferences become very limited when the number of observations is small. If you are very confident in your predictions (and your peers agree), incorporating prior information will give you a benefit. For a discussion of the benefits and risks of such an approach, see @van_de_schoot_analyzing_2015.

Now if you care about efficiency, you might already have incorporated all these things. There is no way to further improve the statistical power of your tests, and by all plausible estimates of effects sizes you can expect or the smallest effect size you would be interested in, statistical power is low. Now what should you do?

### What to do if best practices in study design won’t save you?

The first thing to realize is that you should not look at statistics to save you. There are no secret tricks or magical solutions. Highly informative experiments require a large number of observations. So what should we do then? The solutions below are, regrettably, a lot more work than making a small change to the design of your study. But it is about time we start to take them seriously. This is a list of solutions I see – but there is no doubt more we can/should do, so by all means, let me know your suggestions on twitter or in the comments.

**1) Ask for a lot more money in your grant proposals.**

Some grant organizations distribute funds to be awarded as a function of how much money is requested. If you need more money to collect informative data, ask for it. Obviously grants are incredibly difficult to get, but if you ask for money, include a budget that acknowledges that data collection is not as cheap as you hoped some years ago. In my experience, psychologists are often asking for much less money to collect data than other scientists. Increasing the requested funds for participant payment by a factor of 10 is often reasonable, given the requirements of journals to provide a solid sample size justification, and the more realistic effect size estimates that are emerging from preregistered studies.

**2) Improve management.**

If the implicit or explicit goals that you should meet are still the same now as they were 5 years ago, and you did not receive a miraculous increase in money and time to do research, then an update of the evaluation criteria is long overdue. I sincerely hope your manager is capable of this, but some ‘upward management’ might be needed. In the coda of Lakens & Evers (2014) we wrote “All else being equal, a researcher running properly powered studies will clearly contribute more to cumulative science than a researcher running underpowered studies, and if researchers take their science seriously, it should be the former who is rewarded in tenure systems and reward procedures, not the latter.” and “We believe reliable research should be facilitated above all else, and doing so clearly requires an immediate and irrevocable change from current evaluation practices in academia that mainly focus on quantity.” After publishing this paper, and despite the fact I was an ECR on a tenure track, I thought it would be at least principled if I sent this coda to the head of my own department. He replied that the things we wrote made perfect sense, instituted a recommendation to aim for 90% power in studies our department intends to publish, and has since then tried to make sure quality, and not quantity, is used in evaluations within the faculty (as you might have guessed, I am not on the job market, nor do I ever hope to be).

**3) Change what is expected from PhD students.**

When I did my PhD, there was the assumption that you performed enough research in the 4 years you are employed as a full-time researcher to write a thesis with 3 to 5 empirical chapters (with some chapters having multiple studies). These studies were ideally published, but at least publishable. If we consider it important for PhD students to produce multiple publishable scientific articles during their PhD’s, this will greatly limit the types of research they can do. Instead of evaluating PhD students based on their publications, we can see the PhD as a time where researchers learn skills to become an independent researcher, and evaluate them not based on publishable units, but in terms of clearly identifiable skills. I personally doubt data collection is particularly educational after the 20th participant, and I would probably prefer to  hire a post-doc who had well-developed skills in programming, statistics, and who broadly read the literature, then someone who used that time to collect participant 21 to 200. If we make it easier for PhD students to demonstrate their skills level (which would include at least 1 well written article, I personally think) we can evaluate what they have learned in a more sensible manner than now. Currently, difference in the resources PhD students have at their disposal are a huge confound as we try to judge their skill based on their resume. Researchers at rich universities obviously have more resources – it should not be difficult to develop tools that allow us to judge the skills of people where resources are much less of a confound.

**4) Think about the questions we collectively want answered, instead of the questions we can individually answer.**

Our society has some serious issues that psychologists can help address. These questions are incredibly complex. I have long lost faith in the idea that a bottom-up organized scientific discipline that rewards individual scientists will manage to generate reliable and useful knowledge that can help to solve these societal issues. For some of these questions we need well-coordinated research lines where hundreds of scholars work together, pool their resources and skills, and collectively pursuit answers to these important questions. And if we are going to limit ourselves in our research to the questions we can answer in our own small labs, these big societal challenges are not going to be solved. Call me a pessimist. There is a reason we resort to forming unions and organizations that have to goal to collectively coordinate what we do. If you greatly dislike team science, don’t worry – there will always be options to make scientific contributions by yourself. But now, there are almost no ways for scientists who want to pursue huge challenges in large well-organized collectives of hundreds or thousands of scholars (for a recent exception that proves my rule by remaining unfunded: see the Psychological Science Accelerator). If you honestly believe your research question is important enough to be answered, then get together with everyone who also thinks so, and pursue answers collectively. Doing so should, eventually (I know science funders are slow) also be more convincing as you ask for more resources to do the resource (as in point 1).