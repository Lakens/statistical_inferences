# Replication Studies {#sec-replication}

```{r, include = FALSE}
# needed to make the chapter (not visible)
library(metafor)
library(ggplot2)
library(pwr)
```

In 2015 a team of 270 authors published the results of a research project where they replicated 100 studies published in three psychology journals in 2008. They selected the last study of papers that could feasibly be replicated and were selected by collaborators, performed a study with high power to detect the observed effect size, and attempted to design the best possible replication study. staying close to the original study where possible, but deviating where this was deemed necessary. 97 of the 100 studies were interpreted as significant, and given 92% power for the effect sizes observed in the original studies, 89% of the 97 studies should be expected to observe a significant effect if the effects in the original studies were at least as large those observed. Yet, only 35 out of the 97 original studies that were significant replicated, for a replication rate of 36%. This result was a surprise for most researchers, and led to the realization that for whatever reasons it is much more difficult to replicate findings than researchers intuitively thought. This result solidified the idea of a *replication crisis*, a sudden loss of confidence in the reliability of published results, which led to confusion and uncertainty about how scientists worked.

```{r fig-rpp, echo=FALSE}
#| fig-cap: "Results of the Reproducibility Project:Psychology, with replications in green, and non-replications (based on *p* > .05) in red. "
knitr::include_graphics("images/rpp.png")
```

At the same time, the authors of the replication project acknowledged that a single replication of 100 studies is just the starting point of trying to understand why it was so difficult to replicate findings in the literature. They wrote in their conclusion:  

>After this intensive effort to reproduce a sample of published psychological findings, how many of the effects have we established are true? Zero. And how many of the effects have we established are false? Zero. Is this a limitation of the project design? No. It is the reality of doing science, even if it is not appreciated in daily practice. Humans desire certainty, and science infrequently provides it. As much as we might wish it to be otherwise, a single study almost never provides definitive resolution for or against an effect and its explanation. The original studies examined here offered tentative evidence; the replications we conducted offered additional, confirmatory evidence. In some cases, the replications increase confidence in the reliability of the original results; in other cases, the replications suggest that more investigation is needed to establish the validity of the original findings. Scientific progress is a cumulative process of uncertainty reduction that can only succeed if science itself remains the greatest skeptic of its explanatory claims.

A replication study is an experiment where the methods and procedures in a previous study are repeated by collecting new data. Typically, the term **direct replication study** is used when the methods and measures are as similar to the earlier study as possible. In a **conceptual replication study** a researcher intentionally introduces differences with the original study, either because they aim to systematically explore the impact of this change, or because they are not able to use the same methods and procedures. It is important to distinguish replication, where new data is collected, from **reproducibility**, where the same data is used to reproduce the reported results. In reproducibility checks the goal is to examine the presence of errors in the analysis files. Confusingly, the large-scale collaborative research project where 100 studies in psychology were replicated, and which is often considered an important contributor to the **replication crisis**, was called the Reproducibility Project: Psychology [@opensciencecollaboration_estimating_2015]. It should have been called the Replication Project: Psychology. Our apologies.

The goal of direct replication studies is first of all to identify Type 1 or Type 2 errors in the literature. In any original study with an alpha of 5% and a statistical power of 80% there is a probability of making an erroneous claim. Direct replication studies (especially those with low error rates) have the goal to identify these errors in the scientific literature. As statistical test allow researchers to make claims with a certain error rate, for impactful claims it is important to distinguish erroneous from correct claims. This is especially important given that the scientific literature is biased, which increases the probability that a published claim is a Type 1 error. Schmidt [-@schmidt_shall_2009] also notes that claims can be erroneous because they were based on fraudulent data. Although direct replication can not identify fraud, they can point out erroneous claims due to fraud. The second important goal of direct replications is to identify factors in the study that were deemed irrelevant, but were crucial in generating the observed data [@tunc_falsificationist_2023]. For example, researchers might discover that an original and replication study yielded different results because the experimenter in one study treated the participants in a much more friendly manner than the experimenter in the other study. Such details are typically not reported in the method section, as they are deemed irrelevant, but direct replication studies might identify such factors. This already points out replication studies are never identical in the social sciences. They are designed to be similar in all ways that a researcher thinks will matter.

The goal of conceptual replication studies is to examine the generalizability of effects [@sidman_tactics_1960]. Researchers intentionally vary factors in the experiment to examine if this leads to variability in the results. Sometimes this variability is theoretically predicted, and sometimes researchers simply want to see what will happen. I would define a direct replication as a as a study where a researcher has the goal to not introduce variability compared to the original study, while in a conceptual replication variability is introduced intentionally. This distinction means that it is possible that what one researchers aims to be a direct replication is seen by a peer as a conceptual replication. For example, if a researcher sees no reason why an effect tested in Germany would not be identical in The Netherlands, they will consider it a direct replication. A peer might believe there are theoretically relevant differences between the two countries that make this a conceptual replication, as they would interpret the study as intentionally introducing variability by not keeping an important factor constant. The only way to clarify which factors are deemed theoretically relevant is to write a **constraints on generalizability** statement in the discussion where researchers specify which contexts they expect the effect to replicate in, and where variability in the results would not be considered problematic for their original claim [@simons_constraints_2017]. 

It should be clear that the goals of replication studies are rather modest. At the same time, they are essential for a well-functioning empirical science. They provide a tool to identify false positive or false negative results, and can reveal variability across contexts that might falsify theoretical predictions, or lead to the generation of new theories. Being able to systematically replicate and extend a basic effect is one of the most important ways in which scientists develop and test theories. Although there are theoretically other approaches to generating reliable knowledge, such as **triangulation** where the same theory is tested in different but complementary ways, and such approaches are necessary in situations where it is impossible to replicate a study, in practice the vast majority of scientifically established claims are in part due to replicable effects.

Not all researchers agree that their science has inter-subjectively repeatable observations. In what is called the 'crisis in social psychology' Gergen [-@gergen_social_1973] argued social psychology was not a cumulative science: 

> It is the purpose of this paper to argue that social psychology is primarily an historical inquiry. Unlike the natural sciences, it deals with facts that are largely nonrepeatable and which fluctuate markedly over time. Principles of human interaction cannot readily be developed over time because the facts on which they are based do not generally remain stable. Knowledge cannot accumulate in the usual scientific sense because such knowledge does not generally transcend its historical boundaries.

The belief that basic claims in psychology are not repeatable events lead to **social constructivism**. This approach did not become particularly popular, but it is useful to know of its existence. It is a fact that human behavior can change over time. It is also true that many psychological mechanisms have enabled accurate predictions for more than a century, and this is unlikely to change. Still, if you believe that you are studying unrepeatable events, all you have to do is state this, and such observations can be useful for knowledge generation. In this philosophy of psychology researchers give up the aim to build theories upon which generalizable predictions can be made. It is true that the world changes, and we should not expect all direct replications to yield the same result. For example, in the classic 'foot-in-the-door' effect study, Freedman and Fraser [-@freedman_compliance_1966] first called residents in a local community over the phone to ask them to answer some questions (the small request). If participants agreed, they were asked a larger request, which consisted of "five or six men from our staff coming into your home some morning for about 2 hours to enumerate and classify all the household products that you have. They will have to have full freedom in your house to go through the cupboards and storage places." The idea that anyone would nowadays agree to such a request when called by a stranger over the telephone (let along more than 50% as in the original study) seems highly improbable. But this is the role of theory, where researchers show their theories continue to be able to make predictions, even if some aspects of the world change. As De Groot [-@degroot_methodology_1969, p. 89] writes: "If one knows something to be true, he is in a position to predict; where prediction is impossible there is no knowledge".

## Why replication studies are important

Researchers have repeatedly observed over the last half century that replication studies were rarely performed or published. In an editorial in the Journal of Personality and Social Psychology, Greenwald [@greenwald_editorial_1976] writes: “There may be a crisis in personality and social psychology, associated with the difficulty often experienced by researchers in attempting to replicate published work. A precise statement of the magnitude of this problem cannot be made, since most failures to replicate do not receive public report”. A similar concern about the replicability of findings is expressed by Epstein [@epstein_stability_1980, p. 790]: “Not only are experimental findings often difficult to replicate when there are the slightest alterations in conditions, but even attempts at exact replication frequently fail.” Neher [-@neher_probability_1967, p. 262] concludes: “The general adoption of independent replication as a requirement for acceptance of findings in the behavioral sciences will require the efforts of investigators, readers, and publishing editors alike. It seems clear that such a policy is both long overdue and crucial to the development of a sound body of knowledge concerning human behavior.” Lubin [@lubin_replicability_1957] suggests that, where relevant, manuscripts that demonstrate the replicability of findings should receive a higher publication priority. N. C. Smith [-@smith_replication_1970, p. 974] notes how replication studies are neglected: “The review of the literature on replication and cross-validation research has revealed that psychologists in both research "disciplines" have tended to ignore replication research. Thus, one cannot help but wonder what the impact might be if every investigator repeated the study which he believed to be his most significant contribution to the field.” One problem in the past was the difficulty of describing the methods and analyses in sufficient detail to allow others to repeat the study as closely as possible [@mack_need_1951]. For example, Pereboom [-@pereboom_fundamental_1971, p. 442] writes: “Related to the above is the common difficulty of communicating all important details of a psychological experiment to one's audience. […] Investigators attempting to replicate the work of others are painfully aware of these informational gaps.” Open science practices, such as sharing [computationally reproducible](#sec-computationalreproducibility) code and materials, are an important way to solve this problem. 

Many researchers have suggested that performing replication studies should be common practice. Lykken [-@lykken_statistical_1968, p. 159] writes: “Ideally, all experiments would be replicated before publication but this goal is impractical.”, Loevinger [-@loevinger_information_1968, p. 455] makes a similar point: “Most studies should be replicated prior to publication. This recommendation is particularly pertinent in cases where the results are in the predicted direction, but not significant, or barely so, or only by one-tailed tests.” Samelson [-@samelson_watson_1980, p. 623] notes in the specific context of Watson’s ‘Little Albert’ study: “Beyond this apparent failure of internal criticism of the data is another one that is even less debatable: the clear neglect of a cardinal rule of scientific method, that is, replication.” 

The idea that replication is a 'cardinal rule' or 'cornerstone' of the scientific method follows directly from a methodological falsificationist philosophy of science. Popper [-@popper_logic_2002] discusses how we increase our confidence in theories that make predictions that withstand attempts to falsify the theory. To be able to falsify predictions, predictions need to rule out certain observable data patterns. For example, if our theory predicts that people will be faster at naming the colour of words when their meaning matches the colour (e.g., "blue" written in blue instead of "blue" written in red), the observation that people are not faster (or even slower) would falsify our prediction. A problem is that given variability in observed data any possible data pattern will occur, exactly as often as dictated by chance. Sometimes, purely due to random variation, a study could show people are slower at naming the colour of words when their meaning matches the colour - even though our theory is correct. Popper realized this was a problem for his account of falsification, because it means that "probability statements will not be falsifiable". After all, if all possible data patterns have a non-zero probability, even if they are extremely rare, they are not logically ruled out. This would make falsification impossible if we demanded that science works according to perfectly formal logical rules. However, science does not work following any formal system, and yet, as Popper acknowledges, it still works. Therefore, instead of abandoning the idea of falsification, Popper proposes a more pragmatic approach to falsification. He writes: "It is fairly clear that this ‘practical falsification’ can be obtained only through a methodological decision to regard highly improbable events as ruled out — as prohibited." The logical follow-up question is then "Where are we to draw the line? Where does this ‘high improbability’ begin?" Popper argues that even if any low probability event *can* occur, *they are not reproducible at will*. Any single study can reveal any possible effect, but a prediction should be considered falsified if we fail to see "the predictable and reproducible occurrence of systematic deviations". This is why replication is considered a 'cardinal rule' in methodological falsificationism: When observations are probabilistic, only the reproducible occurrence of low probability events can be taken as the falsification of a prediction. A single *p* < 0.05 is not considered sufficient; only if close replication studies repeatedly observe a low probability event does Popper allow us to 'practically falsify' probabilistic predictions. 

## Direct versus conceptual replications

As Schmidt [-@schmidt_shall_2009] writes "There is no such thing as an exact replication." However, it is possible to 1) repeat an experiment where a researcher stays as closely as possible to the original study, 2) repeat an experiment where there it is likely there is some variation in factors that are deemed irrelevant, and 3) knowingly vary aspects of a study design. Popper agrees: "We can never repeat an experiment precisely — all we can do is to keep certain conditions constant, within certain limits." One of the first extensive treatments of replication comes from Sidman [-@sidman_tactics_1960]. He distinguishes direct replications from **systematic replications** (which I refer to here as conceptual replications). Sidman writes: 

> Where direct   helps to establish generality of a phenomenon amongthe members of a species, systematic replication can accomplish this and, at the same time, extend its generality over a wide range of different situations.

If the same result is observed when systematically varying auxiliary assumptions we build confidence in the general nature of the finding, and therefore, in the finding itself. The more robust an effect is to factors that are deemed irrelevant, the less likely it is that the effect is caused by a confound introduced by one of these factors. If a prediction is confirmed across time, locations, in different samples of participants, by different experimenters, and using different measures of the same variable, then the likelihood of a confound underlying all these effects decreases. If conceptual replications are successful they yield more information than a direct replication, as it a conceptual replication generalizes the finding beyond the original context. This benefit is only present if the conceptual replication is succesful, however. Sidman warns: 

> But this procedure is a gamble. If systematic replication fails, the original experiment will still have to be redone, else there is no way of determining whether the failure to replicate stemmed from the introduction of new variables in the second experiment, or whether the control of relevant factors was inadequate in the first one.

Sometimes there is no need to choose between a direct replication and a conceptual replication, as both can be performed. Researchers can perform **replication and extension studies** where an original study is replicated, but additional conditions are added that test a novel hypotheses. Sidman [-@sidman_tactics_1960] refers to this as the **baseline technique** where an original effect is always part of the experimental design, and variations are tested against the baseline effect. Replication and extension studies are one of the best ways to build cumulative knowledge and develop strong scientific theories [@bonett_replicationextension_2012].

A problem with the distinction between 'direct' and 'conceptual' replications is that it is difficult to reach agreement about how similar studies are by an exclusive focus on operational procedures and methods used in the study. A broader definition of a replication study is "is a study for which any outcome would be considered diagnostic evidence about a claim from prior research" [@nosek_what_2020]. This definition places all the weight on whether a study is a replication on theoretical predictions, and as long as a study is theoretically predicted to show support for the same claim, it is a replication. However, this definition seems too broad to be useful in practice, has limited use when theoretical predictions are vague (which regrettably holds for most research lines in psychology), and does not sufficiently acknowledge the importance of specifying falsifiable auxiliairy hypotheses.

The specification of falsifiable auxiliary hypothesis in replication studies lies at the core of the Systematic Replications Framework [@tunc_falsificationist_2023]. The idea is that researchers should specify the auxiliary hypotheses that are assumed to be relevant. In principle the number of auxiliary hypotheses is infinite, but as Uygun-Tunç and Tunç [-@tunc_falsificationist_2023] write: "it is usually assumed that the exact color of the lab walls, the elevation of the lab above the sea level, the exact design of the chairs used by the subjects, the humidity of the room that the study takes place or many other minute details do not significantly influence the study outcomes." These factors are relagated to the *ceteris paribus clause*, meaning that any differences on these factors are not relevant, and for all purposes studies can be treated as 'all equal' regardless of whether the color of the walls differs. At the same time, no replication study is exactly the same as the original study [@schmidt_shall_2009], and some differences in auxiliary hypotheses are meaningfully different. The challenge is to identify which auxiliary hypotheses explain failures to replicate an original study.

A direct replication study that yields no statistically significant effect can have three interpretations [@schmidt_shall_2009; @tunc_falsificationist_2023]. First, it is possible that the replication study yielded a Type 2 error. Second, it is possible that the original study was a Type 1 error. Third, some of the auxiliary assumptions that have been relegated to the *ceteris paribus* clause actually matter more than researchers might have thought. To resolve disagreements between the results of original and replication studies researchers should perform a set of studies that systematically varies those auxiliary hypotheses that are most crucial for a theoretical viewpoint. Resolving inconsistencies in science is an effortful process that can be facilitated by engaging in an **adversarial collaboration**, where two teams join forces to resolve inconsistencies [@mellers_frequency_2001]. Opposing teams can point out the most crucial auxiliary hypotheses to test, and severely test different theories. 

## Analyzing Replication Studies. 

There are multiple ways to statistically analyze a replication study [@anderson_there_2016]. The most straightforward statistical approach is also the least common: Testing whether the effect sizes in the original and replication studies are statistically different from each other. Two effect sizes can be tested against each other using:


$$
Z_{Diff} = \frac{{{\delta}}_{1}{- {\delta}}_{2}}{\sqrt{{V}_{\delta_{1}} + {V}_{\delta_{2}}}}
$$

where the difference between the two Cohen's *d* effect sizes is divided by the standard error of the difference score, based on the square root of the combined variances of the effect sizes [@borenstein_introduction_2009, formula 19.6 and 19.7]. This formula provides a hint why researchers rarely test for differences between effect sizes. The standard error of the difference score is based on the variance in the original and replication study. If the original study has a small sample size, the variance will be large, and a test of the difference between effect sizes can have very low power. 

Let's take a look at an original non-preregistered study which observed an effect size of d = 0.6 in an independent *t*t-test with 30 participants in each group. A preregistered replication study is performed which observed an effect size of 0 with 150 participants in each group. Testing the two effect sizes against each other can be achieved in three ways that are in principle identical (although the exact implementation, suchas computing Cohen's *d* or Hedges' *g*, might yield slightly different *p*-values). The first is to compute the corresponding *p*-value for the Z-test in the formula above. The other two ways are implemented in the `metafor` package and consist of a heterogeneity analysis and a moderator analysis. These two approaches are mathematically identical. In the heterogeneity analysis we test if the variability in effect sizes (even though there are only two) is greater than expected based only on random variation. 

```{r}
d1 <- escalc(n1i = 30, 
             n2i = 30, 
             di = 0.6, 
             measure = "SMD")
d2 <- escalc(n1i = 100, 
             n2i = 100, 
             di = 0.0, 
             measure = "SMD")
metadata <- data.frame(yi = c(d1$yi, d2$yi), 
                       vi = c(d1$vi, d2$vi), 
                       study = c("original", "replication"))

# Test based on heterogeneity analysis
res_h <- rma(yi, 
             vi, 
             data = metadata, 
             method = "FE")
res_h

# The moderator test would be: rma(yi, vi, mods = ~study, method = "FE", data = metadata, digits = 3)
``` 
The result yields *p*-value of `r round(res_h$QEp, 3)`, which is statistically significant, and therefore allows us to statistically conclude that the two effect sizes differ from each other. We can also perform this test a s amoderator analysis. 

```{r}
rma(yi, 
    vi, 
    mods = ~study, 
    method = "FE", 
    data = metadata, 
    digits = 3)
```

This result yields the same *p*-value of `r round(res_h$QMp, 3)`. This same test was recently repackaged by Spence and Stanley [-@spence_tempered_2024] as a prediction interval, but this approach is just a test of the difference between effect sizes.

It is worth pointing out that the difference between the effect sizes was large, the replication study had a much larger sample size than the original study, but the differences was only just statistically significant. As we see in @fig-rep-1 the original study had large uncertainty, which as explained above is part of the variance of the estimate of the difference between effect sizes. If the replication study had yielded an effect size that was even slightly in the positive direction (which should happen 50% of the time if the true effect size is 0) the difference would not longer have been statistically significant. In general, power is low when original studies have small sample sizes. Despite this limitation, directly testing the difference between effect sizes is the most intuitive and coherent approach to claiming that a study failed to replicate. 

```{r fig-rep-1, echo=FALSE, fig.width  = 8, fig.height = 8}
#| fig-cap: "Forest plot for an original study (N = 60, d = 0.6) and a  replication study (N = 200, d = 0)."

metafor::forest(res_h,
#                efac=c(0,3),
                lty=c(1,1,0),
                cex=0.8,
                shade = TRUE,
                rowadj=0,
                psize = 1,
                colout = "#333333",
                xlim = c(-2, 2.5),
                alim = c(-0.5, 1.5),
                textpos = c(-2, 2.5),
                at = c(-0.5, 0, 0.5, 1, 1.5),
                digits = c(3, 1),
                xlab = "Cohen's d",
#                refline = c(0, 0.2, 0.5),
                header = TRUE,
                slab = c("Original", "Replication")
)


```

Although a statistical difference between effect sizes is one coherent approach to decide whether a study has been replicated, researchers are sometimes interested in a different question: Was there a significant result in the replication study? In this approach to analyzing replication studies there is no direct comparison with the effect observed in the original study. The question is therefore not so much 'is the original effect replicated?' but 'if we repeat the original study is a statistically significant effect observed?'. In other words, we are not testing whether an effect has been replicated, but whether a predicted effect has been observed. Another way of saying this is that we are not asking whether the observed effect replicated, but whether the original ordinal claim of the presence of an non-zero effect is replicated. In the example in @fig-rep-1 the replication study has an effect size of 0, so there is no statistically significant effect. 

Let's take a step back, and consider which statistical test best reflects the question 'did this study replicate'. On the one hand it seems reasonable to consider an effect not replicated if the difference in effect sizes is statistically significant. However, this can mean that a replication study that yields a statistically significant effect that is much smaller than the original study to lead to the claim that the original study did not replicate. If we consider a study a replication if the effect is both statistically different from 0 (i.e., *p* < .05 in a traditional significance test), and the difference in effect sizes is not statistically different from 0 (i.e., *p* > .05 for a test of heterogeneity in a meta-analysis of both effect sizes) we should perform both tests, and only consider a finding replicated if both conditions are met. Logically, a finding should then be considered as a non-replication if the opposite is true (i.e., *p* > .05 for significance test of the replication study, and *p* < .05 for the test of heterogeneity). 

However, due to the low power of the test for a difference between the effect sizes in the original and replication study, it is not easy to meet the bar of an informative result. In practice, many replication studies that do not yield a statistically significant result will also not show a statistically significant difference as part of the test of heterogeneity. In a preprint (that we never bothered to resubmit after we received positive reviews) Richard Morey and I [@morey_why_2016] summarized this problem in our title "Why most of psychology is statistically unfalsiﬁable". If we want to have an informative test for replication following the criteria outlined above, most tests will be uninformative, and lead to inconclusive results. 

As we can never increase the sample size of original studies, and we also can not do science if we can not conclude claims in the literature can not be replicated, we will need to be pragmatic and develop alternative statistical procedures to claim an effect can not be replicated. As the large uncertainty in the original effect sizes are the main cause of the problem, a logical solution is to move to statistical approaches where this uncertainty is not part of the test. An intuitive approach would be to test if the effect size in the replication study is statistically smaller than the effect in the original study, for example in the example above by testing whether the 95% confidence interval around the effect size in the replication study excludes the effect in the original study (i.e., 0.6, or 0.592 when converted to Hedges'*g*). In essence this is a two-sided test against the original effect size. This is the equivalent of changing a independent *t*-test where two groups are compared to a one-sample *t*-test where the 95% confidence interval in one group is compared against the observed mean in the other group, ignoring uncertainty in this other group. We do not accept this when we compare to means, and we should not accept this when we compare two standardized mean differences. The problem is that this test will too easily reject the null-hypothesis that the two effects are equal, because it ignores the variability in one of the two effect sizes. 

An alternative approach is to test against a more conservative effect size, and consider an original finding not replicated if this conservative effect size can be rejected. In practice researchers are primarily interested in the question whether the effect size in the replication study is statistically smaller that the effect size in the original study. This questions is answered by an *inferiority test*, which is statistically significant if the 90% confidence interval around the effect size in the replication study does not contain the conservative effect size estimate. One implementation of an inferiority test is the *small telescopes* approach [@simonsohn_small_2015]. In the small telescopes approach a test is performed against the effect the original study had 33% power to detect. In the example in @fig-rep-1 the original study had 33% power to detect and effect of *d* = 0.4. 

```{r}
pwr::pwr.t.test(
  n = 30,
  sig.level = 0.05,
  power = 0.33,
  type = "two.sample",
  alternative = "two.sided"
)
```

The 95% confidence interval of the replication study shows effects larger than 0.277 can be rejected, so a 90% confidence interval would be able to reject even smaller effect sizes. Therefore, the small telescopes approach would allow researchers to conclude that the original effect could not be replicated. The small telescopes is popular especially whenc replicating small original studies, as the smaller the sample size, the larger the effect size the study had 33% power te detect, and this the easier it is to reject in a replication study. This is the opposite to what happens if we want to test the two effect sizes against each other, where smaller original studies reduce the power of the test. 

A fair point of criticism of the small telescopes approach is that the convention to test against an effect the original study had 33% power to detect is completely arbitrary. Ideally researchers would specify the *smallest effect size of interest* and perform an inferiority test against the smallest effect that would actually matter. Specifying a smallest effect size of interest is difficult, however. In some cases researchers involved in the original study might specify the smallest effect they care about. For example, in a multi-lab replication study of the action-sentence compatibility effect the researchers who published the original study stated that they considered an effect of 50 milliseconds or less theoretically negligible [@morey_preregistered_2021]. The large sample size allowed the authors to conclusively reject the presence of effects as large or larger than were considered theoretically negligible. 

Another approach that has been proposed is to combine the effect size of the original study and the replication study in a meta-analysis, and test whether the meta-analytic effect is statistically different from zero. This approach is interesting if there is no bias, but when there is bias the weakness of this approach outweighs its usefulness. As publication bias and selection bias [inflated effect sizes](#sec-effectinflated) the meta-analytic effect size will also be inflated. This in turn inflates the probability that the meta-analytic effect size is statistically significant, when there is no effect.

## Replication studies or lower alpha levels?

Statistically minded researchers sometimes remark that there is no difference in the Type 1 error probability when a single study with a lower alpha level is used to test a hypothesis (say 0.05 x 0.05 = 0.0025) compared to when the same hypothesis is tested in two studies, each at an alpha level of 0.05. This is correct, but in practice it is not possible to replace the function of replication studies to decrease the Type 1 error probability with directly lowering alpha levels in single studies. Lowering the alpha level to a desired Type 1 error probability would require that scientists 1) can perfectly predict how important a claim will be in the future, and 2) are able to reach consensus on the Type 1 error rate they collectively find acceptable for each claim. Neither is true in practice. First, it is possible that a claim becomes increasingly important in a research area, for example because a large number of follow-up studies cite the study and assume the claim is true. This increase in importance might convince the entire scientific community that it is worthwhile if the Type 1 error probability is reduced by performing a replication study, as the importance of the original claim has made a Type 1 error more costly [@isager_deciding_2023]. For claims no one builds on, no one will care about reducing the Type 1 error probability. How important a claim will be for follow-up research can not be predicted in advance. 

The second reason to reduce the probability of Type 1 errors through replications is that there are individual differences between researchers in when they believe a finding is satisfactorily demonstrated. As Popper writes: "Every test of a theory, whether resulting in its corroboration or falsification, must stop at some basic statement or other which we *decide to accept*." "This procedure has no natural end. Thus if the test is to lead us anywhere, nothing remains but to stop at some point or other and say that we are satisfied, for the time being." Different researchers will have different thresholds for how satisfied they are we are with the error probability associated with a claim. Some researchers are happy to accept a claim when the Type 1 error probability is 10%, while more skeptical researchers would like to see it reduce by 0.1% before building on a finding. Using a lower alpha level for a single study does not give scientists the flexibility to lower Type 1 error probabilities as the need arises, while performing replication studies does. It is a good idea to think carefully about the desired Type 1 error rate for studies, and if Type 1 errors are costly, researchers can decide the use of a lower alpha level than the default level of 0.05 is justified [@maier_justify_2022]. But replication studies will remain necessary in practice to further reduce the probability of a Type 1 error as claims increase in importance, or for researchers who are more skeptical about a claim. 

There is another reason why it is more beneficial to use independent replication studies to reduce the error rate, compared to performing studies with a lower alpha level. If others are able to independently replicate the same effect, it becomes less likely that the original finding was due to systematic error. Systematic errors do not average out to zero in the long run (as random errors do). There can be many sources of systematic error in a study. One source is the measures that are used. For example, if a researcher uses a weighing scale that is limited to a maximum weight of 150 kilo, they might fail to identify an increase in weight, while different researchers who use a weighting scale with a higher maximum weight will identify the difference. An experimenter might be another source of systematic error, if an observed effect is not due to a manipulation, but due to the way the experimenter treats participants in different conditions. Other experimenters who repeat the manipulation, but do not show the same experimenter bias, would observe different results. 

When a researcher repeats their own experiment this is referred to as a **self-replication**, while when in an **independent replication** other researchers repeat the experiment. As explained above, self-replication can reduce the Type 1 error rate of a claim, while independent replication can in addition reduce the probability of a claim being caused by a systematic error. Both self-replication and independent replication is useful in science. For example, research collaborations such as the Large Hadron Collider at CERN prefer to not only replicate studies with the same detector, but also replicate studies across different detectors. The Large Hadron Collider has four detectors (ATLAS, CMS, ALICE, and LHCb). Experiments can be self-replicated in the same detector by collecting more data (referred to by physicists as a 'replica'), but they can also be replicated in a different detector. As Junk and Lyons [-@junk_reproducibility_2020] note, in self-replications: "The statistical variations are expected to be different in the replica and the original, but the sources of systematic errors are expected to be unchanged." One way to examine systematic errors is to perform the same experiment in different detectors. The detectors at CERN are not exact copies of each other, and by performing studies in two different detectors, the research collaboration increases its confidence in the reliability of the conclusions if a replication study yields the same observations, despite minor differences in the experimental set-up. 

The value of an independent replication is not only the reduction in the probability of a Type 1 error, but at the same time the reduction in concerns about systematic error influencing the conclusion [@neher_probability_1967]. As already highlighted by Mack [-@mack_need_1951]: "Indeed, the introduction of different techniques gives the replication the additional advantage of serving as a check on the validity of the original research." Similarly, Lubin [-@lubin_replicability_1957] notes: "Our confidence in a study will be a positive monotonic function of the extent to which replication designs are used which vary these supposedly irrelevant factors." To conclude, although both self-replications (a replication by the same researchers) and independent replication (a replication by different researchers) reduce the probability of a false positive claim, independent replications have the added benefit of testing the generalizability of the findings across factors that are deemed irrelevant to observe the effect. 

From a purely statistical level is it interesting to ask whether it is less costly in terms of the required sample size to perform a single study at an alpha level of 0.0025, or two studies at an alpha level of 0.05. If the sample size required to achieve a desired power is much lower for a single test at an alpha of 0.0025 this would be a reason to perform larger single studies to achieve a low Type 1 error rate instead of performing two smaller studies at an alpha of 0.05. For an independent *t*-test the more efficient approach depends on the power of the test and whether the test is two-sided or one-sided. In @fig-alphalevels1 a ratio of 2 means the sample size required to reach 70% to 95% power for an alpha level of 0.0025 is exactly twice as large as two single studies with an alpha level of 0.05, and the two approaches would be equally efficient for a two-sided independent *t*-test. We see the ratio is below 2 for studies with high power. For example, the total sample size required to detect an effect of d = 0.5 with an alpha level of 0.0025 and with 80% power is 244. With an alpha of 0.05 the total required sample size is 128, which makes the ratio 244/128 = 1.91, and the number of observations saved is (2*128)-244 = 12. 

```{r fig-alphalevels1, echo=FALSE, fig.width  = 8, fig.height = 4}
#| fig-cap: "Ratio of the sample size required to run one study at al alpha of 0.0025 or two studies at an alpha of 0.05 for a two-sided independent *t*-test."

# set up vector of effect sizes
es <- seq(0.1, 3, length = 1000)
# specify power for test

# calculate ns (sample size for xx% power in two-sided test)
n_70_05 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.7,
    sig.level = 0.05,
    type = "two.sample",
    alternative = "two.sided"
  )$n
})

# calculate ns (sample size for xx% power in one-sided test)
n_70_0025 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.7,
    sig.level = 0.0025,
    type = "two.sample",
    alternative = "two.sided"
  )$n
})

# calculate ns (sample size for xx% power in two-sided test)
n_80_05 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.8,
    sig.level = 0.05,
    type = "two.sample",
    alternative = "two.sided"
  )$n
})

# calculate ns (sample size for xx% power in one-sided test)
n_80_0025 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.8,
    sig.level = 0.0025,
    type = "two.sample",
    alternative = "two.sided"
  )$n
})

# calculate ns (sample size for xx% power in two-sided test)
n_90_05 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.90,
    sig.level = 0.05,
    type = "two.sample",
    alternative = "two.sided"
  )$n
})

# calculate ns (sample size for xx% power in one-sided test)
n_90_0025 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.90,
    sig.level = 0.0025,
    type = "two.sample",
    alternative = "two.sided"
  )$n
})

# calculate ns (sample size for xx% power in two-sided test)
n_95_05 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.95,
    sig.level = 0.05,
    type = "two.sample",
    alternative = "two.sided"
  )$n
})

# calculate ns (sample size for xx% power in one-sided test)
n_95_0025 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.95,
    sig.level = 0.0025,
    type = "two.sample",
    alternative = "two.sided"
  )$n
})


ratio <- c(n_70_0025/(n_70_05*2), n_80_0025/(n_80_05*2), n_90_0025/(n_90_05*2) , n_95_0025/(n_95_05*2))

df_1_vs_2 <- data.frame(
  effect_size = c(es, es, es, es),
  ratio = ratio,
  n_05 = c(n_70_05, n_80_05, n_95_05, n_95_05),
  n_0025 = c(n_70_0025, n_80_0025, n_90_0025, n_95_0025),
  power = c(rep("70", 1000), rep("80", 1000), rep("90", 1000), rep("95", 1000))
)

ggplot(df_1_vs_2, aes(x = effect_size, y = ratio, color = power)) +
  geom_line(linewidth = 1.5) +
  labs(x = "Effect size (d)", y = "Ratio 0.05 to 0.0025") +
  theme(
    legend.position = "bottom", legend.direction = "horizontal",
    legend.title = element_blank()
  ) +
  theme_bw() +
  geom_hline(yintercept = 1, colour = "grey", linetype = 2) +
  scale_color_manual(values = c("black", "#E69F00", "#56B4E9", "#009E73")) +
  theme(plot.background = element_rect(fill = "white"))  +
  theme(panel.background = element_rect(fill = "white"))

```

However, there are strong arguments in favor of [directional tests](#sec-onesided) in general, and especially for replication studies. When we examine the same scenario for a one-sided test the ratios change, and one larger study at 0.0025 is slightly more efficient for smaller effect sizes when the desired statistical power is 90%, but for 80% it is more efficient to perform two tests at 0.05. For example, the total sample size required to detect an effect of d = 0.5 with an alpha level of 0.0025 and with 80% power is 218 With an alpha of 0.05 the total required sample size is 102, which makes the ratio 218/102 = 2.14, and the number of observations saved (now when performing two studies) is (2*102)-218 = 14. 

```{r fig-alphalevels2, echo=FALSE, fig.width  = 8, fig.height = 4}
#| fig-cap: "Ratio of the sample size required to run one study at al alpha of 0.0025 or two studies at an alpha of 0.05 for a one-sided independent *t*-test."

# one-sided
# set up vector of effect sizes
es <- seq(0.1, 3, length = 1000)
# specify power for test

# calculate ns (sample size for xx% power in two-sided test)
n_70_05 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.7,
    sig.level = 0.05,
    type = "two.sample",
    alternative = "greater"
  )$n
})

# calculate ns (sample size for xx% power in one-sided test)
n_70_0025 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.7,
    sig.level = 0.0025,
    type = "two.sample",
    alternative = "greater"
  )$n
})

# calculate ns (sample size for xx% power in two-sided test)
n_80_05 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.8,
    sig.level = 0.05,
    type = "two.sample",
    alternative = "greater"
  )$n
})

# calculate ns (sample size for xx% power in one-sided test)
n_80_0025 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.8,
    sig.level = 0.0025,
    type = "two.sample",
    alternative = "greater"
  )$n
})

# calculate ns (sample size for xx% power in two-sided test)
n_90_05 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.90,
    sig.level = 0.05,
    type = "two.sample",
    alternative = "greater"
  )$n
})

# calculate ns (sample size for xx% power in one-sided test)
n_90_0025 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.90,
    sig.level = 0.0025,
    type = "two.sample",
    alternative = "greater"
  )$n
})

# calculate ns (sample size for xx% power in two-sided test)
n_95_05 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.95,
    sig.level = 0.05,
    type = "two.sample",
    alternative = "greater"
  )$n
})

# calculate ns (sample size for xx% power in one-sided test)
n_95_0025 <- sapply(es, function(x) {
  pwr.t.test(
    d = x,
    power = 0.95,
    sig.level = 0.0025,
    type = "two.sample",
    alternative = "greater"
  )$n
})


ratio <- c(n_70_0025/(n_70_05*2), n_80_0025/(n_80_05*2), n_90_0025/(n_90_05*2) , n_95_0025/(n_95_05*2))

df_1_vs_2 <- data.frame(
  effect_size = c(es, es, es, es),
  ratio = ratio,
  n_05 = c(n_70_05, n_80_05, n_95_05, n_95_05),
  n_0025 = c(n_70_0025, n_80_0025, n_90_0025, n_95_0025),
  power = c(rep("70", 1000), rep("80", 1000), rep("90", 1000), rep("95", 1000))
)

ggplot(df_1_vs_2, aes(x = effect_size, y = ratio, color = power)) +
  geom_line(linewidth = 1.5) +
  labs(x = "Effect size (d)", y = "Ratio 0.05 to 0.0025") +
  theme(
    legend.position = "bottom", legend.direction = "horizontal",
    legend.title = element_blank()
  ) +
  theme_bw() +
  geom_hline(yintercept = 1, colour = "grey", linetype = 2) +
  scale_color_manual(values = c("black", "#E69F00", "#56B4E9", "#009E73")) +
  theme(plot.background = element_rect(fill = "white"))  +
  theme(panel.background = element_rect(fill = "white"))
```
The statistical power in these calculations is identical for both scenarios, but one need to reflect on how the two studies at and alpha of 0.05 will be analyzed. If the two studies are direct replications, a fixed effect meta-analysis of both effect sizes has the same power as a single study with twice the sample size. Altogether, the choice of whether to perform two studies at an alpha of 0.05 or a single study at 0.0025 depends on the directionality of the test and the desired power. There is not clear-cut advantage of a single study at an alpha level of 0.0025, and given the non-statistical benefits of an independent replication study, and the fact that replication studies should arguably always be directional tests, a case can be made to prefer the two study approach. This does require that scientists collaborate, and that all studies are shared regardless of the outcome (e.g., as Registered Reports). WE SHOULD DO DIRECTIONAL TESTS FOR REPLICATION MAKE RATIO OF 1

<!-- ## Why Prediction Markets or Machine Learning can not Replace Replications -->

<!-- As explained above, just as a single alpha level attached to a claim is not sufficient, as we need to be able to lower the Type 1 error probability as needed, so will other approaches to quantify the probability that a finding is not replicable not be able to replace replication studies if they can not be lowered as needed. The SCORE project examined 2 approaches: Machine Learning, and Predictions  -->


## When replication studies yield conflicting results

When an original study yielded a significant result, but a replication studies yields a non-significant result, there are three possible causes. First, the result in the replication study can be a Type 2 error. Both studies examined a true effect, and the difference in the test result is due to random variation. The probability of a Type 2 error can be reduced by performing an a-priori power analysis, or even better, by performing an [equivalence test](#sec-equivalencetest) against a smallest effect size of interest with high statistical power, but there is always a probability that the result in a replication study is a false negative. Second, the original study can be a Type 1 error, and there is no true effect in both studies. Third, there is some difference between both studies that moderates the effect. There was a true effect in the original study, but there is no true effect in the replication study.

Some researchers strongly believe failures to replicate published findings can be explained by the presence of hitherto unidentified, or 'hidden', moderators [@stroebe_alleged_2014]. There has been at least one example of researchers who were able to provide modest support for the idea that a previous failure to replicate a finding was due to how personally relevant a message in the study was [@luttrell_replicating_2017]. It is difficult to reliably identify moderator variables that explain failures to replicate published findings, but easy to raise them as an explanation when replication studies do not observe the same effect as the original study. Especially in the social sciences it is easy to point to moderators that are practically impossible to test, such as the fact that society has changed over time, or that effects that work in one culture might not replicate in different cultures. This is an age-old problem, already identified by Galileo in [The Assayer](https://web.archive.org/web/https://web.stanford.edu/~jsabol/certainty/readings/Galileo-Assayer.pdf), one of the first books on the scientific method. In this book, Galileo discusses the claim that Babylonians cooked eggs by whirling them in a sling, which is impossible to replicate, and writes: 

>‘If we do not achieve an effect which others formerly achieved, it must be that we lack something in our operation which was the cause of this effect succeeding, and if we lack one thing only, then this alone can be the true cause. Now we do not lack eggs, or slings, or sturdy fellows to whirl them, and still they do not cook, but rather cool down faster if hot. And since we lack nothing except being Babylonians, then being Babylonian is the cause of the egg hardening.’

At the same time, some failures to replicate are due to a difference in auxiliary hypotheses. In the most interesting study examining whether failures to replicate are due to differences in auxiliary assumptions, Ebersole and colleagues [-@ebersole_many_2020] performed additional replication studies for 10 studies that were replicated in the Reproducibility Project: Psychology (RP:P, @opensciencecollaboration_estimating_2015). For each of these studies authors of the original study had raised concerns about the replication study performed as part of the RP:P. For example, authors raised the concern that the replication study included participants who had taken prior psychology or economics courses or participated in prior psychology studies, and the authors predicted the effects should be larger in 'naive' participants. Other authors pointed out the possibility that the stimuli were not sufficiently pilot tested, the fact that data was collected in a different country, the technical specifications of the computer set-up, or differences in the materials or stimuli. All these concerns involve predictions about the effects of auxiliary hypotheses, and a team of 172 researchers collaborated with original authors to examine these auxiliary hypotheses in Many Labs 5 [@ebersole_many_2020]. What makes this project especially interesting is large replication studies were performed both of the RP:P version of the study, as of the revised protocol that addressed the concerns raised by the researchers. 

The results of this project provide a nice illustration of how difficult it is to predict whether findings will replicate, until you try to replicate them [@miller_what_2009]. Two of the studies that did not replicate in the RP:P also did not replicate when a larger new sample was collected, but did replicate with the revised protocol. However, these replication effect sizes were arguably trivially small (Albarracin et al., Study 5 and Shnabel & Nadler in @fig-manylabs5). A third study (Van Dijk et al) showed a similar pattern but just failed to show a significant effect in the revised protocol. A fourth study (Albarracin et al., Study 7) was just significant in the original study, and the RP:P study found a very similar result which was just non-significant. A meta-analysis pooling these two studies would have yielded a significant meta-analytic effect. And yet, surprisingly, both the replication of the RP:P and the revised protocol based on feedback by the original authors yielded clear null results. A fifth study (Crosby et al) found the same pattern in the original and RP:P study as the second study. Neither the much larger RP:P replication, nor the replication based on the revised protocol yielded a significant result. And yet, the pattern of effect sizes is close to identical across all studies, and a meta-analysis across all studies reveals a small but statistically significant effect. In total six of the studies can clearly be regarded as a non-replication where the original authors' concerns did not matter, as only the original study showed a significant effect, and none of the replication studies yielded a significant result. Note that we can not conclude that the concerns the authors raised in the other four studies mattered. Despite the large sample sizes, only one statistical difference between the RP:P protocol and the revised protocol was observed (Payne et al.), and here the changes suggested by the authors led to an effect sizes that was even more strongly contrasted away from the original study by the authors if we test the meta-analytic effect sizes against each other.  

```{r}
# Payne, Burkley, & Stokes (2008)
# Note that the CI in the figure is wide because there is considerable variability across the sites where data was collected, especially for the revised protocol.

r1 <- escalc(ni = 545, 
             ri = 0.05, 
             measure = "ZCOR")
r2 <- escalc(ni = 558, 
             ri = -0.16, 
             measure = "ZCOR")
metadata <- data.frame(yi = c(r1$yi, r2$yi), 
                       vi = c(r1$vi, r2$vi), 
                       study = c("original", "replication"))

# Test based on heterogeneity analysis
res_h <- rma(yi, 
             vi, 
             data = metadata, 
             method = "FE")
res_h
```
```{r fig-manylabs5, echo=FALSE}
#| fig-cap: "Forest plot for the original study, RP:P replication study, the larger replication of the RP:P replication study, and the revised protocol based on author feedback, and a meta-analysis of all data, for all 10 studies in Many Labs 5"
knitr::include_graphics("images/manylabs5.png")
```

The other difference were not statistically significant, but in one other study the effect size was even further away from the original effect size after the changes to the protocol, and in the other two studies the effect size was more similar as to the original study. Altogether, it seems authors who raise concerns about replication studies do not have a high success rate in predicting which auxiliary hypotheses influence the observed effect size. This is a very important conclusion of Many Labs 5. 

## Why are replication studies so rare?

"It is difficult to deny that there is more thrill, and usually more glory, involved in blazing a new trail than in checking the pioneer's work" [@mack_need_1951]. Throughout history, researchers have pointed out that the reward structures value novel research over replication studies [@koole_rewarding_2012; @fishman_american_1982]. Performing replication studies is a social dilemma: It is good for everyone if scientists perform replication studies, but it is better for an individual scientist to perform a novel study than a replication study. Exact numbers of how many replication studies are performed are difficult to get, as there is no complete database that keeps track of all replication studies (but see [Curate Science](https://curatescience.org/), [Replication WIKI](https://replication.uni-goettingen.de/wiki/index.php/Main_Page) or the [Replication Database](https://metaanalyses.shinyapps.io/replicationdatabase/). 

Although the reward structures have remained the same, there are some positive developments. At the start of the replication crisis failed replications of Bem’s pre-cognition study were desk-rejected by the editor of JPSP, Eliot Smith, who stated “This journal does not publish replication studies, whether successful or unsuccessful” and “We don’t want to be the Journal of Bem Replication” [@aldhous_journal_2011]. This led to public outcry, and numerous journals have started to explicitly state they accept replication studies. An increasing number of studies are accepting Registered Report publications, which can also be replication studies, and Peer Community Inn: Registered Reports initiative is publishing replication studies. The APA has created specific [reporting guidelines for replication studies](https://apastyle.apa.org/jars/quant-table-6.pdf). Some science funders have developed [grants for replication research](https://www.nature.com/articles/nature.2016.20287). At the same time, replication studies are still rewarded less than novel work, which means researchers who want to build a career are still pushed towards novel research instead of replication studies. So despite positive developments, in many disciplines there is still some way to go before replication studies become a normal aspect of scientific research. 
