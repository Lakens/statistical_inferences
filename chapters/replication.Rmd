

A replication study is an experiment where the methods and procedures in a previous study are repeated by collecting new data. Typically, the term *replication study* is used when the methods and measures are as similar to the earlier study as possible. In a *conceptual replication study* a researcher intentionally introduces differences with the original study, either because they aim to systematically explore the impact of this change, or because they are not able to use the same methods and procedures. It is important to distinguish replication, where new data is collected, from *reproducibility*, where the same data is used to reproduce the reported results. In reproducibility checks the goal is to examine the presence of errors in the analysis files. Confusingly, the large-scale collaborative research project where 100 studies in psychology were replicated, and which is often considered an important contributor to the *replication crisis*, was called the Reproducibility Project: Psychology [@opensciencecollaboration_estimating_2015]. It should have been called the Replication Project: Psychology. Our apologies.

Researchers have repeatedly observed over the last half century that replication studies were rarely performed or published. In an editorial in the Journal of Personality and Social Psychology, Greenwald [@greenwald_editorial_1976] writes: “There may be a crisis in personality and social psychology, associated with the difficulty often experienced by researchers in attempting to replicate published work. A precise statement of the magnitude of this problem cannot be made, since most failures to replicate do not receive public report”. A similar concern about the replicability of findings is expressed by Epstein [@epstein_stability_1980, p. 790]: “Not only are experimental findings often difficult to replicate when there are the slightest alterations in conditions, but even attempts at exact replication frequently fail.” Neher [-@neher_probability_1967, p. 262] concludes: “The general adoption of independent replication as a requirement for acceptance of findings in the behavioral sciences will require the efforts of investigators, readers, and publishing editors alike. It seems clear that such a policy is both long overdue and crucial to the development of a sound body of knowledge concerning human behavior.” Lubin [@lubin_replicability_1957] suggests that, where relevant, manuscripts that demonstrate the replicability of findings should receive a higher publication priority. N. C. Smith [-@smith_replication_1970, p. 974] notes how replication studies are neglected: “The review of the literature on replication and cross-validation research has revealed that psychologists in both research "disciplines" have tended to ignore replication research. Thus, one cannot help but wonder what the impact might be if every investigator repeated the study which he believed to be his most significant contribution to the field.” One problem in the past was the difficulty of describing the methods and analyses in sufficient detail to allow others to repeat the study as closely as possible [@mack_need_1951]. For example, Pereboom [-@pereboom_fundamental_1971, p. 442] writes: “Related to the above is the common difficulty of communicating all important details of a psychological experiment to one's audience. […] Investigators attempting to replicate the work of others are painfully aware of these informational gaps.” Open science practices, such as sharing [computationally reproducible](#sec-computationalreproducibility) code and materials, are an important way to solve this problem. 

Many researchers have suggested that performing replication studies should be common practice. Lykken [-@lykken_statistical_1968, p. 159] writes: “Ideally, all experiments would be replicated before publication but this goal is impractical.”, Loevinger [-@loevinger_information_1968, p. 455] makes a similar point: “Most studies should be replicated prior to publication. This recommendation is particularly pertinent in cases where the results are in the predicted direction, but not significant, or barely so, or only by one-tailed tests.” Samelson [-@samelson_watson_1980, p. 623] notes in the specific context of Watson’s ‘Little Albert’ study: “Beyond this apparent failure of internal criticism of the data is another one that is even less debatable: the clear neglect of a cardinal rule of scientific method, that is, replication.” 

The idea that replication is a 'cardinal rule' or 'cornerstone' of the scientific method follows directly from a methodological falsificationist philosophy of science. Popper [-@popper_logic_1959] discusses how we increase our confidence in theories that make predictions that withstand attempts to falsify the theory. To be able to falsify predictions, predictions need to rule out certain observable data patterns. For example, if our theory predicts that people will be faster at naming the colour of words when their meaning matches the colour (e.g., "blue" written in blue instead of "blue" written in red), the observation that people are not faster (or even slower) would falsify our prediction. A problem is that given variability in observed data any possible data pattern will occur, exactly as often as dictated by chance. Sometimes, purely due to random variation, a study could show people are slower at naming the colour of words when their meaning matches the colour - even though our theory is correct. Popper realized this was a problem for his account of falsification, because it means that "probability statements will not be falsifiable". After all, if all possible data patterns have a non-zero probability, even if they are extremely rare, they are not logically ruled out. This would make falsification impossible if we demanded that science works according to perfectly formal logical rules. However, science does not work following any formal system, and yet, as Popper acknowledges, it still works. Therefore, instead of abandoning the idea of falsification, Popper proposes a more pragmatic approach to falsification. He writes: "It is fairly clear that this ‘practical falsification’ can be obtained only through a methodological decision to regard highly improbable events as ruled out — as prohibited." The logical follow-up question is then "Where are we to draw the line? Where does this ‘high improbability’ begin?" Popper argues that even if any low probability event *can* occur, *they are not reproducible at will*. Any single study can reveal any possible effect, but a prediction should be considered falsified if we fail to see "the predictable and reproducible occurrence of systematic deviations". This is why replication is considered a 'cardinal rule' in methodological falsificationism: When observations are probabilistic, only the reproducible occurrence of low probability events can be taken as the falsification of a prediction. A single *p* < 0.05 is not considered sufficient; only if close replication studies repeatedly observe a low probability event does Popper allow us to 'practically falsify' probabilistic predictions. 

# Direct versus conceptual replications

One of the first extensive treatments of replication comes from Sidman [-@sidman_tactics_1960]. He distinguishes direct replications from *systematic replications* (which I refer to here as conceptual replications). Sidman writes: 

> Where direct replication helps to establish generality of a phenomenon among
the members of a species, systematic replication can accomplish this and, at the same time, extend its generality over a wide range of different situations.

If the same result is observed when systematically varying auxiliary assumptions we build confidence in the general nature of the finding, and therefore, in the finding itself. The more robust an effect is to factors that are deemed irrelevant, the less likely it is that the effect is caused by a confound introduced by one of these factors. If a prediction is confirmed across time, locations, in different samples of participants, by different experimenters, and using different measures of the same variable, then the likelihood of a confound underlying all these effects decreases. If conceptual replications are successful they yield more information than a direct replication, as it a conceptual replication generalizes the finding beyond the original context. This benefit is only present if the conceptual replication is succesful, however. Sidman warns: 

> But this procedure is a gamble. If systematic replication fails, the original experiment will still have to be redone, else there is no way of determining whether the failure to replicate stemmed from the introduction of new variables in the second experiment, or whether the control of relevant factors was inadequate in the first
one.

Sometimes there is no need to choose between a direct replication and a conceptual replication, as both can be performed. Researchers can perform **replication and extension studies** where an original study is replicated, but additional conditions are added that test a novel hypotheses. Sidman [-@sidman_tactics_1960] refers to this as the *baseline technique* where an original effect is always part of the experimental design, and variations are tested against the baseline effect. Replication and extension studies are one of the best ways to build cumulative knowledge and develop strong scientific theories [@bonett_replication-extension_2012].



## Replication Studies or Lower Alpha Levels

Statistically minded researchers sometimes remark that there is no difference in the Type 1 error probability when a single study with a lower alpha level is used to test a hypothesis (say 0.05 x 0.05 = 0.0025) compared to when the same hypothesis is tested in two studies, each at an alpha level of 0.05. This is correct, but in practice it is not possible to replace the function of replication studies to decrease the Type 1 error probability with directly lowering alpha levels in single studies. Lowering the alpha level to a desired Type 1 error probability would require that scientists 1) can perfectly predict how important a claim will be in the future, and 2) are able to reach consensus on the Type 1 error rate they collectively find acceptable for each claim. Neither is true in practice. First, it is possible that a claim becomes increasingly important in a research area, for example because a large number of follow-up studies assume the claim is true. This increase in importance might convince the entire scientific community that it is worthwhile if the Type 1 error probability is reduced, as the importance of the original claim has made a Type 1 error more costly. For claims no one builds on, no one will care about reducing the Type 1 error probability. How important a claim will be for follow-up research can not be predicted in advance. Second, some researchers are happy to accept a claim when the Type 1 error probability is 10%, while more skeptical researchers would require 0.1%. While the former will optimistically build on a finding, the latter will want to invest their resources to collect more data, and the two are unlikely to reach agreement about how low the alpha level should be in the initial study. For both these reasons, we need a mechanism to decrease the Type 1 error probability as the need arises. As Popper writes: "Every test of a theory, whether resulting in its corroboration or falsification, must stop at some basic statement or other which we *decide to accept*." "This procedure has no natural end. Thus if the test is to lead us anywhere, nothing remains but to stop at some point or other and say that we are satisfied, for the time being." How satisfied we are with the error probability associated with a claim can change over time, in an unpredictable manner. Using a lower alpha level for a single study does not give scientists the flexibility to lower Type 1 error probabilities as the need arises, while performing replication studies does. It is a good idea to think carefully about the desired Type 1 error rate for studies, and if Type 1 errors are costly, researchers can decide the use of a lower alpha level than the default level of 0.05 is justified [@maier_justify_2022]. But replication studies will remain necessary in practice to further reduce the probability of a Type 1 error as claims increase in importance, or for researchers who are more skeptical about a claim. 

A second benefit of replication studies is that they can be performed by different researchers. If others are able to independently replicate the same effect, it becomes less likely that the original finding was due to systematic error. Systematic errors do not average out to zero in the long run (as random errors do). There can be many sources of systematic error in a study. One source is the measures that are used. For example, if a researcher uses a weighing scale that is limited to a maximum weight of 150 kilo, they might fail to identify an increase in weight, while different researchers who use a weighting scale with a higher maximum weight will identify the difference. An experimenter might be another source of systematic error, if an observed effect is not due to a manipulation, but due to the way the experimenter treats participants in different conditions. Other experimenters who repeat the manipulation, but do not show the same experimenter bias, would observe different results. 

When a researcher repeats their own experiment this is referred to as a **self-replication**, while when in an **independent replication** other researchers repeat the experiment. As explained above, self-replication can reduce the Type 1 error rate of a claim, while independent replication can in addition reduce the probability of a claim being caused by a systematic error. Both self-replication and independent replication is useful in science. For example, research collaborations such as the Large Hadron Collider at CERN prefer to not only replicate studies with the same detector, but also replicate studies across different detectors. The Large Hadron Collider has four detectors (ATLAS, CMS, ALICE, and LHCb). Experiments can be self-replicated in the same detector by collecting more data (referred to by physicists as a 'replica'), but they can also be replicated in a different detector. As Junk and Lyons [-@junk_reproducibility_2020] note, in self-replications: "The statistical variations are expected to be different in the replica and the original, but the sources of systematic errors are expected to be unchanged." One way to examine systematic errors is to perform the same experiment in different detectors. The detectors at CERN are not exact copies of each other, and by performing studies in two different detectors, the research collaboration increases its confidence in the reliability of the conclusions if a replication study yields the same observations, despite minor differences in the experimental set-up. 

The value of an independent replication is not only the reduction in the probability of a Type 1 error, but at the same time the reduction in concerns about systematic error influencing the conclusion [@neher_probability_1967]. As already highlighted by Mack [-@mack_need_1951]: "Indeed, the introduction of different techniques gives the replication the additional advantage of serving as a check on the validity of the original research." Similarly, Lubin [-@lubin_replicability_1957] notes: "Our confidence in a study will be a positive monotonic function of the extent to which replication designs are used which vary these supposedly irrelevant factors." To conclude, although both self-replications (a replication by the same researchers) and independent replication (a replication by different researchers) reduce the probability of a false positive claim, independent replications have the added benefit of testing the generalizability of the findings across factors that are deemed irrelevant to observe the effect. 

## Why Prediction Markets or Machine Learning can not Replace Replications

As explained above, just as a single alpha level attached to a claim is not sufficient, as we need to be able to lower the Type 1 error probability as needed, so will other approaches to quantify the probability that a finding is not replicable not be able to replace replication studies if they can not be lowered as needed. The SCORE project examined 2 approaches: Machine Learning, and Predictions 


## When Replication Studies Yield Conflicting Results

When an original study yielded a significant result, but a replication studies yields a non-significant result, there are three possible causes. First, the result in the replication study can be a Type 2 error. Both studies examined a true effect, and the difference in the test result is due to random variation. The probability of a Type 2 error can be reduced by performing an a-priori power analysis, or even better, by performing an [equivalence test](#sec-equivalencetest) against a smallest effect size of interest with high statistical power, but there is always a probability that the result in a replication study is a false negative. Second, the original study can be a Type 1 error, and there is no true effect in both studies. Third, there is some difference between both studies that moderates the effect. There was a true effect in the original study, but there is no true effect in the replication study.

Some researchers strongly believe failures to replicate published findings can be explained by the presence of hitherto unidentified, or 'hidden', moderators [@stroebe_alleged_2014]. There has been at least one example of researchers who were able to provide modest support for the idea that a previous failure to replicate a finding was due to how personally relevant a message in the study was [@luttrell_replicating_2017]. It is difficult to reliably identify moderator variables that explain failures to replicate published findings, but easy to raise them as an explanation when replication studies do not observe the same effect as the original study. Especially in the social sciences it is easy to point to moderators that are practically impossible to test, such as the fact that society has changed over time, or that effects that work in one culture might not replicate in different cultures. This is an age-old problem, already identified by Galileo in [The Assayer](https://web.archive.org/web/https://web.stanford.edu/~jsabol/certainty/readings/Galileo-Assayer.pdf), one of the first books on the scientific method. In this book, Galileo discusses the claim that Babylonians cooked eggs by whirling them in a sling, which is impossible to replicate, and writes: 

>‘If we do not achieve an effect which others formerly achieved, it must be that we lack something in our operation which was the cause of this effect succeeding, and if we lack one thing only, then this alone can be the true cause. Now we do not lack eggs, or slings, or sturdy fellows to whirl them, and still they do not cook, but rather cool down faster if hot. And since we lack nothing except being Babylonians, then being Babylonian is the cause of the egg hardening.’


Resolving inconsistencies in science is an effortful process that can be facilitated by engaging in an **adversarial collaboration**, where two teams join forces to resolve inconsistencies [@mellers_frequency_2001]. It requires first establishing a reliable empirical basis by reducing the probability of Type 1 and Type 2 errors and bias, and then systematically testing hypotheses that are proposed to explain inconsistencies [@uygun_tunc_falsificationist_2022].



Not all researchers agree that their science has inter-subjectively repeatable observations. In what is called the 'crisis in social psychology' Gergen [-@gergen_social_1973] argued social psychology was not a cumulative science: 

> It is the purpose of this paper to argue that social psychology is primarily an historical inquiry. Unlike the natural sciences, it deals with facts that are largely nonrepeatable and which fluctuate markedly over time. Principles of human interaction cannot readily be developed over time because the facts on which they are based do not generally remain stable. Knowledge cannot accumulate in the usual scientific sense because such knowledge does not generally transcend its historical boundaries.

The belief that basic claims in psychology are not repeatable events lead to **social constructivism**. This approach did not become particularly popular, but it is useful to know of its existence. It is a fact that human behavior can change over time. It is also true that many psychological mechanisms have enabled accurate predictions for more than a century, and this is unlikely to change. Still, if you believe that you are studying unrepeatable events, all you have to do is state this, and such observations can be useful for knowledge generation. 

As Schmidt [-@schmidt_shall_2009] writes "There is no such thing as an exact replication." However, it is possible to 1) repeat an experiment where a researcher stays as closely as possible to the original study, 2) repeat an experiment where there it is likely there is some variation in factors that are deemed irrelevant, and 3) knowingly vary aspects of a study design. Popper agrees: "We can never repeat an experiment precisely — all we can do is to keep certain conditions constant, within certain limits." The same idea goes even further back to Peirce (1872), who writes: "The observations which I made yesterday are not the same which I make today. Nor are the simultaneous observations at different observatories the same, however close together the observatories are placed. Every man's senses are his observatory".

# Publishing Replication Studies

Publishing brief reports of replication studies is one solution to identify false positives in the scientific literature [@tullock_publication_1959]. Exact numbers of how many replication studies are performed are difficult to get, as there is no complete database that keeps track of all replication studies (but see [Curate Science](https://curatescience.org/), [Replication WIKI](https://replication.uni-goettingen.de/wiki/index.php/Main_Page) or the [Replication Database]https://metaanalyses.shinyapps.io/replicationdatabase/). It is clear that replication studies in psychology are rare. One reason is that journals prefer novel findings over replication studies. Over the last years, several journals have started to explicitly state they will publish replication studies. 

## The role of auxiliary assumptions


## Why are they not done

"It is difficult to deny that there is more thrill, and usually more glory, involved in blazing a new trail than in checking the pioneer's work" [@mack_need_1951]]

Replication Value idea Peder Isager

Within a methodological falsificationist philosophy of science, the goal of replication studies is to make claims about observational statements: By repeating a specified methodological procedure, a predicted result was observed (or not). As De Groot [-@de_groot_methodology_1969, p. 89] writes: "If one knows something to be true, he is in a position to predict; where prediction is impossible there is no knowledge". In a replication study, the prediction can be as weak as 'if I repeat the procedure in the original study, the same result should be observed'. 'Close' or 'direct' replication studies are therefore not a test of a theory, and they do not tell us anything about the truth of claims. That is not their role in science. Their only function is to test whether repeating a methodological procedure will lead to a predicted observation. 

Because of Type 1 and Type 2 errors a series of replication studies should yield mixed results, but a series of well-designed studies (e.g., with sufficient power) should be able to distinguish between replicable and non-replicable effects. Of course replicable effects can be due to mechanisms that are unrelated to the theory that is tested, and non-replicable effects might be due to unknown moderators that vary across replication studies. To make *theoretical inferences* based upon *statistical inferences* researchers need to critically reflect on the **validity** of their claims [@shadish_experimental_2001; @schiavone_consensus-based_2023]. 


There are studies that are impossible to replicate directly. 




In this philosophy of psychology researchers give up the aim to build theories upon which generalizable predictions can be made. Most younger researchers no longer know about this 'crisis in psychology' in the 1970's, perhaps because sufficient effects that proved replicable over time were observed. Nevertheless, this does not mean all scientific inquiry in a field such as social psychology should be expected to lead to replicable findings, as it is simply a fact that social behavior changes over time. For example, in the classic 'foot-in-the-door' effect study, Freedman and Fraser [-@freedman_compliance_1966] first called residents in a local community over the phone to ask them to answer some questions (the small request). If participants agreed, they were asked a larger request, which consisted of "five or six men from our staff coming into your home some morning for about 2 hours to enumerate and classify all the household products that you have. They will have to have full freedom in your house to go through the cupboards and storage places." The idea that anyone would nowadays agree to such a request when called by a stranger over the telephone (let along more than 50% as in the original study) seems highly improbable. 

# The Reproducibility Project: Psychology

>After this intensive effort to reproduce a sample of published psychological findings, how many of the effects have we established are true? Zero. And how many of the effects have we established are false? Zero. Is this a limitation of the project design? No. It is the reality of doing science, even if it is not appreciated in daily practice. Humans desire certainty, and science infrequently provides it. As much as we might wish it to be otherwise, a single study almost never provides definitive resolution for or against an effect and its explanation. The original studies examined here offered tentative evidence; the replications we conducted offered additional, confirmatory evidence. In some cases, the replications increase confidence in the reliability of the original results; in other cases, the replications suggest that more investigation is needed to establish the validity of the original findings. Scientific progressisacumulativeprocess of uncertainty reduction that can only succeed if science itself remains the greatest skeptic of its explanatory claims.

It inspired, and was followed up, by many other replication projects. 

## Replication of Qualitative Research

Take from Freese, J., & Peterson, D. (2017). Replication in Social Science.


