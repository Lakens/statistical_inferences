<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>replication</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="replication_files/libs/clipboard/clipboard.min.js"></script>
<script src="replication_files/libs/quarto-html/quarto.js"></script>
<script src="replication_files/libs/quarto-html/popper.min.js"></script>
<script src="replication_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="replication_files/libs/quarto-html/anchor.min.js"></script>
<link href="replication_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="replication_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="replication_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="replication_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="replication_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="sec-replication" class="level1">
<h1>Replication Studies</h1>
<p>A replication study is an experiment where the methods and procedures in a previous study are repeated by collecting new data. Typically, the term <em>direct replication study</em> is used when the methods and measures are as similar to the earlier study as possible. In a <em>conceptual replication study</em> a researcher intentionally introduces differences with the original study, either because they aim to systematically explore the impact of this change, or because they are not able to use the same methods and procedures. It is important to distinguish replication, where new data is collected, from <em>reproducibility</em>, where the same data is used to reproduce the reported results. In reproducibility checks the goal is to examine the presence of errors in the analysis files. Confusingly, the large-scale collaborative research project where 100 studies in psychology were replicated, and which is often considered an important contributor to the <em>replication crisis</em>, was called the Reproducibility Project: Psychology <span class="citation" data-cites="opensciencecollaboration_estimating_2015">[@opensciencecollaboration_estimating_2015]</span>. It should have been called the Replication Project: Psychology. Our apologies.</p>
<p>The goal of direct replication studies is first of all to identify Type 1 or Type 2 errors in the literature. In any original study with an alpha of 5% and a statistical power of 80% there is a probability of making an erroneous claim. Direct replication studies (especially those with low error rates) have the goal to identify these errors in the scientific literature. As statistical test allow researchers to make claims with a certain error rate, for impactful claims it is important to distinguish erroneous from correct claims. This is especially important given that the scientific literature is biased, which increases the probability that a published claim is a Type 1 error. Schmidt <span class="citation" data-cites="schmidt_shall_2009">[-@schmidt_shall_2009]</span> also notes that claims can be erroneous because they were based on fraudulent data. Although direct replication can not identify fraud, they can point out erroneous claims due to fraud. The second important goal of direct replications is to identify factors in the study that were deemed irrelevant, but were crucial in generating the observed data <span class="citation" data-cites="tunc_falsificationist_2023">[@tunc_falsificationist_2023]</span>. For example, researchers might discover that an original and replication study yielded different results because the experimenter in one study treated the participants in a much more friendly manner than the experimenter in the other study. Such details are typically not reported in the method section, as they are deemed irrelevant, but direct replication studies might identify such factors. This already points out replication studies are never identical in the social sciences. They are designed to be similar in all ways that a researcher thinks will matter.</p>
<p>The goal of conceptual replication studies is to examine the generalizability of effects <span class="citation" data-cites="sidman_tactics_1960">[@sidman_tactics_1960]</span>. Researchers intentionally vary factors in the experiment to examine if this leads to variability in the results. Sometimes this variability is theoretically predicted, and sometimes researchers simply want to see what will happen. I see the main distinction between a direct replication and a conceptual replication in the goal the researchers has to intentionally introduce variability. This distinction means that it is possible that what one researchers aims to be a direct replication is seen by a peer as a conceptual replication. For example, if a researcher sees no reason why an effect tested in Germany would not be identical in The Netherlands, they will consider it a direct replication. A peer might believe there are theoretically relevant differences between the two countries that make this a conceptual replication. The only way to clarify which factors are deemed theoretically relevant is to write a <em>constraints on generalizability</em> statement in the discussion where researchers specify which contexts they expect the effect to replicate in, and where variability in the results would not be considered problematic for their original claim <span class="citation" data-cites="simons_constraints_2017">[@simons_constraints_2017]</span>.</p>
<p>It should be clear that the goals of replication studies are rather modest. At the same time, they are essential for a well-functioning empirical science. They provide a tool to identify false positive or false negative results, and can reveal variability across contexts that might falsify theoretical predictions, or lead to the generation of new theories. Being able to systematically replicate and extend a basic effect is one of the most important ways in which scientists develop and test theories. Although there are theoretically other approaches to generating reliable knowledge, such as <em>triangulation</em> where the same theory is tested in different but complementary ways, in practice the vast majority of scientifically established claims are in part due to replicable effects.</p>
</section>
<section id="why-replication-studies-are-important" class="level1">
<h1>Why replication studies are important</h1>
<p>Researchers have repeatedly observed over the last half century that replication studies were rarely performed or published. In an editorial in the Journal of Personality and Social Psychology, Greenwald <span class="citation" data-cites="greenwald_editorial_1976">[@greenwald_editorial_1976]</span> writes: “There may be a crisis in personality and social psychology, associated with the difficulty often experienced by researchers in attempting to replicate published work. A precise statement of the magnitude of this problem cannot be made, since most failures to replicate do not receive public report”. A similar concern about the replicability of findings is expressed by Epstein <span class="citation" data-cites="epstein_stability_1980">[@epstein_stability_1980, p. 790]</span>: “Not only are experimental findings often difficult to replicate when there are the slightest alterations in conditions, but even attempts at exact replication frequently fail.” Neher <span class="citation" data-cites="neher_probability_1967">[-@neher_probability_1967, p. 262]</span> concludes: “The general adoption of independent replication as a requirement for acceptance of findings in the behavioral sciences will require the efforts of investigators, readers, and publishing editors alike. It seems clear that such a policy is both long overdue and crucial to the development of a sound body of knowledge concerning human behavior.” Lubin <span class="citation" data-cites="lubin_replicability_1957">[@lubin_replicability_1957]</span> suggests that, where relevant, manuscripts that demonstrate the replicability of findings should receive a higher publication priority. N. C. Smith <span class="citation" data-cites="smith_replication_1970">[-@smith_replication_1970, p. 974]</span> notes how replication studies are neglected: “The review of the literature on replication and cross-validation research has revealed that psychologists in both research”disciplines” have tended to ignore replication research. Thus, one cannot help but wonder what the impact might be if every investigator repeated the study which he believed to be his most significant contribution to the field.” One problem in the past was the difficulty of describing the methods and analyses in sufficient detail to allow others to repeat the study as closely as possible <span class="citation" data-cites="mack_need_1951">[@mack_need_1951]</span>. For example, Pereboom <span class="citation" data-cites="pereboom_fundamental_1971">[-@pereboom_fundamental_1971, p. 442]</span> writes: “Related to the above is the common difficulty of communicating all important details of a psychological experiment to one’s audience. […] Investigators attempting to replicate the work of others are painfully aware of these informational gaps.” Open science practices, such as sharing <a href="#sec-computationalreproducibility">computationally reproducible</a> code and materials, are an important way to solve this problem.</p>
<p>Many researchers have suggested that performing replication studies should be common practice. Lykken <span class="citation" data-cites="lykken_statistical_1968">[-@lykken_statistical_1968, p. 159]</span> writes: “Ideally, all experiments would be replicated before publication but this goal is impractical.”, Loevinger <span class="citation" data-cites="loevinger_information_1968">[-@loevinger_information_1968, p. 455]</span> makes a similar point: “Most studies should be replicated prior to publication. This recommendation is particularly pertinent in cases where the results are in the predicted direction, but not significant, or barely so, or only by one-tailed tests.” Samelson <span class="citation" data-cites="samelson_watson_1980">[-@samelson_watson_1980, p. 623]</span> notes in the specific context of Watson’s ‘Little Albert’ study: “Beyond this apparent failure of internal criticism of the data is another one that is even less debatable: the clear neglect of a cardinal rule of scientific method, that is, replication.”</p>
<p>The idea that replication is a ‘cardinal rule’ or ‘cornerstone’ of the scientific method follows directly from a methodological falsificationist philosophy of science. Popper <span class="citation" data-cites="popper_logic_1959">[-@popper_logic_1959]</span> discusses how we increase our confidence in theories that make predictions that withstand attempts to falsify the theory. To be able to falsify predictions, predictions need to rule out certain observable data patterns. For example, if our theory predicts that people will be faster at naming the colour of words when their meaning matches the colour (e.g., “blue” written in blue instead of “blue” written in red), the observation that people are not faster (or even slower) would falsify our prediction. A problem is that given variability in observed data any possible data pattern will occur, exactly as often as dictated by chance. Sometimes, purely due to random variation, a study could show people are slower at naming the colour of words when their meaning matches the colour - even though our theory is correct. Popper realized this was a problem for his account of falsification, because it means that “probability statements will not be falsifiable”. After all, if all possible data patterns have a non-zero probability, even if they are extremely rare, they are not logically ruled out. This would make falsification impossible if we demanded that science works according to perfectly formal logical rules. However, science does not work following any formal system, and yet, as Popper acknowledges, it still works. Therefore, instead of abandoning the idea of falsification, Popper proposes a more pragmatic approach to falsification. He writes: “It is fairly clear that this ‘practical falsification’ can be obtained only through a methodological decision to regard highly improbable events as ruled out — as prohibited.” The logical follow-up question is then “Where are we to draw the line? Where does this ‘high improbability’ begin?” Popper argues that even if any low probability event <em>can</em> occur, <em>they are not reproducible at will</em>. Any single study can reveal any possible effect, but a prediction should be considered falsified if we fail to see “the predictable and reproducible occurrence of systematic deviations”. This is why replication is considered a ‘cardinal rule’ in methodological falsificationism: When observations are probabilistic, only the reproducible occurrence of low probability events can be taken as the falsification of a prediction. A single <em>p</em> &lt; 0.05 is not considered sufficient; only if close replication studies repeatedly observe a low probability event does Popper allow us to ‘practically falsify’ probabilistic predictions.</p>
</section>
<section id="direct-versus-conceptual-replications" class="level1">
<h1>Direct versus conceptual replications</h1>
<p>One of the first extensive treatments of replication comes from Sidman <span class="citation" data-cites="sidman_tactics_1960">[-@sidman_tactics_1960]</span>. He distinguishes direct replications from <em>systematic replications</em> (which I refer to here as conceptual replications). Sidman writes:</p>
<blockquote class="blockquote">
<p>Where direct replication helps to establish generality of a phenomenon among the members of a species, systematic replication can accomplish this and, at the same time, extend its generality over a wide range of different situations.</p>
</blockquote>
<p>If the same result is observed when systematically varying auxiliary assumptions we build confidence in the general nature of the finding, and therefore, in the finding itself. The more robust an effect is to factors that are deemed irrelevant, the less likely it is that the effect is caused by a confound introduced by one of these factors. If a prediction is confirmed across time, locations, in different samples of participants, by different experimenters, and using different measures of the same variable, then the likelihood of a confound underlying all these effects decreases. If conceptual replications are successful they yield more information than a direct replication, as it a conceptual replication generalizes the finding beyond the original context. This benefit is only present if the conceptual replication is succesful, however. Sidman warns:</p>
<blockquote class="blockquote">
<p>But this procedure is a gamble. If systematic replication fails, the original experiment will still have to be redone, else there is no way of determining whether the failure to replicate stemmed from the introduction of new variables in the second experiment, or whether the control of relevant factors was inadequate in the first one.</p>
</blockquote>
<p>Sometimes there is no need to choose between a direct replication and a conceptual replication, as both can be performed. Researchers can perform <strong>replication and extension studies</strong> where an original study is replicated, but additional conditions are added that test a novel hypotheses. Sidman <span class="citation" data-cites="sidman_tactics_1960">[-@sidman_tactics_1960]</span> refers to this as the <em>baseline technique</em> where an original effect is always part of the experimental design, and variations are tested against the baseline effect. Replication and extension studies are one of the best ways to build cumulative knowledge and develop strong scientific theories <span class="citation" data-cites="bonett_replication-extension_2012">[@bonett_replication-extension_2012]</span>.</p>
<p>A problem with the distinction between ‘direct’ and ‘conceptual’ replications is that it is difficult to reach agreement about how similar studies are by an exclusive focus on operational procedures and methods used in the study. A broader definition of a replication study is “is a study for which any outcome would be considered diagnostic evidence about a claim from prior research” <span class="citation" data-cites="nosek_what_2020">[@nosek_what_2020]</span>. This definition places all the weight on whether a study is a replication on theoretical predictions, and as long as a study is theoretically predicted to show support for the same claim, it is a replication. However, this definition seems too broad to be useful in practice, has limited use when theoretical predictions are vague (which regrettably holds for most research lines in psychology), and does not sufficiently acknowledge the importance of specifying falsifiable auxiliairy hypotheses.</p>
<p>The specification of falsifiable auxiliary hypothesis in replication studies lies at the core of the Systematic Replications Framework <span class="citation" data-cites="tunc_falsificationist_2023">[@tunc_falsificationist_2023]</span>. The idea is that researchers should specify the auxiliary hypotheses that are assumed to be relevant. In principle the number of auxiliary hypotheses is infinite, but as Uygun-Tunç and Tunç <span class="citation" data-cites="tunc_falsificationist_2023">[-@tunc_falsificationist_2023]</span> write: “it is usually assumed that the exact color of the lab walls, the elevation of the lab above the sea level, the exact design of the chairs used by the subjects, the humidity of the room that the study takes place or many other minute details do not significantly influence the study outcomes.” These factors are relagated to the <em>ceteris paribus clause</em>, meaning that any differences on these factors are not relevant, and for all purposes studies can be treated as ‘all equal’ regardless of whether the color of the walls differs. At the same time, no replication study is exactly the same as the original study <span class="citation" data-cites="schmidt_shall_2009">[@schmidt_shall_2009]</span>, and some differences in auxiliary hypotheses are meaningfully different. The challenge is to identify which auxiliary hypotheses explain failures to replicate an original study. A direct replication study that yields no statistically significant effect can have three interpretations <span class="citation" data-cites="schmidt_shall_2009 tunc_falsificationist_2023">[@schmidt_shall_2009; @tunc_falsificationist_2023]</span>. First, it is possible that the replication study yielded a Type 2 error. Second, it is possible that the original study was a Type 1 error. Third, some of the auxiliary assumptions that have been relegated to the ceteris paribus clause actually matter more than researchers might have thought. To resolve disagreements between the results of original and replication studies researchers should perform a set of studies that systematically varies those auxiliary hypotheses that are most crucial for a theoretical viewpoint. Resolving inconsistencies in science is an effortful process that can be facilitated by engaging in an <strong>adversarial collaboration</strong>, where two teams join forces to resolve inconsistencies <span class="citation" data-cites="mellers_frequency_2001">[@mellers_frequency_2001]</span>. Opposing teams can point out the most crucial auxiliary hypotheses to test, and severely test different theories.</p>
</section>
<section id="analyzing-replication-studies." class="level1">
<h1>Analyzing Replication Studies.</h1>
<p>There are multiple ways to statistically analyze a replication study <span class="citation" data-cites="anderson_there_2016">[@anderson_there_2016]</span>. The most straightforward statistical approach is also the least common: Testing whether the effect sizes in the original and replication studies are statistically different from each other. Two effect sizes can be tested against each other using:</p>
<p><span class="math display">\[
Z_{Diff} = \frac{{{\delta}}_{1}{- {\delta}}_{2}}{\sqrt{{V}_{\delta_{1}} + {V}_{\delta_{2}}}}
\]</span></p>
<p>where the difference between the two Cohen’s <em>d</em> effect sizes is divided by the standard error of the difference score, based on the square root of the combined variances of the effect sizes <span class="citation" data-cites="borenstein_introduction_2009">[@borenstein_introduction_2009, formula 19.6 and 19.7]</span>. This formula provides a hint why researchers rarely test for differences between effect sizes. The standard error of the difference score is based on the variance in the original and replication study. If the original study has a small sample size, the variance will be large, and a test of the difference between effect sizes can have very low power.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">escalc</span>(<span class="at">n1i =</span> <span class="dv">50</span>, </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">n2i =</span> <span class="dv">50</span>, </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">di =</span> <span class="fl">0.4</span>, </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">measure =</span> <span class="st">"SMD"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">escalc</span>(<span class="at">n1i =</span> <span class="dv">250</span>, </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">n2i =</span> <span class="dv">250</span>, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">di =</span> <span class="fl">0.0</span>, </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">measure =</span> <span class="st">"SMD"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>metadata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">yi =</span> <span class="fu">c</span>(d1<span class="sc">$</span>yi, d2<span class="sc">$</span>yi), </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                       <span class="at">vi =</span> <span class="fu">c</span>(d1<span class="sc">$</span>vi, d2<span class="sc">$</span>vi), </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                       <span class="at">study =</span> <span class="fu">c</span>(<span class="st">"original"</span>, <span class="st">"replication"</span>))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Test based on heterogeneity analysis</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>res_h <span class="ot">&lt;-</span> <span class="fu">rma</span>(yi, </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>             vi, </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> metadata, </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">method =</span> <span class="st">"FE"</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>res_h</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Fixed-Effects Model (k = 2)

I^2 (total heterogeneity / total variability):   69.03%
H^2 (total variability / sampling variability):  3.23

Test for Heterogeneity:
Q(df = 1) = 3.2294, p-val = 0.0723

Model Results:

estimate      se    zval    pval    ci.lb   ci.ub    
  0.0651  0.0818  0.7959  0.4261  -0.0952  0.2254    

---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>forest(res_h)</p>
<p>metafor::forest(res_h, # efac=c(0,3), lty=c(1,1,0), cex=0.8, shade = TRUE, rowadj=0, psize = 1, colout = “#333333”, xlim = c(-2, 2.5), alim = c(-0.5, 1), textpos = c(-2, 2.5), at = c(-0.5, 0, 0.5, 1), digits = c(3, 1), xlab = “Cohen’s d”, # refline = c(0, 0.2, 0.5), header = TRUE )</p>
<p>```</p>
<section id="replication-studies-or-lower-alpha-levels" class="level2">
<h2 class="anchored" data-anchor-id="replication-studies-or-lower-alpha-levels">Replication Studies or Lower Alpha Levels</h2>
<p>Statistically minded researchers sometimes remark that there is no difference in the Type 1 error probability when a single study with a lower alpha level is used to test a hypothesis (say 0.05 x 0.05 = 0.0025) compared to when the same hypothesis is tested in two studies, each at an alpha level of 0.05. This is correct, but in practice it is not possible to replace the function of replication studies to decrease the Type 1 error probability with directly lowering alpha levels in single studies. Lowering the alpha level to a desired Type 1 error probability would require that scientists 1) can perfectly predict how important a claim will be in the future, and 2) are able to reach consensus on the Type 1 error rate they collectively find acceptable for each claim. Neither is true in practice. First, it is possible that a claim becomes increasingly important in a research area, for example because a large number of follow-up studies assume the claim is true. This increase in importance might convince the entire scientific community that it is worthwhile if the Type 1 error probability is reduced, as the importance of the original claim has made a Type 1 error more costly. For claims no one builds on, no one will care about reducing the Type 1 error probability. How important a claim will be for follow-up research can not be predicted in advance. Second, some researchers are happy to accept a claim when the Type 1 error probability is 10%, while more skeptical researchers would require 0.1%. While the former will optimistically build on a finding, the latter will want to invest their resources to collect more data, and the two are unlikely to reach agreement about how low the alpha level should be in the initial study. For both these reasons, we need a mechanism to decrease the Type 1 error probability as the need arises. As Popper writes: “Every test of a theory, whether resulting in its corroboration or falsification, must stop at some basic statement or other which we <em>decide to accept</em>.” “This procedure has no natural end. Thus if the test is to lead us anywhere, nothing remains but to stop at some point or other and say that we are satisfied, for the time being.” How satisfied we are with the error probability associated with a claim can change over time, in an unpredictable manner. Using a lower alpha level for a single study does not give scientists the flexibility to lower Type 1 error probabilities as the need arises, while performing replication studies does. It is a good idea to think carefully about the desired Type 1 error rate for studies, and if Type 1 errors are costly, researchers can decide the use of a lower alpha level than the default level of 0.05 is justified <span class="citation" data-cites="maier_justify_2022">[@maier_justify_2022]</span>. But replication studies will remain necessary in practice to further reduce the probability of a Type 1 error as claims increase in importance, or for researchers who are more skeptical about a claim.</p>
<p>A second benefit of replication studies is that they can be performed by different researchers. If others are able to independently replicate the same effect, it becomes less likely that the original finding was due to systematic error. Systematic errors do not average out to zero in the long run (as random errors do). There can be many sources of systematic error in a study. One source is the measures that are used. For example, if a researcher uses a weighing scale that is limited to a maximum weight of 150 kilo, they might fail to identify an increase in weight, while different researchers who use a weighting scale with a higher maximum weight will identify the difference. An experimenter might be another source of systematic error, if an observed effect is not due to a manipulation, but due to the way the experimenter treats participants in different conditions. Other experimenters who repeat the manipulation, but do not show the same experimenter bias, would observe different results.</p>
<p>When a researcher repeats their own experiment this is referred to as a <strong>self-replication</strong>, while when in an <strong>independent replication</strong> other researchers repeat the experiment. As explained above, self-replication can reduce the Type 1 error rate of a claim, while independent replication can in addition reduce the probability of a claim being caused by a systematic error. Both self-replication and independent replication is useful in science. For example, research collaborations such as the Large Hadron Collider at CERN prefer to not only replicate studies with the same detector, but also replicate studies across different detectors. The Large Hadron Collider has four detectors (ATLAS, CMS, ALICE, and LHCb). Experiments can be self-replicated in the same detector by collecting more data (referred to by physicists as a ‘replica’), but they can also be replicated in a different detector. As Junk and Lyons <span class="citation" data-cites="junk_reproducibility_2020">[-@junk_reproducibility_2020]</span> note, in self-replications: “The statistical variations are expected to be different in the replica and the original, but the sources of systematic errors are expected to be unchanged.” One way to examine systematic errors is to perform the same experiment in different detectors. The detectors at CERN are not exact copies of each other, and by performing studies in two different detectors, the research collaboration increases its confidence in the reliability of the conclusions if a replication study yields the same observations, despite minor differences in the experimental set-up.</p>
<p>The value of an independent replication is not only the reduction in the probability of a Type 1 error, but at the same time the reduction in concerns about systematic error influencing the conclusion <span class="citation" data-cites="neher_probability_1967">[@neher_probability_1967]</span>. As already highlighted by Mack <span class="citation" data-cites="mack_need_1951">[-@mack_need_1951]</span>: “Indeed, the introduction of different techniques gives the replication the additional advantage of serving as a check on the validity of the original research.” Similarly, Lubin <span class="citation" data-cites="lubin_replicability_1957">[-@lubin_replicability_1957]</span> notes: “Our confidence in a study will be a positive monotonic function of the extent to which replication designs are used which vary these supposedly irrelevant factors.” To conclude, although both self-replications (a replication by the same researchers) and independent replication (a replication by different researchers) reduce the probability of a false positive claim, independent replications have the added benefit of testing the generalizability of the findings across factors that are deemed irrelevant to observe the effect.</p>
</section>
<section id="why-prediction-markets-or-machine-learning-can-not-replace-replications" class="level2">
<h2 class="anchored" data-anchor-id="why-prediction-markets-or-machine-learning-can-not-replace-replications">Why Prediction Markets or Machine Learning can not Replace Replications</h2>
<p>As explained above, just as a single alpha level attached to a claim is not sufficient, as we need to be able to lower the Type 1 error probability as needed, so will other approaches to quantify the probability that a finding is not replicable not be able to replace replication studies if they can not be lowered as needed. The SCORE project examined 2 approaches: Machine Learning, and Predictions</p>
</section>
<section id="when-replication-studies-yield-conflicting-results" class="level2">
<h2 class="anchored" data-anchor-id="when-replication-studies-yield-conflicting-results">When Replication Studies Yield Conflicting Results</h2>
<p>When an original study yielded a significant result, but a replication studies yields a non-significant result, there are three possible causes. First, the result in the replication study can be a Type 2 error. Both studies examined a true effect, and the difference in the test result is due to random variation. The probability of a Type 2 error can be reduced by performing an a-priori power analysis, or even better, by performing an <a href="#sec-equivalencetest">equivalence test</a> against a smallest effect size of interest with high statistical power, but there is always a probability that the result in a replication study is a false negative. Second, the original study can be a Type 1 error, and there is no true effect in both studies. Third, there is some difference between both studies that moderates the effect. There was a true effect in the original study, but there is no true effect in the replication study.</p>
<p>Some researchers strongly believe failures to replicate published findings can be explained by the presence of hitherto unidentified, or ‘hidden’, moderators <span class="citation" data-cites="stroebe_alleged_2014">[@stroebe_alleged_2014]</span>. There has been at least one example of researchers who were able to provide modest support for the idea that a previous failure to replicate a finding was due to how personally relevant a message in the study was <span class="citation" data-cites="luttrell_replicating_2017">[@luttrell_replicating_2017]</span>. It is difficult to reliably identify moderator variables that explain failures to replicate published findings, but easy to raise them as an explanation when replication studies do not observe the same effect as the original study. Especially in the social sciences it is easy to point to moderators that are practically impossible to test, such as the fact that society has changed over time, or that effects that work in one culture might not replicate in different cultures. This is an age-old problem, already identified by Galileo in <a href="https://web.archive.org/web/https://web.stanford.edu/~jsabol/certainty/readings/Galileo-Assayer.pdf">The Assayer</a>, one of the first books on the scientific method. In this book, Galileo discusses the claim that Babylonians cooked eggs by whirling them in a sling, which is impossible to replicate, and writes:</p>
<blockquote class="blockquote">
<p>‘If we do not achieve an effect which others formerly achieved, it must be that we lack something in our operation which was the cause of this effect succeeding, and if we lack one thing only, then this alone can be the true cause. Now we do not lack eggs, or slings, or sturdy fellows to whirl them, and still they do not cook, but rather cool down faster if hot. And since we lack nothing except being Babylonians, then being Babylonian is the cause of the egg hardening.’</p>
</blockquote>
<p>Not all researchers agree that their science has inter-subjectively repeatable observations. In what is called the ‘crisis in social psychology’ Gergen <span class="citation" data-cites="gergen_social_1973">[-@gergen_social_1973]</span> argued social psychology was not a cumulative science:</p>
<blockquote class="blockquote">
<p>It is the purpose of this paper to argue that social psychology is primarily an historical inquiry. Unlike the natural sciences, it deals with facts that are largely nonrepeatable and which fluctuate markedly over time. Principles of human interaction cannot readily be developed over time because the facts on which they are based do not generally remain stable. Knowledge cannot accumulate in the usual scientific sense because such knowledge does not generally transcend its historical boundaries.</p>
</blockquote>
<p>The belief that basic claims in psychology are not repeatable events lead to <strong>social constructivism</strong>. This approach did not become particularly popular, but it is useful to know of its existence. It is a fact that human behavior can change over time. It is also true that many psychological mechanisms have enabled accurate predictions for more than a century, and this is unlikely to change. Still, if you believe that you are studying unrepeatable events, all you have to do is state this, and such observations can be useful for knowledge generation.</p>
<p>As Schmidt <span class="citation" data-cites="schmidt_shall_2009">[-@schmidt_shall_2009]</span> writes “There is no such thing as an exact replication.” However, it is possible to 1) repeat an experiment where a researcher stays as closely as possible to the original study, 2) repeat an experiment where there it is likely there is some variation in factors that are deemed irrelevant, and 3) knowingly vary aspects of a study design. Popper agrees: “We can never repeat an experiment precisely — all we can do is to keep certain conditions constant, within certain limits.” The same idea goes even further back to Peirce (1872), who writes: “The observations which I made yesterday are not the same which I make today. Nor are the simultaneous observations at different observatories the same, however close together the observatories are placed. Every man’s senses are his observatory”.</p>
</section>
</section>
<section id="publishing-replication-studies" class="level1">
<h1>Publishing Replication Studies</h1>
<p>Publishing brief reports of replication studies is one solution to identify false positives in the scientific literature <span class="citation" data-cites="tullock_publication_1959">[@tullock_publication_1959]</span>. Exact numbers of how many replication studies are performed are difficult to get, as there is no complete database that keeps track of all replication studies (but see <a href="https://curatescience.org/">Curate Science</a>, <a href="https://replication.uni-goettingen.de/wiki/index.php/Main_Page">Replication WIKI</a> or the [Replication Database]https://metaanalyses.shinyapps.io/replicationdatabase/). It is clear that replication studies in psychology are rare. One reason is that journals prefer novel findings over replication studies. Over the last years, several journals have started to explicitly state they will publish replication studies.</p>
<section id="the-role-of-auxiliary-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-auxiliary-assumptions">The role of auxiliary assumptions</h2>
</section>
<section id="why-are-they-not-done" class="level2">
<h2 class="anchored" data-anchor-id="why-are-they-not-done">Why are they not done</h2>
<p>“It is difficult to deny that there is more thrill, and usually more glory, involved in blazing a new trail than in checking the pioneer’s work” <span class="citation" data-cites="mack_need_1951">[@mack_need_1951]</span>]</p>
<p>Replication Value idea Peder Isager</p>
<p>Within a methodological falsificationist philosophy of science, the goal of replication studies is to make claims about observational statements: By repeating a specified methodological procedure, a predicted result was observed (or not). As De Groot <span class="citation" data-cites="de_groot_methodology_1969">[-@de_groot_methodology_1969, p. 89]</span> writes: “If one knows something to be true, he is in a position to predict; where prediction is impossible there is no knowledge”. In a replication study, the prediction can be as weak as ‘if I repeat the procedure in the original study, the same result should be observed’. ‘Close’ or ‘direct’ replication studies are therefore not a test of a theory, and they do not tell us anything about the truth of claims. That is not their role in science. Their only function is to test whether repeating a methodological procedure will lead to a predicted observation.</p>
<p>Because of Type 1 and Type 2 errors a series of replication studies should yield mixed results, but a series of well-designed studies (e.g., with sufficient power) should be able to distinguish between replicable and non-replicable effects. Of course replicable effects can be due to mechanisms that are unrelated to the theory that is tested, and non-replicable effects might be due to unknown moderators that vary across replication studies. To make <em>theoretical inferences</em> based upon <em>statistical inferences</em> researchers need to critically reflect on the <strong>validity</strong> of their claims <span class="citation" data-cites="shadish_experimental_2001 schiavone_consensus-based_2023">[@shadish_experimental_2001; @schiavone_consensus-based_2023]</span>.</p>
<p>There are studies that are impossible to replicate directly.</p>
<p>In this philosophy of psychology researchers give up the aim to build theories upon which generalizable predictions can be made. Most younger researchers no longer know about this ‘crisis in psychology’ in the 1970’s, perhaps because sufficient effects that proved replicable over time were observed. Nevertheless, this does not mean all scientific inquiry in a field such as social psychology should be expected to lead to replicable findings, as it is simply a fact that social behavior changes over time. For example, in the classic ‘foot-in-the-door’ effect study, Freedman and Fraser <span class="citation" data-cites="freedman_compliance_1966">[-@freedman_compliance_1966]</span> first called residents in a local community over the phone to ask them to answer some questions (the small request). If participants agreed, they were asked a larger request, which consisted of “five or six men from our staff coming into your home some morning for about 2 hours to enumerate and classify all the household products that you have. They will have to have full freedom in your house to go through the cupboards and storage places.” The idea that anyone would nowadays agree to such a request when called by a stranger over the telephone (let along more than 50% as in the original study) seems highly improbable.</p>
</section>
</section>
<section id="the-reproducibility-project-psychology" class="level1">
<h1>The Reproducibility Project: Psychology</h1>
<blockquote class="blockquote">
<p>After this intensive effort to reproduce a sample of published psychological findings, how many of the effects have we established are true? Zero. And how many of the effects have we established are false? Zero. Is this a limitation of the project design? No.&nbsp;It is the reality of doing science, even if it is not appreciated in daily practice. Humans desire certainty, and science infrequently provides it. As much as we might wish it to be otherwise, a single study almost never provides definitive resolution for or against an effect and its explanation. The original studies examined here offered tentative evidence; the replications we conducted offered additional, confirmatory evidence. In some cases, the replications increase confidence in the reliability of the original results; in other cases, the replications suggest that more investigation is needed to establish the validity of the original findings. Scientific progressisacumulativeprocess of uncertainty reduction that can only succeed if science itself remains the greatest skeptic of its explanatory claims.</p>
</blockquote>
<p>It inspired, and was followed up, by many other replication projects.</p>
<section id="replication-of-qualitative-research" class="level2">
<h2 class="anchored" data-anchor-id="replication-of-qualitative-research">Replication of Qualitative Research</h2>
<p>Take from Freese, J., &amp; Peterson, D. (2017). Replication in Social Science.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>