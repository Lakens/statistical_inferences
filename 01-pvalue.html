<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.3.310" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

<meta name="description" content="This open educational resource contains information to improve statistical inferences, design better experiments, and report scientific research more transparently." />

<title>Improving Your Statistical Inferences – 1  Using p-values to test a hypothesis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<!-- htmldependencies:E3FAD763 -->
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0MK2WTGRM3"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-0MK2WTGRM3', { 'anonymize_ip': true});
</script>

<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="include/booktem.css" />
<link rel="stylesheet" href="include/style.css" />
<link rel="stylesheet" href="include/webex.css" />
</head>

<body>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn"
      data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" 
      aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation"
      onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <h1 class="quarto-secondary-nav-title"></h1>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" 
      aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation"
      onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="/">
      Improving Your Statistical Inferences
      </a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/Lakens/statistical_inferences" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="/Improving-Your-Statistical-Inferences.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="/Improving-Your-Statistical-Inferences.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <a href="https://twitter.com/intent/tweet?url=|url|" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/01-pvalue.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;1&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Using *p*-values to test a hypothesis&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/02-errorcontrol.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;2&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Error control&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/03-likelihoods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;3&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Likelihoods&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/04-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;4&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Bayesian statistics&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/05-questions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;5&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Asking Statistical Questions&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/06-effectsize.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;6&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Effect Sizes&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/07-CI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;7&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Confidence Intervals&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/08-samplesizejustification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;8&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Sample Size Justification&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/09-equivalencetest.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;9&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Equivalence Testing and Interval Hypotheses&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/10-sequential.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;10&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Sequential Analysis&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/11-meta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;11&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Meta-analysis&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/12-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;12&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Bias detection&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/13-prereg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;13&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Preregistration and Transparency&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/14-computationalreproducibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;14&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Computational Reproducibility&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/15-researchintegrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;15&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Research Integrity&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" ></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <div id="quarto-toc-target"></div>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-pvalue" class="quarto-section-identifier"><span class="chapter-number">1</span>  <span class="chapter-title">Using <em>p</em>-values to test a hypothesis</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>
<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#philosophical-approaches-to-p-values" id="toc-philosophical-approaches-to-p-values"><span class="header-section-number">1.1</span> Philosophical approaches to <em>p</em>-values</a></li>
  <li><a href="#creating-a-null-model" id="toc-creating-a-null-model"><span class="header-section-number">1.2</span> Creating a null model</a></li>
  <li><a href="#calculating-a-p-value" id="toc-calculating-a-p-value"><span class="header-section-number">1.3</span> Calculating a <em>p</em>-value</a></li>
  <li><a href="#sec-whichpexpect" id="toc-sec-whichpexpect"><span class="header-section-number">1.4</span> Which <em>p</em>-values can you expect?</a></li>
  <li><a href="#sec-lindley" id="toc-sec-lindley"><span class="header-section-number">1.5</span> Lindley’s paradox</a></li>
  <li><a href="#sec-correctlyinterpreting" id="toc-sec-correctlyinterpreting"><span class="header-section-number">1.6</span> Correctly reporting and interpreting <em>p</em>-values</a></li>
  <li><a href="#sec-misconceptions" id="toc-sec-misconceptions"><span class="header-section-number">1.7</span> Preventing common misconceptions about <em>p</em>-values</a>
  <ul>
  <li><a href="#sec-misconception1" id="toc-sec-misconception1"><span class="header-section-number">1.7.1</span> Misunderstanding 1: A non-significant <em>p</em>-value means that the null hypothesis is true.</a></li>
  <li><a href="#misunderstanding-2-a-significant-p-value-means-that-the-null-hypothesis-is-false." id="toc-misunderstanding-2-a-significant-p-value-means-that-the-null-hypothesis-is-false."><span class="header-section-number">1.7.2</span> Misunderstanding 2: A significant <em>p</em>-value means that the null hypothesis is false.</a></li>
  <li><a href="#misunderstanding-3-a-significant-p-value-means-that-a-practically-important-effect-has-been-discovered." id="toc-misunderstanding-3-a-significant-p-value-means-that-a-practically-important-effect-has-been-discovered."><span class="header-section-number">1.7.3</span> Misunderstanding 3: A significant <em>p</em>-value means that a practically important effect has been discovered.</a></li>
  <li><a href="#sec-misconception4" id="toc-sec-misconception4"><span class="header-section-number">1.7.4</span> Misunderstanding 4: If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%.</a></li>
  <li><a href="#misunderstanding-5-one-minus-the-p-value-is-the-probability-that-the-effect-will-replicate-when-repeated." id="toc-misunderstanding-5-one-minus-the-p-value-is-the-probability-that-the-effect-will-replicate-when-repeated."><span class="header-section-number">1.7.5</span> Misunderstanding 5: One minus the <em>p</em>-value is the probability that the effect will replicate when repeated.</a></li>
  </ul></li>
  <li><a href="#test-yourself" id="toc-test-yourself"><span class="header-section-number">1.8</span> Test Yourself</a>
  <ul>
  <li><a href="#questions-about-which-p-values-you-can-expect" id="toc-questions-about-which-p-values-you-can-expect"><span class="header-section-number">1.8.1</span> Questions about which <em>p</em>-values you can expect</a></li>
  <li><a href="#questions-about-p-value-misconceptions" id="toc-questions-about-p-value-misconceptions"><span class="header-section-number">1.8.2</span> Questions about <em>p</em>-value misconceptions</a></li>
  <li><a href="#open-questions" id="toc-open-questions"><span class="header-section-number">1.8.3</span> Open Questions</a></li>
  </ul></li>
  </ul>
</nav>
<p>Scientists can attempt to answer a wide range of questions by collecting data. One question that interests scientists is whether measurements that have been collected under different conditions differ, or not. The answer to such a question is an <em>ordinal claim</em>, where a researcher states the average of the measurements is larger, or smaller, or the same, when comparing conditions. For example, a researcher might be interested in the hypothesis that students learn better if they do tests, during which they need to retrieve information they have learned (condition A), compared to not getting tests, but spending all of their time studying (condition B). After collecting data, and observing that the mean grade is higher for students who spent part of their time doing tests, the researcher can make the ordinal claim that student performance was <em>better</em> in condition A compared to condition B. Ordinal claims can only be used to state there is a difference between conditions. They do not quantify the <strong>size of the effect</strong>.</p>
<p>To make ordinal claims, researchers typically rely on a methodological procedure known as a <strong>hypothesis test</strong>. One part of a hypothesis test consists of computing a <strong><em>p</em>-value</strong> and examining whether there is a statistically <strong>significant</strong> difference. ‘Significant’ means that something is worthy of attention. A hypothesis test is used to distinguish a signal (that is worth paying attention to) from random noise in empirical data. It is worth distinguishing <strong>statistical significance</strong>, which is only used to claim whether an observed effect is a signal or noise, from <strong>practical significance</strong>, which depends on whether the size of the effect is large enough to have any worthwhile consequences in real life. Researchers use a methodological procedure to decide whether to make an ordinal claim or not as a safeguard against <a href="https://www.youtube.com/watch?v=G1juPBoxBdc">confirmation bias</a>. In an internal report for Guinness brewery on the use of statistical tests in an applied setting, William Gosset (or ‘Student’, who developed the <em>t</em>-test) already wrote <span class="citation" data-cites="gosset_application_1904">(<a href="#ref-gosset_application_1904" role="doc-biblioref">1904</a>)</span>:</p>
<blockquote>
<p>On the other hand, it is generally agreed that to leave the rejection of experiments entirely to the discretion of the experimenter is dangerous, as he is likely to be biassed. Hence it has been proposed to adopt a criterion depending on the probability of such a wide error occurring in the given number of observations.</p>
</blockquote>
<p>Depending on their desires, scientists might be tempted to interpret data as support for their hypothesis, even when it is not. As Benjamini <span class="citation" data-cites="benjamini_its_2016">(<a href="#ref-benjamini_its_2016" role="doc-biblioref">2016</a>)</span> notes, a <em>p</em>-value “offers a first line of defense against being fooled by randomness, separating signal from noise”. There are indications that banning the use of <em>p</em>-values increases the ability of researchers to present erroneous claims. Based on qualitative analyses of scientific articles published after the null hypothesis significance ban in the journal Basic and Applied Social Psychology <span class="citation" data-cites="fricker_assessing_2019">Fricker et al. (<a href="#ref-fricker_assessing_2019" role="doc-biblioref">2019</a>)</span> conclude: “When researchers only employ descriptive statistics we found that they are likely to overinterpret and/or overstate their results compared to a researcher who uses hypothesis testing with the p &lt; 0.05 threshold”. A hypothesis test, when used correctly, controls the amount of time researchers will fool themselves when they make ordinal claims.</p>
<section id="philosophical-approaches-to-p-values" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Philosophical approaches to <em>p</em>-values</h2>
<p>Before we look at how <em>p</em>-values are computed, it is important to examine how they are supposed to help us make ordinal claims when testing hypotheses. The definition of a <em>p</em>-value is the probability of observing the sample data, or more extreme data, assuming the null hypothesis is true. But this definition does not tell us much about how we should interpret a <em>p</em>-value.</p>
<p>The interpretation of a <em>p</em>-value depends on the statistical philosophy one subscribes to. <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a> published ‘Statistical Methods for Research Workers’ in 1925 which popularized the use of <em>p</em>-values. In a Fisherian framework a <em>p</em>-value is interpreted as a descriptive continuous measure of compatibility between the observed data and the null hypothesis <span class="citation" data-cites="greenland_statistical_2016">(<a href="#ref-greenland_statistical_2016" role="doc-biblioref">Greenland et al., 2016</a>)</span>. The compatibility of observed data with the null model falls between 1 (perfectly compatible) and 0 (extremely incompatible), and every individual can interpret the <em>p</em>-value with “statistical thoughtfulness”. According to Fisher <span class="citation" data-cites="fisher_statistical_1956">(<a href="#ref-fisher_statistical_1956" role="doc-biblioref">1956</a>)</span>, <em>p</em>-values “do not generally lead to any probability statement about the real world, but to a rational and well-defined measure of the reluctance to accept the hypotheses they test”. Fisher tried to formalize his philosophy in an approach called ‘fiducial inference’, but this has not received the same widespread adoption of other approaches, such as decision theory, likelihoods, and Bayesian inference. Indeed, Zabell <span class="citation" data-cites="zabell_r_1992">(<a href="#ref-zabell_r_1992" role="doc-biblioref">1992</a>)</span> writes “The fiducial argument stands as Fisher’s one great failure”, although others have expressed the hope that it might be developed into a useful approach in the future <span class="citation" data-cites="schweder_confidence_2016">(<a href="#ref-schweder_confidence_2016" role="doc-biblioref">Schweder &amp; Hjort, 2016</a>)</span>. A Fisherian <em>p</em>-value describes the incompatibility of the data with a single hypothesis, and is known as <em>significance testing</em>. The main reason a <em>significance test</em> is limited is because researchers only specify a null hypothesis (<span class="math inline">\(H_0\)</span>), but not the alternative hypothesis (<span class="math inline">\(H_1\)</span>).</p>
<p>Neyman and Pearson <span class="citation" data-cites="neyman_problem_1933">(<a href="#ref-neyman_problem_1933" role="doc-biblioref">1933</a>)</span> built on insights about <em>p</em>-values by William Gosset and Ronald Fisher, and developed an approach called <em>statistical hypothesis testing</em>. The main difference with the significance testing approach developed by Fisher was that in a statistical hypothesis test both a null hypothesis and an alternative hypothesis is specified. In a Neyman-Pearson framework, the goal of statistical tests is to guide the behavior of researchers with respect to these two hypotheses. Based on the results of a statistical test, and without ever knowing whether the hypothesis is true or not, researchers choose to tentatively act as if the null hypothesis or the alternative hypothesis is true. In psychology, researchers often use an imperfect hybrid of the Fisherian and Neyman-Pearson frameworks, but the Neyman-Pearson approach is, according to Dienes <span class="citation" data-cites="dienes_understanding_2008">(<a href="#ref-dienes_understanding_2008" role="doc-biblioref">2008</a>)</span> “the logic underlying all the statistics you see in the professional journals of psychology”.</p>
<p>When a Neyman-Pearson hypothesis test is performed, the observed <em>p</em>-value is only used to check if it is smaller than the chosen alpha level, but it does not matter how much smaller it is. For example, if an alpha level of 0.01 is used, both a <em>p</em> = 0.006 and a <em>p</em> = 0.000001 will lead researchers to decide to act as if the state of the world is best described by the alternative hypothesis. This differs from a Fisherian approach to <em>p</em>-values, where the lower the <em>p</em>-value, the greater the psychological reluctance of a researcher to accept the null hypothesis they are testing. A Neyman-Pearson hypothesis test does not see the goal of an inference as quantifying a continuous measure of compatibility or evidence. Instead, as Neyman <span class="citation" data-cites="neyman_inductive_1957">(<a href="#ref-neyman_inductive_1957" role="doc-biblioref">1957</a>)</span> writes:</p>
<blockquote>
<p>The content of the concept of inductive behavior is the recognition that the purpose of every piece of serious research is to provide grounds for the selection of one of several contemplated courses of action.</p>
</blockquote>
<p>Intuitively, one might feel that decisions about how to act should not be based on the results of a single statistical test, and this point is often raised as a criticism of a Neyman-Pearson approach to statistical inferences. However, such criticisms rarely use the same definition of an ‘act’ as Neyman used. It is true that, for example, the decision to implement a new government policy should not be based on a single study result. However, Neyman considered making a scientific claim an ‘act’ as well, and wrote (1957, p. 10) that the concluding phase of a study involves:</p>
<blockquote>
<p>an act of will or a decision to take a particular action, perhaps to assume a particular attitude towards the various sets of hypotheses</p>
</blockquote>
<p>Cox <span class="citation" data-cites="cox_problems_1958">(<a href="#ref-cox_problems_1958" role="doc-biblioref">1958</a>)</span> writes:</p>
<blockquote>
<p>it might be argued that in making an inference we are ‘deciding’ to make a statement of a certain type about the populations and that therefore, provided that the word decision is not interpreted too narrowly, the study of statistical decisions embraces that of inference. The point here is that one of the main general problems of statistical inference consists in deciding what types of statement can usefully be made and exactly what they mean.</p>
</blockquote>
<p>Thus, in a Neyman-Pearson approach, <em>p</em>-values form the basis of decisions about which claims to make. In science, such claims underlie most novel experiments in the form of <strong>auxiliary hypotheses</strong>, or the assumptions about underlying hypotheses that are assumed to be accurate in order for a test to work as planned <span class="citation" data-cites="hempel_philosophy_1966">(<a href="#ref-hempel_philosophy_1966" role="doc-biblioref">Hempel, 1966</a>)</span>. For example, if it is important that participants can see color in a planned experiment, we assume it is true that the <a href="https://en.wikipedia.org/wiki/Ishihara_test">Ishihara test</a> successfully identifies which participants are colorblind.</p>
</section>
<section id="creating-a-null-model" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Creating a null model</h2>
<p>Assume I ask two groups of 10 people how much they liked the extended directors cut of the Lord of the Rings (LOTR) trilogy. This means our <strong>total sample size</strong> (<em>N</em>) is 20, and the sample size in each group (<em>n</em>) is 10. The first group consists of my friends, and the second groups consists of friends of my wife. Our friends rate the trilogy on a score from 1 to 10. We can calculate the average rating by my friends, which is 8.7, and the average rating by my wife’s friends, which is 7.7. We can compare the scores in both groups by looking at the raw data, and by plotting the data.</p>
<div class="cell" data-layout-align="center" data-tab.cap="Ratings for the Lord of the Rings extended trilogy by two groups of friends.">
<div class="cell-output-display">
<table class="table table-striped" data-quarto-postprocess="true" style="width: auto !important; ">
<caption>Ratings for the Lord of the Rings extended trilogy by two groups of friends.</caption>
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Friends Daniel</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">Friends Kyra</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">friend_1</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
</tr>
<tr class="even">
<td style="text-align: left;">friend_2</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">friend_3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">friend_4</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">friend_5</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">friend_6</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">friend_7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">friend_8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">friend_9</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">friend_10</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">7</td>
</tr>
</tbody>
</table>


</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" style="width:100.0%" /></p>
</figure>
</div>
</div>
</div>
<p>We can see the groups overlap but the mean ratings differ by 1 whole point. The question we are now faced with is the following: Is the difference between the two groups just random variation, or can we claim that my friends like the extended directors cut of the Lord of the Rings (LOTR) trilogy more than my wife’s friends?</p>
<p>In a <strong>null hypothesis significance test</strong> we try to answer this question by calculating the probability of the observed difference (in this case, a mean difference of 1) or a more extreme difference, under the assumption that there is no real difference between how much my friends and my wife’s friends like the extended directors cut of LOTR, and we are just looking at random noise. This probability is called the <em>p</em>-value. If this probability is low enough, we decide to claim there is a difference. If this probability is not low enough, we refrain from making a claim about a difference.</p>
<p>The null hypothesis assumes that if we would ask an infinite number of my friends and an infinite number of my wife’s friends how much they like LOTR, the difference between these huge groups is exactly 0. However, in any sample drawn from the population, random variation is very likely to lead to a difference somewhat larger or smaller than 0. We can create a <strong>null model</strong> that quantifies the expected variation in the observed data, just due to random noise, to tell us what constitutes a reasonable expectation about how much the differences between groups can vary if there is no difference in the population.</p>
<p>It is practical to create a null model in terms of a <strong>standardized</strong> distribution, as this makes it easier to calculate the probability that specific values will occur, regardless of the scale that is used to collect the measurements. One version of a null model for differences is the <em>t</em>-distribution, which can be used to describe which differences should be expected when drawing samples from a population. Such a null model is built on <strong>assumptions</strong>. In the case of the <em>t</em>-distribution, the assumption is that scores are normally distributed. In reality, the assumptions upon which statistical methods are built are never met perfectly, which is why statisticians examine the impact of violations of assumptions on methodological procedures. Statistical tests are still useful in practice when the impact of violations on statistical inferences is small enough.</p>
<p>We can quantify the distribution of <em>t</em>-values that is expected when there is no difference in the population by a <em>probability density function</em>. Below is a plot of the probability density function for a <em>t</em>-distribution with 18 <strong>degrees of freedom</strong> (df), which corresponds to our example where we collect data from 20 friends (df = N - 2 for two independent groups). For a continuous distribution, where probabilities are defined for an infinite number of points, the probability of observing any single point (e.g., <em>t</em> = 2.5) is always zero. Probabilities are measured over intervals. For this reason, when a <em>p</em>-value is computed, it is not defined as ‘the probability of observing the data’, but as ‘the probability of observing the data, <em>or more extreme data</em>’. This creates an interval (a tail of a distribution) for which a probability can be calculated.</p>
</section>
<section id="calculating-a-p-value" class="level2" data-number="1.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Calculating a <em>p</em>-value</h2>
<p>A <em>t</em>-value can be computed from the mean in the sample, the mean in the population, the standard deviation in the sample, and the sample size. By then computing the probability of observing a <em>t</em>-value as extreme or more extreme as the one observed, we get a <em>p</em>-value. For the comparison of the movie ratings for the two groups of friends above, performing a two-sided Student’s <em>t</em>-test yields a <em>t</em>-value of 2.5175 and a <em>p</em>-value of 0.02151.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">t.test</span>(df_long<span class="sc">$</span>rating <span class="sc">~</span> df_long<span class="sc">$</span><span class="st">`</span><span class="at">Friend Group</span><span class="st">`</span>, <span class="at">var.equal =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Two Sample t-test

data:  df_long$rating by df_long$`Friend Group`
t = 2.5175, df = 18, p-value = 0.02151
alternative hypothesis: true difference in means between group Friends Daniel and group Friends Kyra is not equal to 0
95 percent confidence interval:
 0.1654875 1.8345125
sample estimates:
mean in group Friends Daniel   mean in group Friends Kyra 
                         8.7                          7.7 </code></pre>
</div>
</div>
<p>We can graph the <em>t</em>-distribution (for df = 18) and highlight the two tail areas that start at the <em>t</em>-values of 2.5175 and -2.5175.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-tdist" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-tdist-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.1: A <em>t</em>-distribution with 18 degrees of freedom.</figcaption></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-whichpexpect" class="level2" data-number="1.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Which <em>p</em>-values can you expect?</h2>
<p>In a very educational video about the ‘<a href="https://www.youtube.com/watch?v=5OL1RqHrZQ8">Dance of the <em>p</em>-values</a>’, Geoff Cumming explains that <em>p</em>-values vary from experiment to experiment. However, this is not a reason to ‘not trust p’ as he mentions in the video. Instead, it is important to clearly understand <strong><em>p</em>-value distributions</strong> to prevent misconceptions. Because <em>p</em>-values are part of frequentist statistics, we need to examine what we can expect <em>in the long run</em>. Because we never do the same experiment hundreds of times, and we do only a very limited number of studies in our lifetime, the best way to learn about what we should expect in the long run is through computer simulations.</p>
<p>Take a moment to try to answer the following two questions. Which <em>p</em>-values can you expect to observe if there is a true effect, and you repeat the same study one-hundred thousand times? And which <em>p</em>-values can you expect if there is no true effect, and you repeat the same study one-hundred thousand times? If you don’t know the answer, don’t worry - you will learn it now. But if you don’t know the answer, it is worth reflecting on why you don’t know the answer about such an essential aspect of <em>p</em>-values. If you are like me, you were simply never taught this. But as we will see, it is essential to a solid understanding of how to interpret <em>p</em>-values.</p>
<p>Which <em>p</em>-values you can expect is completely determined by the statistical power of the study, or the probability that you will observe a significant effect, if there is a true effect. The statistical power ranges from 0 to 1. We can illustrate this by simulating independent <em>t</em>-tests. The idea is that we simulate IQ scores for a group of people. We know the standard deviation of IQ scores is 15. For now, we will set the mean IQ score in one simulated group to 100, and in the other simulated group to 105. We are testing if the people in one group have an IQ that differs from the other group (and we know the correct answer is ‘yes’, because we made it so in the simulation).</p>
<div class="cell" data-layout-align="center" data-hash="01-pvalue_cache/html/unnamed-chunk-5_c76d311b541d744aed05cd28d1318f20">
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>p <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">100000</span>) <span class="co"># store all simulated *p*-values</span></span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100000</span>) { <span class="co"># for each simulated experiment</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>  x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">71</span>, <span class="at">mean =</span> <span class="dv">100</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co"># Simulate data</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>  y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">71</span>, <span class="at">mean =</span> <span class="dv">105</span>, <span class="at">sd =</span> <span class="dv">15</span>) <span class="co"># Simulate data</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>  p[i] <span class="ot">&lt;-</span> <span class="fu">t.test</span>(x, y)<span class="sc">$</span>p.value <span class="co"># store the *p*-value</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>}</span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a>(<span class="fu">sum</span>(p <span class="sc">&lt;</span> <span class="fl">0.05</span>) <span class="sc">/</span> <span class="dv">100000</span>) <span class="co"># compute power</span></span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="fu">hist</span>(p, <span class="at">breaks =</span> <span class="dv">20</span>) <span class="co"># plot a histogram</span></span></code></pre></div>
</div>
<p>In the simulation, we generate n = 71 normally distributed IQ scores with means of M (100 and 105 by default) and a standard deviation of 15. We then perform an independent <em>t</em>-test, store the <em>p</em>-value, and generate a plot of the <em>p</em>-value distribution.</p>
<div class="cell" data-layout-align="center" data-hash="01-pvalue_cache/html/fig-pdistr1_9dc4eeeed6409a8931bfd47768497257">
<div class="cell-output-display">
<div id="fig-pdistr1" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-pdistr1-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.2: Distribution of <em>p</em>-values when power = 50%.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>On the x-axis we see <em>p</em>-values from 0 to 1 in 20 bars, and on the y-axis we see how frequently these <em>p</em>-values were observed. There is a horizontal red dotted line that indicates an alpha of 5% (located at a frequency of 100.000*0.05 = 5000) – but you can ignore this line for now. In the title of the graph, the statistical power that is achieved in the simulated studies is given (assuming an alpha of 0.05): The studies have 50% power.</p>
<p>The simulation result illustrates the <strong>probability density function</strong> of <em>p</em>-values. A probability density function provides the probability that a random variable has a specific value (such as <a href="#fig-tdist">Figure <span class="quarto-unresolved-ref">fig-tdist</span></a>) of the <em>t</em>-distribution). Because the <em>p</em>-value is a random variable, we can use its probability density function to plot the <em>p</em>-value distribution <span class="citation" data-cites="hung_behavior_1997 ulrich_properties_2018">(<a href="#ref-hung_behavior_1997" role="doc-biblioref">Hung et al., 1997</a>; <a href="#ref-ulrich_properties_2018" role="doc-biblioref">Ulrich &amp; Miller, 2018</a>)</span>, as in <a href="#fig-pdft">Figure <span class="quarto-unresolved-ref">fig-pdft</span></a>). In <a href="http://shiny.ieis.tue.nl/d_p_power/">this online Shiny app</a> you can vary the sample size, effect size, and alpha level to examine the effect on the <em>p</em>-value distribution. Increasing the sample size or the effect size will increase the steepness of the <em>p</em>-value distribution, which means that the probability to observe small <em>p</em>-values increases. The <em>p</em>-value distribution is a function of the statistical power of the test.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pdft" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-pdft-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.3: Probability density function for <em>p</em>-values from a two-sided <em>t</em>-test.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>When there is no true effect, <em>p</em>-values are <strong>uniformly distributed</strong>. This means that every <em>p</em>-value is equally likely to be observed when the null hypothesis is true. In other words, when there is no true effect, a <em>p</em>-value of 0.08 is just as likely as a <em>p</em>-value of 0.98. I remember thinking this was very counterintuitive when I first learned about uniform <em>p</em>-value distributions (well after completing my PhD). But it makes sense that <em>p</em>-values are unifromly distributed when we think about the goal to guarantee that when <span class="math inline">\(H_0\)</span> is true, alpha % of the <em>p</em>-values should fall below the alpha level. If we set alpha to 0.01, 1% of the observed <em>p</em>-values should fall below 0.01, and if we set alpha to 0.12, 12% of the observed <em>p</em>-values should fall below 0.12. This can only happen if <em>p</em>-values are uniformly distributed when the null hypothesis is true.</p>
<div class="cell" data-layout-align="center" data-hash="01-pvalue_cache/html/fig-pdistr2_62337b60f1119189c14c16363c2782fd">
<div class="cell-output-display">
<div id="fig-pdistr2" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-pdistr2-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.4: Distribution of <em>p</em>-values when the null hypothesis is true.</figcaption></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-lindley" class="level2" data-number="1.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Lindley’s paradox</h2>
<p>As the statistical power increases, some <em>p</em>-values below 0.05 (e.g., <em>p</em> = 0.04) can be more likely when there is <em>no</em> effect than when there <em>is</em> an effect. This is known as Lindley’s paradox <span class="citation" data-cites="lindley_statistical_1957">(<a href="#ref-lindley_statistical_1957" role="doc-biblioref">Lindley, 1957</a>)</span>, or sometimes the Jeffreys-Lindley paradox <span class="citation" data-cites="spanos_who_2013">(<a href="#ref-spanos_who_2013" role="doc-biblioref">Spanos, 2013</a>)</span>. Because the distribution of <em>p</em>-values is a function of the statistical power <span class="citation" data-cites="cumming_replication_2008">(<a href="#ref-cumming_replication_2008" role="doc-biblioref">Cumming, 2008</a>)</span>, the higher the power, the more right-skewed the <em>p</em>-value distribution becomes (i.e., the more likely it becomes that small <em>p</em>-values are observed). When there is no true effect, <em>p</em>-values are uniformly distributed, and 1% of observed <em>p</em>-values fall between 0.04 and 0.05. When the statistical power is extremely high, not only will most <em>p</em>-values fall below 0.05, most <em>p</em>-values will fall below 0.01. In <a href="#fig-paradox">Figure <span class="quarto-unresolved-ref">fig-paradox</span></a>) we see that with high power very small <em>p</em>-values (e.g., 0.001) are more likely to be observed when there <em>is</em> an effect than when there is <em>no</em> effect (e.g., the dotted black curve representing 99% power falls above the grey horizontal line representing the uniform distribution when the null is true for a <em>p</em>-value of 0.01).</p>
<p>Yet perhaps surprisingly, observing a <em>p</em>-value of 0.04 is more likely when the null hypothesis (<span class="math inline">\(H_0\)</span>) is true than when the alternative hypothesis (<span class="math inline">\(H_1\)</span>) is true and we have very high power, as illustrated by the fact that in <a href="#fig-paradox">Figure <span class="quarto-unresolved-ref">fig-paradox</span></a>) the density of the <em>p</em>-value distribution is higher when the null is true, than when a test has 99% power, at 0.04. Lindley’s paradox shows that a <em>p</em>-value of for example 0.04 can be statistically significant, but at the same time provides evidence for the null hypothesis. From a Neyman-Pearson approach we have made a claim that has a maximum error rate of 5%, but from a likelihood or Bayesian approach, we should conclude our data provides evidence in favor of the null hypothesis, relative to the alternative hypothesis. Lindley’s paradox illustrates when different statistical philosophies would reach different conclusions, and why a <em>p</em>-value cannot directly be interpreted as a measure of evidence, without taking the power of the test into account. Although it is not necessary, researchers might desire to prevent situations where a frequentist rejects the null hypothesis based on <em>p</em> &lt; 0.05, when the evidence in the test favors the null hypothesis over the alternative hypothesis. This can be achieved by lowering the alpha level as a function of the sample size <span class="citation" data-cites="leamer_specification_1978 maier_justify_2022 good_bayesnon-bayes_1992">(<a href="#ref-good_bayesnon-bayes_1992" role="doc-biblioref">Good, 1992</a>; <a href="#ref-leamer_specification_1978" role="doc-biblioref">Leamer, 1978</a>; <a href="#ref-maier_justify_2022" role="doc-biblioref">Maier &amp; Lakens, 2022</a>)</span>, as explained in the chapter on <a href="#sec-errorcontrol">error control</a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-paradox" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-paradox-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.5: <em>P</em>-value distribution for 0 (grey horizontal line, 50 percent power (black solid curve), and 99 percent power (black dotted curve, where <em>p</em>-values just below 0.05 are more likely when <span class="math inline">\(H_0\)</span> is true than when <span class="math inline">\(H_1\)</span> is true).</figcaption></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-correctlyinterpreting" class="level2" data-number="1.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Correctly reporting and interpreting <em>p</em>-values</h2>
<p>Although from a strict Neyman-Pearson perspective it is sufficient to report that <em>p</em> &lt; <span class="math inline">\(\alpha\)</span> or that <em>p</em> &gt; <span class="math inline">\(\alpha\)</span>, researchers should report exact <em>p</em>-values. This facilitates the re-use of results for secondary analyses <span class="citation" data-cites="appelbaum_journal_2018">(<a href="#ref-appelbaum_journal_2018" role="doc-biblioref">Appelbaum et al., 2018</a>)</span>, and allows other researchers to compare the <em>p</em>-value to an alpha level they would have preferred to use <span class="citation" data-cites="lehmann_testing_2005">(<a href="#ref-lehmann_testing_2005" role="doc-biblioref">Lehmann &amp; Romano, 2005</a>)</span>. Because claims are made using a methodological procedure with known maximum error rates, a <em>p</em>-value never allows you state anything with certainty. Even if we set the alpha level to 0.000001 any single claim can be an error, Fisher <span class="citation" data-cites="fisher_design_1935">(<a href="#ref-fisher_design_1935" role="doc-biblioref">1935</a>)</span> reminds us, ‘for the “one chance in a million” will undoubtedly occur, with no less and no more than its appropriate frequency, however surprised we may be that it should occur to <em>us</em>”. This is the reason that <strong>replication studies</strong> are important in science. Any single finding could be a fluke, but this probability quickly becomes very small if several replication studies observe the same finding. This uncertainty is sometimes not reflected in academic writing, where researchers can be seen using words as ’prove’, ‘show’, or ‘it is known’. A slightly longer but more accurate statement after a hypothesis test would read:</p>
<blockquote>
<p>We claim there is a/no meaningful effect, while acknowledging that if scientists make claims using this methodological procedure, they will be misled, in the long run, at most alpha % or beta % of the time, which we deem acceptable. We will for the foreseeable future, until new data or information emerges that proves us wrong, assume this claim is correct.</p>
</blockquote>
<p>Remember that in a Neyman-Pearson framework researchers make claims, but do not necessarily <em>believe</em> in the truth of these claims. For example, the OPERA collaboration reported in 2011 that they had observed data that seemed to suggest neutrinos traveled faster than the speed of light. This claim was made with a with a 0.2-in-a-million Type 1 error rate, <em>assuming the error was purely due to random noise</em>. However, none of the researchers actually believed this claim was true, because it is theoretically impossible for neutrinos to move faster than the speed of light. Indeed, it was later confirmed that equipment failures were the cause of the anomalous data: a fiber optic cable had been attached improperly, and a clock oscillator was ticking too fast. Nevertheless, the claim was made with the explicit invitation to the scientific community to provide new data or information that would prove this claim wrong.</p>
<p>When researchers “accept” or “reject” a hypothesis in a Neyman-Pearson approach to statistical inferences, they do not communicate any belief or conclusion about the substantive hypothesis. Instead, they utter a Popperian <strong>basic statement</strong> based on a prespecified decision rule that the observed data reflect a certain state of the world. Basic statements describe an observation that has been made (e.g., “I have observed a black swan”) or an event that has occurred (e.g., “students performed better at the exam when being trained in spaced practice, than when not”).</p>
<p>The claim is about the data we have observed, but not about the theory we used to make predictions. The claim is about observed data, as it is a statistical inference, and not about the theory, which requires a theoretical inference. Data never ‘proves’ a theory is true or false. A basic statement can <strong>corroborate</strong> a prediction derived from a theory, or not. If many predictions deduced from a theory are corroborated, we can become increasingly convinced the theory is close to the truth. This ‘truth-likeness’ of theories is called <strong>verisimilitude</strong> <span class="citation" data-cites="niiniluoto_verisimilitude_1998 popper_logic_2002">(<a href="#ref-niiniluoto_verisimilitude_1998" role="doc-biblioref">Niiniluoto, 1998</a>; <a href="#ref-popper_logic_2002" role="doc-biblioref">Popper, 2002</a>)</span>. A shorter statement when a hypothesis test is presented would therefore read ‘p = .xx, which corroborates our prediction, at an alpha level of y%’, or ‘p = .xx, which does not corroborate our prediction, at a statistical power of y% for our effect size of interest’. Often, the alpha level or the statistical power is only mentioned in the experimental design section of an article, but repeating them in the results section might remind readers of the error rates associated with your claims.</p>
<p>Even when we have made correct claims, the underlying theory can be false. Popper <span class="citation" data-cites="popper_logic_2002">(<a href="#ref-popper_logic_2002" role="doc-biblioref">2002</a>)</span> reminds us that “The empirical basis of objective science has thus nothing ‘absolute’ basis about it”. He argues science is not built on a solid bedrock, but on piles driven in a swamp and notes that “We simply stop when we are satisfied that the piles are firm enough to carry the structure, at least for the time being.” As Hacking <span class="citation" data-cites="hacking_logic_1965">(<a href="#ref-hacking_logic_1965" role="doc-biblioref">1965</a>)</span> writes: “Rejection is not refutation. Plenty of rejections must be only tentative.” So when we reject the null model, we do so tentatively, aware of the fact we might have done so in error, without necessarily believing the null model is false, and without believing the theory we have used to make predictions is true. For Neyman <span class="citation" data-cites="neyman_inductive_1957">(<a href="#ref-neyman_inductive_1957" role="doc-biblioref">1957</a>)</span> inferential behavior is an: “act of will to behave in the future (perhaps until new experiments are performed) in a particular manner, conforming with the outcome of the experiment”. All knowledge in science is provisional.</p>
<p>Some statisticians recommend interpreting <em>p</em>-values as measures of <em>evidence</em>. For example, Bland <span class="citation" data-cites="bland_introduction_2015">(<a href="#ref-bland_introduction_2015" role="doc-biblioref">2015</a>)</span> teaches that <em>p</em>-values can be interpreted as a ‘rough and ready’ guide for the strength of evidence, and that <em>p</em> &gt; 0.1 indicates ‘little or no evidence’, 0.01 &lt; <em>p</em> &lt; 0.05 indicates ‘evidence’, <em>p</em> &lt; 0.001 is ‘very strong evidence’. This is incorrect <span class="citation" data-cites="johansson_hail_2011 lakens_why_2022">(<a href="#ref-johansson_hail_2011" role="doc-biblioref">Johansson, 2011</a>; <a href="#ref-lakens_why_2022" role="doc-biblioref">Lakens, 2022</a>)</span>, as is clear from the previous discussions of Lindley’s paradox and uniform <em>p</em>-value distributions. If you want to quantify <em>evidence</em>, see the chapters on <a href="#sec-likelihoods">likelihoods</a> or <a href="#sec-bayes">Bayesian statistics</a>.</p>
</section>
<section id="sec-misconceptions" class="level2" data-number="1.7">
<h2 data-number="1.7"><span class="header-section-number">1.7</span> Preventing common misconceptions about <em>p</em>-values</h2>
<p>A <em>p</em>-value is the probability of the observed data, or more extreme data, under the assumption that the null hypothesis is true. To understand what this means, it might be especially useful to know what this doesn’t mean. First, we need to know what ‘the assumption that the null hypothesis is true’ looks like, and which data we should expect if the null hypothesis is true. Although the null hypothesis can be any value, in this assignment we will assume the null hypothesis is specified as a mean difference of 0. For example, we might be interested in calculating the difference between a control condition and an experimental condition on a dependent variable.</p>
<p>It is useful to distinguish the null hypothesis (the prediction that the mean difference in the population is exactly 0) and the null model (a model of the data we should expect when we collect data when the null hypothesis is true). The null hypothesis is a point at 0, but the null model is a distribution. It is visualized in textbooks or power analysis software using pictures as you can see below, with <em>t</em>-values on the horizontal axis, and a critical <em>t</em>-value somewhere between 1.96 – 2.00 (depending on the sample size). This is done because the statistical test when comparing two groups is based on the <em>t</em>-distribution, and the <em>p</em>-value is statistically significant when the <em>t</em>-value is larger than a critical <em>t</em>-value.</p>
<p>I personally find things become a lot clearer if you plot the null model as mean differences instead of <em>t</em>-values. So below, you can see a null model for the mean differences we can expect when comparing two groups of 50 observations where the true difference between the two groups is 0, and the standard deviation in each group is 1. Because the standard deviation is 1, you can also interpret the mean differences as a Cohen’s <em>d</em> effect size. So this is also the distribution you can expect for a Cohen’s <em>d</em> of 0, when collecting 50 observations per group in an independent <em>t</em>-test.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fig131" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-fig131-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.6: Distribution of observed Cohen’s <em>d</em> effect sizes when collecting 50 observations per group in an independent <em>t</em>-test.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>The first thing to notice is that we expect that the mean of the null model is 0. Looking at the x-axis, we see the plotted distribution is centered on 0. But even if the mean difference in the population is 0 that does not imply every sample we draw from the population will give a mean difference of exactly zero. There is variation around the population value, as a function of the standard deviation and the sample size.</p>
<p>The y-axis of the graph represents the density, which provides an indication of the relative likelihood of measuring a particular value of a continuous distribution. We can see that the most likely mean difference is the true population value of zero, and that larger differences from zero become increasingly less likely. The graph has two areas that are colored red. These areas represent 2.5% of the most extreme values in the left tail of the distribution, and 2.5% of the most extreme values in the right tail of the distribution. Together, they make up 5% of the most extreme mean differences we would expect to observe, given our number of observations, when the true mean difference is exactly 0. When a mean difference in the red area is observed, the corresponding statistical test will be statistically significant at a 5% alpha level. In other words, not more than 5% of the observed mean differences will be far enough away from 0 to be considered surprising. Because the null hypothesis is true, observing a ‘surprising’ mean difference in the red areas is a Type 1 error.</p>
<p>Let’s assume that the null model in the Figure above is true, and that we observe a mean difference of 0.5 between the two groups. This observed difference falls in the red area in the right tail of the distribution. This means that the observed mean difference is relatively surprising, under the assumption that the true mean difference is 0. If the true mean difference is 0, the probability density functions shows that we should not expect a mean difference of 0.5 very often. If we calculate a <em>p</em>-value for this observation, it would be lower than 5%. The probability of observing a mean difference that is at least as far away from 0 as 0.5 (either to the left from the mean, or to the right, when we do a two-tailed test) is less than 5%.</p>
<p>One reason why I prefer to plot the null model in raw scores instead of <em>t</em>-values is that you can see how the null model changes when the sample size increases. When we collect 5000 instead of 50 observations, we see the null model is still centered on 0 – but in our null model we now expect most values will fall very close around 0.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fig132" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-fig132-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.7: Distribution of observed Cohen’s <em>d</em> effect sizes when collecting 5000 observations per group in an independent <em>t</em>-test when <em>d</em> = 0.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>The distribution is much narrower because the distribution of mean differences is based on the standard error of the difference between means. This value is calculated based on the standard deviation and the sample size, as follows:</p>
<p><span class="math display">\[\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}\]</span></p>
<p>This formula shows that the standard deviations of each group (σ) are squared and divided by the sample size of that group, added together, after which the square root is taken. The larger the sample size the bigger the number we divide by, and thus the smaller standard error of the difference between means. In our n = 50 example this is:</p>
<p><span class="math display">\[\sqrt{\frac{1^{2}}{50}+\frac{1^{2}}{50}}\]</span></p>
<p>The standard error of the differences between means is thus 0.2 for n = 50 in each group, and for n = 5000 it is 0.02. Assuming a normal distribution, 95% of the observations fall between 1.96 SE. So for 50 samples per group, the mean differences should fall between -1.96 * 0.2 = -0.392, and +1.96 * 0.2 = 0.392, and we can see the red areas start from approximately -0.392 to 0.392 for n = 50. For 5000 samples per group, the mean differences should fall between -1.96 * 0.02, and +1.96 * 0.02; in other words between -0.0392 to 0.0392 for n = 5000. Due to the larger sample size with n = 5000 observations per group, we should expect to observe mean differences in our sample closer to 0 compared to our null model when we had only 50 observations.</p>
<p>If we collected n = 5000, and we would again observe a mean difference of 0.5, it should be clear that this same difference is even more surprising than it was when we collected 50 observations. We are now almost ready to address common misconceptions about <em>p</em>-values, but before we can do this, we need to introduce a model of the data when the null is not true. If we are not sampling data from a model where the true mean difference is 0, what does our alternative model look like? Some software (such as G*power, see <a href="#fig-gpowerscreenshot">Figure <span class="quarto-unresolved-ref">fig-gpowerscreenshot</span></a>)) will visualize both the null model (red curve) and the alternative model (blue curve) in their output:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gpowerscreenshot" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="images/1.3.3.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.8: Screenshot from G*Power software visualizing the null model (red distribution) and alternative model (blue distribution) and the critical <em>t</em>-value (1.66055) that is the threshold distinguishing significant and non-significant results.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>When we do a study, we rarely already know what the true mean difference is (if we already knew, why would we do the study?). But let’s assume there is an all-knowing entity. Following Paul Meehl, we will call this all-knowing entity ‘Omniscient Jones’. Before we collect our sample of 50 observations, Omniscient Jones already knows that the true mean difference in the population is 0.5. Again, we should expect some variation around 0.5 in this alternative model. The figure below shows the expected data pattern when the null hypothesis is true (now indicated by a grey line) and it shows an alternative model, assuming a true mean difference of 0.5 exists in the population (indicated by a black line).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fig134" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-fig134-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.9: Distribution of observed Cohen’s <em>d</em> effect sizes when collecting 50 observations per group in an independent <em>t</em>-test when <em>d</em> = 0.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>But Omniscient Jones could have said the true difference was much larger. Let’s assume we do another study, but now before we collect our 50 observations, Omniscient Jones tells us that the true mean difference is 1.5. The null model does not change, but the alternative model now moves over to the right.</p>
<p>You can play around with the alternative and null models in this online app: http://shiny.ieis.tue.nl/d_p_power/. The app allows you to specify the sample size in each group of an independent <em>t</em>-test (from 2 to infinity), the mean difference (from 0 to 2), and the alpha level. In the plot, the red areas visualize Type 1 errors. The blue area visualizes the Type 2 error rate (which we will discuss below). The app also tells you the critical value: There is a vertical line (with n = 50 this line falls at a mean difference of 0.4) and a sentence that says: “Effects larger than 0.4 will be statistically significant”. Note that the same is true for effects smaller than -0.4, even though there is no second label there, but the app shows the situation for a two-sided independent <em>t</em>-test.</p>
<p>You can see that on the left of the vertical line that indicates the critical mean difference there is a blue area that is part of the alternative model. This is the Type 2 error rate (or 1 - the power of the study). If a study has 80% power, 80% of the mean differences we will observe should fall on the right of the critical value indicated by the line. If the alternative model is true, but we observe an effect smaller than the critical value, the observed <em>p</em>-value will be larger than 0.05, even when there is a true effect. You can check in the app that the larger the sample size, the further to the right the entire alternative distribution falls, and thus the higher the power. You can also see that the larger the sample size, the narrower the distribution, and the less of the distribution will fall below the critical value (as long as the true population mean is larger than the critical value). Finally, the larger the alpha level, the further to the left the critical mean difference moves, and the smaller the area of the alternative distribution that falls below the critical value.</p>
<p>The app also plots 3 graphs that illustrate the power curves as a function of different alpha levels, sample sizes, or true mean differences. Play around in the app by changing the values. Get a feel for how each variable impacts the null and alternative models, the mean difference that will be statistically significant, and the Type 1 and Type 2 error rates.</p>
<p>So far, several aspects of null models should have become clear. First of all, the population value in a traditional null hypothesis is a value of 0, but in any sample you draw, the observed difference falls in a distribution centered on 0, and will thus most often be slightly larger or smaller than 0. Second, the width of this distribution depends on the sample size and the standard deviation. The larger the sample size in the study, the narrower the distribution will be around 0. Finally, when a mean difference is observed that falls in the tails of the null model, this can be considered surprising. The further away from the null value, the more surprising this result is. But when the null model is true, these surprising values will happen with a probability specified by the alpha level (and are called Type 1 errors). Remember that a Type 1 error occurs when a researcher concludes there is a difference in the population, while the true mean difference in the population is zero.</p>
<p>We are now finally ready to address some common misconceptions about <em>p</em>-values. Let’s go through a list of common misconceptions that have been reported in the scientific literature. Some of these examples might sound like semantics. It is easy to at first glance think that the statement communicates the right idea, even if the written version is not formally correct. However, when a statement is not formally correct, it is wrong. And exactly because people so often misunderstand <em>p</em>-values, it is worth it to be formally correct about how they should be interpreted.</p>
<section id="sec-misconception1" class="level3" data-number="1.7.1">
<h3 data-number="1.7.1"><span class="header-section-number">1.7.1</span> Misunderstanding 1: A non-significant <em>p</em>-value means that the null hypothesis is true.</h3>
<p>A common version of this misconception is reading a sentence such as ‘because <em>p</em> &gt; 0.05 we can conclude that there is no effect’. Another version of such a sentence is ‘there was no difference, (<em>p</em> &gt; 0.05)’.</p>
<p>Before we look at this misconception in some detail, I want to remind you of one fact that is easy to remember, and will enable you to recognize many misconceptions about <em>p</em>-values: <em>p</em>-values are a statement about the probability of data, not a statement about the probability of a hypothesis or the probability of a theory. Whenever you see <em>p</em>-values interpreted as a probability of a theory or a hypothesis, you know something is not right. Examples of statements about a hypothesis are ‘The null hypothesis is true’, or ‘The alternative hypothesis is true’, because both these statements say that the probability that the null or alternative model is true is 100%. A subtler version is a statement such as ‘the observed difference is not due to chance’. The observed difference is only ‘due to chance’ (instead of due to the presence of a real difference) when the null hypothesis is true, and as before, this statement implies it is 100% probable that the null hypothesis is true.</p>
<p>When you conclude that ‘there is no effect’ or that ‘there is no difference’ you are similarly claiming that it is 100% probable that the null hypothesis is true. But since <em>p</em>-values are statements about the probability of data, you should refrain from making statements about the probability of a theory solely based on a <em>p</em>-value. That’s ok. <em>p</em>-values were designed to help you identify surprising results from a noisy data generation process (aka the real world). They were not designed to quantify the probability that a hypothesis is true.</p>
<p>Let’s take a concrete example that will illustrate why a non-significant result does not mean that the null hypothesis is true. In the figure below, Omniscient Jones tells us the true mean difference is 0.5. We can see this, because the alternative distribution which visualizes the probability of the mean differences we should expect when the alternative hypothesis is true is centered on 0.5. We have observed a mean difference of 0.35. This value is not extreme enough to be statistically different from 0. We can see this, because the value does not fall within the red area of the null model (and hence, the <em>p</em>-value is not smaller than our alpha level).</p>
<p>Nevertheless, we see that observing a mean difference of 0.35 is not only quite likely given that the true mean difference is 0.5, but observing a mean difference of 0.35 is much more likely under the alternative model than under the null model. You can see this by comparing the height of the density curve at a difference of 0.35 for the null model, which is approximately 0.5, and the height of the density curve for the alternative model, which is approximately 1.5. See the chapter on <a href="#sec-likettest">likelihoods</a> for further details.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fig136" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-fig136-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.10: Distribution of observed Cohen’s <em>d</em> effect sizes when collecting 50 observations per group in an independent <em>t</em>-test for <em>d</em> = 0 and <em>d</em> = 0.5 when observing <em>d</em> = 0.35.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>All the <em>p</em>-value tells us is that a mean difference of 0.35 is not extremely surprising, if we assume the null hypothesis is true. There can be many reasons for this. In the real world, where we have no Omniscient Jones to tell us about the true mean difference, it is possible that there is a true effect, as illustrated in the figure above.</p>
<p>So what should we say instead? The solution is subtle, but important. Let’s revisit the two examples of incorrect statements we made earlier. First, ‘because <em>p</em> &gt; 0.05 we can conclude that there is no effect’ is incorrect, because there might very well be an effect (and remember <em>p</em>-values are statements about data, not about the probability that there is an effect or is no effect). Fisher’s interpretation of a <em>p</em>-value was that we can conclude a rare event has happened, or that the null hypothesis is false (he writes literally: “Either an exceptionally rare chance has occurred, or the theory of random distribution is not true”). This might sound like it is a statement about the probability of a theory, but it is really just stating the two possible scenarios under which low <em>p</em>-values occur (when you have made a Type 1 error, or when the alternative hypothesis is true). Both a true positive as a false positive remain possible, and we do not quantify the probability of either possible reality (e.g., we are not saying it is 95% probable that the null hypothesis is false). From a Neyman-Pearson perspective, a <em>p</em> &gt; .05 means that we cannot act as if the null hypothesis can be rejected, without maintaining our desired error rate of 5%.</p>
<p>If you are interested in concluding an effect is absent, null hypothesis testing is not the tool to use. A null hypothesis test answers the question ‘can I reject the null hypothesis with a desired error rate?’. If you cannot do this, and p &gt; 0.05, no conclusion can be drawn based only on the <em>p</em>-value (remember the concept of 無 ‘mu’: the answer is neither yes nor no). Luckily, statistical approaches have been developed to ask questions about the absence of an effect such as <a href="#sec-equivalencetest">equivalence testing</a>, Bayes factors, and Bayesian estimation (see <span class="citation" data-cites="harms_making_2018">Harms &amp; Lakens (<a href="#ref-harms_making_2018" role="doc-biblioref">2018</a>)</span>, for an overview).</p>
<p>The second incorrect statement was ‘there was no difference’. This statement is somewhat easier to correct. You can instead write ‘there was no statistically significant difference’. Granted, this is a bit tautological, because you are basically saying that the <em>p</em>-value was larger than the alpha level in two different ways, but at least this statement is formally correct. The difference between ‘there was no difference’ and ‘there was no statistically significant difference’ might sound like semantics, but in the first case you are formally saying ‘the difference was 0’ while in the second you are saying ‘there was no difference large enough to yield a <em>p</em> &lt; .05’. Although I have never seen anyone do this, a more informative message might be ‘because given our sample size of 50 per group, and our alpha level of 0.05, only observed differences more extreme than 0.4 could be statistically significant, and our observed mean difference was 0.35 thus we could not reject the null hypothesis’. If this feels like a very unsatisfactory conclusion, remember that a null hypothesis test was not designed to draw interesting conclusions about the absence of effects – you will need to learn about equivalence tests to get a more satisfactory answers about null effects.</p>
</section>
<section id="misunderstanding-2-a-significant-p-value-means-that-the-null-hypothesis-is-false." class="level3" data-number="1.7.2">
<h3 data-number="1.7.2"><span class="header-section-number">1.7.2</span> Misunderstanding 2: A significant <em>p</em>-value means that the null hypothesis is false.</h3>
<p>This is the opposite misconception from the one we discussed previously. Examples of incorrect statements based on this misconception are ‘<em>p</em> &lt; .05, therefore there is an effect’, or ‘there is a difference between the two groups, <em>p</em> &lt; .05’. As before, both these statements imply it is 100% probable that the null model is false, and an alternative model is true.</p>
<p>As a simple example of why such extreme statements are incorrect, imagine we generate a series of numbers in R using the following command:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">50</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] -1.933737354  0.833574776  0.215210384 -1.420298621 -2.173008815
 [6]  0.594572303 -0.755546980  0.013497083 -0.873689147 -0.634266537
[11]  1.652850563 -0.103092189 -0.825630753 -2.044684944  3.704181152
[16] -0.554937052  0.660450633  0.814324937 -0.186301010  0.265158994
[21]  2.119859250  0.310806151 -0.665607760 -0.328149936  0.971384511
[26]  0.221626172 -0.510990241  1.044466915 -0.470665286 -2.017300062
[31] -0.111352627 -0.001534238  1.006420761  0.187909622 -0.944293543
[36] -0.589530823 -0.601660999  0.061614493  1.163750752  2.224886748
[41] -0.902472138  2.435338239 -0.169003918 -0.951271701  0.886062960
[46] -1.622608355 -0.802226231  0.459385505  0.138067814  0.660618215</code></pre>
</div>
</div>
<p>This command generates 50 random observations from a distribution with a mean of 0 and a standard deviation of 1 (in the long run – the mean and standard deviation will vary in each sample that is generated). Imagine we run this command once, and we observe a mean of 0.5. The figure below visualizes this scenario. We can perform a one-sample <em>t</em>-test against 0, and this test tells us, with a <em>p</em> &lt; .05, that the data we have observed is surprisingly different from 0, assuming the random number generator in R functions as it should and generates data with a true mean of 0.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fig137" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-fig137-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.11: Distribution of observed Cohen’s <em>d</em> effect sizes when collecting 50 observations per group in an independent <em>t</em>-test when <em>d</em> = 0 and observing <em>d</em> = 0.5.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>The significant <em>p</em>-value does not allow us to conclude that the null hypothesis (“the random number generator works”) is false. It is true that the mean of the 50 samples we generated was surprisingly extreme. But a low <em>p</em>-value simply tells us that an observation is surprising. We should observe such surprising observations with a low probability when the null hypothesis is true – when the null is true, they still happen. Therefore, a significant result does not mean an alternative hypothesis is true – the result can also be a Type 1 error, and in the example above, Omniscient Jones knows that this is the case.</p>
<p>Let’s revisit the incorrect statement ‘<em>p</em> &lt; .05, therefore there is an effect’. A correct interpretation of a significant <em>p</em>-value requires us to acknowledge the possibility that our significant result might be a Type 1 error. Remember that Fisher would conclude that “Either an exceptionally rare chance has occurred, or the theory of random distribution is not true”. A correct interpretation in terms of Neyman-Pearson statistics would be: “we can act as if the null hypothesis is false, and we would not be wrong more than 5% of the time in the long run”. Note the specific use of the word ‘act’, which does not imply anything about whether this specific hypothesis is true or false, but merely states that if we act as if the null hypothesis is false any time we observe <em>p</em> &lt; alpha, we will not make an error more than alpha percent of the time.</p>
<p>Both these formally correct statements are a bit long. In scientific articles, we often read a shorter statement such as: ‘we can reject the null hypothesis’, or ‘we can accept the alternative hypothesis’. These statements might be made with the assumption that readers will themselves add ‘with a 5% probability of being wrong, in the long run’. But it might be useful to add ‘with a 5% long run error rate’ at least the first time you make such a statement in your article to remind readers.</p>
<p>In the example above we have a very strong subjective prior probability that the random number generator in R works. Alternative statistical procedures to incorporate such prior beliefs are <a href="#sec-bayes">Bayesian statistics</a> or <a href="#sec-ppv">false positive report probabilities</a>. In frequentist statistics, the idea is that you need to replicate your study several times. You will observe a Type 1 error every now and then, but you are unlikely to observe a Type 1 error three times in a row. Alternatively, you can lower the alpha level in a single study to reduce the probability of a Type 1 error rate.</p>
</section>
<section id="misunderstanding-3-a-significant-p-value-means-that-a-practically-important-effect-has-been-discovered." class="level3" data-number="1.7.3">
<h3 data-number="1.7.3"><span class="header-section-number">1.7.3</span> Misunderstanding 3: A significant <em>p</em>-value means that a practically important effect has been discovered.</h3>
<p>A common concern when interpreting <em>p</em>-values is that ‘significant’ in normal language implies ‘important’, and thus a ‘significant’ effect is interpreted as an ‘important’ effect. However, the question whether an effect is important is completely orthogonal to the question whether it is different from zero, or even how large the effect is. Not all effects have practical impact. The smaller the effect, the less likely such effects will be noticed by individuals, but such effects might still have a large impact on a societal level. Therefore, the general take home message is that statistical significance does not answer the question whether an effect matters in practice, or is ‘practically important’. To answer the question whether an effect matters, you need to present a cost-benefit analysis.</p>
<p>This issue of practical significance most often comes up in studies with a very large sample size. As we have seen before, with an increasing sample size, the width of the density distribution around the null value becomes more and more narrow, and the values that are considered surprising fall closer and closer to zero.</p>
<p>If we plot the null model for a very large sample size (e.g., n = 10000 per group) we see that even very small mean differences (differences more extreme than a mean difference of 0.04) will be considered ‘surprising’. This still means that if there really is no difference in the population, you will observe differences larger than 0.04 less than 5% of the time, in the long run, and 95% of the observed differences will be smaller than a mean difference of 0.04. But it becomes more difficult to argue for the practical significance of such effects. Imagine that a specific intervention is successful in changing people’s spending behavior, and when implementing some intervention people save 12 cents per year. It is difficult to argue how this effect will make any individual happier. However, if this money is combined, it will yield over 2 million, which could be used to treat diseases in developing countries, where it would have a real impact. The cost of the intervention might be considered too high if the goal is to make individuals happier, but it might be consider worthwhile if the goal is to raise 2 million for charity.</p>
<p>Not all effects in psychology are additive (we cannot combine or transfer an increase in happiness of 0.04 scale points), so it is often more difficult to argue for the importance of small effects in subjective feelings <span class="citation" data-cites="anvari_not_2021">(<a href="#ref-anvari_not_2021" role="doc-biblioref">Anvari et al., 2021</a>)</span>. A cost-benefit analysis might show small effects matter a lot, but whether or not this is the case cannot be inferred from a <em>p</em>-value.</p>
<p>Note that nothing about this is a problem with the interpretation of a <em>p</em>-value per se: A <em>p</em> &lt; 0.05 still correctly indicates that, if the null hypothesis is true, we have observed data that should be considered surprising. However, just because data is surprising, does not mean we need to care about it. It is mainly the verbal label ‘significant’ that causes confusion here – it is perhaps less confusing to think of a ‘significant’ effect as a ‘surprising’ effect, but not necessarily as an ‘important’ effect.</p>
</section>
<section id="sec-misconception4" class="level3" data-number="1.7.4">
<h3 data-number="1.7.4"><span class="header-section-number">1.7.4</span> Misunderstanding 4: If you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%.</h3>
<p>This misinterpretation is one possible explanation of the incorrect statement that a <em>p</em>-value is ‘the probability that the data are observed by chance.’ Assume we collect 20 observations, and Omniscient Jones tells us the null hypothesis is true (as in the example above where we generated random numbers in R). This means we are sampling from the distribution in the figure below.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-fig138" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/fig-fig138-1.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.12: Distribution of observed Cohen’s <em>d</em> effect sizes when collecting 20 observations per group in an independent <em>t</em>-test when <em>d</em> = 0.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>If this is our reality, it means that 100% of the time that we observe a significant result, it is a false positive (or Type I error). Thus, 100% of our significant results are Type 1 errors.</p>
<p>It is important to distinguish probabilities before collecting the data and analyzing the result, and probabilities after collecting data and analyzing the results. What the Type 1 error rate controls is that from all studies we will perform in the future where the null hypothesis is true, not more than 5% of our observed mean differences will fall in the red tail areas. But after we have seen that our data falls in the tail areas with <em>p</em> &lt; alpha, and we know that the null hypothesis is true, these observed significant effects are always a Type 1 error. If you read carefully, you will notice that this misunderstanding is cause by differences in the question that is asked. “If I have observed a <em>p</em> &lt; .05, what is the probability that the null hypothesis is true?” is a different question than “If the null hypothesis is true, what is the probability of observing this (or more extreme) data?”. Only the latter question is answered by a <em>p</em>-value. The first question cannot be answered without making a subjective judgment about the probability that the null hypothesis is true prior to collecting the data.</p>
</section>
<section id="misunderstanding-5-one-minus-the-p-value-is-the-probability-that-the-effect-will-replicate-when-repeated." class="level3" data-number="1.7.5">
<h3 data-number="1.7.5"><span class="header-section-number">1.7.5</span> Misunderstanding 5: One minus the <em>p</em>-value is the probability that the effect will replicate when repeated.</h3>
<p>It is impossible to calculate the probability that an effect will replicate <span class="citation" data-cites="miller_what_2009">(<a href="#ref-miller_what_2009" role="doc-biblioref">Miller, 2009</a>)</span>. There are too many unknown factors to accurately predict the replication probability, and one of the main factors is the true mean difference. If we were Omniscient Jones, and we knew the true mean difference (e.g., a difference between the two groups of 0.5 scale points) we would know the statistical power of our test. The statistical power is the probability that we will find a significant result, if the alternative model is true (i.e. if there is a true effect). For example, reading the text in the left bar in the app, we see that with N = 50 per group, alpha level of 0.05, and a true mean difference of 0.5, the probability of finding a significant result (or the statistical power) is 69.69%. If we would observe a significant effect in this scenario (e.g., <em>p</em> = 0.03) it is not true that there is a 97% probability that an exact replication of the study (with the same sample size) would again yield a significant effect. The probability that a study yields a significant effect is determined by the statistical power - not by the <em>p</em>-value in a previous study.</p>
<p>What we can generally take away from this last misunderstanding is the fact that the probability of replication depends on the presence versus the absence of a true effect. In other words, as stated above, if a true effect exists then the level of statistical power informs us about how frequently we should observe a significant result (e.g., 80% power means we should observe significant result 80% of the time). On the other hand, if the null hypothesis is true (e.g., the effect is 0) then significant results will be observed only with a frequency approaching the chosen alpha level in the long run (i.e. a 5% Type 1 error rate if an an alpha of 0.05 is chosen). Therefore, if the original study correctly observed an effect, the probability of a significant result in a replication study is determined by the statistical power, and if the original study correctly observed no significant effect, the probability of a significant effect in a replication study is determined by the alpha level. In practice, many other factors determine if an effect will replicate. The only way to know if an effect will replicate, is to replicate it. If you want to explore how difficult it is to predict if findings in the literature will replicate you can perform <a href="https://80000hours.org/psychology-replication-quiz/">this test by 80.000 Hours</a>.</p>
</section>
</section>
<section id="test-yourself" class="level2" data-number="1.8">
<h2 data-number="1.8"><span class="header-section-number">1.8</span> Test Yourself</h2>
<section id="questions-about-which-p-values-you-can-expect" class="level3 webex-check webex-box" data-number="1.8.1">
<h3 data-number="1.8.1"><span class="header-section-number">1.8.1</span> Questions about which <em>p</em>-values you can expect</h3>
<p>Answer each question. Then click the ‘Show Answers’ button at the bottom of this set of questions to check which questions you answered correctly.</p>
<p>Copy the code below to R and run the code. You can click the ‘clipboard’ icon on the top right of the code section to copy all the code to your clipboard, so you can easily paste it into R.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>nsims <span class="ot">&lt;-</span> <span class="dv">100000</span> <span class="co"># number of simulations</span></span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a>m <span class="ot">&lt;-</span> <span class="dv">106</span> <span class="co"># mean sample</span></span>
<span id="cb6-4"><a href="#cb6-4"></a>n <span class="ot">&lt;-</span> <span class="dv">26</span> <span class="co"># set sample size</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>sd <span class="ot">&lt;-</span> <span class="dv">15</span> <span class="co"># SD of the simulated data</span></span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a>p <span class="ot">&lt;-</span> <span class="fu">numeric</span>(nsims) <span class="co"># set up empty vector</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>bars <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsims) { <span class="co"># for each simulated experiment</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>  x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> n, <span class="at">mean =</span> m, <span class="at">sd =</span> sd)</span>
<span id="cb6-12"><a href="#cb6-12"></a>  z <span class="ot">&lt;-</span> <span class="fu">t.test</span>(x, <span class="at">mu =</span> <span class="dv">100</span>) <span class="co"># perform the t-test</span></span>
<span id="cb6-13"><a href="#cb6-13"></a>  p[i] <span class="ot">&lt;-</span> z<span class="sc">$</span>p.value <span class="co"># get the p-value</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>}</span>
<span id="cb6-15"><a href="#cb6-15"></a>power <span class="ot">&lt;-</span> <span class="fu">round</span>((<span class="fu">sum</span>(p <span class="sc">&lt;</span> <span class="fl">0.05</span>) <span class="sc">/</span> nsims), <span class="dv">2</span>) <span class="co"># power</span></span>
<span id="cb6-16"><a href="#cb6-16"></a></span>
<span id="cb6-17"><a href="#cb6-17"></a><span class="co"># Plot figure</span></span>
<span id="cb6-18"><a href="#cb6-18"></a><span class="fu">hist</span>(p,</span>
<span id="cb6-19"><a href="#cb6-19"></a>  <span class="at">breaks =</span> bars, <span class="at">xlab =</span> <span class="st">&quot;P-values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;number of p-values</span><span class="sc">\n</span><span class="st">&quot;</span>, </span>
<span id="cb6-20"><a href="#cb6-20"></a>  <span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">&quot;P-value Distribution with&quot;</span>, </span>
<span id="cb6-21"><a href="#cb6-21"></a>                             <span class="fu">round</span>(power <span class="sc">*</span> <span class="dv">100</span>, <span class="at">digits =</span> <span class="dv">1</span>), <span class="st">&quot;% Power&quot;</span>),</span>
<span id="cb6-22"><a href="#cb6-22"></a>  <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, nsims))</span>
<span id="cb6-23"><a href="#cb6-23"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>), <span class="at">labels =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>))</span>
<span id="cb6-24"><a href="#cb6-24"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">2</span>, <span class="at">at =</span> <span class="fu">seq</span>(<span class="dv">0</span>, nsims, nsims <span class="sc">/</span> <span class="dv">4</span>), </span>
<span id="cb6-25"><a href="#cb6-25"></a>     <span class="at">labels =</span> <span class="fu">seq</span>(<span class="dv">0</span>, nsims, nsims <span class="sc">/</span> <span class="dv">4</span>), <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb6-26"><a href="#cb6-26"></a><span class="fu">abline</span>(<span class="at">h =</span> nsims <span class="sc">/</span> bars, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="01-pvalue_files/figure-html/q1-1.png" class="img-fluid" style="width:100.0%" /></p>
</figure>
</div>
</div>
</div>
<p>On the x-axis we see <em>p</em>-values from 0 to 1 in 20 bars, and on the y-axis we see how frequently these <em>p</em>-values were observed. There is a horizontal red dotted line that indicates an alpha of 5% (located at a frequency of 100.000*0.05 = 5000) – but you can ignore this line for now. In the title of the graph, the statistical power that is achieved in the simulated studies is given (assuming an alpha of 0.05): The studies have 50% power (with minor variations for each simulation).</p>
<p><strong>Q1</strong>: Since the statistical power is the probability of observing a statistically significant result, if there is a true effect, we can also see the power in the figure itself. Where?</p>
<div class="cell" data-layout-align="center">
<div id="radio_JEZURTTNNK" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_JEZURTTNNK" value=""></input> <span>We can calculate the number of <em>p</em>-values larger than 0.5, and divide them by the number of simulations.</span></label><label><input type="radio" autocomplete="off" name="radio_JEZURTTNNK" value="answer"></input> <span>We can calculate the number of <em>p</em>-values in the first bar (which contains all 'significant' <em>p</em>-values from 0.00 to 0.05) and divide the <em>p</em>-values in this bar by the total number of simulations.</span></label><label><input type="radio" autocomplete="off" name="radio_JEZURTTNNK" value=""></input> <span>We can calculate the difference between <em>p</em>-values above 0.5 minus the <em>p</em>-values below 0.5, and divide this number by the total number of simulations.</span></label><label><input type="radio" autocomplete="off" name="radio_JEZURTTNNK" value=""></input> <span>We can calculate the difference between <em>p</em>-values above 0.5 minus the <em>p</em>-values below 0.05, and divide this number by the number of simulations.</span></label>
</div>
</div>
<p><strong>Q2</strong>: Change the sample size on line 4 in the code from n &lt;- 26 to n &lt;- 51. Run the simulation by selecting all lines and pressing CTRL+Enter. What is the power in the simulation now that we have increased the sample size from 26 people to 51 people? Remember that simulations can sometimes yield slightly varying answers, so choose the answer option closest to the simulation results.</p>
<div class="cell" data-layout-align="center">
<div id="radio_MDWMJMLEOU" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_MDWMJMLEOU" value=""></input> <span>55%</span></label><label><input type="radio" autocomplete="off" name="radio_MDWMJMLEOU" value=""></input> <span>60%</span></label><label><input type="radio" autocomplete="off" name="radio_MDWMJMLEOU" value="answer"></input> <span>80%</span></label><label><input type="radio" autocomplete="off" name="radio_MDWMJMLEOU" value=""></input> <span>95%</span></label>
</div>
</div>
<p><strong>Q3</strong>: If you look at the distribution of <em>p</em>-values, what do you notice?</p>
<div class="cell" data-layout-align="center">
<div id="radio_CIWCSBAOKE" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_CIWCSBAOKE" value=""></input> <span>The <em>p</em>-value distribution is exactly the same as with 50% power</span></label><label><input type="radio" autocomplete="off" name="radio_CIWCSBAOKE" value="answer"></input> <span>The <em>p</em>-value distribution is much steeper than with 50% power</span></label><label><input type="radio" autocomplete="off" name="radio_CIWCSBAOKE" value=""></input> <span>The <em>p</em>-value distribution is much flatter than with 50% power</span></label><label><input type="radio" autocomplete="off" name="radio_CIWCSBAOKE" value=""></input> <span>The <em>p</em>-value distribution is much more normally distributed than with 50% power</span></label>
</div>
</div>
<p>Feel free to increase and decrease the sample size and see what happens if you run the simulation. When you are done exploring, make sure that n &lt;- 51 again in line 4 before you continue.</p>
<p><strong>Q4</strong>: What would happen when there is no true difference between our simulated samples and the average IQ score? In this situation, we have no probability to observe an effect, so you might say we have ‘0 power’. Formally, power is not defined when there is no true effect. However, we can casually refer to this as 0 power. Change the mean in the sample to 100 (set m &lt;- 106 to m &lt;- 100) There is now no difference between the mean in our sample, and the population value we are testing against in the one-sample <em>t</em>-test. Run the script again. What do you notice?</p>
<div class="cell" data-layout-align="center">
<div id="radio_OQGSWYYDON" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_OQGSWYYDON" value=""></input> <span>The <em>p</em>-value distribution is exactly the same as with 50% powe</span></label><label><input type="radio" autocomplete="off" name="radio_OQGSWYYDON" value=""></input> <span>The <em>p</em>-value distribution is much steeper than with 50% power</span></label><label><input type="radio" autocomplete="off" name="radio_OQGSWYYDON" value="answer"></input> <span>The <em>p</em>-value distribution is basically completely flat (ignoring some minor variation due to random noise in the simulation)</span></label><label><input type="radio" autocomplete="off" name="radio_OQGSWYYDON" value=""></input> <span>The <em>p</em>-value distribution is normally (i.e., bell-shaped) distributed</span></label>
</div>
</div>
<p>The question below builds on the simulation above where there was no true difference between the groups.</p>
<p><strong>Q5</strong>: Look at the leftmost bar in the plot produced for Q4, and look at the frequency of <em>p</em>-values in this bar. What is the formal name for this bar?</p>
<div class="cell" data-layout-align="center">
<div id="radio_LETBJKXPVB" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_LETBJKXPVB" value=""></input> <span>The power (or true positives)</span></label><label><input type="radio" autocomplete="off" name="radio_LETBJKXPVB" value=""></input> <span>The true negatives</span></label><label><input type="radio" autocomplete="off" name="radio_LETBJKXPVB" value="answer"></input> <span>The Type 1 error (or false positives)</span></label><label><input type="radio" autocomplete="off" name="radio_LETBJKXPVB" value=""></input> <span>The Type 2 error (or false negatives)</span></label>
</div>
</div>
<p>Let’s take a look at just the <em>p</em>-values below 0.05. Bear with me for the next few steps – it will be worth it. Find the variable that determines how many bars there are, in the statement bars &lt;- 20 in line 8. Change it to bars &lt;- 100. We will now get 1 bar for <em>p</em>-values between 0 and 0.01, one bar for <em>p</em>-values between 0.01 and 0.02, and 100 bars in total. The red dotted line will now indicate the frequency of <em>p</em>-values when the null hypothesis is true, where every bar contains 1% of the total number of <em>p</em>-values. We only want to look at <em>p</em>-values below 0.05, and we will cut off the plot at 0.05. Change xlim = c(0, 1) to xlim = c(0, 0.05). Instead of seeing all <em>p</em>-values between 0 and 1, we will only see <em>p</em>-values between 0 and 0.05. Re-run the simulation (still with m &lt;- 100). We see the same uniform distribution, but now every bar contains 1% of the <em>p</em>-values, so the <em>p</em>-value distribution is very flat and almost impossible to see (we will zoom in on the y-axis later this assignment). The red line now clearly gives the frequency for each bar, assuming the null hypothesis is true.</p>
<p>Change the mean in the simulation in line 3 to m &lt;- 107 (remember n is still 51). Re-run the simulation. It’s clear we have very high power. Most <em>p</em>-values are in the left-most bar, which contains all <em>p</em>-values between 0.00 and 0.01.</p>
<p><strong>Q6</strong>: The plot from the last simulation tells you we have approximately 90.5% power (the number in your simulation might vary a bit due to random variation). This is the power if we use an alpha of 5%. But we can also use an alpha of 1%. What is the statistical power we have in the simulated studies when we would use an alpha of 1%, looking at the graph? Pick the answer closest to the answer from your simulations. Note that you can also compute the power for an alpha of 0.01 by changing p &lt; 0.05 to p &lt; 0.01 in line 15, just make sure to set it back to 0.05 before your continue.</p>
<div class="cell" data-layout-align="center">
<div id="radio_LSGTCXUICB" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_LSGTCXUICB" value=""></input> <span>~90%</span></label><label><input type="radio" autocomplete="off" name="radio_LSGTCXUICB" value="answer"></input> <span>~75%</span></label><label><input type="radio" autocomplete="off" name="radio_LSGTCXUICB" value=""></input> <span>~50%</span></label><label><input type="radio" autocomplete="off" name="radio_LSGTCXUICB" value=""></input> <span>~5%</span></label>
</div>
</div>
<p>To be able to look at the <em>p</em>-values around 0.03 and 0.04, we will zoom in on the y-axis as well. In the part of the code where the plot is draw, change ylim = c(0, nSims) to ylim = c(0, 10000). Re-run the script.</p>
<p>Change the mean in the sample to 108 in m &lt;- 108), and leave the sample size at 51. Run the simulation. Look at how the distribution has changed compared to the graph above.</p>
<p>Look at the fifth bar from the left. This bar now contains all the <em>p</em>-values between 0.04 and 0.05. You will notice something peculiar. Remember that the red dotted line indicates the frequency in each bar, assuming the null hypothesis is true. See how the bar with <em>p</em>-values between 0.04 and 0.05 is lower than the red line. We have simulated studies with 96% power. When power is very high, <em>p</em>-values between 0.04 and 0.05 are very rare – they occur less than 1% of the time (most <em>p</em>-values are smaller than 0.01). When the null hypothesis is true, <em>p</em>-values between 0.04 and 0.05 occur exactly 1% of the time (because <em>p</em>-values are uniformly distributed). Now ask yourself: When you have very high power, and you observe a <em>p</em>-value between 0.04 and 0.05, is it more likely that the null hypothesis is true, or that the alternative hypothesis is true? Given that you are more likely to observe <em>p</em>-values between 0.04 and 0.05 when the null hypothesis is true, than when the alternative hypothesis is true, you should interpret a <em>p</em>-value significant with an alpha of 0.05 as more likely when the null hypothesis is true, than when the alternative hypothesis is true.</p>
<p>In our simulations, we know there is a true effect or not, but in the real world, you don’t know. When you have very high power, use an alpha level of 0.05, and find a <em>p</em>-value of <em>p</em> = .045, the data is surprising, assuming the null hypothesis is true, but it is even <em>more</em> surprising, assuming the alternative hypothesis is true. This shows how a significant <em>p</em>-value is not always evidence for the alternative hypothesis.</p>
<p><strong>Q7</strong>: When you know you have very high (e.g., 98%) power for the smallest effect size you care about, and you observe a <em>p</em>-value of 0.045, what is the correct conclusion?</p>
<div class="cell" data-layout-align="center">
<div id="radio_FZTIVQIUPB" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_FZTIVQIUPB" value=""></input> <span>The effect is significant, and provides strong support for the alternative hypothesis.</span></label><label><input type="radio" autocomplete="off" name="radio_FZTIVQIUPB" value="answer"></input> <span>The effect is significant, but it is without any doubt a Type 1 error.</span></label><label><input type="radio" autocomplete="off" name="radio_FZTIVQIUPB" value=""></input> <span>With high power, you should use an alpha level that is smaller than 0.05, and therefore, this effect cannot be considered significant. </span></label><label><input type="radio" autocomplete="off" name="radio_FZTIVQIUPB" value="answer"></input> <span>The effect is significant, but the data are more likely under the null hypothesis than under the alternative hypothesis.</span></label>
</div>
</div>
<p><strong>Q8</strong>: Play around with the sample size (n) and the mean (m) by changing the numerical values or both (and thus, vary the statistical power in the simulated studies). Look at the simulation result for the bar that contains <em>p</em>-values between 0.04 and 0.05. The red line indicates how many <em>p</em>-values would be found in this bar if the null hypothesis was true (and is always at 1%). At the very best, how much more likely is a <em>p</em>-value between 0.04 and 0.05 to come from a <em>p</em>-value distribution representing a true effect, than it is to come from a <em>p</em>-value distribution when there is no effect? You can answer this question by seeing how much higher the bar of <em>p</em>-values between 0.04 and 0.05 can become. If at best the bar in the simulation is five times as high at the red line (so the bar shows 5% of <em>p</em>-values end up between 0.04 and 0.05, while the red line remains at 1%), then at best <em>p</em>-values between 0.04 and 0.05 are five times as likely when there is a true effect than when there is no true effect.</p>
<div class="cell" data-layout-align="center">
<div id="radio_IAGPDDDYKW" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_IAGPDDDYKW" value=""></input> <span>At best, <em>p</em>-values between 0.04 and 0.05 are equally likely under the alternative hypothesis, and under the null hypothesis.</span></label><label><input type="radio" autocomplete="off" name="radio_IAGPDDDYKW" value="answer"></input> <span>At best, <em>p</em>-values between 0.04 and 0.05 are approximately 4 times more likely under the alternative hypothesis, than under the null hypothesis.</span></label><label><input type="radio" autocomplete="off" name="radio_IAGPDDDYKW" value=""></input> <span>At best, <em>p</em>-values between 0.04 and 0.05 are ~10 times more likely under the alternative hypothesis, than under the null hypothesis.</span></label><label><input type="radio" autocomplete="off" name="radio_IAGPDDDYKW" value=""></input> <span>At best, <em>p</em>-values between 0.04 and 0.05 are ~30 times more likely under the alternative hypothesis, than under the null hypothesis.</span></label>
</div>
</div>
<p>For this reason, statisticians warn that <em>p</em>-values just below 0.05 (e.g., between 0.04 and 0.05) are at the very best weak support for the alternative hypothesis. If you find <em>p</em>-values in this range, consider replicating the study, or if that’s not possible, interpret the result at least a bit cautiously. Of course, you can make a claim in a Neyman-Pearson approach that has at most a 5% Type 1 error rate. The Lindley’s paradox therefore nicely illustrates the difference between different philosophical approaches to statistical inferences.</p>
</section>
<section id="questions-about-p-value-misconceptions" class="level3 webex-check webex-box" data-number="1.8.2">
<h3 data-number="1.8.2"><span class="header-section-number">1.8.2</span> Questions about <em>p</em>-value misconceptions</h3>
<p>Answer each question. Then click the ‘Show Answers’ button at the bottom of this set of questions to check which questions you answered correctly.</p>
<p><strong>Q1</strong>: When the sample size in each group of an independent <em>t</em>-test is 50 observations (see <a href="#fig-fig131">Figure <span class="quarto-unresolved-ref">fig-fig131</span></a>), which statement is correct?</p>
<div class="cell" data-layout-align="center">
<div id="radio_QTLWGGWLSX" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_QTLWGGWLSX" value=""></input> <span>The mean of the differences you will observe between the two groups is always 0.</span></label><label><input type="radio" autocomplete="off" name="radio_QTLWGGWLSX" value=""></input> <span>The mean of the differences you will observe between the two groups is always different from 0.</span></label><label><input type="radio" autocomplete="off" name="radio_QTLWGGWLSX" value="answer"></input> <span>Observing a mean difference of +0.5 or -0.5 is considered surprising, assuming the null hypothesis is true.</span></label><label><input type="radio" autocomplete="off" name="radio_QTLWGGWLSX" value=""></input> <span>Observing a mean difference of +0.1 or -0.1 is considered surprising, assuming the null hypothesis is true.</span></label>
</div>
</div>
<p><strong>Q2:</strong> In what sense are the null models in <a href="#fig-fig131">Figure <span class="quarto-unresolved-ref">fig-fig131</span></a> and <a href="#fig-fig132">Figure <span class="quarto-unresolved-ref">fig-fig132</span></a> similar, and in what sense are they different?</p>
<div class="cell" data-layout-align="center">
<div id="radio_HUROAUNGGU" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_HUROAUNGGU" value="answer"></input> <span>In both cases, the distributions are centered on zero, and the critical <em>t</em>-value is between 1.96 and 2 (for a two-sided test, depending on the sample size). But the larger the sample size, the closer to 0 the mean differences fall that are considered ‘surprising’.</span></label><label><input type="radio" autocomplete="off" name="radio_HUROAUNGGU" value=""></input> <span>In both cases, a <em>t</em>-value of 0 is the most likely outcome, but the critical <em>t</em>-value is around 0.4 for n = 50, and around 0.05 for n = 5000.</span></label><label><input type="radio" autocomplete="off" name="radio_HUROAUNGGU" value=""></input> <span>In both cases, means will vary in exactly the same way around 0, but the Type 1 error rate is much smaller when n = 5000 than when n = 50.</span></label><label><input type="radio" autocomplete="off" name="radio_HUROAUNGGU" value=""></input> <span>Because the standard error is much larger for n = 50 than for n = 5000, it is much more likely that the null hypothesis is true for n = 50.</span></label>
</div>
</div>
<p><strong>Q3:</strong> You can play around with the alternative and null models in this online app: <a href="http://shiny.ieis.tue.nl/d_p_power/" class="uri">http://shiny.ieis.tue.nl/d_p_power/</a>. The app allows you to specify the sample size in each group of an independent <em>t</em>-test (from 2 to infinity), the mean difference (from 0 to 2), and the alpha level. In the plot, the red areas visualize Type 1 errors. The blue area visualizes the Type 2 error rate. The app also tells you the critical value: There is a vertical line (with n = 50 this line falls at a mean difference of 0.4) and a verbal label that says: “Effects larger than 0.4 will be statistically significant”. Note that the same is true for effects smaller than -0.4, even though there is no second label there, but the app shows the situation for a two-sided independent <em>t</em>-test.</p>
<p>You can see that on the left of the vertical line that indicates the critical mean difference there is a blue area that is part of the alternative model. This is the Type 2 error rate (or 1 - the power of the study). If a study has 80% power, 80% of the mean differences we will observe should fall on the right of the critical value indicated by the line. If the alternative model is true, but we observe an effect smaller than the critical value, the observed <em>p</em>-value will be larger than 0.05, even when there is a true effect. You can check in the app that the larger the effect size, the further to the right the entire alternative distribution falls, and thus the higher the power. You can also see that the larger the sample size, the narrower the distribution, and the less of the distribution will fall below the critical value (as long as the true population mean is larger than the critical value). Finally, the larger the alpha level, the further to the left the critical mean difference moves, and the smaller the area of the alternative distribution that falls below the critical value.</p>
<p>The app also plots 3 graphs that illustrate the power curves as a function of different alpha levels, sample sizes, or true mean differences. Play around in the app by changing the values. Get a feel for how each variable impacts the null and alternative models, the mean difference that will be statistically significant, and the Type 1 and Type 2 error rates.</p>
<p>Open the app, and make sure it is set to the default settings of a sample size of 50 and an alpha level of 0.05. Look at the distribution of the null model. Set the sample size to 2. Set the sample size to 5000. The app will not allow you to plot data for a ‘group’ size of 1, but with n = 2 you will get a pretty good idea of the range of values you can expect when the true effect is 0, and when you collect single observations (n = 1). Given your experiences with the app as you change different parameters, which statement is true?</p>
<div class="cell" data-layout-align="center">
<div id="radio_XBSXMILQDC" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_XBSXMILQDC" value=""></input> <span>When the null hypothesis is true and the standard deviation is 1, if you randomly take 1 observation from each group and calculate the difference score, the differences will fall between -0.4 and 0.4 for 95% of the pairs of observations you will draw.</span></label><label><input type="radio" autocomplete="off" name="radio_XBSXMILQDC" value="answer"></input> <span>When the null hypothesis is true and the standard deviation is 1, with n = 50 per group, 95% of studies where data is collected will observe in the long run a mean difference between -0.4 and 0.4.</span></label><label><input type="radio" autocomplete="off" name="radio_XBSXMILQDC" value=""></input> <span>In any study with n = 50 per group, even when the SD is unknown and it is not known if the null hypothesis is true, you should rarely observe a mean difference more extreme than -0.4 or 0.4.</span></label><label><input type="radio" autocomplete="off" name="radio_XBSXMILQDC" value=""></input> <span>As the sample size increases, the expected distribution of means become narrower for the null model, but not for the alternative model.</span></label>
</div>
</div>
<p><strong>Q4:</strong> Open the app once more with the default settings. Set the slider for the alpha level to 0.01 (while keeping the mean difference at 0.5 and the sample size at 50). Compared to the critical value when alpha = 0.05, which statement is true?</p>
<div class="cell" data-layout-align="center">
<div id="radio_PRVNQDGJBT" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_PRVNQDGJBT" value="answer"></input> <span>Compared to an alpha of 0.05, only <em>less</em> extreme values are considered surprising when an alpha of 0.01 is used, and only differences larger than 0.53 scale points (or smaller than -0.53) will now be statistically significant.</span></label><label><input type="radio" autocomplete="off" name="radio_PRVNQDGJBT" value=""></input> <span>Compared to an alpha of 0.05, only <em>less</em> extreme values are considered surprising when an alpha of 0.01 is used, and only differences larger than 0.33 scale points (or smaller than -0.33) will now be statistically significant.</span></label><label><input type="radio" autocomplete="off" name="radio_PRVNQDGJBT" value=""></input> <span>Compared to an alpha of 0.05, only <em>more</em> extreme values are considered surprising when an alpha of 0.01 is used, and only differences larger than 0.53 scale points (or smaller than -0.53) will be statistically significant.</span></label><label><input type="radio" autocomplete="off" name="radio_PRVNQDGJBT" value=""></input> <span>Compared to an alpha of 0.05, only <em>more</em> extreme values are considered surprising when an alpha of 0.01 is used, and only differences larger than 0.33 scale points (or smaller than -0.33) will now be statistically significant.</span></label>
</div>
</div>
<p><strong>Q5:</strong> Why can’t you conclude that the null hypothesis is true, when you observe a statistically non-significant <em>p</em>-value (<em>p</em> &gt; alpha)?</p>
<div class="cell" data-layout-align="center">
<div id="radio_CFKSHPGWSP" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_CFKSHPGWSP" value=""></input> <span>When calculating <em>p</em>-values you always need to take the prior probability into account.</span></label><label><input type="radio" autocomplete="off" name="radio_CFKSHPGWSP" value=""></input> <span>You need to acknowledge the probability that you have observed a Type 1 error.</span></label><label><input type="radio" autocomplete="off" name="radio_CFKSHPGWSP" value=""></input> <span>The alternative hypothesis is never true.</span></label><label><input type="radio" autocomplete="off" name="radio_CFKSHPGWSP" value="answer"></input> <span>You need to acknowledge the probability that you have observed a Type 2 error.</span></label>
</div>
</div>
<ol type="A">
<li>When calculating <em>p</em>-values you always need to take the prior probability into account.</li>
<li>You need to acknowledge the probability that you have observed a Type 1 error.</li>
<li>The null hypothesis is never true.</li>
<li>You need to acknowledge the probability that you have observed a Type 2 error.</li>
</ol>
<p><strong>Q6:</strong> Why can’t you conclude that the alternative hypothesis is true, when you observe a statistically significant <em>p</em>-value (<em>p</em> &lt; alpha)?</p>
<div class="cell" data-layout-align="center">
<div id="radio_QIXFETPQTI" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_QIXFETPQTI" value=""></input> <span>When calculating <em>p</em>-values you always need to take the prior probability into account.</span></label><label><input type="radio" autocomplete="off" name="radio_QIXFETPQTI" value="answer"></input> <span>You need to acknowledge the probability that you have observed a Type 1 error.</span></label><label><input type="radio" autocomplete="off" name="radio_QIXFETPQTI" value=""></input> <span>The alternative hypothesis is never true.</span></label><label><input type="radio" autocomplete="off" name="radio_QIXFETPQTI" value=""></input> <span>You need to acknowledge the probability that you have observed a Type 2 error.</span></label>
</div>
</div>
<p><strong>Q7:</strong> A common concern when interpreting <em>p</em>-values is that ‘significant’ in normal language implies ‘important’, and thus a ‘significant’ effect is interpreted as an ‘important’ effect. However, <strong>the question whether an effect is important is completely orthogonal to the question whether it is different from zero, or even how large the effect is</strong>. Not all effects have practical impact. The smaller the effect, the less likely such effects will be noticed by individuals, but such effects might still have a large impact on a societal level. Therefore, the general take home message is that <strong>statistical significance does not answer the question whether an effect matters in practice, or is ‘practically important’</strong>. To answer the question whether an effect matters, you need to present a <strong>cost-benefit analysis</strong>.</p>
<p>Go to the app: <a href="http://shiny.ieis.tue.nl/d_p_power/" class="uri">http://shiny.ieis.tue.nl/d_p_power/</a>. Set the sample size to 50000, the mean difference to 0.5, and the alpha level to 0.05. Which effects will, when observed, be statistically different from 0?</p>
<div class="cell" data-layout-align="center">
<div id="radio_UJGKDVURWF" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_UJGKDVURWF" value="answer"></input> <span>Effects more extreme than -0.01 and 0.01</span></label><label><input type="radio" autocomplete="off" name="radio_UJGKDVURWF" value=""></input> <span>ffects more extreme than -0.04 and 0.04</span></label><label><input type="radio" autocomplete="off" name="radio_UJGKDVURWF" value=""></input> <span>Effects more extreme than -0.05 and 0.05</span></label><label><input type="radio" autocomplete="off" name="radio_UJGKDVURWF" value=""></input> <span>Effects more extreme than -0.12 and 0.12</span></label>
</div>
</div>
<p>If we plot the null model for a very large sample size (e.g., n = 10000 per group) we see that even very small mean differences (differences more extreme than a mean difference of 0.04) will be considered ‘surprising’. This still means that if there really is no difference in the population, you will observe differences larger than 0.04 less than 5% of the time, in the long run, and 95% of the observed differences will be smaller than a mean difference of 0.04. But it becomes more difficult to argue for the practical significance of such effects. Imagine that a specific intervention is successful in changing people’s spending behavior, and when implementing some intervention people save 12 cents per year. It is difficult to argue how this effect will make any individual happier. However, if this money is combined, it will yield over 2 million, which could be used to treat diseases in developing countries, where it would have a real impact. The cost of the intervention might be considered too high if the goal is to make individuals happier, but it might be consider worthwhile if the goal is to raise 2 million for charity.</p>
<p>Not all effects in psychology are additive (we cannot combine or transfer an increase in happiness of 0.04 scale points), so it is often more difficult to argue for the importance of small effects in subjective feelings. A cost-benefit analysis might show small effects matter a lot, but whether or not this is the case cannot be inferred from a <em>p</em>-value. Instead, you need to report and interpret the <a href="#sec-effectsize">effect size</a>,</p>
<p><strong>Q8:</strong> Let’s assume that the random number generator in R works, and we use rnorm(n = 50, mean = 0, sd = 1) to generate 50 observations, and the mean of these observations is 0.5, which in a one-sample <em>t</em>-test against an effect of 0 yields a <em>p</em>-value of 0.03, which is smaller than the alpha level (which we have set to 0.05). What is the probability that we have observed a significant difference (<em>p</em> &lt; alpha) just by chance?</p>
<div class="cell" data-layout-align="center">
<div id="radio_UFIUXTQYQS" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_UFIUXTQYQS" value=""></input> <span>3%</span></label><label><input type="radio" autocomplete="off" name="radio_UFIUXTQYQS" value=""></input> <span>5%</span></label><label><input type="radio" autocomplete="off" name="radio_UFIUXTQYQS" value=""></input> <span>95%</span></label><label><input type="radio" autocomplete="off" name="radio_UFIUXTQYQS" value="answer"></input> <span>100%</span></label>
</div>
</div>
<p><strong>Q9:</strong> Which statement is true?</p>
<div class="cell" data-layout-align="center">
<div id="radio_PTYZNYDABV" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_PTYZNYDABV" value=""></input> <span>The probability that a replication study will yield a significant result is 1-<em>p</em>.</span></label><label><input type="radio" autocomplete="off" name="radio_PTYZNYDABV" value="answer"></input> <span>he probability that a replication study will yield a significant result is 1-<em>p</em> multiplied by the probability that the null hypothesis is true.</span></label><label><input type="radio" autocomplete="off" name="radio_PTYZNYDABV" value=""></input> <span>The probability that a replication study will yield a significant result is equal to the statistical power of the replication study (if there is a true effect), or the alpha level (if there is no true effect).</span></label><label><input type="radio" autocomplete="off" name="radio_PTYZNYDABV" value=""></input> <span>The probability that a replication study will yield a significant result is equal to the statistical power of the replication study + the alpha level.</span></label>
</div>
</div>
<p>This question is conceptually very similar to that asked by Tversky and Kahneman <span class="citation" data-cites="tversky_belief_1971">(<a href="#ref-tversky_belief_1971" role="doc-biblioref">1971</a>)</span> in article ‘Belief in the law of small numbers’:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-smallnumbers" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="images/belieflawsmallnumers.png" class="img-fluid" style="width:100.0%" /></p>
<p><figcaption>Figure 1.13: Screenshot of first paragraph in Tversky and Kahneman, 1971.</figcaption></p>
</figure>
</div>
</div>
</div>
<blockquote>
<p>Suppose you have run an experiment on 20 subjects, and have obtained a significant result which confirms your theory (<em>z</em> = 2.23, <em>p</em> &lt; .05, two-tailed). You now have cause to run an additional group of 10 subjects. What do you think the probability is that the results will be significant, by a one-tailed test, separately for this group?</p>
</blockquote>
<p>Tversky and Kahneman argue a reasonable answer is 48%, but the only correct response is the same as the correct response to question 9, and the exact probability cannot be known <span class="citation" data-cites="miller_what_2009">(<a href="#ref-miller_what_2009" role="doc-biblioref">Miller, 2009</a>)</span>.</p>
<p><strong>Q10:</strong> Does a non-significant <em>p</em>-value (i.e., <em>p</em> = 0.65) mean that the null hypothesis is true?</p>
<div class="cell" data-layout-align="center">
<div id="radio_GLQDZIEICI" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_GLQDZIEICI" value="answer"></input> <span>No - the result could be a Type 2 error, or a false negative.</span></label><label><input type="radio" autocomplete="off" name="radio_GLQDZIEICI" value=""></input> <span>Yes, because it is a true negative.</span></label><label><input type="radio" autocomplete="off" name="radio_GLQDZIEICI" value=""></input> <span>Yes, if the <em>p</em>-value is larger than the alpha level the null hypothesis is true.</span></label><label><input type="radio" autocomplete="off" name="radio_GLQDZIEICI" value=""></input> <span>No, because you need at least two non-significant <em>p</em>-values to conclude the null hypothesis is true.</span></label>
</div>
</div>
<p><strong>Q11:</strong> What is a correct way to present a non-significant <em>p</em>-value (e.g., <em>p</em> = 0.34 assuming an alpha level of 0.05 is used in an independent <em>t</em>-test)?</p>
<div class="cell" data-layout-align="center">
<div id="radio_BSPCIVPCEC" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_BSPCIVPCEC" value=""></input> <span>The null hypothesis was confirmed, <em>p</em> &gt; 0.05</span></label><label><input type="radio" autocomplete="off" name="radio_BSPCIVPCEC" value=""></input> <span>There was no difference between the two conditions, <em>p</em> &gt; 0.05</span></label><label><input type="radio" autocomplete="off" name="radio_BSPCIVPCEC" value="answer"></input> <span>The observed difference was not statistically different from 0.</span></label><label><input type="radio" autocomplete="off" name="radio_BSPCIVPCEC" value=""></input> <span>The null hypothesis is true.</span></label>
</div>
</div>
<p><strong>Q12:</strong> Does observing a significant <em>p</em>-value (<em>p</em> &lt; .05) mean that the null hypothesis is false?</p>
<div class="cell" data-layout-align="center">
<div id="radio_BNUTFELHBK" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_BNUTFELHBK" value=""></input> <span>No, because <em>p</em> &lt; .05 only means that the alternative is true, not that the null hypothesis is wrong.</span></label><label><input type="radio" autocomplete="off" name="radio_BNUTFELHBK" value="answer"></input> <span>No, because <em>p</em>-values are never a statement about the probability of a hypothesis or theory.</span></label><label><input type="radio" autocomplete="off" name="radio_BNUTFELHBK" value=""></input> <span>Yes, because an exceptionally rare event has occurred.</span></label><label><input type="radio" autocomplete="off" name="radio_BNUTFELHBK" value=""></input> <span>Yes, because the difference is statistically significant.</span></label>
</div>
</div>
<p><strong>Q13:</strong> Is a statistically significant effect always a practically important effect?</p>
<div class="cell" data-layout-align="center">
<div id="radio_OGTXTESIJU" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_OGTXTESIJU" value=""></input> <span>No, because in extremely large samples, extremely small effects can be statistically significant, and small effects are never practically important.</span></label><label><input type="radio" autocomplete="off" name="radio_OGTXTESIJU" value=""></input> <span>No, because the alpha level could in theory be set to 0.20, and in that case a significant effect is not practically important.</span></label><label><input type="radio" autocomplete="off" name="radio_OGTXTESIJU" value="answer"></input> <span>No, because how important an effect is depends on a cost-benefit analysis, not on how surprising the data is under the null hypothesis.</span></label><label><input type="radio" autocomplete="off" name="radio_OGTXTESIJU" value=""></input> <span>All of the above are true.</span></label>
</div>
</div>
<p><strong>Q14:</strong> What is the correct definition of a <em>p</em>-value?</p>
<div class="cell" data-layout-align="center">
<div id="radio_BLVOUIUKGJ" class="webex-radiogroup">
<label><input type="radio" autocomplete="off" name="radio_BLVOUIUKGJ" value=""></input> <span>A <em>p</em>-value is the probability that the null hypothesis is true, given data that is as extreme or more extreme than the data you have observed. </span></label><label><input type="radio" autocomplete="off" name="radio_BLVOUIUKGJ" value=""></input> <span>A <em>p</em>-value is the probability that the alternative hypothesis is true, given data that is as extreme or more extreme than the data you have observed.</span></label><label><input type="radio" autocomplete="off" name="radio_BLVOUIUKGJ" value=""></input> <span>A <em>p</em>-value is the probability of observing data that is as extreme or more extreme than the data you have observed, assuming the alternative hypothesis is true.</span></label><label><input type="radio" autocomplete="off" name="radio_BLVOUIUKGJ" value="answer"></input> <span>A <em>p</em>-value is the probability of observing data that is as extreme or more extreme than the data you have observed, assuming the null hypothesis is true. </span></label>
</div>
</div>
</section>
<section id="open-questions" class="level3" data-number="1.8.3">
<h3 data-number="1.8.3"><span class="header-section-number">1.8.3</span> Open Questions</h3>
<ol type="1">
<li><p>What determines the shape of the <em>p</em>-value distribution?</p></li>
<li><p>How does the shape of the <em>p</em>-value distribution change when there is a true effect and the sample size increases?</p></li>
<li><p>What is Lindley’s paradox?</p></li>
<li><p>How are <em>p</em>-values distributed when there is no true effect?</p></li>
<li><p>What is the correct definition of a <em>p</em>-value?</p></li>
<li><p>Why is it incorrect to think that a non-significant <em>p</em>-value means that the null hypothesis is true?</p></li>
<li><p>Why is it incorrect to think that a significant <em>p</em>-value means that the null hypothesis is false?</p></li>
<li><p>Why is it incorrect to think that a significant <em>p</em>-value means that a practically important effect has been discovered?</p></li>
<li><p>Why is it incorrect to think that if you have observed a significant finding, the probability that you have made a Type 1 error (a false positive) is 5%?</p></li>
<li><p>Why is it incorrect to think that 1 – <em>p</em> (e.g., 1 – 0.05 = 0.95) is the probability that the effect will replicate when repeated?</p></li>
</ol>
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden" data-render-id="quarto-int-sidebar-title">Improving Your Statistical Inferences</span> <span class="hidden" data-render-id="quarto-int-navbar-title">Improving Your Statistical Inferences</span> <span class="hidden" data-render-id="quarto-int-next"><span class="chapter-number">2</span>  <span class="chapter-title">Error control</span></span> <span class="hidden" data-render-id="quarto-int-prev">Overview</span> <span class="hidden" data-render-id="quarto-int-sidebar:/index.html">Overview</span> <span class="hidden" data-render-id="quarto-int-sidebar:/01-pvalue.html"><span class="chapter-number">1</span>  <span class="chapter-title">Using <em>p</em>-values to test a hypothesis</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/02-errorcontrol.html"><span class="chapter-number">2</span>  <span class="chapter-title">Error control</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/03-likelihoods.html"><span class="chapter-number">3</span>  <span class="chapter-title">Likelihoods</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/04-bayes.html"><span class="chapter-number">4</span>  <span class="chapter-title">Bayesian statistics</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/05-questions.html"><span class="chapter-number">5</span>  <span class="chapter-title">Asking Statistical Questions</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/06-effectsize.html"><span class="chapter-number">6</span>  <span class="chapter-title">Effect Sizes</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/07-CI.html"><span class="chapter-number">7</span>  <span class="chapter-title">Confidence Intervals</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/08-samplesizejustification.html"><span class="chapter-number">8</span>  <span class="chapter-title">Sample Size Justification</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/09-equivalencetest.html"><span class="chapter-number">9</span>  <span class="chapter-title">Equivalence Testing and Interval Hypotheses</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/10-sequential.html"><span class="chapter-number">10</span>  <span class="chapter-title">Sequential Analysis</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/11-meta.html"><span class="chapter-number">11</span>  <span class="chapter-title">Meta-analysis</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/12-bias.html"><span class="chapter-number">12</span>  <span class="chapter-title">Bias detection</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/13-prereg.html"><span class="chapter-number">13</span>  <span class="chapter-title">Preregistration and Transparency</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/14-computationalreproducibility.html"><span class="chapter-number">14</span>  <span class="chapter-title">Computational Reproducibility</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/15-researchintegrity.html"><span class="chapter-number">15</span>  <span class="chapter-title">Research Integrity</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/references.html">References</span> <span class="hidden" data-render-id="footer-left">Lakens, D. (2022). Improving Your Statistical Inferences. Retrieved from https://lakens.github.io/statistical_inferences/. https://doi.org/10.5281/zenodo.6409077</span> <span class="hidden" data-render-id="quarto-breadcrumbs-09f898ad46c9272bae0abdc1159c54d8"><span class="chapter-number">1</span>  <span class="chapter-title">Using <em>p</em>-values to test a hypothesis</span></span></p>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden" data-render-id="quarto-metatitle">Improving Your Statistical Inferences - <span id="sec-pvalue" class="quarto-section-identifier"><span class="chapter-number">1</span>  <span class="chapter-title">Using <em>p</em>-values to test a hypothesis</span></span></span> <span class="hidden" data-render-id="quarto-twittercardtitle">Improving Your Statistical Inferences - <span id="sec-pvalue" class="quarto-section-identifier"><span class="chapter-number">1</span>  <span class="chapter-title">Using <em>p</em>-values to test a hypothesis</span></span></span> <span class="hidden" data-render-id="quarto-ogcardtitle">Improving Your Statistical Inferences - <span id="sec-pvalue" class="quarto-section-identifier"><span class="chapter-number">1</span>  <span class="chapter-title">Using <em>p</em>-values to test a hypothesis</span></span></span> <span class="hidden" data-render-id="quarto-metasitename">Improving Your Statistical Inferences</span> <span class="hidden" data-render-id="quarto-twittercarddesc">This open educational resource contains information to improve statistical inferences, design better experiments, and report scientific research more transparently.</span> <span class="hidden" data-render-id="quarto-ogcardddesc">This open educational resource contains information to improve statistical inferences, design better experiments, and report scientific research more transparently.</span></p>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list">
<div id="ref-anvari_not_2021" class="csl-entry" role="listitem">
Anvari, F., Kievit, R., Lakens, D., Pennington, C. R., Przybylski, A. K., Tiokhin, L., Wiernik, B. M., &amp; Orben, A. (2021). Not all effects are indispensable: <span>Psychological</span> science requires verifiable lines of reasoning for whether an effect matters. <em>Perspectives on Psychological Science</em>. <a href="https://doi.org/10.31234/osf.io/g3vtr">https://doi.org/10.31234/osf.io/g3vtr</a>
</div>
<div id="ref-appelbaum_journal_2018" class="csl-entry" role="listitem">
Appelbaum, M., Cooper, H., Kline, R. B., Mayo-Wilson, E., Nezu, A. M., &amp; Rao, S. M. (2018). Journal article reporting standards for quantitative research in psychology: <span>The APA Publications</span> and <span>Communications Board</span> task force report. <em>American Psychologist</em>, <em>73</em>(1), 3. <a href="https://doi.org/10.1037/amp0000191">https://doi.org/10.1037/amp0000191</a>
</div>
<div id="ref-benjamini_its_2016" class="csl-entry" role="listitem">
Benjamini, Y. (2016). It’s <span>Not</span> the p-values’ <span>Fault</span>. <em>The American Statistician: Supplemental Material to the ASA Statement on P-Values and Statistical Significance</em>, <em>70</em>, 1–2.
</div>
<div id="ref-bland_introduction_2015" class="csl-entry" role="listitem">
Bland, M. (2015). <em>An introduction to medical statistics</em> (Fourth edition). <span>Oxford University Press</span>.
</div>
<div id="ref-cox_problems_1958" class="csl-entry" role="listitem">
Cox, D. R. (1958). Some <span>Problems Connected</span> with <span>Statistical Inference</span>. <em>Annals of Mathematical Statistics</em>, <em>29</em>(2), 357–372. <a href="https://doi.org/10.1214/aoms/1177706618">https://doi.org/10.1214/aoms/1177706618</a>
</div>
<div id="ref-cumming_replication_2008" class="csl-entry" role="listitem">
Cumming, G. (2008). Replication and <span><em>p</em></span> <span>Intervals</span>: <span><em>p</em></span> <span>Values Predict</span> the <span>Future Only Vaguely</span>, but <span>Confidence Intervals Do Much Better</span>. <em>Perspectives on Psychological Science</em>, <em>3</em>(4), 286–300. <a href="https://doi.org/10.1111/j.1745-6924.2008.00079.x">https://doi.org/10.1111/j.1745-6924.2008.00079.x</a>
</div>
<div id="ref-dienes_understanding_2008" class="csl-entry" role="listitem">
Dienes, Z. (2008). <em>Understanding psychology as a science: <span>An</span> introduction to scientific and statistical inference</em>. <span>Palgrave Macmillan</span>.
</div>
<div id="ref-fisher_design_1935" class="csl-entry" role="listitem">
Fisher, Ronald Aylmer. (1935). <em>The design of experiments</em>. <span>Oliver And Boyd; Edinburgh; London</span>.
</div>
<div id="ref-fisher_statistical_1956" class="csl-entry" role="listitem">
Fisher, Ronald A. (1956). <em>Statistical methods and scientific inference: Vol. viii</em>. <span>Hafner Publishing Co.</span>
</div>
<div id="ref-fricker_assessing_2019" class="csl-entry" role="listitem">
Fricker, R. D., Burke, K., Han, X., &amp; Woodall, W. H. (2019). Assessing the <span>Statistical Analyses Used</span> in <span>Basic</span> and <span>Applied Social Psychology After Their</span> p-<span>Value Ban</span>. <em>The American Statistician</em>, <em>73</em>(sup1), 374–384. <a href="https://doi.org/10.1080/00031305.2018.1537892">https://doi.org/10.1080/00031305.2018.1537892</a>
</div>
<div id="ref-good_bayesnon-bayes_1992" class="csl-entry" role="listitem">
Good, I. J. (1992). The <span>Bayes</span>/<span>Non-Bayes</span> compromise: <span>A</span> brief review. <em>Journal of the American Statistical Association</em>, <em>87</em>(419), 597–606. <a href="https://doi.org/10.2307/2290192">https://doi.org/10.2307/2290192</a>
</div>
<div id="ref-gosset_application_1904" class="csl-entry" role="listitem">
Gosset, W. S. (1904). <em>The <span>Application</span> of the "<span>Law</span> of <span>Error</span>" to the <span>Work</span> of the <span>Brewery</span></em> (1 vol 8; pp. 3–16). <span>Arthur Guinness &amp; Son, Ltd.</span>
</div>
<div id="ref-greenland_statistical_2016" class="csl-entry" role="listitem">
Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., &amp; Altman, D. G. (2016). Statistical tests, <span>P</span> values, confidence intervals, and power: A guide to misinterpretations. <em>European Journal of Epidemiology</em>, <em>31</em>(4), 337–350. <a href="https://doi.org/10.1007/s10654-016-0149-3">https://doi.org/10.1007/s10654-016-0149-3</a>
</div>
<div id="ref-hacking_logic_1965" class="csl-entry" role="listitem">
Hacking, I. (1965). <em>Logic of <span>Statistical Inference</span></em>. <span>Cambridge University Press</span>.
</div>
<div id="ref-harms_making_2018" class="csl-entry" role="listitem">
Harms, C., &amp; Lakens, D. (2018). Making ’null effects’ informative: Statistical techniques and inferential frameworks. <em>Journal of Clinical and Translational Research</em>, <em>3</em>, 382–393. <a href="https://doi.org/10.18053/jctres.03.2017S2.007">https://doi.org/10.18053/jctres.03.2017S2.007</a>
</div>
<div id="ref-hempel_philosophy_1966" class="csl-entry" role="listitem">
Hempel, C. G. (1966). <em>Philosophy of natural science</em> (Nachdr.). <span>Prentice-Hall</span>.
</div>
<div id="ref-hung_behavior_1997" class="csl-entry" role="listitem">
Hung, H. M. J., O’Neill, R. T., Bauer, P., &amp; Kohne, K. (1997). The <span>Behavior</span> of the <span>P-Value When</span> the <span>Alternative Hypothesis</span> is <span>True</span>. <em>Biometrics</em>, <em>53</em>(1), 11–22. <a href="https://doi.org/10.2307/2533093">https://doi.org/10.2307/2533093</a>
</div>
<div id="ref-johansson_hail_2011" class="csl-entry" role="listitem">
Johansson, T. (2011). Hail the impossible: P-values, evidence, and likelihood. <em>Scandinavian Journal of Psychology</em>, <em>52</em>(2), 113–125. <a href="https://doi.org/10.1111/j.1467-9450.2010.00852.x">https://doi.org/10.1111/j.1467-9450.2010.00852.x</a>
</div>
<div id="ref-lakens_why_2022" class="csl-entry" role="listitem">
Lakens, D. (2022). Why <span>P</span> values are not measures of evidence. <em>Trends in Ecology &amp; Evolution</em>. <a href="https://doi.org/10.1016/j.tree.2021.12.006">https://doi.org/10.1016/j.tree.2021.12.006</a>
</div>
<div id="ref-leamer_specification_1978" class="csl-entry" role="listitem">
Leamer, E. E. (1978). <em>Specification <span>Searches</span>: <span>Ad Hoc Inference</span> with <span>Nonexperimental Data</span></em> (1 edition). <span>Wiley</span>.
</div>
<div id="ref-lehmann_testing_2005" class="csl-entry" role="listitem">
Lehmann, E. L., &amp; Romano, J. P. (2005). <em>Testing statistical hypotheses</em> (3rd ed). <span>Springer</span>.
</div>
<div id="ref-lindley_statistical_1957" class="csl-entry" role="listitem">
Lindley, D. V. (1957). A statistical paradox. <em>Biometrika</em>, <em>44</em>(1/2), 187–192.
</div>
<div id="ref-maier_justify_2022" class="csl-entry" role="listitem">
Maier, M., &amp; Lakens, D. (2022). Justify your alpha: <span>A</span> primer on two practical approaches. <em>Advances in Methods and Practices in Psychological Science</em>. <a href="https://doi.org/10.31234/osf.io/ts4r6">https://doi.org/10.31234/osf.io/ts4r6</a>
</div>
<div id="ref-miller_what_2009" class="csl-entry" role="listitem">
Miller, J. (2009). What is the probability of replicating a statistically significant effect? <em>Psychonomic Bulletin &amp; Review</em>, <em>16</em>(4), 617–640. <a href="https://doi.org/10.3758/PBR.16.4.617">https://doi.org/10.3758/PBR.16.4.617</a>
</div>
<div id="ref-neyman_inductive_1957" class="csl-entry" role="listitem">
Neyman, J. (1957). "<span>Inductive Behavior</span>" as a <span>Basic Concept</span> of <span>Philosophy</span> of <span>Science</span>. <em>Revue de l’Institut International de Statistique / Review of the International Statistical Institute</em>, <em>25</em>(1/3), 7. <a href="https://doi.org/10.2307/1401671">https://doi.org/10.2307/1401671</a>
</div>
<div id="ref-neyman_problem_1933" class="csl-entry" role="listitem">
Neyman, J., &amp; Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. <em>Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</em>, <em>231</em>(694-706), 289–337. <a href="https://doi.org/10.1098/rsta.1933.0009">https://doi.org/10.1098/rsta.1933.0009</a>
</div>
<div id="ref-niiniluoto_verisimilitude_1998" class="csl-entry" role="listitem">
Niiniluoto, I. (1998). Verisimilitude: <span>The Third Period</span>. <em>The British Journal for the Philosophy of Science</em>, <em>49</em>, 1–29.
</div>
<div id="ref-popper_logic_2002" class="csl-entry" role="listitem">
Popper, K. R. (2002). <em><span>The logic of scientific discovery</span></em>. <span>Routledge</span>.
</div>
<div id="ref-schweder_confidence_2016" class="csl-entry" role="listitem">
Schweder, T., &amp; Hjort, N. L. (2016). <em>Confidence, <span>Likelihood</span>, <span>Probability</span>: <span>Statistical Inference</span> with <span>Confidence Distributions</span></em>. <span>Cambridge University Press</span>. <a href="https://doi.org/10.1017/CBO9781139046671">https://doi.org/10.1017/CBO9781139046671</a>
</div>
<div id="ref-spanos_who_2013" class="csl-entry" role="listitem">
Spanos, A. (2013). Who should be afraid of the <span>Jeffreys-Lindley</span> paradox? <em>Philosophy of Science</em>, <em>80</em>(1), 73–93. <a href="https://doi.org/10.1086/668875">https://doi.org/10.1086/668875</a>
</div>
<div id="ref-tversky_belief_1971" class="csl-entry" role="listitem">
Tversky, A., &amp; Kahneman, D. (1971). Belief in the law of small numbers. <em>Psychological Bulletin</em>, <em>76</em>(2), 105–110. <a href="https://doi.org/10.1037/h0031322">https://doi.org/10.1037/h0031322</a>
</div>
<div id="ref-ulrich_properties_2018" class="csl-entry" role="listitem">
Ulrich, R., &amp; Miller, J. (2018). Some properties of p-curves, with an application to gradual publication bias. <em>Psychological Methods</em>, <em>23</em>(3), 546–560. <a href="https://doi.org/10.1037/met0000125">https://doi.org/10.1037/met0000125</a>
</div>
<div id="ref-zabell_r_1992" class="csl-entry" role="listitem">
Zabell, S. L. (1992). R. <span>A</span>. <span>Fisher</span> and <span>Fiducial Argument</span>. <em>Statistical Science</em>, <em>7</em>(3), 369–387. <a href="https://doi.org/10.1214/ss/1177011233">https://doi.org/10.1214/ss/1177011233</a>
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>

<script>

/* update total correct if #webex-total_correct exists */
update_total_correct = function() {
  console.log("webex: update total_correct");

  var t = document.getElementsByClassName("webex-total_correct");
  for (var i = 0; i < t.length; i++) {
    p = t[i].parentElement;
    var correct = p.getElementsByClassName("webex-correct").length;
    var solvemes = p.getElementsByClassName("webex-solveme").length;
    var radiogroups = p.getElementsByClassName("webex-radiogroup").length;
    var selects = p.getElementsByClassName("webex-select").length;

    t[i].innerHTML = correct + " of " + (solvemes + radiogroups + selects) + " correct";
  }
}

/* webex-solution button toggling function */
b_func = function() {
  console.log("webex: toggle hide");

  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* check answers */
check_func = function() {
  console.log("webex: check answers");

  var cl = this.parentElement.classList;
  if (cl.contains('unchecked')) {
    cl.remove("unchecked");
    this.innerHTML = "Hide Answers";
  } else {
    cl.add("unchecked");
    this.innerHTML = "Show Answers";
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  console.log("webex: check solveme");

  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "")
  }

  if (my_answer == "") {
    cl.remove("webex-correct");
    cl.remove("webex-incorrect");
  } else if (real_answers.includes(my_answer)) {
    cl.add("webex-correct");
    cl.remove("webex-incorrect");
  } else {
    cl.add("webex-incorrect");
    cl.remove("webex-correct");
  }

  // match numeric answers within a specified tolerance
  if(this.dataset.tol > 0){
    var tol = JSON.parse(this.dataset.tol);
    var matches = real_answers.map(x => Math.abs(x - my_answer) < tol)
    if (matches.reduce((a, b) => a + b, 0) > 0) {
      cl.add("webex-correct");
    } else {
      cl.remove("webex-correct");
    }
  }

  // added regex bit
  if (cl.contains("regex")){
    answer_regex = RegExp(real_answers.join("|"))
    if (answer_regex.test(my_answer)) {
      cl.add("webex-correct");
    }
  }

  update_total_correct();
}

/* function for checking select answers */
select_func = function(e) {
  console.log("webex: check select");

  var cl = this.classList

  /* add style */
  cl.remove("webex-incorrect");
  cl.remove("webex-correct");
  if (this.value == "answer") {
    cl.add("webex-correct");
  } else if (this.value != "blank") {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

/* function for checking radiogroups answers */
radiogroups_func = function(e) {
  console.log("webex: check radiogroups");

  var checked_button = document.querySelector('input[name=' + this.id + ']:checked');
  var cl = checked_button.parentElement.classList;
  var labels = checked_button.parentElement.parentElement.children;

  /* get rid of styles */
  for (i = 0; i < labels.length; i++) {
    labels[i].classList.remove("webex-incorrect");
    labels[i].classList.remove("webex-correct");
  }

  /* add style */
  if (checked_button.value == "answer") {
    cl.add("webex-correct");
  } else {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

window.onload = function() {
  console.log("webex onload");
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('webex-solution')) {
      buttons[i].onclick = b_func;
    }
  }

  var check_sections = document.getElementsByClassName("webex-check");
  console.log("check:", check_sections.length);
  for (var i = 0; i < check_sections.length; i++) {
    check_sections[i].classList.add("unchecked");

    let btn = document.createElement("button");
    btn.innerHTML = "Show Answers";
    btn.classList.add("webex-check-button");
    btn.onclick = check_func;
    check_sections[i].appendChild(btn);

    let spn = document.createElement("span");
    spn.classList.add("webex-total_correct");
    check_sections[i].appendChild(spn);
  }

  /* set up webex-solveme inputs */
  var solveme = document.getElementsByClassName("webex-solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off");
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";

    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;

    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;

    $(solveme[i]).after(" <span class='webex-icon'></span>");
  }

  /* set up radiogroups */
  var radiogroups = document.getElementsByClassName("webex-radiogroup");
  for (var i = 0; i < radiogroups.length; i++) {
    radiogroups[i].onchange = radiogroups_func;
  }

  /* set up selects */
  var selects = document.getElementsByClassName("webex-select");
  for (var i = 0; i < selects.length; i++) {
    selects[i].onchange = select_func;
    $(selects[i]).after(" <span class='webex-icon'></span>");
  }

  update_total_correct();
}

</script>
<script>
// open rdrr links externally ----

var exlinks = document.querySelectorAll("a[href^='https://rdrr.io']");
var exlink_func = function(){
  window.open(this.href);
  return false;
};
for (var i = 0; i < exlinks.length; i++) {
    exlinks[i].addEventListener('click', exlink_func, false);
}

// visible second sidebar in mobile ----

function move_sidebar() {
  var toc = document.getElementById("TOC");
  var small_sidebar = document.querySelector("#quarto-sidebar .sidebar-menu-container");
  var right_sidebar = document.getElementById("quarto-margin-sidebar");

  if (window.innerWidth < 768) {
    small_sidebar.append(toc);
  } else {
    right_sidebar.append(toc);
  }
}
move_sidebar();
window.onresize = move_sidebar;
</script>
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a  href="/index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Overview</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a  href="/02-errorcontrol.html" class="pagination-link">
        <span class="nav-page-text"><span class='chapter-number'>2</span>  <span class='chapter-title'>Error control</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <div class='footer-contents'>Lakens, D. (2022). Improving Your Statistical Inferences. Retrieved from https://lakens.github.io/statistical_inferences/. https://doi.org/10.5281/zenodo.6409077</div>  
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>

</body>

</html>